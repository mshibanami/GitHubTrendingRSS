<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Scala Daily Trending</title>
    <description>Daily Trending of Scala in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:38:37 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>apache/spark</title>
      <link>https://github.com/apache/spark</link>
      <description>&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; 
&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R (Deprecated), and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Official version: &lt;a href=&quot;https://spark.apache.org/&quot;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Development version: &lt;a href=&quot;https://apache.github.io/spark/&quot;&gt;https://apache.github.io/spark/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_main.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/apache/spark&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&quot; alt=&quot;PySpark Coverage&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/pyspark/&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/personalized-badge/pyspark?period=month&amp;amp;units=international_system&amp;amp;left_color=black&amp;amp;right_color=orange&amp;amp;left_text=PyPI%20downloads&quot; alt=&quot;PyPI Downloads&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Online Documentation&lt;/h2&gt; 
&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&quot;https://spark.apache.org/documentation.html&quot;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; 
&lt;h2&gt;Build Pipeline Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Branch&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;master&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_non_ansi.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_non_ansi.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_uds.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_uds.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_rockdb_as_ui_backend.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_rockdb_as_ui_backend.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_macos15.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_macos15.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_arm.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_arm.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_coverage.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_coverage.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_pypy3.10.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_pypy3.10.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.10.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.10.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_classic_only.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_classic_only.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_arm.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_arm.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_macos.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_macos.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_numpy_2.1.3.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_numpy_2.1.3.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.12.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.12.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13_nogil.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13_nogil.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_minimum.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_minimum.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_ps_minimum.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_ps_minimum.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect35.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect35.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_sparkr_window.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_sparkr_window.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/publish_snapshot.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/publish_snapshot.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;branch-4.0&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_non_ansi.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_non_ansi.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python_pypy3.10.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python_pypy3.10.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;branch-3.5&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch35.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch35.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch35_python.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch35_python.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Building Spark&lt;/h2&gt; 
&lt;p&gt;Spark is built using &lt;a href=&quot;https://maven.apache.org/&quot;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./build/mvn -DskipTests clean package
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; 
&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&quot;https://spark.apache.org/docs/latest/building-spark.html&quot;&gt;&quot;Building Spark&quot;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&quot;https://spark.apache.org/developer-tools.html&quot;&gt;&quot;Useful Developer Tools&quot;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; 
&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/spark-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; 
&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/pyspark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example Programs&lt;/h2&gt; 
&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will run the Pi example locally.&lt;/p&gt; 
&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be spark:// URL, &quot;yarn&quot; to run on YARN, and &quot;local&quot; to run locally with one thread, or &quot;local[N]&quot; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; 
&lt;h2&gt;Running Tests&lt;/h2&gt; 
&lt;p&gt;Testing first requires &lt;a href=&quot;https://raw.githubusercontent.com/apache/spark/master/#building-spark&quot;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./dev/run-tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see the guidance on how to &lt;a href=&quot;https://spark.apache.org/developer-tools.html#individual-tests&quot;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; 
&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; 
&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; 
&lt;p&gt;Please refer to the build documentation at &lt;a href=&quot;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&quot;&gt;&quot;Specifying the Hadoop Version and Enabling YARN&quot;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Please refer to the &lt;a href=&quot;https://spark.apache.org/docs/latest/configuration.html&quot;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please review the &lt;a href=&quot;https://spark.apache.org/contributing.html&quot;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>scala/scala3</title>
      <link>https://github.com/scala/scala3</link>
      <description>&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/scala/scala3/actions?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/scala/scala3/workflows/Dotty/badge.svg?branch=main&quot; alt=&quot;Dotty CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.com/invite/scala&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/632150470000902164&quot; alt=&quot;Join the chat at https://discord.com/invite/scala&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://develocity.scala-lang.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&amp;amp;labelColor=02303A&quot; alt=&quot;Revved up by Develocity&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.scala-lang.org/scala3/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try it out&lt;/h1&gt; 
&lt;p&gt;To try it in your project see also the &lt;a href=&quot;https://docs.scala-lang.org/scala3/getting-started.html&quot;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;sbt dist/Universal/packageBin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Code of Conduct&lt;/h1&gt; 
&lt;p&gt;Dotty uses the &lt;a href=&quot;https://www.scala-lang.org/conduct.html&quot;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; 
&lt;h1&gt;How to Contribute&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&quot;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/scala/scala3/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&quot;&gt;Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Dotty is licensed under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>joernio/joern</title>
      <link>https://github.com/joernio/joern</link>
      <description>&lt;p&gt;Open-source code analysis platform for C/C++/Java/Binary/Javascript/Python/Kotlin based on code property graphs. Discord https://discord.gg/vv4MH284Hc&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Joern - The Bug Hunter&#39;s Workbench&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/joernio/joern/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/joernio/joern/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;release&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://index.scala-lang.org/joernio/joern&quot;&gt;&lt;img src=&quot;https://index.scala-lang.org/joernio/joern/latest.svg?sanitize=true&quot; alt=&quot;Joern SBT&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/joernio/joern/releases/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/joernio/joern/total.svg?sanitize=true&quot; alt=&quot;Github All Releases&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.com/invite/vv4MH284Hc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/-Discord-lime?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;color=black&quot; alt=&quot;Gitter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Joern is a platform for analyzing source code, bytecode, and binary executables. It generates code property graphs (CPGs), a graph representation of code for cross-language code analysis. Code property graphs are stored in a custom graph database. This allows code to be mined using search queries formulated in a Scala-based domain-specific query language. Joern is developed with the goal of providing a useful tool for vulnerability discovery and research in static program analysis.&lt;/p&gt; 
&lt;p&gt;Website: &lt;a href=&quot;https://joern.io&quot;&gt;https://joern.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Documentation: &lt;a href=&quot;https://docs.joern.io/&quot;&gt;https://docs.joern.io/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Specification: &lt;a href=&quot;https://cpg.joern.io&quot;&gt;https://cpg.joern.io&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;News / Changelog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Joern v4.0.0 &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/changelog/4.0.0-flatgraph.md&quot;&gt;migrates from overflowdb to flatgraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Joern v2.0.0 &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/changelog/2.0.0-scala3.md&quot;&gt;upgrades from Scala2 to Scala3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Joern v1.2.0 removes the &lt;code&gt;overflowdb.traversal.Traversal&lt;/code&gt; class. This change is not completely backwards compatible. See &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/changelog/traversal_removal.md&quot;&gt;here&lt;/a&gt; for a detailed writeup.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;JDK 21 (other versions &lt;em&gt;might&lt;/em&gt; work, but have not been properly tested)&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;optional&lt;/em&gt;: gcc and g++ (for auto-discovery of C/C++ system header files if included/used in your C/C++ code)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;wget https://github.com/joernio/joern/releases/latest/download/joern-install.sh
chmod +x ./joern-install.sh
sudo ./joern-install.sh
joern

     ██╗ ██████╗ ███████╗██████╗ ███╗   ██╗
     ██║██╔═══██╗██╔════╝██╔══██╗████╗  ██║
     ██║██║   ██║█████╗  ██████╔╝██╔██╗ ██║
██   ██║██║   ██║██╔══╝  ██╔══██╗██║╚██╗██║
╚█████╔╝╚██████╔╝███████╗██║  ██║██║ ╚████║
 ╚════╝  ╚═════╝ ╚══════╝╚═╝  ╚═╝╚═╝  ╚═══╝
Version: 2.0.1
Type `help` to begin

joern&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the installation script fails for any reason, try&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./joern-install --interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://jdk.java.net/&quot;&gt;java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.scala-sbt.org&quot;&gt;sbt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run unit and integration tests locally&lt;/h2&gt; 
&lt;p&gt;Unit tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sbt test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sbt joerncli/stage querydb/createDistribution
python -m pip install requests pexpect # wexpect on Windows
python -u ./testDistro.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker based execution&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v /tmp:/tmp -v $(pwd):/app:rw -w /app -t ghcr.io/joernio/joern joern
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run joern in server mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v /tmp:/tmp -v $(pwd):/app:rw -w /app -t ghcr.io/joernio/joern joern --server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Almalinux 9 requires the CPU to support SSE4.2. For kvm64 VM use the Almalinux 8 version instead.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v /tmp:/tmp -v $(pwd):/app:rw -w /app -t ghcr.io/joernio/joern-alma8 joern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;A new release is &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/.github/workflows/release.yml&quot;&gt;created automatically&lt;/a&gt; once per day. Contributers can also manually run the &lt;a href=&quot;https://github.com/joernio/joern/actions/workflows/release.yml&quot;&gt;release workflow&lt;/a&gt; if they need the release sooner.&lt;/p&gt; 
&lt;h2&gt;Developers&lt;/h2&gt; 
&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; 
&lt;p&gt;Thank you for taking time to contribute to Joern! Here are a few guidelines to ensure your pull request will get merged as soon as possible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try to make use of the templates as far as possible, however they may not suit all needs. The minimum we would like to see is: 
  &lt;ul&gt; 
   &lt;li&gt;A title that briefly describes the change and purpose of the PR, preferably with the affected module in square brackets, e.g. &lt;code&gt;[javasrc2cpg] Addition Operator Fix&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;A short description of the changes in the body of the PR. This could be in bullet points or paragraphs.&lt;/li&gt; 
   &lt;li&gt;A link or reference to the related issue, if any exists.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Do not: 
  &lt;ul&gt; 
   &lt;li&gt;Immediately CC/@/email spam other contributors, the team will review the PR and assign the most appropriate contributor to review the PR. Joern is maintained by industry partners and researchers alike, for the most part with their own goals and priorities, and additional help is largely volunteer work. If your PR is going stale, then reach out to us in follow-up comments with @&#39;s asking for an explanation of priority or planning of when it may be addressed (if ever, depending on quality).&lt;/li&gt; 
   &lt;li&gt;Leave the description body empty, this makes reviewing the purpose of the PR difficult.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Remember to: 
  &lt;ul&gt; 
   &lt;li&gt;Remember to format your code, i.e. run &lt;code&gt;sbt scalafmt Test/scalafmt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Add a unit test to verify your change.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;IDE setup&lt;/h3&gt; 
&lt;h4&gt;Intellij IDEA&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.jetbrains.com/idea/download&quot;&gt;Download Intellij Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install and run it&lt;/li&gt; 
 &lt;li&gt;Install the &lt;a href=&quot;https://plugins.jetbrains.com/plugin/1347-scala&quot;&gt;Scala Plugin&lt;/a&gt; - just search and install from within Intellij.&lt;/li&gt; 
 &lt;li&gt;Important: open &lt;code&gt;sbt&lt;/code&gt; in your local joern repository, run &lt;code&gt;compile&lt;/code&gt; and keep it open - this will allow us to use the BSP build in the next step&lt;/li&gt; 
 &lt;li&gt;Back to Intellij: open project: select your local joern clone: select to open as &lt;code&gt;BSP project&lt;/code&gt; (i.e. &lt;em&gt;not&lt;/em&gt; &lt;code&gt;sbt project&lt;/code&gt;!)&lt;/li&gt; 
 &lt;li&gt;Await the import and indexing to complete, then you can start, e.g. &lt;code&gt;Build -&amp;gt; build project&lt;/code&gt; or run a test&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;VSCode&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install VSCode and Docker&lt;/li&gt; 
 &lt;li&gt;Install the plugin &lt;code&gt;ms-vscode-remote.remote-containers&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Open Joern project folder in VSCode 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure-sphere/app-development/container-build-vscode#build-and-debug-the-project&quot;&gt;Option 1&lt;/a&gt;: Visual Studio Code detects the new files and opens a message box saying: &lt;code&gt;Folder contains a Dev Container configuration file. Reopen to folder to develop in a container.&lt;/code&gt;. Select the &lt;code&gt;Reopen in Container&lt;/code&gt; button to reopen the folder in the container created by the &lt;code&gt;.devcontainer/Dockerfile&lt;/code&gt; file.&lt;/li&gt; 
   &lt;li&gt;Option 2: press &lt;code&gt;Ctrl + Shift + P&lt;/code&gt; then select &lt;code&gt;Dev Containers: Reopen in Container&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Press &lt;code&gt;Ctrl + Shift + P&lt;/code&gt; then select &lt;code&gt;Metals: Import build&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;After &lt;code&gt;Metals: Import build&lt;/code&gt; succeeds, you are ready to start writing code for Joern&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;QueryDB (queries plugin)&lt;/h2&gt; 
&lt;p&gt;Quick way to develop and test QueryDB:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sbt stage
./querydb-install.sh
./joern-scan --list-query-names
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The last command prints all available queries - add your own in querydb, run the above commands again to see that your query got deployed. More details in the &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/querydb/README.md&quot;&gt;separate querydb readme&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gatling/gatling</title>
      <link>https://github.com/gatling/gatling</link>
      <description>&lt;p&gt;Modern Load Testing as Code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gatling &lt;a href=&quot;https://github.com/gatling/gatling/actions/workflows/build.yml?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/gatling/gatling/actions/workflows/build.yml/badge.svg?branch=main&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://central.sonatype.com/search?q=gatling-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/maven-central/v/io.gatling/gatling-core&quot; alt=&quot;Maven Central&quot; /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;h2&gt;What is Gatling ?&lt;/h2&gt; 
&lt;p&gt;Gatling is a load test tool. It officially supports HTTP, WebSocket, Server-Sent-Events, JMS, gRPC and MQTT.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Finding fancy GUIs not that convenient for describing load tests, what you want is a friendly expressive DSL in Java, Scala, Kotlin, JavaScript or TypeScript?&lt;/li&gt; 
 &lt;li&gt;Wanting something more convenient than huge XML dumps to store in your source version control system?&lt;/li&gt; 
 &lt;li&gt;Fed up with having to host a farm of injecting servers because your tool uses blocking IO and one-thread-per-user architecture?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Gatling is for you!&lt;/p&gt; 
&lt;h2&gt;Questions, help?&lt;/h2&gt; 
&lt;p&gt;Read the &lt;a href=&quot;https://docs.gatling.io&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href=&quot;https://community.gatling.io&quot;&gt;Gatling Community Forum&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Found a real bug? Raise an &lt;a href=&quot;https://github.com/gatling/gatling/issues&quot;&gt;issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Partners&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/gatling/gatling/main/images/highsoft_logo.png&quot; alt=&quot;Highsoft AS&quot; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rtyley/bfg-repo-cleaner</title>
      <link>https://github.com/rtyley/bfg-repo-cleaner</link>
      <description>&lt;p&gt;Removes large or troublesome blobs like git-filter-branch does, but faster. And written in Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BFG Repo-Cleaner&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/ci.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Removes large or troublesome blobs like git-filter-branch does, but faster - and written in Scala&lt;/em&gt; - &lt;a href=&quot;https://j.mp/fund-bfg&quot;&gt;Fund the BFG&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ bfg --strip-blobs-bigger-than 1M --replace-text banned.txt repo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The BFG is a simpler, faster (&lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0AsR1d5Zpes8HdER3VGU1a3dOcmVHMmtzT2dsS2xNenc&quot;&gt;10 - 720x&lt;/a&gt; faster) alternative to &lt;code&gt;git-filter-branch&lt;/code&gt; for cleansing bad data out of your Git repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Removing &lt;strong&gt;Crazy Big Files&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Removing &lt;strong&gt;Passwords, Credentials&lt;/strong&gt; &amp;amp; other &lt;strong&gt;Private data&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Main documentation for The BFG is here : &lt;strong&gt;&lt;a href=&quot;https://rtyley.github.io/bfg-repo-cleaner/&quot;&gt;https://rtyley.github.io/bfg-repo-cleaner/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/incubator-gluten</title>
      <link>https://github.com/apache/incubator-gluten</link>
      <description>&lt;p&gt;Gluten is a middle layer responsible for offloading JVM-based SQL engines&#39; execution to native engines.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/image/gluten-logo.svg?sanitize=true&quot; alt=&quot;Gluten&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Apache Gluten (Incubating): A Middle Layer for Offloading JVM-based SQL Engines&#39; Execution to Native Engines&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.bestpractices.dev/projects/8452&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/8452/badge&quot; alt=&quot;OpenSSF Best Practices&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;1. Introduction&lt;/h1&gt; 
&lt;h2&gt;Problem Statement&lt;/h2&gt; 
&lt;p&gt;Apache Spark is a stable, mature project that has been developed for many years. It is one of the best frameworks to scale out for processing petabyte-scale datasets. However, the Spark community has had to address performance challenges that require various optimizations over time. As a key optimization in Spark 2.0, Whole Stage Code Generation is introduced to replace Volcano Model, which achieves 2x speedup. Henceforth, most optimizations are at query plan level. Single operator&#39;s performance almost stops growing.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/47296334/199853029-b6d0ea19-f8e4-4f62-9562-2838f7f159a7.png&quot; width=&quot;800&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;On the other side, native SQL engines have been developed for a few years, such as Clickhouse, Arrow and Velox, etc. With features like native execution, columnar data format and vectorized data processing, these native engines can outperform Spark&#39;s JVM based SQL engine. However, they only support single node execution.&lt;/p&gt; 
&lt;h2&gt;Gluten&#39;s Basic Design&lt;/h2&gt; 
&lt;p&gt;“Gluten” is Latin for &quot;glue&quot;. The main goal of Gluten project is to glue native engines with SparkSQL. Thus, we can benefit from high scalability of Spark SQL framework and high performance of native engines.&lt;/p&gt; 
&lt;p&gt;The basic design rule is that we would reuse Spark&#39;s whole control flow and as much JVM code as possible but offload the compute-intensive data processing to native side. Here is what Gluten does basically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transform Spark’s physical plan to Substrait plan, then transform it to native engine&#39;s plan.&lt;/li&gt; 
 &lt;li&gt;Offload performance-critical data processing to native engine.&lt;/li&gt; 
 &lt;li&gt;Define clear JNI interfaces for native SQL engines.&lt;/li&gt; 
 &lt;li&gt;Switch available native backends easily.&lt;/li&gt; 
 &lt;li&gt;Reuse Spark’s distributed control flow.&lt;/li&gt; 
 &lt;li&gt;Manage data sharing between JVM and native.&lt;/li&gt; 
 &lt;li&gt;Extensible to support more native engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Target User&lt;/h2&gt; 
&lt;p&gt;Gluten&#39;s target user is anyone who aspires to accelerate SparkSQL fundamentally. As a plugin to Spark, Gluten doesn&#39;t require any change for dataframe API or SQL query, but only requires user to make correct configuration. See Gluten configuration properties &lt;a href=&quot;https://github.com/apache/incubator-gluten/raw/main/docs/Configuration.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;You can click below links for more related information.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0Q6gHT_N-1U&quot;&gt;Gluten Intro Video at Data AI Summit 2022&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/intel-analytics-software/accelerate-spark-sql-queries-with-gluten-9000b65d1b4e&quot;&gt;Gluten Intro Article at Medium.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cn.kyligence.io/blog/gluten-spark/&quot;&gt;Gluten Intro Article at Kyligence.io(in Chinese)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://engineering.fb.com/2023/03/09/open-source/velox-open-source-execution-engine/&quot;&gt;Velox Intro from Meta&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;2. Architecture&lt;/h1&gt; 
&lt;p&gt;The overview chart is like below. Substrait provides a well-defined cross-language specification for data compute operations (see more details &lt;a href=&quot;https://substrait.io/&quot;&gt;here&lt;/a&gt;). Spark physical plan is transformed to Substrait plan. Then Substrait plan is passed to native through JNI call. On native side, the native operator chain will be built out and offloaded to native engine. Gluten will return Columnar Batch to Spark and Spark Columnar API (since Spark-3.0) will be used at execution time. Gluten uses Apache Arrow data format as its basic data format, so the returned data to Spark JVM is ArrowColumnarBatch.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/47296334/199617207-1140698a-4d53-462d-9bc7-303d14be060b.png&quot; width=&quot;800&quot; /&gt; &lt;/p&gt; Currently, Gluten only supports Clickhouse backend &amp;amp; Velox backend. Velox is a C++ database acceleration library which provides reusable, extensible and high-performance data processing components. More details can be found from https://github.com/facebookincubator/velox/. Gluten can also be extended to support more backends. 
&lt;p&gt;There are several key components in Gluten:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Query Plan Conversion&lt;/strong&gt;: converts Spark&#39;s physical plan to Substrait plan.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Memory Management&lt;/strong&gt;: controls native memory allocation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Columnar Shuffle&lt;/strong&gt;: shuffles Gluten columnar data. The shuffle service still reuses the one in Spark core. A kind of columnar exchange operator is implemented to support Gluten columnar data format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fallback Mechanism&lt;/strong&gt;: supports falling back to Vanilla spark for unsupported operators. Gluten ColumnarToRow (C2R) and RowToColumnar (R2C) will convert Gluten columnar data and Spark&#39;s internal row data if needed. Both C2R and R2C are implemented in native code as well&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: collected from Gluten native engine to help identify bugs, performance bottlenecks, etc. The metrics are displayed in Spark UI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Shim Layer&lt;/strong&gt;: supports multiple Spark versions. We plan to only support Spark&#39;s latest 2 or 3 releases. Currently, Spark-3.2, Spark-3.3 &amp;amp; Spark-3.4 (experimental) are supported.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;3. User Guide&lt;/h1&gt; 
&lt;p&gt;Here is a basic configuration to enable Gluten in Spark.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export GLUTEN_JAR=/PATH/TO/GLUTEN_JAR
spark-shell \
  --master yarn --deploy-mode client \
  --conf spark.plugins=org.apache.gluten.GlutenPlugin \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=20g \
  --conf spark.driver.extraClassPath=${GLUTEN_JAR} \
  --conf spark.executor.extraClassPath=${GLUTEN_JAR} \
  --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are two ways to acquire Gluten jar for the above configuration.&lt;/p&gt; 
&lt;h3&gt;Use Released Jar&lt;/h3&gt; 
&lt;p&gt;Please download a tar package &lt;a href=&quot;https://downloads.apache.org/incubator/gluten/&quot;&gt;here&lt;/a&gt;, then extract out Gluten jar from it. Additionally, Gluten offers nightly builds based on the main branch, which are available for early testing. You can find these release jars at this link: &lt;a href=&quot;https://nightlies.apache.org/gluten/&quot;&gt;Apache Gluten Nightlies&lt;/a&gt;. It was verified on Centos-7, Centos-8, Centos-9, Ubuntu-20.04 and Ubuntu-22.04.&lt;/p&gt; 
&lt;h3&gt;Build From Source&lt;/h3&gt; 
&lt;p&gt;For &lt;strong&gt;Velox&lt;/strong&gt; backend, please refer to &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/get-started/Velox.md&quot;&gt;Velox.md&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/get-started/build-guide.md&quot;&gt;build-guide.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;ClickHouse&lt;/strong&gt; backend, please refer to &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/get-started/ClickHouse.md&quot;&gt;ClickHouse.md&lt;/a&gt;. ClickHouse backend is developed by &lt;a href=&quot;https://kyligence.io/&quot;&gt;Kyligence&lt;/a&gt;, please visit &lt;a href=&quot;https://github.com/Kyligence/ClickHouse&quot;&gt;https://github.com/Kyligence/ClickHouse&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;Gluten jar will be generated under &lt;code&gt;/PATH/TO/GLUTEN/package/target/&lt;/code&gt; after the build.&lt;/p&gt; 
&lt;h3&gt;Configurations&lt;/h3&gt; 
&lt;p&gt;Common configurations used by Gluten is listed in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/Configuration.md&quot;&gt;Configuration.md&lt;/a&gt;. Velox specific configurations is listed in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/velox-configuration.md&quot;&gt;velox-configuration.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Some of the spark configurations are hornored by Gluten Velox backend, some of them are ignored, and many are transparent to Gluten. The detail can be found in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/velox-spark-configuration.md&quot;&gt;velox-spark-configuration.md&lt;/a&gt; and parquet write ones can be found in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/velox-parquet-write-configuration.md&quot;&gt;velox-parquet-write-configuration.md&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;4. Gluten Website&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://gluten.apache.org/&quot;&gt;https://gluten.apache.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;5. Contribution&lt;/h1&gt; 
&lt;p&gt;Welcome to contribute to Gluten project! See &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; about how to make contributions.&lt;/p&gt; 
&lt;h1&gt;6. Community&lt;/h1&gt; 
&lt;p&gt;Gluten successfully became Apache incubator project in March 2024. Here are several ways to contact us:&lt;/p&gt; 
&lt;h2&gt;GitHub&lt;/h2&gt; 
&lt;p&gt;Welcome to report any issue or create any discussion related to Gluten in GitHub. Please do a search from GitHub issue list before creating a new one to avoid repetition.&lt;/p&gt; 
&lt;h2&gt;Mail Lists&lt;/h2&gt; 
&lt;p&gt;For any technical discussion, please send email to &lt;a href=&quot;mailto:dev@gluten.apache.org&quot;&gt;dev@gluten.apache.org&lt;/a&gt;. You can go to &lt;a href=&quot;https://lists.apache.org/list.html?dev@gluten.apache.org&quot;&gt;archives&lt;/a&gt; for getting historical discussions. Please click &lt;a href=&quot;mailto:dev-subscribe@gluten.apache.org&quot;&gt;here&lt;/a&gt; to subscribe the mail list.&lt;/p&gt; 
&lt;h2&gt;Slack Channel (English communication)&lt;/h2&gt; 
&lt;p&gt;Please click &lt;a href=&quot;https://github.com/apache/incubator-gluten/discussions/8429&quot;&gt;here&lt;/a&gt; to get invitation for ASF Slack workspace where you can find &quot;incubator-gluten&quot; channel.&lt;/p&gt; 
&lt;p&gt;The ASF Slack login entry: &lt;a href=&quot;https://the-asf.slack.com/&quot;&gt;https://the-asf.slack.com/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;WeChat Group (Chinese communication)&lt;/h2&gt; 
&lt;p&gt;For PRC developers/users, please contact weitingchen at apache.org or zhangzc at apache.org for getting invited to the WeChat group.&lt;/p&gt; 
&lt;h1&gt;7. Performance&lt;/h1&gt; 
&lt;p&gt;We use Decision Support Benchmark1 (TPC-H like) to evaluate Gluten&#39;s performance. Decision Support Benchmark1 is a query set modified from &lt;a href=&quot;http://tpc.org/tpch/default5.asp&quot;&gt;TPC-H benchmark&lt;/a&gt;. We use Parquet file format for Velox testing &amp;amp; MergeTree file format for Clickhouse testing, compared to Parquet file format as baseline. See &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/tools/workload/tpch&quot;&gt;Decision Support Benchmark1&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The below test environment: single node with 2TB data; Spark-3.3.2 for both baseline and Gluten. The Decision Support Benchmark1 result (tested in Jun. 2023) shows an overall speedup of 2.71x and up to 14.53x speedup in a single query with Gluten Velox backend used.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/image/velox_decision_support_bench1_22queries_performance.png&quot; alt=&quot;Performance&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The below testing environment: a 8-nodes AWS cluster with 1TB data; Spark-3.1.1 for both baseline and Gluten. The Decision Support Benchmark1 result shows an average speedup of 2.12x and up to 3.48x speedup with Gluten Clickhouse backend.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/image/clickhouse_decision_support_bench1_22queries_performance.png&quot; alt=&quot;Performance&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;8. Qualification Tool&lt;/h1&gt; 
&lt;p&gt;The Qualification Tool is a utility to analyze Spark event log files and assess the compatibility and performance of SQL workloads with Gluten. This tool helps users understand how their workloads can benefit from Gluten.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Analyzes Spark SQL execution plans for compatibility with Gluten.&lt;/li&gt; 
 &lt;li&gt;Supports various types of event log files, including single files, folders, compressed files, and rolling event logs.&lt;/li&gt; 
 &lt;li&gt;Generates detailed reports highlighting supported and unsupported operations.&lt;/li&gt; 
 &lt;li&gt;Provides metrics on SQL execution times and operator impact.&lt;/li&gt; 
 &lt;li&gt;Offers configurable options such as threading, output directory, and date-based filtering.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To use the Qualification Tool, follow the instructions in its &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/tools/qualification-tool/README.MD&quot;&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Example Command&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;java -jar target/qualification-tool-1.3.0-SNAPSHOT-jar-with-dependencies.jar -f /path/to/eventlog
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed usage instructions and advanced options, see the Qualification Tool README.&lt;/p&gt; 
&lt;h1&gt;9. License&lt;/h1&gt; 
&lt;p&gt;Gluten is licensed under &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;10. Acknowledgements&lt;/h1&gt; 
&lt;p&gt;Gluten was initiated by Intel and Kyligence in 2022. Several companies are also actively participating in the development, such as BIGO, Meituan, Alibaba Cloud, NetEase, Baidu, Microsoft, IBM, Google, etc.&lt;/p&gt; 
&lt;a href=&quot;https://github.com/apache/incubator-gluten/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=apache/incubator-gluten&amp;amp;columns=25&quot; /&gt; &lt;/a&gt; 
&lt;h5&gt;* LEGAL NOTICE: Your use of this software and any required dependent software (the &quot;Software Package&quot;) is subject to the terms and conditions of the software license agreements for the Software Package, which may also include notices, disclaimers, or license terms for third party or open source software included in or with the Software Package, and your use indicates your acceptance of all such terms. Please refer to the &quot;TPP.txt&quot; or other similarly-named text file included with the Software Package for additional details.&lt;/h5&gt;</description>
    </item>
    
    <item>
      <title>playframework/playframework</title>
      <link>https://github.com/playframework/playframework</link>
      <description>&lt;p&gt;The Community Maintained High Velocity Web Framework For Java and Scala.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Play Framework - The High Velocity Web Framework&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://x.com/playframework&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/playframework?label=follow&amp;amp;style=flat&amp;amp;logo=x&amp;amp;color=brightgreen&quot; alt=&quot;X (formerly Twitter) Follow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/g5s2vtZ4Fa&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/931647755942776882?logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/playframework/playframework/discussions&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/discussions/playframework/playframework?&amp;amp;logo=github&amp;amp;color=brightgreen&quot; alt=&quot;GitHub Discussions&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://stackoverflow.com/tags/playframework&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=stackoverflow&amp;amp;logo=stackoverflow&amp;amp;logoColor=fe7a16&amp;amp;color=brightgreen&amp;amp;message=playframework&quot; alt=&quot;StackOverflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/channel/UCRp6QDm5SDjbIuisUpxV9cg&quot;&gt;&lt;img src=&quot;https://img.shields.io/youtube/channel/views/UCRp6QDm5SDjbIuisUpxV9cg?label=watch&amp;amp;logo=youtube&amp;amp;style=flat&amp;amp;color=brightgreen&amp;amp;logoColor=ff0000&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.twitch.tv/playframework&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitch/status/playframework?logo=twitch&amp;amp;logoColor=white&amp;amp;color=brightgreen&amp;amp;label=live%20stream&quot; alt=&quot;Twitch Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://opencollective.com/playframework&quot;&gt;&lt;img src=&quot;https://img.shields.io/opencollective/all/playframework?label=financial%20contributors&amp;amp;logo=open-collective&quot; alt=&quot;OpenCollective&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/playframework/playframework/actions/workflows/build-test.yml&quot;&gt;&lt;img src=&quot;https://github.com/playframework/playframework/actions/workflows/build-test.yml/badge.svg?sanitize=true&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://mvnrepository.com/artifact/org.playframework/play_2.13&quot;&gt;&lt;img src=&quot;https://img.shields.io/maven-central/v/org.playframework/play_2.13.svg?logo=apache-maven&quot; alt=&quot;Maven&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/playframework/playframework&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/repo-size/playframework/playframework.svg?logo=git&quot; alt=&quot;Repository size&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://scala-steward.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Scala_Steward-helping-blue.svg?style=flat&amp;amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAQCAMAAAARSr4IAAAAVFBMVEUAAACHjojlOy5NWlrKzcYRKjGFjIbp293YycuLa3pYY2LSqql4f3pCUFTgSjNodYRmcXUsPD/NTTbjRS+2jomhgnzNc223cGvZS0HaSD0XLjbaSjElhIr+AAAAAXRSTlMAQObYZgAAAHlJREFUCNdNyosOwyAIhWHAQS1Vt7a77/3fcxxdmv0xwmckutAR1nkm4ggbyEcg/wWmlGLDAA3oL50xi6fk5ffZ3E2E3QfZDCcCN2YtbEWZt+Drc6u6rlqv7Uk0LdKqqr5rk2UCRXOk0vmQKGfc94nOJyQjouF9H/wCc9gECEYfONoAAAAASUVORK5CYII=&quot; alt=&quot;Scala Steward badge&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://mergify.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint.svg?url=https://api.mergify.com/v1/badges/playframework/playframework&amp;amp;style=flat&quot; alt=&quot;Mergify Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The Play Framework combines productivity and performance making it easy to build scalable web applications with Java and Scala. Play is developer friendly with a &quot;just hit refresh&quot; workflow and built-in testing support. With Play, applications scale predictably due to a stateless and non-blocking architecture. By being RESTful by default, including assets compilers, JSON &amp;amp; WebSocket support, Play is a perfect fit for modern web &amp;amp; mobile applications.&lt;/p&gt; 
&lt;h2&gt;Learn More&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com&quot;&gt;www.playframework.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/download&quot;&gt;Download&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/documentation/latest/Installing&quot;&gt;Install&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/documentation/latest/NewApplication&quot;&gt;Create a new application&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/documentation/latest/ScalaHome&quot;&gt;Play for Scala developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/documentation/latest/JavaHome&quot;&gt;Play for Java developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/documentation/latest/BuildingFromSource&quot;&gt;Build from source&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/playframework/playframework/issues&quot;&gt;Search or create issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/tagged/playframework&quot;&gt;Get help&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.playframework.com/contributing&quot;&gt;Contribute&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors &amp;amp; Backers&lt;/h2&gt; 
&lt;p&gt;If you find Play useful for work, please consider asking your company to support this Open Source project by &lt;a href=&quot;https://www.playframework.com/sponsors&quot;&gt;becoming a sponsor&lt;/a&gt;.&lt;br /&gt; You can also individually sponsor the project by &lt;a href=&quot;https://www.playframework.com/sponsors&quot;&gt;becoming a backer&lt;/a&gt;.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://opencollective.com/playframework&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://opencollective.com/playframework/donate/button@2x.png?color=blue&quot; width=&quot;250&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Thank you to our premium sponsors!&lt;/h3&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://theguardian.com/&quot;&gt;&lt;img src=&quot;https://www.playframework.com/assets/images/home/sponsors/b15eb0f249dbc45089872e268d8ea5ad-the_guardian.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href=&quot;https://pronto.net/&quot;&gt;&lt;img src=&quot;https://www.playframework.com/assets/images/home/sponsors/c77b1d664f10a1c9cb19b97c6d8bd204-pronto-software.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://depop.com/&quot;&gt;&lt;img src=&quot;https://www.playframework.com/assets/images/home/sponsors/483f7622215dc240d6e6fc52fe167bc0-depop.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://cedarlakeventures.com/&quot;&gt;&lt;img src=&quot;https://www.playframework.com/assets/images/home/sponsors/bec2b526c9ce52c051f9089a10044867-cedar-lake-ventures.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt; 
 &lt;br /&gt; 
 &lt;a href=&quot;https://informaticon.com/&quot;&gt; 
  &lt;picture&gt; 
   &lt;source width=&quot;250&quot; media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://www.playframework.com/assets/images/home/sponsors/d180a3ad763aaf69b46bff18fb110d9e-informaticon-logo-white.png&quot; /&gt; 
   &lt;source width=&quot;250&quot; media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://www.playframework.com/assets/images/home/sponsors/ad86c68a917e70a42440b4feb413c68d-informaticon-logo-black.png&quot; /&gt; 
   &lt;img width=&quot;250&quot; alt=&quot;informaticon logo fallback&quot; src=&quot;https://www.playframework.com/assets/images/home/sponsors/ad86c68a917e70a42440b4feb413c68d-informaticon-logo-black.png&quot; /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://nulab.com/&quot;&gt;&lt;img src=&quot;https://www.playframework.com/assets/images/home/sponsors/6152e584aa8625eedca1c4accf8f8b63-nulab_logo_color.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://sprypoint.com/&quot;&gt;&lt;img src=&quot;https://www.playframework.com/assets/images/home/sponsors/3fdf14f6369cf9d69f4a2a29ce26c2f8-sprypoint-logo-lrg-transparent.png&quot; width=&quot;250&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Thank you to all our backers!&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://opencollective.com/playframework#section-contributors&quot;&gt;&lt;img src=&quot;https://opencollective.com/playframework/organizations.svg?width=890&amp;amp;button=false&amp;amp;avatarHeight=46&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://opencollective.com/playframework#section-contributors&quot;&gt;&lt;img src=&quot;https://opencollective.com/playframework/individuals.svg?width=890&amp;amp;button=false&amp;amp;avatarHeight=46&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Copyright (C) from 2022 The Play Framework Contributors &lt;a href=&quot;https://github.com/playframework&quot;&gt;https://github.com/playframework&lt;/a&gt;, 2011-2021 Lightbend Inc. &lt;a href=&quot;https://www.lightbend.com&quot;&gt;https://www.lightbend.com&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this project except in compliance with the License. You may obtain a copy of the License at &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;https://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ucb-bar/berkeley-hardfloat</title>
      <link>https://github.com/ucb-bar/berkeley-hardfloat</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Berkeley Hardware Floating-Point Units&lt;/h1&gt; 
&lt;p&gt;This repository contains hardware floating-point units written in Chisel. This library contains parameterized floating-point units for fused multiply-add operations, conversions between integer and floating-point numbers, and conversions between floating-point conversions with different precision.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: These units are works in progress. They may not be yet completely free of bugs, nor are they fully optimized.&lt;/p&gt; 
&lt;h2&gt;Recoded Format&lt;/h2&gt; 
&lt;p&gt;The floating-point units in this repository work on an internal recoded format (exponent has an additional bit) to handle subnormal numbers more efficiently in a microprocessor. A more detailed explanation will come soon, but in the mean time here are some example mappings for single-precision numbers.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;IEEE format                           Recoded format
----------------------------------    -----------------------------------
s 00000000 00000000000000000000000    s 000------ 00000000000000000000000
s 00000000 00000000000000000000001    s 001101011 00000000000000000000000
s 00000000 0000000000000000000001f    s 001101100 f0000000000000000000000
s 00000000 000000000000000000001ff    s 001101101 ff000000000000000000000
    ...              ...                   ...              ... 
s 00000000 001ffffffffffffffffffff    s 001111111 ffffffffffffffffffff000
s 00000000 01fffffffffffffffffffff    s 010000000 fffffffffffffffffffff00
s 00000000 1ffffffffffffffffffffff    s 010000001 ffffffffffffffffffffff0
s 00000001 fffffffffffffffffffffff    s 010000010 fffffffffffffffffffffff
s 00000010 fffffffffffffffffffffff    s 010000011 fffffffffffffffffffffff
    ...              ...                   ...              ... 
s 11111101 fffffffffffffffffffffff    s 101111110 fffffffffffffffffffffff
s 11111110 fffffffffffffffffffffff    s 101111111 fffffffffffffffffffffff
s 11111111 00000000000000000000000    s 110------ -----------------------
s 11111111 fffffffffffffffffffffff    s 111------ fffffffffffffffffffffff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Unit-Testing&lt;/h2&gt; 
&lt;p&gt;To unit-test these floating-point units, you need the berkeley-testfloat-3 package.&lt;/p&gt; 
&lt;p&gt;To test floating-point units with the C simulator:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/spark-rapids</title>
      <link>https://github.com/NVIDIA/spark-rapids</link>
      <description>&lt;p&gt;Spark RAPIDS plugin - accelerate Apache Spark with GPUs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RAPIDS Accelerator For Apache Spark&lt;/h1&gt; 
&lt;p&gt;NOTE: For the latest stable &lt;a href=&quot;https://github.com/nvidia/spark-rapids/raw/main/README.md&quot;&gt;README.md&lt;/a&gt; ensure you are on the main branch.&lt;/p&gt; 
&lt;p&gt;The RAPIDS Accelerator for Apache Spark provides a set of plugins for &lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt; that leverage GPUs to accelerate processing via the &lt;a href=&quot;https://rapids.ai&quot;&gt;RAPIDS&lt;/a&gt; libraries.&lt;/p&gt; 
&lt;p&gt;Documentation on the current release can be found &lt;a href=&quot;https://nvidia.github.io/spark-rapids/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To get started and try the plugin out use the &lt;a href=&quot;https://docs.nvidia.com/spark-rapids/user-guide/latest/getting-started/overview.html&quot;&gt;getting started guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://deepwiki.com/NVIDIA/spark-rapids&quot;&gt;&lt;img src=&quot;https://deepwiki.com/badge.svg?sanitize=true&quot; alt=&quot;Ask DeepWiki&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Compatibility&lt;/h2&gt; 
&lt;p&gt;The SQL plugin tries to produce results that are bit for bit identical with Apache Spark. Operator compatibility is documented &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/spark-rapids/branch-25.10/docs/compatibility.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Tuning&lt;/h2&gt; 
&lt;p&gt;To get started tuning your job and get the most performance out of it please start with the &lt;a href=&quot;https://docs.nvidia.com/spark-rapids/user-guide/latest/tuning-guide.html&quot;&gt;tuning guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;The plugin has a set of Spark configs that control its behavior and are documented &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/spark-rapids/branch-25.10/docs/configs.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Issues &amp;amp; Questions&lt;/h2&gt; 
&lt;p&gt;We use github to track bugs, feature requests, and answer questions. File an &lt;a href=&quot;https://github.com/NVIDIA/spark-rapids/issues/new/choose&quot;&gt;issue&lt;/a&gt; for a bug or feature request. Ask or answer a question on the &lt;a href=&quot;https://github.com/NVIDIA/spark-rapids/discussions&quot;&gt;discussion board&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Download&lt;/h2&gt; 
&lt;p&gt;The jar files for the most recent release can be retrieved from the &lt;a href=&quot;https://nvidia.github.io/spark-rapids/docs/download.html&quot;&gt;download&lt;/a&gt; page.&lt;/p&gt; 
&lt;h2&gt;Building From Source&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/spark-rapids/branch-25.10/CONTRIBUTING.md#building-from-source&quot;&gt;build instructions in the contributing guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Testing&lt;/h2&gt; 
&lt;p&gt;Tests are described &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/spark-rapids/branch-25.10/tests/README.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Integration&lt;/h2&gt; 
&lt;p&gt;The RAPIDS Accelerator For Apache Spark does provide some APIs for doing zero copy data transfer into other GPU enabled applications. It is described &lt;a href=&quot;https://docs.nvidia.com/spark-rapids/user-guide/latest/additional-functionality/ml-integration.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, we are working with XGBoost to try to provide this integration out of the box.&lt;/p&gt; 
&lt;p&gt;You may need to disable RMM caching when exporting data to an ML library as that library will likely want to use all of the GPU&#39;s memory and if it is not aware of RMM it will not have access to any of the memory that RMM is holding.&lt;/p&gt; 
&lt;h2&gt;Qualification and Profiling tools&lt;/h2&gt; 
&lt;p&gt;The Qualification and Profiling tools have been moved to &lt;a href=&quot;https://github.com/NVIDIA/spark-rapids-tools&quot;&gt;nvidia/spark-rapids-tools&lt;/a&gt; repo.&lt;/p&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://docs.nvidia.com/spark-rapids/user-guide/latest/qualification/overview.html&quot;&gt;Qualification tool documentation&lt;/a&gt; and &lt;a href=&quot;https://docs.nvidia.com/spark-rapids/user-guide/latest/profiling/overview.html&quot;&gt;Profiling tool documentation&lt;/a&gt; for more details on how to use the tools.&lt;/p&gt; 
&lt;h2&gt;Dependency for External Projects&lt;/h2&gt; 
&lt;p&gt;If you need to develop some functionality on top of RAPIDS Accelerator For Apache Spark (we currently limit support to GPU-accelerated UDFs) we recommend you declare our distribution artifact as a &lt;code&gt;provided&lt;/code&gt; dependency.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-xml&quot;&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;com.nvidia&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;rapids-4-spark_2.12&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;25.10.0-SNAPSHOT&amp;lt;/version&amp;gt;
    &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>ucb-bar/chipyard</title>
      <link>https://github.com/ucb-bar/chipyard</link>
      <description>&lt;p&gt;An Agile RISC-V SoC Design Framework with in-order cores, out-of-order cores, accelerators, and more&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://github.com/ucb-bar/chipyard/raw/main/docs/_static/images/chipyard-logo-full.png&quot; alt=&quot;CHIPYARD&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Chipyard Framework &lt;a href=&quot;https://github.com/ucb-bar/chipyard/actions&quot;&gt;&lt;img src=&quot;https://github.com/ucb-bar/chipyard/actions/workflows/chipyard-run-tests.yml/badge.svg?sanitize=true&quot; alt=&quot;Test&quot; /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Latest Documentation&lt;/strong&gt;: &lt;a href=&quot;https://chipyard.readthedocs.io/&quot;&gt;https://chipyard.readthedocs.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User Question Forum&lt;/strong&gt;: &lt;a href=&quot;https://groups.google.com/forum/#!forum/chipyard&quot;&gt;https://groups.google.com/forum/#!forum/chipyard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bugs and Feature Requests&lt;/strong&gt;: &lt;a href=&quot;https://github.com/ucb-bar/chipyard/issues&quot;&gt;https://github.com/ucb-bar/chipyard/issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using Chipyard&lt;/h2&gt; 
&lt;p&gt;To get started using Chipyard, see the documentation on the Chipyard documentation site: &lt;a href=&quot;https://chipyard.readthedocs.io/&quot;&gt;https://chipyard.readthedocs.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Chipyard&lt;/h2&gt; 
&lt;p&gt;Chipyard is an open source framework for agile development of Chisel-based systems-on-chip. It will allow you to leverage the Chisel HDL, Rocket Chip SoC generator, and other &lt;a href=&quot;https://berkeley.edu&quot;&gt;Berkeley&lt;/a&gt; projects to produce a &lt;a href=&quot;https://riscv.org/&quot;&gt;RISC-V&lt;/a&gt; SoC with everything from MMIO-mapped peripherals to custom accelerators. Chipyard contains processor cores (&lt;a href=&quot;https://github.com/freechipsproject/rocket-chip&quot;&gt;Rocket&lt;/a&gt;, &lt;a href=&quot;https://github.com/riscv-boom/riscv-boom&quot;&gt;BOOM&lt;/a&gt;, &lt;a href=&quot;https://github.com/openhwgroup/cva6/&quot;&gt;CVA6 (Ariane)&lt;/a&gt;), vector units (&lt;a href=&quot;https://github.com/ucb-bar/saturn-vectors&quot;&gt;Saturn&lt;/a&gt;, &lt;a href=&quot;https://github.com/pulp-platform/ara&quot;&gt;Ara&lt;/a&gt;), accelerators (&lt;a href=&quot;https://github.com/ucb-bar/gemmini&quot;&gt;Gemmini&lt;/a&gt;, &lt;a href=&quot;http://nvdla.org/&quot;&gt;NVDLA&lt;/a&gt;), memory systems, and additional peripherals and tooling to help create a full featured SoC. Chipyard supports multiple concurrent flows of agile hardware development, including software RTL simulation, FPGA-accelerated simulation (&lt;a href=&quot;https://fires.im&quot;&gt;FireSim&lt;/a&gt;), automated VLSI flows (&lt;a href=&quot;https://github.com/ucb-bar/hammer&quot;&gt;Hammer&lt;/a&gt;), and software workload generation for bare-metal and Linux-based systems (&lt;a href=&quot;https://github.com/firesim/FireMarshal/&quot;&gt;FireMarshal&lt;/a&gt;). Chipyard is actively developed in the &lt;a href=&quot;http://bar.eecs.berkeley.edu&quot;&gt;Berkeley Architecture Research Group&lt;/a&gt; in the &lt;a href=&quot;https://eecs.berkeley.edu&quot;&gt;Electrical Engineering and Computer Sciences Department&lt;/a&gt; at the &lt;a href=&quot;https://berkeley.edu&quot;&gt;University of California, Berkeley&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Chipyard Documentation: &lt;a href=&quot;https://chipyard.readthedocs.io/&quot;&gt;https://chipyard.readthedocs.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chipyard (x FireSim) Tutorial: &lt;a href=&quot;https://fires.im/tutorial-recent/&quot;&gt;https://fires.im/tutorial-recent/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chipyard Basics slides: &lt;a href=&quot;https://fires.im/asplos23-slides-pdf/02_chipyard_basics.pdf&quot;&gt;https://fires.im/asplos23-slides-pdf/02_chipyard_basics.pdf&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Need help?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the Chipyard Mailing List: &lt;a href=&quot;https://groups.google.com/forum/#!forum/chipyard&quot;&gt;https://groups.google.com/forum/#!forum/chipyard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;If you find a bug or would like propose a feature, post an issue on this repo: &lt;a href=&quot;https://github.com/ucb-bar/chipyard/issues&quot;&gt;https://github.com/ucb-bar/chipyard/issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chipyard/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Attribution and Chipyard-related Publications&lt;/h2&gt; 
&lt;p&gt;If used for research, please cite Chipyard by the following publication:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{chipyard,
  author={Amid, Alon and Biancolin, David and Gonzalez, Abraham and Grubb, Daniel and Karandikar, Sagar and Liew, Harrison and Magyar,   Albert and Mao, Howard and Ou, Albert and Pemberton, Nathan and Rigge, Paul and Schmidt, Colin and Wright, John and Zhao, Jerry and Shao, Yakun Sophia and Asanovi\&#39;{c}, Krste and Nikoli\&#39;{c}, Borivoje},
  journal={IEEE Micro},
  title={Chipyard: Integrated Design, Simulation, and Implementation Framework for Custom SoCs},
  year={2020},
  volume={40},
  number={4},
  pages={10-21},
  doi={10.1109/MM.2020.2996616},
  ISSN={1937-4143},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Chipyard&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A. Amid, et al. &lt;em&gt;IEEE Micro&#39;20&lt;/em&gt; &lt;a href=&quot;https://ieeexplore.ieee.org/document/9099108&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;A. Amid, et al. &lt;em&gt;DAC&#39;20&lt;/em&gt; &lt;a href=&quot;https://ieeexplore.ieee.org/document/9218756&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;A. Amid, et al. &lt;em&gt;ISCAS&#39;21&lt;/em&gt; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9401515&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These additional publications cover many of the internal components used in Chipyard. However, for the most up-to-date details, users should refer to the Chipyard docs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Generators&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Rocket Chip&lt;/strong&gt;: K. Asanovic, et al., &lt;em&gt;UCB EECS TR&lt;/em&gt;. &lt;a href=&quot;http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;BOOM&lt;/strong&gt;: C. Celio, et al., &lt;em&gt;Hot Chips 30&lt;/em&gt;. &lt;a href=&quot;https://old.hotchips.org/hc30/1conf/1.03_Berkeley_BROOM_HC30.Berkeley.Celio.v02.pdf&quot;&gt;PDF&lt;/a&gt;. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;SonicBOOM (BOOMv3)&lt;/strong&gt;: J. Zhao, et al., &lt;em&gt;CARRV&#39;20&lt;/em&gt;. &lt;a href=&quot;https://carrv.github.io/2020/papers/CARRV2020_paper_15_Zhao.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;COBRA (BOOM Branch Prediction)&lt;/strong&gt;: J. Zhao, et al., &lt;em&gt;ISPASS&#39;21&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/9408173&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Gemmini&lt;/strong&gt;: H. Genc, et al., &lt;em&gt;DAC&#39;21&lt;/em&gt;. &lt;a href=&quot;https://arxiv.org/pdf/1911.09925&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sims&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;FireSim&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;ISCA&#39;18&lt;/em&gt;. &lt;a href=&quot;https://sagark.org/assets/pubs/firesim-isca2018.pdf&quot;&gt;PDF&lt;/a&gt;. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;FireSim Micro Top Picks&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;IEEE Micro, Top Picks 2018&lt;/em&gt;. &lt;a href=&quot;https://sagark.org/assets/pubs/firesim-micro-top-picks2018.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;FASED&lt;/strong&gt;: D. Biancolin, et al., &lt;em&gt;FPGA&#39;19&lt;/em&gt;. &lt;a href=&quot;https://people.eecs.berkeley.edu/~biancolin/papers/fased-fpga19.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Golden Gate&lt;/strong&gt;: A. Magyar, et al., &lt;em&gt;ICCAD&#39;19&lt;/em&gt;. &lt;a href=&quot;https://davidbiancolin.github.io/papers/goldengate-iccad19.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;FirePerf&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;ASPLOS&#39;20&lt;/em&gt;. &lt;a href=&quot;https://sagark.org/assets/pubs/fireperf-asplos2020.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;FireSim ISCA@50 Retrospective&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;ISCA@50 Retrospective: 1996-2020&lt;/em&gt;. &lt;a href=&quot;https://sites.coecis.cornell.edu/isca50retrospective/files/2023/06/Karandikar_2018_FireSim.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Chisel&lt;/strong&gt;: J. Bachrach, et al., &lt;em&gt;DAC&#39;12&lt;/em&gt;. &lt;a href=&quot;https://people.eecs.berkeley.edu/~krste/papers/chisel-dac2012.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;FIRRTL&lt;/strong&gt;: A. Izraelevitz, et al., &lt;em&gt;ICCAD&#39;17&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/8203780&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Chisel DSP&lt;/strong&gt;: A. Wang, et al., &lt;em&gt;DAC&#39;18&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/8465790&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;FireMarshal&lt;/strong&gt;: N. Pemberton, et al., &lt;em&gt;ISPASS&#39;21&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/9408192&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VLSI&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Hammer&lt;/strong&gt;: E. Wang, et al., &lt;em&gt;ISQED&#39;20&lt;/em&gt;. &lt;a href=&quot;https://www.isqed.org/English/Archives/2020/Technical_Sessions/113.html&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Hammer&lt;/strong&gt;: H. Liew, et al., &lt;em&gt;DAC&#39;22&lt;/em&gt;. &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3489517.3530672&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This work is supported by the NSF CCRI ENS Chipyard Award #2016662.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>awslabs/deequ</title>
      <link>https://github.com/awslabs/deequ</link>
      <description>&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &quot;unit tests for data&quot;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/awslabs/deequ/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/awslabs/deequ/actions/workflows/maven.yml?query=branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/awslabs/deequ/actions/workflows/maven.yml/badge.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&quot;&gt;&lt;img src=&quot;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&quot; alt=&quot;Maven Central&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &quot;unit tests for data&quot;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&quot;&gt;contributions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&quot;https://github.com/awslabs/python-deequ&quot;&gt;GitHub&lt;/a&gt;, &lt;a href=&quot;https://pydeequ.readthedocs.io/en/latest/README.html&quot;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&quot;https://pypi.org/project/pydeequ/&quot;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Requirements and Installation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; 
&lt;p&gt;Available via &lt;a href=&quot;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&quot;&gt;maven central&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&quot;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&quot;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;libraryDependencies += &quot;com.amazon.deequ&quot; % &quot;deequ&quot; % &quot;2.0.0-spark-3.1&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &quot;unit-test&quot; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;case class Item(
  id: Long,
  productName: String,
  description: String,
  priority: String,
  numViews: Long
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Our library is built on &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val rdd = spark.sparkContext.parallelize(Seq(
  Item(1, &quot;Thingy A&quot;, &quot;awesome thing.&quot;, &quot;high&quot;, 0),
  Item(2, &quot;Thingy B&quot;, &quot;available at http://thingb.com&quot;, null, 0),
  Item(3, null, null, &quot;low&quot;, 5),
  Item(4, &quot;Thingy D&quot;, &quot;checkout https://thingd.ca&quot;, &quot;low&quot;, 10),
  Item(5, &quot;Thingy E&quot;, null, &quot;high&quot;, 12)))

val data = spark.createDataFrame(rdd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &quot;unit-test&quot; for data, which can be verified on a piece of data at hand. If the data has errors, we can &quot;quarantine&quot; and fix it, before we feed it to an application.&lt;/p&gt; 
&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&quot;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&quot;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;there are 5 rows in total&lt;/li&gt; 
 &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; 
 &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &quot;high&quot; or &quot;low&quot; as value&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; 
 &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; 
 &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In code this looks as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import com.amazon.deequ.VerificationSuite
import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}


val verificationResult = VerificationSuite()
  .onData(data)
  .addCheck(
    Check(CheckLevel.Error, &quot;unit testing my data&quot;)
      .hasSize(_ == 5) // we expect 5 rows
      .isComplete(&quot;id&quot;) // should never be NULL
      .isUnique(&quot;id&quot;) // should not contain duplicates
      .isComplete(&quot;productName&quot;) // should never be NULL
      // should only contain the values &quot;high&quot; and &quot;low&quot;
      .isContainedIn(&quot;priority&quot;, Array(&quot;high&quot;, &quot;low&quot;))
      .isNonNegative(&quot;numViews&quot;) // should not contain negative values
      // at least half of the descriptions should contain a url
      .containsURL(&quot;description&quot;, _ &amp;gt;= 0.5)
      // half of the items should have less than 10 views
      .hasApproxQuantile(&quot;numViews&quot;, 0.5, _ &amp;lt;= 10))
    .run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&quot;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import com.amazon.deequ.constraints.ConstraintStatus


if (verificationResult.status == CheckStatus.Success) {
  println(&quot;The data passed the test, everything is fine!&quot;)
} else {
  println(&quot;We found errors in the data:\n&quot;)

  val resultsForAllConstraints = verificationResult.checkResults
    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }

  resultsForAllConstraints
    .filter { _.status != ConstraintStatus.Success }
    .foreach { result =&amp;gt; println(s&quot;${result.constraint}: ${result.message.get}&quot;) }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;We found errors in the data:

CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!
PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; 
&lt;h2&gt;More examples&lt;/h2&gt; 
&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&quot;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&quot;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&quot;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&quot;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&quot;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&quot;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;DQDL (Data Quality Definition Language)&lt;/h2&gt; 
&lt;p&gt;Deequ also supports &lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/dqdl.html&quot;&gt;DQDL&lt;/a&gt;, a declarative language for defining data quality rules. DQDL allows you to express data quality constraints in a simple, readable format.&lt;/p&gt; 
&lt;h3&gt;Supported DQDL Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RowCount&lt;/strong&gt;: &lt;code&gt;RowCount &amp;lt; 100&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Completeness&lt;/strong&gt;: &lt;code&gt;Completeness &quot;column&quot; &amp;gt; 0.9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IsComplete&lt;/strong&gt;: &lt;code&gt;IsComplete &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Uniqueness&lt;/strong&gt;: &lt;code&gt;Uniqueness &quot;column&quot; = 1.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IsUnique&lt;/strong&gt;: &lt;code&gt;IsUnique &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ColumnCorrelation&lt;/strong&gt;: &lt;code&gt;ColumnCorrelation &quot;col1&quot; &quot;col2&quot; &amp;gt; 0.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DistinctValuesCount&lt;/strong&gt;: &lt;code&gt;DistinctValuesCount &quot;column&quot; = 5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Entropy&lt;/strong&gt;: &lt;code&gt;Entropy &quot;column&quot; &amp;gt; 2.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mean&lt;/strong&gt;: &lt;code&gt;Mean &quot;column&quot; between 10 and 50&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;StandardDeviation&lt;/strong&gt;: &lt;code&gt;StandardDeviation &quot;column&quot; &amp;lt; 5.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sum&lt;/strong&gt;: &lt;code&gt;Sum &quot;column&quot; = 100&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UniqueValueRatio&lt;/strong&gt;: &lt;code&gt;UniqueValueRatio &quot;column&quot; &amp;gt; 0.7&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CustomSql&lt;/strong&gt;: &lt;code&gt;CustomSql &quot;SELECT COUNT(*) FROM primary&quot; &amp;gt; 0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IsPrimaryKey&lt;/strong&gt;: &lt;code&gt;IsPrimaryKey &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ColumnLength&lt;/strong&gt;: &lt;code&gt;ColumnLength &quot;column&quot; between 1 and 5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ColumnExists&lt;/strong&gt;: &lt;code&gt;ColumnExists &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Scala Example&lt;/h3&gt; 
&lt;p&gt;ScalaDQDLExample.scala&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import com.amazon.deequ.dqdl.EvaluateDataQuality
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName(&quot;DQDL Example&quot;)
  .master(&quot;local[*]&quot;)
  .getOrCreate()

import spark.implicits._

// Sample data
val df = Seq(
  (&quot;1&quot;, &quot;a&quot;, &quot;c&quot;),
  (&quot;2&quot;, &quot;a&quot;, &quot;c&quot;),
  (&quot;3&quot;, &quot;a&quot;, &quot;c&quot;),
  (&quot;4&quot;, &quot;b&quot;, &quot;d&quot;)
).toDF(&quot;item&quot;, &quot;att1&quot;, &quot;att2&quot;)

// Define rules using DQDL syntax
val ruleset = &quot;&quot;&quot;Rules=[IsUnique &quot;item&quot;, RowCount &amp;lt; 10, Completeness &quot;item&quot; &amp;gt; 0.8, Uniqueness &quot;item&quot; = 1.0]&quot;&quot;&quot;

// Evaluate data quality
val results = EvaluateDataQuality.process(df, ruleset)
results.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Java Example&lt;/h3&gt; 
&lt;p&gt;JavaDQDLExample.java&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import com.amazon.deequ.dqdl.EvaluateDataQuality;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

SparkSession spark = SparkSession.builder()
    .appName(&quot;DQDL Java Example&quot;)
    .master(&quot;local[*]&quot;)
    .getOrCreate();

// Create sample data
Dataset&amp;lt;Row&amp;gt; df = spark.sql(
    &quot;SELECT * FROM VALUES &quot; +
    &quot;(&#39;1&#39;, &#39;a&#39;, &#39;c&#39;), &quot; +
    &quot;(&#39;2&#39;, &#39;a&#39;, &#39;c&#39;), &quot; +
    &quot;(&#39;3&#39;, &#39;a&#39;, &#39;c&#39;), &quot; +
    &quot;(&#39;4&#39;, &#39;b&#39;, &#39;d&#39;) &quot; +
    &quot;AS t(item, att1, att2)&quot;
);

// Define rules using DQDL syntax
String ruleset = &quot;Rules=[IsUnique \&quot;item\&quot;, RowCount &amp;lt; 10, Completeness \&quot;item\&quot; &amp;gt; 0.8, Uniqueness \&quot;item\&quot; = 1.0]&quot;;

// Evaluate data quality
Dataset&amp;lt;Row&amp;gt; results = EvaluateDataQuality.process(df, ruleset);
results.show();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; 
&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&quot;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&quot;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>twitter/the-algorithm</title>
      <link>https://github.com/twitter/the-algorithm</link>
      <description>&lt;p&gt;Source code for the X Recommendation Algorithm&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;X&#39;s Recommendation Algorithm&lt;/h1&gt; 
&lt;p&gt;X&#39;s Recommendation Algorithm is a set of services and jobs that are responsible for serving feeds of posts and other content across all X product surfaces (e.g. For You Timeline, Search, Explore, Notifications). For an introduction to how the algorithm works, please refer to our &lt;a href=&quot;https://blog.x.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm&quot;&gt;engineering blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Product surfaces at X are built on a shared set of data, models, and software frameworks. The shared components included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Data&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/tweetypie/server/README.md&quot;&gt;tweetypie&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Core service that handles the reading and writing of post data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/unified_user_actions/README.md&quot;&gt;unified-user-actions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time stream of user actions on X.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/user-signal-service/README.md&quot;&gt;user-signal-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Centralized platform to retrieve explicit (e.g. likes, replies) and implicit (e.g. profile visits, tweet clicks) user signals.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/simclusters_v2/README.md&quot;&gt;SimClusters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Community detection and sparse embeddings into those communities.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/twitter/the-algorithm-ml/raw/main/projects/twhin/README.md&quot;&gt;TwHIN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dense knowledge graph embeddings for Users and Posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/trust_and_safety_models/README.md&quot;&gt;trust-and-safety-models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Models for detecting NSFW or abusive content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/interaction_graph/README.md&quot;&gt;real-graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model to predict the likelihood of an X User interacting with another User.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/graph/batch/job/tweepcred/README&quot;&gt;tweepcred&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Page-Rank algorithm for calculating X User reputation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/recos-injector/README.md&quot;&gt;recos-injector&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Streaming event processor for building input streams for &lt;a href=&quot;https://github.com/twitter/GraphJet&quot;&gt;GraphJet&lt;/a&gt; based services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/graph-feature-service/README.md&quot;&gt;graph-feature-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Serves graph features for a directed pair of users (e.g. how many of User A&#39;s following liked posts from User B).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/topic-social-proof/README.md&quot;&gt;topic-social-proof&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Identifies topics related to individual posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-scorer/README.md&quot;&gt;representation-scorer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Compute scores between pairs of entities (Users, Posts, etc.) using embedding similarity.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Software framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/navi/README.md&quot;&gt;navi&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;High performance, machine learning model serving written in Rust.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md&quot;&gt;product-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Software framework for building feeds of content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/timelines/data_processing/ml_util/aggregation_framework/README.md&quot;&gt;timelines-aggregation-framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Framework for generating aggregate features in batch or real time.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-manager/README.md&quot;&gt;representation-manager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Service to retrieve embeddings (i.e. SimClusers and TwHIN).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/twml/README.md&quot;&gt;twml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy machine learning framework built on TensorFlow v1.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The product surfaces currently included in this repository are the For You Timeline and Recommended Notifications.&lt;/p&gt; 
&lt;h3&gt;For You Timeline&lt;/h3&gt; 
&lt;p&gt;The diagram below illustrates how major services and jobs interconnect to construct a For You Timeline.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/docs/system-diagram.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The core components of the For You Timeline included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Candidate Source&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/java/com/twitter/search/README.md&quot;&gt;search-index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Find and rank In-Network posts. ~50% of posts come from this candidate source.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/tweet-mixer&quot;&gt;tweet-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Coordination layer for fetching Out-of-Network tweet candidates from underlying compute services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos/user_tweet_entity_graph/README.md&quot;&gt;user-tweet-entity-graph&lt;/a&gt; (UTEG)&lt;/td&gt; 
   &lt;td&gt;Maintains an in memory User to Post interaction graph, and finds candidates based on traversals of this graph. This is built on the &lt;a href=&quot;https://github.com/twitter/GraphJet&quot;&gt;GraphJet&lt;/a&gt; framework. Several other GraphJet based features and candidate sources are located &lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos&quot;&gt;here&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/follow-recommendations-service/README.md&quot;&gt;follow-recommendation-service&lt;/a&gt; (FRS)&lt;/td&gt; 
   &lt;td&gt;Provides Users with recommendations for accounts to follow, and posts from those accounts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/README.md&quot;&gt;light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by search index (Earlybird) to rank posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/README.md&quot;&gt;heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Neural network for ranking candidate posts. One of the main signals used to select timeline posts post candidate sourcing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Post mixing &amp;amp; filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/home-mixer/README.md&quot;&gt;home-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main service used to construct and serve the Home Timeline. Built on &lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md&quot;&gt;product-mixer&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/visibilitylib/README.md&quot;&gt;visibility-filters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Responsible for filtering X content to support legal compliance, improve product quality, increase user trust, protect revenue through the use of hard-filtering, visible product treatments, and coarse-grained downranking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/timelineranker/README.md&quot;&gt;timelineranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy service which provides relevance-scored posts from the Earlybird Search Index and UTEG service.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Recommended Notifications&lt;/h3&gt; 
&lt;p&gt;The core components of Recommended Notifications included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Service&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/README.md&quot;&gt;pushservice&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main recommendation service at X used to surface recommendations to our users via notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/light_ranking/README.md&quot;&gt;pushservice-light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by pushservice to rank posts. Bridges candidate generation and heavy ranking by pre-selecting highly-relevant candidates from the initial huge candidate pool.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/heavy_ranking/README.md&quot;&gt;pushservice-heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-task learning model to predict the probabilities that the target users will open and engage with the sent notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Build and test code&lt;/h2&gt; 
&lt;p&gt;We include Bazel BUILD files for most components, but not a top-level BUILD or WORKSPACE file. We plan to add a more complete build and test system in the future.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We invite the community to submit GitHub issues and pull requests for suggestions on improving the recommendation algorithm. We are working on tools to manage these suggestions and sync changes to our internal repository. Any security concerns or issues should be routed to our official &lt;a href=&quot;https://hackerone.com/x&quot;&gt;bug bounty program&lt;/a&gt; through HackerOne. We hope to benefit from the collective intelligence and expertise of the global community in helping us identify issues and suggest improvements, ultimately leading to a better X.&lt;/p&gt; 
&lt;p&gt;Read our blog on the open source initiative &lt;a href=&quot;https://blog.x.com/en_us/topics/company/2023/a-new-era-of-transparency-for-twitter&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>softwaremill/sttp</title>
      <link>https://github.com/softwaremill/sttp</link>
      <description>&lt;p&gt;The Scala HTTP client you always wanted!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://github.com/softwaremill/sttp/raw/master/banner.png&quot; alt=&quot;sttp&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://softwaremill.community/c/sttp-client&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Discourse-ask%20question-blue&quot; alt=&quot;Ideas, suggestions, problems, questions&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/softwaremill/sttp/actions?query=workflow%3ACI+branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/softwaremill/sttp/workflows/CI/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://maven-badges.herokuapp.com/maven-central/com.softwaremill.sttp.client4/core_3&quot;&gt;&lt;img src=&quot;https://maven-badges.herokuapp.com/maven-central/com.softwaremill.sttp.client4/core_3/badge.svg?sanitize=true&quot; alt=&quot;Maven Central&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;The Scala HTTP client you always wanted!&lt;/h1&gt; 
&lt;h2&gt;Welcome!&lt;/h2&gt; 
&lt;p&gt;sttp client is an open-source HTTP client for Scala, supporting various approaches to writing Scala code: synchronous (direct-style), &lt;code&gt;Future&lt;/code&gt;-based, and using functional effect systems (cats-effect, ZIO, Monix, Kyo, scalaz).&lt;/p&gt; 
&lt;p&gt;The library is available for Scala 2.12, 2.13 and 3. Supported platforms are the JVM (Java 11+), Scala.JS and Scala Native.&lt;/p&gt; 
&lt;p&gt;Here&#39;s a quick example of sttp client in action, runnable using &lt;a href=&quot;https://scala-cli.virtuslab.org&quot;&gt;scala-cli&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//&amp;gt; using dep com.softwaremill.sttp.client4::core:4.0.11

import sttp.client4.quick.*

@main def run(): Unit =
  println(quickRequest.get(uri&quot;http://httpbin.org/ip&quot;).send())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;sttp client addresses common HTTP client use cases, such as interacting with JSON APIs (with automatic serialization of request bodies and deserialization of response bodies), uploading and downloading files, submitting form data, handling multipart requests, and working with WebSockets.&lt;/p&gt; 
&lt;p&gt;The driving principle of sttp client&#39;s design is to provide a clean, programmer-friendly API to describe HTTP requests, along with response handling. This ensures that resources, such as HTTP connections, are used safely, also in the presence of errors.&lt;/p&gt; 
&lt;p&gt;sttp client integrates with a number of lower-level Scala and Java HTTP client implementations through backends (using Java&#39;s &lt;code&gt;HttpClient&lt;/code&gt;, Akka HTTP, Pekko HTTP, http4s, OkHttp, Armeria), offering a wide range of choices when it comes to protocol support, connectivity settings and programming stack compatibility.&lt;/p&gt; 
&lt;p&gt;Additionally, sttp client seamlessly integrates with popular libraries for JSON handling (e.g., circe, uPickle, jsoniter, json4s, play-json, ZIO Json), logging, metrics, and tracing (e.g., slf4j, scribe, OpenTelemetry, Prometheus). It also supports streaming libraries (e.g., fs2, ZIO Streams, Akka Streams, Pekko Streams) and provides tools for testing HTTP interactions.&lt;/p&gt; 
&lt;p&gt;Some more features: URI interpolation, a self-managed backend, and type-safe HTTP error/success representation, are demonstrated by the below example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;//&amp;gt; using dep com.softwaremill.sttp.client4::core:4.0.11

import sttp.client4.*

@main def sttpDemo(): Unit =
  val sort: Option[String] = None
  val query = &quot;http language:scala&quot;

  // the `query` parameter is automatically url-encoded
  // `sort` is removed, as the value is not defined
  val request = basicRequest.get(
    uri&quot;https://api.github.com/search/repositories?q=$query&amp;amp;sort=$sort&quot;)

  val backend = DefaultSyncBackend()
  val response = request.send(backend)

  // response.header(...): Option[String]
  println(response.header(&quot;Content-Length&quot;)) 

  // response.body: read into an Either[String, String] to indicate failure or success 
  println(response.body)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;But that&#39;s just a small glimpse of sttp client&#39;s features! For more examples, see the &lt;a href=&quot;https://sttp.softwaremill.com/en/latest/examples.html&quot;&gt;usage examples&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;sttp (v4) documentation is available at &lt;a href=&quot;https://sttp.softwaremill.com&quot;&gt;sttp.softwaremill.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;sttp (v3) documentation is available at &lt;a href=&quot;https://sttp.softwaremill.com/en/v3&quot;&gt;sttp.softwaremill.com/en/v3&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;sttp (v2) documentation is available at &lt;a href=&quot;https://sttp.softwaremill.com/en/v2&quot;&gt;sttp.softwaremill.com/en/v2&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;sttp (v1) documentation is available at &lt;a href=&quot;https://sttp.softwaremill.com/en/v1&quot;&gt;sttp.softwaremill.com/en/v1&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;scaladoc is available at &lt;a href=&quot;https://www.javadoc.io/doc/com.softwaremill.sttp.client4/core_3/4.0.11&quot;&gt;https://www.javadoc.io&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quickstart with scala-cli&lt;/h2&gt; 
&lt;p&gt;Add the following directive to the top of your scala file to add the core sttp dependency: If you are using &lt;a href=&quot;https://scala-cli.virtuslab.org&quot;&gt;scala-cli&lt;/a&gt;, you can quickly start experimenting with sttp by copy-pasting the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;//&amp;gt; using dep &quot;com.softwaremill.sttp.client4::core:4.0.11&quot;
import sttp.client4.quick.*
quickRequest.get(uri&quot;http://httpbin.org/ip&quot;).send()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The &lt;code&gt;quick&lt;/code&gt; package import brings in the sttp API and a pre-configured, global synchronous backend instance.&lt;/p&gt; 
&lt;h2&gt;Quickstart with Ammonite&lt;/h2&gt; 
&lt;p&gt;Similarly, using &lt;a href=&quot;http://ammonite.io&quot;&gt;Ammonite&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import $ivy.`com.softwaremill.sttp.client4::core:4.0.11`
import sttp.client4.quick.*
quickRequest.get(uri&quot;http://httpbin.org/ip&quot;).send()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quickstart with sbt&lt;/h2&gt; 
&lt;p&gt;Add the following dependency:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;&quot;com.softwaremill.sttp.client4&quot; %% &quot;core&quot; % &quot;4.0.11&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then, import:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import sttp.client4.*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Type &lt;code&gt;basicRequest.&lt;/code&gt; and see where your IDE’s auto-complete gets you!&lt;/p&gt; 
&lt;h2&gt;Other sttp projects&lt;/h2&gt; 
&lt;p&gt;sttp is a family of Scala HTTP-related projects, and currently includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;sttp client: this project&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/softwaremill/tapir&quot;&gt;sttp tapir&lt;/a&gt;: rapid development of self-documenting APIs&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/softwaremill/sttp-model&quot;&gt;sttp model&lt;/a&gt;: simple HTTP model classes (used by client &amp;amp; tapir)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/softwaremill/sttp-shared&quot;&gt;sttp shared&lt;/a&gt;: shared web socket, FP abstractions, capabilities and streaming code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/softwaremill/sttp-apispec&quot;&gt;sttp apispec&lt;/a&gt;: OpenAPI, AsyncAPI and JSON Schema models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/softwaremill/sttp-openai&quot;&gt;sttp openai&lt;/a&gt;: Scala client wrapper for OpenAI and OpenAI-compatible APIs. Use the power of ChatGPT inside your code!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you have a question, suggestion, or hit a problem, feel free to ask on our &lt;a href=&quot;https://softwaremill.community/c/sttp-client&quot;&gt;discourse forum&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Or, if you encounter a bug, something is unclear in the code or documentation, don’t hesitate and open an issue on GitHub.&lt;/p&gt; 
&lt;p&gt;We are also always looking for contributions and new ideas, so if you’d like to get into the project, check out the &lt;a href=&quot;https://github.com/softwaremill/sttp/issues&quot;&gt;open issues&lt;/a&gt;, or post your own suggestions!&lt;/p&gt; 
&lt;p&gt;Note that running the default &lt;code&gt;compile&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; tasks will build all of the JVM, Native and JS backends, and is likely to run out of memory, or to encounter missing native dependencies. If you&#39;d like to run compilation/tests using &lt;em&gt;only&lt;/em&gt; the JVM backend, run: &lt;code&gt;sbt &quot;compileScoped 3 JVM&quot;&lt;/code&gt; or &lt;code&gt;sbt &quot;testScoped 3 JVM&quot;&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;When you have a PR ready, take a look at our &lt;a href=&quot;https://softwaremill.community/t/how-to-prepare-a-good-pr-to-a-library/448&quot;&gt;&quot;How to prepare a good PR&quot; guide&lt;/a&gt;. Thanks! :)&lt;/p&gt; 
&lt;h3&gt;Importing into IntelliJ&lt;/h3&gt; 
&lt;p&gt;By default, when importing to IntelliJ or Metals, only the Scala 2.13/JVM subprojects will be imported. This is controlled by the &lt;code&gt;ideSkipProject&lt;/code&gt; setting in &lt;code&gt;build.sbt&lt;/code&gt; (inside &lt;code&gt;commonSettings&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;If you&#39;d like to work on a different platform or Scala version, simply change this setting temporarily so that the correct subprojects are imported. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;// import only Scala 2.13, JS projects
ideSkipProject := (scalaVersion.value != scala2_13) || !thisProjectRef.value.project.contains(&quot;JS&quot;)

// import only Scala 3, JVM projects
ideSkipProject := (scalaVersion.value != scala3) || thisProjectRef.value.project.contains(&quot;JS&quot;) || thisProjectRef.value.project.contains(&quot;Native&quot;),

// import only Scala 2.13, Native projects
ideSkipProject := (scalaVersion.value != scala2_13) || !thisProjectRef.value.project.contains(&quot;Native&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Modifying documentation&lt;/h3&gt; 
&lt;p&gt;The documentation is typechecked using &lt;a href=&quot;https://scalameta.org/mdoc/&quot;&gt;mdoc&lt;/a&gt;. The sources for the documentation exist in &lt;code&gt;docs&lt;/code&gt;. Don&#39;t modify the generated documentation in &lt;code&gt;generated-docs&lt;/code&gt;, as these files will get overwritten!&lt;/p&gt; 
&lt;p&gt;When generating documentation, it&#39;s best to set the version to the current one, so that the generated doc files don&#39;t include modifications with the current snapshot version.&lt;/p&gt; 
&lt;p&gt;That is, in sbt run: &lt;code&gt;set ThisBuild/version := &quot;4.0.11&quot;&lt;/code&gt;, before running &lt;code&gt;mdoc&lt;/code&gt; in &lt;code&gt;docs&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Testing the Scala.JS backend&lt;/h3&gt; 
&lt;p&gt;In order to run tests against JS backend you will need to install &lt;a href=&quot;https://www.google.com/chrome/&quot;&gt;Google Chrome&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Building &amp;amp; testing the scala-native backend&lt;/h3&gt; 
&lt;p&gt;By default, sttp-native will &lt;strong&gt;not&lt;/strong&gt; be included in the aggregate build of the root project. To include it, define the &lt;code&gt;STTP_NATIVE&lt;/code&gt; environmental variable before running sbt, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;STTP_NATIVE=1 sbt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You might need to install some additional libraries, see the &lt;a href=&quot;http://www.scala-native.org/en/latest/user/setup.html&quot;&gt;scala native&lt;/a&gt; documentation site. On macos, you might additionally need:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ln -s /usr/local/opt/openssl/lib/libcrypto.dylib /usr/local/lib/
ln -s /usr/local/opt/openssl/lib/libssl.dylib /usr/local/lib/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Commercial Support&lt;/h2&gt; 
&lt;p&gt;We offer commercial support for sttp and related technologies, as well as development services. &lt;a href=&quot;https://softwaremill.com&quot;&gt;Contact us&lt;/a&gt; to learn more about our offer!&lt;/p&gt; 
&lt;h2&gt;Copyright&lt;/h2&gt; 
&lt;p&gt;Copyright (C) 2017-2025 SoftwareMill &lt;a href=&quot;https://softwaremill.com&quot;&gt;https://softwaremill.com&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>chipsalliance/rocket-chip</title>
      <link>https://github.com/chipsalliance/rocket-chip</link>
      <description>&lt;p&gt;Rocket Chip Generator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rocket Chip Generator &lt;span&gt;🚀&lt;/span&gt; &lt;img src=&quot;https://github.com/chipsalliance/rocket-chip/workflows/Continuous%20Integration/badge.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/h1&gt; 
&lt;p&gt;This repository contains the Rocket chip generator necessary to instantiate the RISC-V Rocket Core. For more information on Rocket Chip, please consult our &lt;a href=&quot;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html&quot;&gt;technical report&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;RocketChip Dev Meeting&lt;/h2&gt; 
&lt;p&gt;RocketChip development meetings happen every 2 weeks on Wednesday 17:00 – 18:00am CST (Pacific Time - Los Angeles) with meeting notes &lt;a href=&quot;https://docs.google.com/document/d/1NjDnf-i10QE0y-qI94A67uCspDRdCIS_IRTm4jc0Ycc&quot;&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Click &lt;a href=&quot;https://calendar.google.com/calendar/ical/c_699527d804418f900468a49b413d1f9c08e13c0f3f872ce551fc0470d4cdf983%40group.calendar.google.com/public/basic.ics&quot;&gt;here&lt;/a&gt; to subscribe Meeting Schedule(iCal format)&lt;/li&gt; 
 &lt;li&gt;Click &lt;a href=&quot;https://calendar.google.com/calendar/embed?src=c_699527d804418f900468a49b413d1f9c08e13c0f3f872ce551fc0470d4cdf983%40group.calendar.google.com&quot;&gt;here&lt;/a&gt; to view Meeting Schedule via Google Calendar&lt;/li&gt; 
 &lt;li&gt;Click &lt;a href=&quot;https://sifive.zoom.us/j/93899365000?pwd=UG1HSFJ4ODFzR2dhMHU2bUNqbXc3Zz09&quot;&gt;here&lt;/a&gt; to join Zoom meeting (ID: 93899365000, passcode: 754340)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For possible time adjustments, they will be negotiated in Slack and published in the calendar.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#quick&quot;&gt;Quick instructions&lt;/a&gt; for those who want to dive directly into the details without knowing exactly what&#39;s in the repository.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#what&quot;&gt;What&#39;s in the Rocket chip generator repository?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#how&quot;&gt;How should I use the Rocket chip generator?&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#emulator&quot;&gt;Using the cycle-accurate Verilator simulation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#fpga&quot;&gt;Mapping a Rocket core down to an FPGA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#vlsi&quot;&gt;Pushing a Rocket core through the VLSI tools&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#param&quot;&gt;How can I parameterize my Rocket chip?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#debug&quot;&gt;Debugging with GDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#ide&quot;&gt;Building Rocket Chip with an IDE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a name=&quot;quick&quot;&gt;&lt;/a&gt; Quick Instructions&lt;/h2&gt; 
&lt;h3&gt;Checkout The Code&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ucb-bar/rocket-chip.git
$ cd rocket-chip
$ git submodule update --init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Necessary Dependencies&lt;/h3&gt; 
&lt;p&gt;You may need to install some additional packages to use this repository. Rather than list all dependencies here, please see the appropriate section of the READMEs for each of the subprojects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/freechipsproject/rocket-tools/raw/master/README.md&quot;&gt;rocket-tools &quot;Ubuntu Packages Needed&quot;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ucb-bar/chisel3#installation&quot;&gt;chisel3 &quot;Installation&quot;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Building The Project&lt;/h3&gt; 
&lt;p&gt;Generating verilog&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make verilog
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generating verilog for a specific Config&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make verilog CONFIG=DefaultSmallConfig
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Keeping Your Repo Up-to-Date&lt;/h3&gt; 
&lt;p&gt;If you are trying to keep your repo up to date with this GitHub repo, you also need to keep the submodules and tools up to date.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ # Get the newest versions of the files in this repo
$ git pull origin master
$ # Make sure the submodules have the correct versions
$ git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If rocket-tools version changes, you should recompile and install rocket-tools according to the directions in the &lt;a href=&quot;https://github.com/freechipsproject/rocket-tools/raw/master/README.md&quot;&gt;rocket-tools/README&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ cd rocket-tools
$ ./build.sh
$ ./build-rv32ima.sh (if you are using RV32)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a name=&quot;what&quot;&gt;&lt;/a&gt; What&#39;s in the Rocket chip generator repository?&lt;/h2&gt; 
&lt;p&gt;The rocket-chip repository is a meta-repository that points to several sub-repositories using &lt;a href=&quot;http://git-scm.com/book/en/Git-Tools-Submodules&quot;&gt;Git submodules&lt;/a&gt;. Those repositories contain tools needed to generate and test SoC designs. This respository also contains code that is used to generate RTL. Hardware generation is done using &lt;a href=&quot;http://chisel.eecs.berkeley.edu&quot;&gt;Chisel&lt;/a&gt;, a hardware construction language embedded in Scala. The rocket-chip generator is a Scala program that invokes the Chisel compiler in order to emit RTL describing a complete SoC. The following sections describe the components of this repository.&lt;/p&gt; 
&lt;h3&gt;&lt;a name=&quot;what_submodules&quot;&gt;&lt;/a&gt;Git Submodules&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://git-scm.com/book/en/v2/Git-Tools-Submodules&quot;&gt;Git submodules&lt;/a&gt; allow you to keep a Git repository as a subdirectory of another Git repository. For projects being co-developed with the Rocket Chip Generator, we have often found it expedient to track them as submodules, allowing for rapid exploitation of new features while keeping commit histories separate. As submoduled projects adopt stable public APIs, we transition them to external dependencies. Here are the submodules that are currently being tracked in the rocket-chip repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;chisel3&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/chisel3&quot;&gt;https://github.com/ucb-bar/chisel3&lt;/a&gt;): The Rocket Chip Generator uses &lt;a href=&quot;http://chisel.eecs.berkeley.edu&quot;&gt;Chisel&lt;/a&gt; to generate RTL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;firrtl&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/firrtl&quot;&gt;https://github.com/ucb-bar/firrtl&lt;/a&gt;): &lt;a href=&quot;http://bar.eecs.berkeley.edu/projects/2015-firrtl.html&quot;&gt;Firrtl (Flexible Internal Representation for RTL)&lt;/a&gt; is the intermediate representation of RTL constructions used by Chisel3. The Chisel3 compiler generates a Firrtl representation, from which the final product (Verilog code, C code, etc) is generated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hardfloat&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/berkeley-hardfloat&quot;&gt;https://github.com/ucb-bar/berkeley-hardfloat&lt;/a&gt;): Hardfloat holds Chisel code that generates parameterized IEEE 754-2008 compliant floating-point units used for fused multiply-add operations, conversions between integer and floating-point numbers, and conversions between floating-point conversions with different precision.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;rocket-tools&lt;/strong&gt; (&lt;a href=&quot;https://github.com/freechipsproject/rocket-tools&quot;&gt;https://github.com/freechipsproject/rocket-tools&lt;/a&gt;): We tag a version of RISC-V software tools that work with the RTL committed in this repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;torture&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/riscv-torture&quot;&gt;https://github.com/ucb-bar/riscv-torture&lt;/a&gt;): This module is used to generate and execute constrained random instruction streams that can be used to stress-test both the core and uncore portions of the design.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a name=&quot;what_packages&quot;&gt;&lt;/a&gt;Scala Packages&lt;/h3&gt; 
&lt;p&gt;In addition to submodules that track independent Git repositories, the rocket-chip code base is itself factored into a number of Scala packages. These packages are all found within the src/main/scala directory. Some of these packages provide Scala utilities for generator configuration, while other contain the actual Chisel RTL generators themselves. Here is a brief description of what can be found in each package:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;amba&lt;/strong&gt; This RTL package uses diplomacy to generate bus implementations of AMBA protocols, including AXI4, AHB-lite, and APB.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;config&lt;/strong&gt; This utility package provides Scala interfaces for configuring a generator via a dynamically-scoped parameterization library.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;coreplex&lt;/strong&gt; This RTL package generates a complete coreplex by gluing together a variety of components from other packages, including: tiled Rocket cores, a system bus network, coherence agents, debug devices, interrupt handlers, externally-facing peripherals, clock-crossers and converters from TileLink to external bus protocols (e.g. AXI or AHB).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;devices&lt;/strong&gt; This RTL package contains implementations for peripheral devices, including the Debug module and various TL slaves.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;diplomacy&lt;/strong&gt; This utility package extends Chisel by allowing for two-phase hardware elaboration, in which certain parameters are dynamically negotiated between modules. For more information about diplomacy, see &lt;a href=&quot;https://carrv.github.io/2017/papers/cook-diplomacy-carrv2017.pdf&quot;&gt;this paper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;groundtest&lt;/strong&gt; This RTL package generates synthesizable hardware testers that emit randomized memory access streams in order to stress-tests the uncore memory hierarchy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;jtag&lt;/strong&gt; This RTL package provides definitions for generating JTAG bus interfaces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;regmapper&lt;/strong&gt; This utility package generates slave devices with a standardized interface for accessing their memory-mapped registers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;rocket&lt;/strong&gt; This RTL package generates the Rocket in-order pipelined core, as well as the L1 instruction and data caches. This library is intended to be used by a chip generator that instantiates the core within a memory system and connects it to the outside world.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tile&lt;/strong&gt; This RTL package contains components that can be combined with cores to construct tiles, such as FPUs and accelerators.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tilelink&lt;/strong&gt; This RTL package uses diplomacy to generate bus implementations of the TileLink protocol. It also contains a variety of adapters and protocol converters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;system&lt;/strong&gt; This top-level utility package invokes Chisel to elaborate a particular configuration of a coreplex, along with the appropriate testing collateral.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unittest&lt;/strong&gt; This utility package contains a framework for generateing synthesizable hardware testers of individual modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;util&lt;/strong&gt; This utility package provides a variety of common Scala and Chisel constructs that are re-used across multiple other packages,&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a name=&quot;what_else&quot;&gt;&lt;/a&gt;Other Resources&lt;/h3&gt; 
&lt;p&gt;Outside of Scala, we also provide a variety of resources to create a complete SoC implementation and test the generated designs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;bootrom&lt;/strong&gt; Sources for the first-stage bootloader included in the BootROM.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;csrc&lt;/strong&gt; C sources for use with Verilator simulation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;docs&lt;/strong&gt; Documentation, tutorials, etc for specific parts of the codebase.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;emulator&lt;/strong&gt; Directory in which Verilator simulations are compiled and run.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;regression&lt;/strong&gt; Defines continuous integration and nightly regression suites.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scripts&lt;/strong&gt; Utilities for parsing the output of simulations or manipulating the contents of source files.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vsim&lt;/strong&gt; Directory in which Synopsys VCS simulations are compiled and run.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vsrc&lt;/strong&gt; Verilog sources containing interfaces, harnesses and VPI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a name=&quot;ide&quot;&gt;&lt;/a&gt; IDEs Support&lt;/h2&gt; 
&lt;p&gt;The Rocket Chip Scala build uses &lt;a href=&quot;https://github.com/com-lihaoyi/mill&quot;&gt;mill&lt;/a&gt; as build tool.&lt;/p&gt; 
&lt;p&gt;IDEs like &lt;a href=&quot;https://www.jetbrains.com/idea/&quot;&gt;IntelliJ&lt;/a&gt; and &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;VSCode&lt;/a&gt; are popular in the Scala community and work with Rocket Chip.&lt;/p&gt; 
&lt;p&gt;The Rocket Chip currently uses &lt;code&gt;nix&lt;/code&gt; to configure the build and/or development environment, you need to install it first depending on your OS distro.&lt;/p&gt; 
&lt;p&gt;Then follow the steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Generate BSP config by running:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mill mill.bsp.BSP/install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Patch the &lt;code&gt;argv&lt;/code&gt; in &lt;code&gt;.bsp/mill-bsp.json&lt;/code&gt;, from&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;name&quot;:&quot;mill-bsp&quot;,&quot;argv&quot;:[&quot;/usr/bin/mill&quot;,&quot;--bsp&quot;,&quot;--disable-ticker&quot;,&quot;--color&quot;,&quot;false&quot;,&quot;--jobs&quot;,&quot;1&quot;],&quot;millVersion&quot;:&quot;0.10.9&quot;,&quot;bspVersion&quot;:&quot;2.0.0&quot;,&quot;languages&quot;:[&quot;scala&quot;,&quot;java&quot;]}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;name&quot;:&quot;mill-bsp&quot;,&quot;argv&quot;:[&quot;/usr/bin/nix&quot;,&quot;develop&quot;,&quot;-c&quot;,&quot;mill&quot;,&quot;--bsp&quot;,&quot;--disable-ticker&quot;,&quot;--color&quot;,&quot;false&quot;,&quot;--jobs&quot;,&quot;1&quot;],&quot;millVersion&quot;:&quot;0.10.9&quot;,&quot;bspVersion&quot;:&quot;2.0.0&quot;,&quot;languages&quot;:[&quot;scala&quot;,&quot;java&quot;]}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;For IntelliJ users&lt;/h3&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Install and configure &lt;a href=&quot;https://plugins.jetbrains.com/plugin/1347-scala&quot;&gt;Scala&lt;/a&gt; plugin.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BSP should be automatically run. If it doesn&#39;t, click &lt;code&gt;bsp&lt;/code&gt; on the right bar, then right-click on your project to reload.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;For VSCode users&lt;/h3&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Install and configure &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=scalameta.metals&quot;&gt;Metals&lt;/a&gt; extension.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Execute VSCode command &lt;code&gt;Metals: Import build&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;a name=&quot;contributors&quot;&gt;&lt;/a&gt; Contributors&lt;/h2&gt; 
&lt;p&gt;Contributing guidelines can be found in &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;A list of contributors can be found &lt;a href=&quot;https://github.com/chipsalliance/rocket-chip/graphs/contributors&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a name=&quot;attribution&quot;&gt;&lt;/a&gt; Attribution&lt;/h2&gt; 
&lt;p&gt;If used for research, please cite Rocket Chip by the technical report:&lt;/p&gt; 
&lt;p&gt;Krste Asanović, Rimas Avižienis, Jonathan Bachrach, Scott Beamer, David Biancolin, Christopher Celio, Henry Cook, Palmer Dabbelt, John Hauser, Adam Izraelevitz, Sagar Karandikar, Benjamin Keller, Donggyu Kim, John Koenig, Yunsup Lee, Eric Love, Martin Maas, Albert Magyar, Howard Mao, Miquel Moreto, Albert Ou, David Patterson, Brian Richards, Colin Schmidt, Stephen Twigg, Huy Vo, and Andrew Waterman, &lt;em&gt;&lt;a href=&quot;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html&quot;&gt;The Rocket Chip Generator&lt;/a&gt;&lt;/em&gt;, Technical Report UCB/EECS-2016-17, EECS Department, University of California, Berkeley, April 2016&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>delta-io/delta</title>
      <link>https://github.com/delta-io/delta</link>
      <description>&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://docs.delta.io/latest/_static/delta-lake-white.png&quot; width=&quot;200&quot; alt=&quot;Delta Lake Logo&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/delta-io/delta/actions/workflows/test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&quot; alt=&quot;Test&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/delta-spark/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&quot; alt=&quot;PyPI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/delta-spark&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/delta-spark&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&quot;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&quot;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See the &lt;a href=&quot;https://docs.delta.io&quot;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/quick-start.html&quot;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; 
 &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&quot;https://github.com/delta-io&quot;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&quot;https://github.com/delta-io/delta&quot;&gt;delta&lt;/a&gt;, &lt;a href=&quot;https://github.com/delta-io/delta-rs&quot;&gt;delta-rs&lt;/a&gt;, &lt;a href=&quot;https://github.com/delta-io/delta-sharing&quot;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&quot;https://github.com/delta-io/kafka-delta-ingest&quot;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&quot;https://github.com/delta-io/website&quot;&gt;website&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&quot;https://delta.io/integrations/&quot;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/&quot;&gt;Apache Spark™&lt;/a&gt;: This connector allows Apache Spark™ to read from and write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/delta-io/delta/tree/master/connectors/flink&quot;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://prestodb.io/docs/current/connector/deltalake.html&quot;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://trino.io/docs/current/connector/delta-lake.html&quot;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/delta-standalone.html&quot;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/hive-integration.html&quot;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.rs/deltalake/latest/deltalake/&quot;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&quot;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&quot;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&quot;&gt;Compatibility&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&quot;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&quot;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#building&quot;&gt;Building&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&quot;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&quot;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&quot;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&quot;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#community&quot;&gt;Community&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Latest Binaries&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/&quot;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; 
&lt;h2&gt;API Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/delta-apidoc.html&quot;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/api/java/index.html&quot;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/api/python/index.html&quot;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Compatibility&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.delta.io/latest/delta-standalone.html&quot;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table’s metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; 
&lt;h3&gt;API Compatibility&lt;/h3&gt; 
&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&quot;https://docs.delta.io/latest/delta-apidoc.html&quot;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; 
 &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/releases.html&quot;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; 
&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; 
&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/spark/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&quot;&gt;action&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&quot;http://delta.io/roadmap&quot;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For the detailed timeline, see the &lt;a href=&quot;https://github.com/delta-io/delta/milestones&quot;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Transaction Protocol&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&quot;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; 
&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; 
&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/delta-storage.html&quot;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Concurrency Control&lt;/h2&gt; 
&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&quot;https://docs.delta.io/latest/delta-concurrency.html&quot;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Reporting issues&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href=&quot;https://github.com/delta-io/delta/issues&quot;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#community&quot;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;We also adhere to the &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&quot;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;Delta Lake is compiled using &lt;a href=&quot;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&quot;&gt;SBT&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To compile, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt compile
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To generate artifacts, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt package
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute tests, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute a single test suite, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt spark/&#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSQLSuite&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt spark/&#39;testOnly *.OptimizeCompactionSQLSuite -- -z &quot;optimize command: on partitioned table - all partitions&quot;&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;a href=&quot;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&quot;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; 
&lt;h2&gt;Running python tests locally&lt;/h2&gt; 
&lt;h3&gt;Setup Environment&lt;/h3&gt; 
&lt;h4&gt;Install Conda (Skip if you already installed it)&lt;/h4&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://www.anaconda.com/download/&quot;&gt;Conda Download&lt;/a&gt; to install Anaconda.&lt;/p&gt; 
&lt;h4&gt;Create an environment from environment file&lt;/h4&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-from-file&quot;&gt;Create Environment From Environment file&lt;/a&gt; to create a Conda environment from &lt;code&gt;&amp;lt;repo-root&amp;gt;/python/environment.yml&lt;/code&gt; and activate the newly created &lt;code&gt;delta_python_tests&lt;/code&gt; environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Note the `--file` argument should be a fully qualified path. Using `~` in file
# path doesn&#39;t work. Example valid path: `/Users/macuser/delta/python/environment.yml`

conda env create --name delta_python_tests --file=&amp;lt;absolute_path_to_delta_repo&amp;gt;/python/environment.yml`
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;JDK Setup&lt;/h4&gt; 
&lt;p&gt;Build needs JDK 1.8. Make sure to setup &lt;code&gt;JAVA_HOME&lt;/code&gt; that points to JDK 1.8.&lt;/p&gt; 
&lt;h4&gt;Running tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;conda activate delta_python_tests
python3 &amp;lt;delta-root&amp;gt;/python/run-tests.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; 
&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In your terminal, run &lt;code&gt;build/sbt clean package&lt;/code&gt;. Make sure you use Java &lt;code&gt;1.8&lt;/code&gt;. The build will generate files that are necessary for Intellij to index the repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Setup Verification&lt;/h3&gt; 
&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;If you see errors of the form&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser
import io.delta.sql.parser.DeltaSqlBaseParser._
...
Error:(91, 22) not found: type DeltaSqlBaseParser
    val parser = new DeltaSqlBaseParser(tokenStream)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ensure you are using Java &lt;code&gt;1.8&lt;/code&gt;. You can set this using&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;export JAVA_HOME=`/usr/libexec/java_home -v 1.8`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt clean compile&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-spark&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache License 2.0, see &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/LICENSE.txt&quot;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Public Slack Channel 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://go.delta.io/slack&quot;&gt;Register here&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://delta-users.slack.com/&quot;&gt;Login here&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/company/deltalake&quot;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/deltalake&quot;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Public &lt;a href=&quot;https://groups.google.com/forum/#!forum/delta-users&quot;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>ucb-bar/chiseltest</title>
      <link>https://github.com/ucb-bar/chiseltest</link>
      <description>&lt;p&gt;The batteries-included testing and formal verification library for Chisel-based RTL designs.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chiseltest&lt;/h1&gt; 
&lt;h2&gt;Archived Project Status&lt;/h2&gt; 
&lt;p&gt;After six long and exciting years, we unfortunately had to stop development of chiseltest since we no longer have a maintainer. Please feel free to take advantage of the existing code, either by creating a community fork or by using it to &lt;a href=&quot;https://github.com/chipsalliance/chisel/pull/4209&quot;&gt;improve other Chisel testing solutions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Thanks!&lt;/p&gt; 
&lt;h2&gt;What is &lt;code&gt;chiseltest&lt;/code&gt;?&lt;/h2&gt; 
&lt;p&gt;Chiseltest is the &lt;em&gt;batteries-included&lt;/em&gt; testing and formal verification library for &lt;a href=&quot;https://github.com/chipsalliance/chisel3&quot;&gt;Chisel&lt;/a&gt;-based RTL designs. Chiseltest emphasizes tests that are lightweight (minimizes boilerplate code), easy to read and write (understandability), and compose (for better test code reuse).&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To use chisel-testers as a managed dependency, add this in your build.sbt:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;libraryDependencies += &quot;edu.berkeley.cs&quot; %% &quot;chiseltest&quot; % &quot;5.0-SNAPSHOT&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Starting with &lt;code&gt;chisel5&lt;/code&gt;, please make sure to pick a matching major version, to avoid linking errors. For older versions, if you are also directly depending on the &lt;code&gt;chisel3&lt;/code&gt; library, please &lt;a href=&quot;https://www.chisel-lang.org/chisel3/docs/appendix/versioning.html&quot;&gt;make sure that your chisel3 and chiseltest versions match&lt;/a&gt; to avoid linking errors.&lt;/p&gt; 
&lt;h2&gt;Writing a Test&lt;/h2&gt; 
&lt;p&gt;ChiselTest integrates with the &lt;a href=&quot;http://scalatest.org&quot;&gt;ScalaTest&lt;/a&gt; framework, which provides good IDE and continuous integration support for launching unit tests.&lt;/p&gt; 
&lt;p&gt;Assuming a typical Chisel project with &lt;code&gt;MyModule&lt;/code&gt; defined in &lt;code&gt;src/main/scala/MyModule.scala&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class MyModule extend Module {
    val io = IO(new Bundle {
        val in = Input(UInt(16.W))
        val out = Output(UInt(16.W))
    })

    io.out := RegNext(io.in)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a new file in &lt;code&gt;src/test/scala/&lt;/code&gt;, for example, &lt;code&gt;BasicTest.scala&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In this file:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Add the necessary imports: &lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import chisel3._
import chiseltest._
import org.scalatest.flatspec.AnyFlatSpec
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Create a test class: &lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;class BasicTest extends AnyFlatSpec with ChiselScalatestTester {
  behavior of &quot;MyModule&quot;
  // test class body here
}
&lt;/code&gt;&lt;/pre&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;AnyFlatSpec&lt;/code&gt; is the &lt;a href=&quot;http://www.scalatest.org/user_guide/selecting_a_style&quot;&gt;default and recommended ScalaTest style for unit testing&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ChiselScalatestTester&lt;/code&gt; provides testdriver functionality and integration (like signal value assertions) within the context of a ScalaTest environment.&lt;/li&gt; 
   &lt;li&gt;For those interested in additional ScalaTest assertion expressibility, &lt;code&gt;Matchers&lt;/code&gt; provides additional &lt;a href=&quot;http://www.scalatest.org/user_guide/using_matchers&quot;&gt;assertion syntax options&lt;/a&gt;. &lt;code&gt;Matchers&lt;/code&gt; is optional as it&#39;s mainly for Scala-land assertions and does not inter-operate with circuit operations.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;In the test class, define a test case: &lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;it should &quot;do something&quot; in {
  // test case body here
}
&lt;/code&gt;&lt;/pre&gt; There can be multiple test cases per test class, and we recommend one test class per Module being tested, and one test case per individual test.&lt;/li&gt; 
 &lt;li&gt;In the test case, define the module being tested: &lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;test(new MyModule) { c =&amp;gt;
  // test body here
}
&lt;/code&gt;&lt;/pre&gt; &lt;code&gt;test&lt;/code&gt; automatically runs the default simulator (which is &lt;a href=&quot;https://github.com/freechipsproject/treadle&quot;&gt;treadle&lt;/a&gt;), and runs the test stimulus in the block. The argument to the test stimulus block (&lt;code&gt;c&lt;/code&gt; in this case) is a handle to the module under test.&lt;/li&gt; 
 &lt;li&gt;In the test body, use &lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;step&lt;/code&gt;, and &lt;code&gt;expect&lt;/code&gt; operations to write the test: &lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;c.io.in.poke(0.U)
c.clock.step()
c.io.out.expect(0.U)
c.io.in.poke(42.U)
c.clock.step()
c.io.out.expect(42.U)
println(&quot;Last output value :&quot; + c.io.out.peek().litValue)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;With your test case complete, you can run all the test cases in your project by invoking ScalaTest. If you&#39;re using &lt;a href=&quot;http://scala-sbt.org&quot;&gt;sbt&lt;/a&gt;, you can either run &lt;code&gt;sbt test&lt;/code&gt; from the command line, or &lt;code&gt;test&lt;/code&gt; from the sbt console. &lt;code&gt;testOnly&lt;/code&gt; can also be used to run specific tests.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Usage References&lt;/h3&gt; 
&lt;p&gt;See the test cases for examples:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/BasicTest.scala&quot;&gt;BasicTest&lt;/a&gt; shows basic &lt;code&gt;peek&lt;/code&gt;, &lt;code&gt;poke&lt;/code&gt;, and &lt;code&gt;step&lt;/code&gt; functionality&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/QueueTest.scala&quot;&gt;QueueTest&lt;/a&gt; shows example uses of the DecoupledDriver library, providing functions like &lt;code&gt;enqueueNow&lt;/code&gt;, &lt;code&gt;expectDequeueNow&lt;/code&gt;, their sequence variants, &lt;code&gt;expectPeek&lt;/code&gt;, and &lt;code&gt;expectInvalid&lt;/code&gt;. Also, check out the &lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/main/scala/chiseltest/DecoupledDriver.scala&quot;&gt;DecoupledDriver&lt;/a&gt; implementation, and note that it is not a special case, but code that any user can write.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/BundleLiteralsSpec.scala&quot;&gt;BundleLiteralsSpec&lt;/a&gt; shows examples of using bundle literals to poke and expect bundle wires. 
  &lt;ul&gt; 
   &lt;li&gt;Note: Bundle literals are still an experimental chisel3 feature and need to be explicitly imported: &lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import chisel3.experimental.BundleLiterals._
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/AluTest.scala&quot;&gt;AlutTest&lt;/a&gt; shows an example of re-using the same test for different data&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/src/test/scala/chiseltest/tests/ShiftRegisterTest.scala&quot;&gt;ShiftRegisterTest&lt;/a&gt; shows an example of using fork/join to define a test helper function, where multiple invocations of it are pipelined using &lt;code&gt;fork&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;New Constructs&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;fork&lt;/code&gt; to spawn threads, and &lt;code&gt;join&lt;/code&gt; to block (wait) on a thread. Pokes and peeks/expects to wires from threads are checked during runtime to ensure no collisions or unexpected behavior. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;fork&lt;/code&gt;ed threads provide a concurrency abstraction for writing testbenches only, without real parallelism. The test infrastructure schedules threads one at a time, with threads running once per simulation cycle.&lt;/li&gt; 
   &lt;li&gt;Thread order is deterministic, and attempts to follow lexical order (as it would appear from the code text): &lt;code&gt;fork&lt;/code&gt;ed (child) threads run immediately, then return to the spawning (parent) thread. On future cycles, child threads run before their parent, in the order they were spawned.&lt;/li&gt; 
   &lt;li&gt;Only cross-thread operations that round-trip through the simulator (eg, peek-after-poke) are checked. You can do cross-thread operations in Scala (eg, using shared variables) that aren&#39;t checked, but it is up to you to make sure they are correct and intuitive. This is not recommended. In the future, we may provide checked mechanisms for communicating between test threads.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Regions can be associated with a thread, with &lt;code&gt;fork.withRegion(...)&lt;/code&gt;, which act as a synchronization barrier within simulator time steps. This can be used to create monitors that run after other main testdriver threads have been run, and can read wires those threads have poked.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Simulator Backends&lt;/h2&gt; 
&lt;p&gt;One of our goals is to keep your tests independent of the underlying simulator as much as possible. Thus, in most cases you should be able to choose from one of our four supported backends and get the exact same test results albeit with differences in execution speed and wave dump quality.&lt;/p&gt; 
&lt;p&gt;We provide full bindings to two popular open-source simulator:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chipsalliance/treadle&quot;&gt;treadle&lt;/a&gt;: default, fast startup times, slow execution for larger circuits, supports only VCD&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.veripool.org/wiki/verilator&quot;&gt;verilator&lt;/a&gt;: enable with &lt;code&gt;VerilatorBackendAnnotation&lt;/code&gt;, slow startup, fast execution, supports VCD and FST&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We also provide bindings with some feature limitations to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://iverilog.icarus.com/&quot;&gt;iverilog&lt;/a&gt;: open-source, enable with &lt;code&gt;IcarusBackendAnnotation&lt;/code&gt;, supports VCD, FST and LXT&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.synopsys.com/verification/simulation/vcs.html&quot;&gt;vcs&lt;/a&gt;: commercial, enable with &lt;code&gt;VcsBackendAnnotation&lt;/code&gt;, supports VCD and FSDB&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Verilator Versions&lt;/h3&gt; 
&lt;p&gt;We currently support the following versions of the &lt;a href=&quot;https://www.veripool.org/wiki/verilator&quot;&gt;verilator&lt;/a&gt; simulator:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;v4.028&lt;/code&gt;: &lt;a href=&quot;https://packages.ubuntu.com/focal/verilator&quot;&gt;Ubuntu 20.04&lt;/a&gt;, &lt;a href=&quot;https://src.fedoraproject.org/rpms/verilator&quot;&gt;Fedora 32&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4.032&lt;/code&gt;: &lt;a href=&quot;https://src.fedoraproject.org/rpms/verilator&quot;&gt;Fedora 33&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4.034&lt;/code&gt;: &lt;a href=&quot;https://chipyard.readthedocs.io/en/latest/Chipyard-Basics/Initial-Repo-Setup.html#requirements&quot;&gt;Chipyard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4.038&lt;/code&gt;: &lt;a href=&quot;https://packages.ubuntu.com/groovy/verilator&quot;&gt;Ubuntu 20.10&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4.108&lt;/code&gt;: &lt;a href=&quot;https://src.fedoraproject.org/rpms/verilator&quot;&gt;Fedora 34&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;v4.202&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; 
&lt;h3&gt;How do I rerun with --full-stacktrace?&lt;/h3&gt; 
&lt;p&gt;Whereas Chisel accepts command-line arguments, chiseltest exposes the underlying annotation interface. You can pass annotations to a test by using &lt;code&gt;.withAnnotations(...)&lt;/code&gt;, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// Top of file
import chisel3.stage.PrintFullStackTraceAnnotation

// ...

    // Inside your test spec
    test(new MyModule).withChiselAnnotations(Seq(PrintFullStackTraceAnnotation)) { c =&amp;gt;
      // test body here
    }
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will remove the chisel3 stacktrace suppression (ie. &lt;code&gt;at ... ()&lt;/code&gt;). However, if you are using ScalaTest, you may notice a shortened stack trace with &lt;code&gt;...&lt;/code&gt; at the end. You can tell ScalaTest to stop suppressing the stack trace by passing &lt;code&gt;-oF&lt;/code&gt; to it. For example (using SBT):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ sbt
&amp;gt; testOnly &amp;lt;spec name&amp;gt; -- -oF
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Any arguments after &lt;code&gt;--&lt;/code&gt; pass to ScalaTest directly instead of being interpreted by SBT.&lt;/p&gt; 
&lt;h2&gt;Stability&lt;/h2&gt; 
&lt;p&gt;Most APIs that can be accessed through &lt;code&gt;import chiseltest._&lt;/code&gt; are going to remain stable. We are also trying to keep the API provided through &lt;code&gt;import chiseltest.formal._&lt;/code&gt; relatively stable. All other packages are considered internal and thus might change at any time.&lt;/p&gt; 
&lt;h2&gt;Migrating from chisel-testers / iotesters&lt;/h2&gt; 
&lt;h3&gt;Port to new API&lt;/h3&gt; 
&lt;p&gt;The core abstractions (&lt;code&gt;poke&lt;/code&gt;, &lt;code&gt;expect&lt;/code&gt;, &lt;code&gt;step&lt;/code&gt;) are similar to &lt;a href=&quot;https://github.com/freechipsproject/chisel-testers&quot;&gt;chisel-testers&lt;/a&gt;, but the syntax is inverted: instead of doing &lt;code&gt;tester.poke(wire, value)&lt;/code&gt; with a Scala number value, in ChiselTest you would write &lt;code&gt;wire.poke(value)&lt;/code&gt; with a Chisel literal value. Furthermore, as no reference to the tester context is needed, test helper functions can be defined outside a test class and written as libraries.&lt;/p&gt; 
&lt;h3&gt;PeekPokeTester compatibility&lt;/h3&gt; 
&lt;p&gt;&lt;code&gt;chiseltest&lt;/code&gt; now provides a compatibility layer that makes it possible to re-use old &lt;code&gt;PeekPokeTester&lt;/code&gt; based tests with little to no changes to the code. We ported the majority of &lt;a href=&quot;https://github.com/freechipsproject/chisel-testers/tree/master/src/test/scala&quot;&gt;tests from the chisel-testers repository&lt;/a&gt; to our &lt;a href=&quot;https://github.com/ucb-bar/chiseltest/tree/main/src/test/scala/chiseltest/iotesters&quot;&gt;new compatibility layer&lt;/a&gt;. While the test itself can mostly remain unchanged, the old &lt;code&gt;Driver&lt;/code&gt; is removed and instead tests are launched with the new &lt;code&gt;test&lt;/code&gt; syntax.&lt;/p&gt; 
&lt;h3&gt;Hardware testers&lt;/h3&gt; 
&lt;p&gt;Hardware testers are synthesizeable tests, most often extending the &lt;code&gt;BasicTester&lt;/code&gt; class provided by &lt;code&gt;chisel3&lt;/code&gt;. You can now directly &lt;a href=&quot;https://github.com/ucb-bar/chiseltest/raw/main/src/test/scala/chiseltest/tests/HardwareTestsTest.scala&quot;&gt;use these tests with &lt;code&gt;chiseltest&lt;/code&gt; through the &lt;code&gt;runUntilStop&lt;/code&gt; function&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Contributions submitted on behalf of the Regents of the University of California, are licensed under &lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/LICENSE&quot;&gt;the 3-Clause BSD License&lt;/a&gt;. Contributions submitted by developers on behalf of themselves or any other organization or employer, are licensed under &lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chiseltest/main/LICENSE.contributors&quot;&gt;the Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/pekko</title>
      <link>https://github.com/apache/pekko</link>
      <description>&lt;p&gt;Build highly concurrent, distributed, and resilient message-driven applications using Java/Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Pekko&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/apache/pekko/actions/workflows/nightly-builds.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/pekko/actions/workflows/nightly-builds.yml/badge.svg?sanitize=true&quot; alt=&quot;Nightly Builds&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/apache/pekko/actions/workflows/nightly-builds-aeron.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/pekko/actions/workflows/nightly-builds-aeron.yml/badge.svg?branch=main&quot; alt=&quot;Nightly Aeron Tests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.bestpractices.dev/projects/9032&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/9032/badge&quot; alt=&quot;OpenSSF Best Practices&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Apache Pekko is an open-source framework for building applications that are concurrent, distributed, resilient and elastic. Pekko uses the Actor Model to provide more intuitive high-level abstractions for concurrency. Using these abstractions, Pekko also provides libraries for persistence, streams, HTTP, and more.&lt;/p&gt; 
&lt;p&gt;Pekko is a fork of &lt;a href=&quot;https://github.com/akka/akka&quot;&gt;Akka&lt;/a&gt; 2.6.x, prior to the Akka project&#39;s adoption of the Business Source License.&lt;/p&gt; 
&lt;h2&gt;Reference Documentation&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://pekko.apache.org&quot;&gt;https://pekko.apache.org&lt;/a&gt; for the documentation including the API docs. The docs for all the Apache Pekko modules can be found there.&lt;/p&gt; 
&lt;h2&gt;Building from Source&lt;/h2&gt; 
&lt;p&gt;The CI build is Linux based (Ubuntu) and most Pekko developers use Macs or Linux machines. There have been reports of issues when building with Windows (&lt;a href=&quot;https://github.com/apache/pekko/issues/829&quot;&gt;#829&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Using IntelliJ IDEA&lt;/h2&gt; 
&lt;p&gt;To use IntelliJ IDEA, you need to configure your project to use JDK 8. For a visual guide, refer to this &lt;a href=&quot;https://github.com/apache/pekko/discussions/1847#discussioncomment-13166066&quot;&gt;Q&amp;amp;A&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;File &amp;gt; Project Structure &amp;gt; Project Settings &amp;gt; Project &amp;gt; Set your JDK to version 17&lt;/li&gt; 
 &lt;li&gt;Settings &amp;gt; ... &amp;gt; Scala Compiler Server &amp;gt; Set your JDK to version 17&lt;/li&gt; 
 &lt;li&gt;Settings &amp;gt; ... &amp;gt; sbt &amp;gt; Set your JRE to version 17&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure you have installed a Java Development Kit (JDK) version 17 or later.&lt;/li&gt; 
 &lt;li&gt;Make sure you have &lt;a href=&quot;https://www.scala-sbt.org/&quot;&gt;sbt&lt;/a&gt; installed and using this JDK.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://graphviz.gitlab.io/download/&quot;&gt;Graphviz&lt;/a&gt; is needed for the scaladoc generation build task, which is part of the release.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Running the Build&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open a command window and change directory to your preferred base directory&lt;/li&gt; 
 &lt;li&gt;Use git to clone the &lt;a href=&quot;https://github.com/apache/pekko&quot;&gt;repo&lt;/a&gt; or download a source release from &lt;a href=&quot;https://pekko.apache.org&quot;&gt;https://pekko.apache.org&lt;/a&gt; (and unzip or untar it, as appropriate)&lt;/li&gt; 
 &lt;li&gt;Change directory to the directory where you installed the source (you should have a file called &lt;code&gt;build.sbt&lt;/code&gt; in this directory)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt compile&lt;/code&gt; compiles the main source for project default version of Scala (2.13) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;sbt +compile&lt;/code&gt; will compile for all supported versions of Scala&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt test&lt;/code&gt; will compile the code and run the unit tests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt testQuick&lt;/code&gt; similar to test but when repeated in shell mode will only run failing tests&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt testQuickUntilPassed&lt;/code&gt; similar to testQuick but will loop until tests pass.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt package&lt;/code&gt; will build the jars 
  &lt;ul&gt; 
   &lt;li&gt;the jars will be built into target dirs of the various modules&lt;/li&gt; 
   &lt;li&gt;for the &#39;actor&#39; module, the jar will be built to &lt;code&gt;actor/target/scala-2.13/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt publishLocal&lt;/code&gt; will push the jars to your local Apache Ivy repository&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt publishM2&lt;/code&gt; will push the jars to your local Apache Maven repository&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt docs/paradox&lt;/code&gt; will build the docs (the ones describing the module features) 
  &lt;ul&gt; 
   &lt;li&gt;Requires Java 11 minimum&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;sbt docs/paradoxBrowse&lt;/code&gt; does the same but will open the docs in your browser when complete&lt;/li&gt; 
   &lt;li&gt;the &lt;code&gt;index.html&lt;/code&gt; file will appear in &lt;code&gt;target/paradox/site/main/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt unidoc&lt;/code&gt; will build the Javadocs for all the modules and load them to one place (may require Graphviz, see Prerequisites above) 
  &lt;ul&gt; 
   &lt;li&gt;the &lt;code&gt;index.html&lt;/code&gt; file will appear in &lt;code&gt;target/scala-2.13/unidoc/&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;sbt sourceDistGenerate&lt;/code&gt; will generate source release to &lt;code&gt;target/dist/&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;The version number that appears in filenames and docs is derived, by default. The derived version contains the most git commit id or the date/time (if the directory is not under git control). 
  &lt;ul&gt; 
   &lt;li&gt;You can set the version number explicitly when running sbt commands 
    &lt;ul&gt; 
     &lt;li&gt;eg &lt;code&gt;sbt &quot;set ThisBuild / version := \&quot;1.0.0\&quot;; sourceDistGenerate&quot;&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Or you can add a file called &lt;code&gt;version.sbt&lt;/code&gt; to the same directory that has the &lt;code&gt;build.sbt&lt;/code&gt; containing something like 
    &lt;ul&gt; 
     &lt;li&gt;&lt;code&gt;ThisBuild / version := &quot;1.0.0&quot;&lt;/code&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;There are several ways to interact with the Pekko community:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/apache/pekko/discussions&quot;&gt;GitHub discussions&lt;/a&gt;: for questions and general discussion.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/list.html?dev@pekko.apache.org&quot;&gt;Pekko dev mailing list&lt;/a&gt;: for Pekko development discussions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lists.apache.org/list.html?users@pekko.apache.org&quot;&gt;Pekko users mailing list&lt;/a&gt;: for Pekko user discussions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/apache/pekko/issues&quot;&gt;GitHub issues&lt;/a&gt;: for bug reports and feature requests. Please search the existing issues before creating new ones. If you are unsure whether you have found a bug, consider asking in GitHub discussions or the mailing list first.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are very welcome. If you have an idea on how to improve Pekko, don&#39;t hesitate to create an issue or submit a pull request.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/apache/pekko/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for details on the development workflow and how to create your pull request.&lt;/p&gt; 
&lt;h2&gt;Code of Conduct&lt;/h2&gt; 
&lt;p&gt;Apache Pekko is governed by the &lt;a href=&quot;https://www.apache.org/foundation/policies/conduct.html&quot;&gt;Apache code of conduct&lt;/a&gt;. By participating in this project you agree to abide by its terms.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache Pekko is available under the Apache License, version 2.0. See &lt;a href=&quot;https://github.com/apache/pekko/raw/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>akka/akka</title>
      <link>https://github.com/akka/akka</link>
      <description>&lt;p&gt;A platform to build and run apps that are elastic, agile, and resilient. SDK, libraries, and hosted environments.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Akka&lt;/h1&gt; 
&lt;p&gt;The Akka family of projects is managed by teams at &lt;a href=&quot;https://akka.io/&quot;&gt;Akka&lt;/a&gt; with help from the community.&lt;/p&gt; 
&lt;p&gt;We believe that writing correct concurrent &amp;amp; distributed, resilient and elastic applications is too hard. Most of the time it&#39;s because we are using the wrong tools and the wrong level of abstraction.&lt;/p&gt; 
&lt;p&gt;Akka is here to change that.&lt;/p&gt; 
&lt;p&gt;Using the Actor Model we raise the abstraction level and provide a better platform to build correct concurrent and scalable applications. This model is a perfect match for the principles laid out in the &lt;a href=&quot;https://www.reactivemanifesto.org/&quot;&gt;Reactive Manifesto&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For resilience, we adopt the &quot;Let it crash&quot; model which the telecom industry has used with great success to build applications that self-heal and systems that never stop.&lt;/p&gt; 
&lt;p&gt;Actors also provide the abstraction for transparent distribution and the basis for truly scalable and fault-tolerant applications.&lt;/p&gt; 
&lt;p&gt;Learn more at &lt;a href=&quot;https://akka.io/&quot;&gt;akka.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reference Documentation&lt;/h2&gt; 
&lt;p&gt;The reference documentation is available at &lt;a href=&quot;https://doc.akka.io&quot;&gt;doc.akka.io&lt;/a&gt;, for &lt;a href=&quot;https://doc.akka.io/libraries/akka-core/current/?language=scala&quot;&gt;Scala&lt;/a&gt; and &lt;a href=&quot;https://doc.akka.io/libraries/akka-core/current/?language=java&quot;&gt;Java&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Current versions of all Akka libraries&lt;/h2&gt; 
&lt;p&gt;The current versions of all Akka libraries are listed on the &lt;a href=&quot;https://doc.akka.io/libraries/akka-dependencies/current/&quot;&gt;Akka Dependencies&lt;/a&gt; page. Releases of the Akka core libraries in this repository are listed on the &lt;a href=&quot;https://github.com/akka/akka/releases&quot;&gt;GitHub releases&lt;/a&gt; page.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;You can join these groups and chats to discuss and ask Akka related questions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Forums: &lt;a href=&quot;https://discuss.akka.io&quot;&gt;discuss.akka.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Issue tracker: &lt;a href=&quot;https://github.com/akka/akka/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/github%3A-issues-blue.svg?style=flat-square&quot; alt=&quot;github: akka/akka&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to that, you may enjoy following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Questions tagged &lt;a href=&quot;https://stackoverflow.com/questions/tagged/akka&quot;&gt;#akka on StackOverflow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Contributions are &lt;em&gt;very&lt;/em&gt; welcome!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you see an issue that you&#39;d like to see fixed, or want to shape out some ideas, the best way to make it happen is to help out by submitting a pull request implementing it. We welcome contributions from all, even you are not yet familiar with this project, We are happy to get you started, and will guide you through the process once you&#39;ve submitted your PR.&lt;/p&gt; 
&lt;p&gt;Refer to the &lt;a href=&quot;https://github.com/akka/akka/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; file for more details about the workflow, and general hints on how to prepare your pull request. You can also ask for clarifications or guidance in GitHub issues directly, or in the akka/dev chat if a more real time communication would be of benefit.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Akka is licensed under the Business Source License 1.1, please see the &lt;a href=&quot;https://akka.io/bsl-license-faq&quot;&gt;Akka License FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Tests and documentation are under a separate license, see the LICENSE file in each documentation and test root directory for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>apache/kyuubi</title>
      <link>https://github.com/apache/kyuubi</link>
      <description>&lt;p&gt;Apache Kyuubi is a distributed and multi-tenant gateway to provide serverless SQL on data warehouses and lakehouses.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://svn.apache.org/repos/asf/comdev/project-logos/originals/kyuubi-1.svg?sanitize=true&quot; alt=&quot;Kyuubi logo&quot; height=&quot;120px&quot; /&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/raw/master/LICENSE&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/license/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://kyuubi.apache.org/releases.html&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/v/release/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/apache/kyuubi&quot;&gt; &lt;img src=&quot;https://img.shields.io/docker/pulls/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/graphs/contributors&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/contributors/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;a class=&quot;github-button&quot; href=&quot;https://github.com/apache/kyuubi&quot; data-icon=&quot;octicon-star&quot; aria-label=&quot;Star apache/kyuubi on GitHub&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/stars/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://kyuubi.apache.org/&quot;&gt;Project&lt;/a&gt; - &lt;a href=&quot;https://kyuubi.readthedocs.io/&quot;&gt;Documentation&lt;/a&gt; - &lt;a href=&quot;https://kyuubi.apache.org/powered_by.html&quot;&gt;Who&#39;s using&lt;/a&gt; &lt;/p&gt; 
&lt;h1&gt;Apache Kyuubi&lt;/h1&gt; 
&lt;p&gt;Apache Kyuubi™ is a distributed and multi-tenant gateway to provide serverless SQL on data warehouses and lakehouses.&lt;/p&gt; 
&lt;h2&gt;What is Kyuubi?&lt;/h2&gt; 
&lt;p&gt;Kyuubi provides a pure SQL gateway through Thrift JDBC/ODBC interface for end-users to manipulate large-scale data with pre-programmed and extensible Spark SQL engines. This &quot;out-of-the-box&quot; model minimizes the barriers and costs for end-users to use Spark at the client side. At the server-side, Kyuubi server and engines&#39; multi-tenant architecture provides the administrators a way to achieve computing resource isolation, data security, high availability, high client concurrency, etc.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/kyuubi/master/docs/imgs/kyuubi_positioning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; A HiveServer2-like API&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Multi-tenant Spark Support&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Running Spark in a serverless way&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Target Users&lt;/h3&gt; 
&lt;p&gt;Kyuubi&#39;s goal is to make it easy and efficient for &lt;code&gt;anyone&lt;/code&gt; to use Spark(maybe other engines soon) and facilitate users to handle big data like ordinary data. Here, &lt;code&gt;anyone&lt;/code&gt; means that users do not need to have a Spark technical background but a human language, SQL only. Sometimes, SQL skills are unnecessary when integrating Kyuubi with Apache Superset, which supports rich visualizations and dashboards.&lt;/p&gt; 
&lt;p&gt;In typical big data production environments with Kyuubi, there should be system administrators and end-users.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;System administrators: A small group consists of Spark experts responsible for Kyuubi deployment, configuration, and tuning.&lt;/li&gt; 
 &lt;li&gt;End-users: Focus on business data of their own, not where it stores, how it computes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Additionally, the Kyuubi community will continuously optimize the whole system with various features, such as History-Based Optimizer, Auto-tuning, Materialized View, SQL Dialects, Functions, etc.&lt;/p&gt; 
&lt;h3&gt;Usage scenarios&lt;/h3&gt; 
&lt;h4&gt;Port workloads from HiveServer2 to Spark SQL&lt;/h4&gt; 
&lt;p&gt;In typical big data production environments, especially secured ones, all bundled services manage access control lists to restricting access to authorized users. For example, Hadoop YARN divides compute resources into queues. With Queue ACLs, it can identify and control which users/groups can take actions on particular queues. Similarly, HDFS ACLs control access of HDFS files by providing a way to set different permissions for specific users/groups.&lt;/p&gt; 
&lt;p&gt;Apache Spark is a unified analytics engine for large-scale data processing. It provides a Distributed SQL Engine, a.k.a, the Spark Thrift Server(STS), designed to be seamlessly compatible with HiveServer2 and get even better performance.&lt;/p&gt; 
&lt;p&gt;HiveServer2 can identify and authenticate a caller, and then if the caller also has permissions for the YARN queue and HDFS files, it succeeds. Otherwise, it fails. However, on the one hand, STS is a single Spark application. The user and queue to which STS belongs are uniquely determined at startup. Consequently, STS cannot leverage cluster managers such as YARN and Kubernetes for resource isolation and sharing or control the access for callers by the single user inside the whole system. On the other hand, the Thrift Server is coupled in the Spark driver&#39;s JVM process. This coupled architecture puts a high risk on server stability and makes it unable to handle high client concurrency or apply high availability such as load balancing as it is stateful.&lt;/p&gt; 
&lt;p&gt;Kyuubi extends the use of STS in a multi-tenant model based on a unified interface and relies on the concept of multi-tenancy to interact with cluster managers to finally gain the ability of resources sharing/isolation and data security. The loosely coupled architecture of the Kyuubi server and engine dramatically improves the client concurrency and service stability of the service itself.&lt;/p&gt; 
&lt;h4&gt;DataLake/Lakehouse Support&lt;/h4&gt; 
&lt;p&gt;The vision of Kyuubi is to unify the portal and become an easy-to-use data lake management platform. Different kinds of workloads, such as ETL processing and BI analytics, can be supported by one platform, using one copy of data, with one SQL interface.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Logical View support via Kyuubi DataLake Metadata APIs&lt;/li&gt; 
 &lt;li&gt;Multiple Catalogs support&lt;/li&gt; 
 &lt;li&gt;SQL Standard Authorization support for DataLake(coming)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Cloud Native Support&lt;/h4&gt; 
&lt;p&gt;Kyuubi can deploy its engines on different kinds of Cluster Managers, such as, Hadoop YARN, Kubernetes, etc.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/kyuubi/master/docs/imgs/kyuubi_migrating_yarn_to_k8s.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;The Kyuubi Ecosystem(present and future)&lt;/h3&gt; 
&lt;p&gt;The figure below shows our vision for the Kyuubi Ecosystem. Some of them have been realized, some in development, and others would not be possible without your help.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/kyuubi/master/docs/imgs/kyuubi_ecosystem.drawio.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Online Documentation &lt;a href=&quot;https://kyuubi.readthedocs.io/en/master/?badge=master?style=plastic&quot;&gt; &lt;img src=&quot;https://readthedocs.org/projects/kyuubi/badge/?version=master&quot; alt=&quot;Documentation Status&quot; /&gt; &lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;Ready? &lt;a href=&quot;https://kyuubi.readthedocs.io/en/master/quick_start/&quot;&gt;Getting Started&lt;/a&gt; with Kyuubi.&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/kyuubi/master/CONTRIBUTING.md&quot;&gt;Contributing&lt;/a&gt;&lt;/h2&gt; 
&lt;h2&gt;Project &amp;amp; Community Status&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/issues?q=is%3Aissue+is%3Aclosed&quot;&gt; &lt;img src=&quot;http://isitmaintained.com/badge/resolution/apache/kyuubi.svg?sanitize=true&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/issues&quot;&gt; &lt;img src=&quot;http://isitmaintained.com/badge/open/apache/kyuubi.svg?sanitize=true&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/pulls&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/issues-pr-closed/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/commit-activity/y/apache/kyuubi?style=plastic&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/commit-activity/m/apache/kyuubi?style=plastic&quot; /&gt; &lt;img src=&quot;https://codecov.io/gh/apache/kyuubi/branch/master/graph/badge.svg?sanitize=true&quot; /&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/actions/workflows/master.yml&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/apache/kyuubi/master.yml?style=plastic&quot; /&gt; &lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/languages/top/apache/kyuubi?style=plastic&quot; /&gt; &lt;a href=&quot;https://github.com/apache/kyuubi/pulse&quot;&gt; &lt;img src=&quot;https://img.shields.io/tokei/lines/github/apache/kyuubi?style=plastic&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://contributor-graph-api.apiseven.com/contributors-svg?chart=contributorOverTime&amp;amp;repo=apache/kyuubi&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Aside&lt;/h2&gt; 
&lt;p&gt;The project took its name from a character of a popular Japanese manga - &lt;code&gt;Naruto&lt;/code&gt;. The character is named &lt;code&gt;Kyuubi Kitsune/Kurama&lt;/code&gt;, which is a nine-tailed fox in mythology. &lt;code&gt;Kyuubi&lt;/code&gt; spread the power and spirit of fire, which is used here to represent the powerful &lt;a href=&quot;http://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;. Its nine tails stand for end-to-end multi-tenancy support of this project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenXiangShan/XiangShan</title>
      <link>https://github.com/OpenXiangShan/XiangShan</link>
      <description>&lt;p&gt;Open-source high-performance RISC-V processor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XiangShan&lt;/h1&gt; 
&lt;p&gt;XiangShan (香山) is an open-source high-performance RISC-V processor project.&lt;/p&gt; 
&lt;p&gt;中文说明&lt;a href=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/readme.zh-cn.md&quot;&gt;在此&lt;/a&gt;。&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;XiangShan&#39;s documentation is available at &lt;a href=&quot;https://docs.xiangshan.cc&quot;&gt;docs.xiangshan.cc&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;XiangShan Design Document for Kunminghu V2R2 has been published separately. You can find it at &lt;a href=&quot;https://docs.xiangshan.cc/projects/design/&quot;&gt;docs.xiangshan.cc/projects/design&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;XiangShan User Guide has been published separately. You can find it at &lt;a href=&quot;https://docs.xiangshan.cc/projects/user-guide/&quot;&gt;docs.xiangshan.cc/projects/user-guide&lt;/a&gt; or &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-User-Guide/releases&quot;&gt;XiangShan-User-Guide/releases&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We are using &lt;a href=&quot;https://hosted.weblate.org/projects/openxiangshan/&quot;&gt;Weblate&lt;/a&gt; to translate documentation into English and other languages. Your contributions are welcome—come and help us improve it!&lt;/p&gt; 
&lt;p&gt;All XiangShan documents are licensed under the CC-BY-4.0.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;h3&gt;MICRO 2022: Towards Developing High Performance RISC-V Processors Using Agile Methodology&lt;/h3&gt; 
&lt;p&gt;Our paper introduces XiangShan and the practice of agile development methodology on high performance RISC-V processors. It covers some representative tools we have developed and used to accelerate the chip development process, including design, functional verification, debugging, performance validation, etc. This paper is awarded all three available badges for artifact evaluation (Available, Functional, and Reproduced).&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/images/artifacts_available_dl.jpg&quot; alt=&quot;Artifacts Available&quot; /&gt; &lt;img src=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/images/artifacts_evaluated_functional_dl.jpg&quot; alt=&quot;Artifacts Evaluated — Functional&quot; /&gt; &lt;img src=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/images/results_reproduced_dl.jpg&quot; alt=&quot;Results Reproduced&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/micro2022-xiangshan.pdf&quot;&gt;Paper PDF&lt;/a&gt; | &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9923860&quot;&gt;IEEE Xplore&lt;/a&gt; | &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/micro2022-xiangshan.bib&quot;&gt;BibTeX&lt;/a&gt; | &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/micro2022-xiangshan-slides.pdf&quot;&gt;Presentation Slides&lt;/a&gt; | &lt;a href=&quot;https://www.bilibili.com/video/BV1FB4y1j7Jy&quot;&gt;Presentation Video&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Follow us&lt;/h2&gt; 
&lt;p&gt;Wechat/微信：香山开源处理器&lt;/p&gt; 
&lt;div align=&quot;left&quot;&gt;
 &lt;img width=&quot;340&quot; height=&quot;117&quot; src=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/wechat.png&quot; /&gt;
&lt;/div&gt; 
&lt;p&gt;Zhihu/知乎：&lt;a href=&quot;https://www.zhihu.com/people/openxiangshan&quot;&gt;香山开源处理器&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Weibo/微博：&lt;a href=&quot;https://weibo.com/u/7706264932&quot;&gt;香山开源处理器&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can contact us through &lt;a href=&quot;mailto:xiangshan-all@ict.ac.cn&quot;&gt;our mailing list&lt;/a&gt;. All mails from this list will be archived &lt;a href=&quot;https://www.mail-archive.com/xiangshan-all@ict.ac.cn/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The first stable micro-architecture of XiangShan is called Yanqihu (雁栖湖) and is &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan/tree/yanqihu&quot;&gt;on the yanqihu branch&lt;/a&gt;, which has been developed since June 2020.&lt;/p&gt; 
&lt;p&gt;The second stable micro-architecture of XiangShan is called Nanhu (南湖) and is &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan/tree/nanhu&quot;&gt;on the nanhu branch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The current version of XiangShan, also known as Kunminghu (昆明湖), is still under development on the master branch.&lt;/p&gt; 
&lt;p&gt;The micro-architecture overview of Kunminghu (昆明湖) is shown below.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/xs-arch-kunminghu.svg?sanitize=true&quot; alt=&quot;xs-arch-kunminghu&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Sub-directories Overview&lt;/h2&gt; 
&lt;p&gt;Some of the key directories are shown below.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.
├── src
│   └── main/scala         # design files
│       ├── device         # virtual device for simulation
│       ├── system         # SoC wrapper
│       ├── top            # top module
│       ├── utils          # utilization code
│       └── xiangshan      # main design code
│           └── transforms # some useful firrtl transforms
├── scripts                # scripts for agile development
├── fudian                 # floating unit submodule of XiangShan
├── huancun                # L2/L3 cache submodule of XiangShan
├── difftest               # difftest co-simulation framework
└── ready-to-run           # pre-built simulation images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IDE Support&lt;/h2&gt; 
&lt;h3&gt;bsp&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;make bsp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;IDEA&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;make idea
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Generate Verilog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run &lt;code&gt;make verilog&lt;/code&gt; to generate verilog code. This generates multiple &lt;code&gt;.sv&lt;/code&gt; files in the &lt;code&gt;build/rtl/&lt;/code&gt; folder (e.g., &lt;code&gt;build/rtl/XSTop.sv&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run Programs by Simulation&lt;/h2&gt; 
&lt;h3&gt;Prepare environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set environment variable &lt;code&gt;NEMU_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&quot;https://github.com/OpenXiangShan/NEMU&quot;&gt;NEMU project&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set environment variable &lt;code&gt;NOOP_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the XiangShan project.&lt;/li&gt; 
 &lt;li&gt;Set environment variable &lt;code&gt;AM_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&quot;https://github.com/OpenXiangShan/nexus-am&quot;&gt;AM project&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;mill&lt;/code&gt;. Refer to &lt;a href=&quot;https://mill-build.org/mill/cli/installation-ide.html#_bootstrap_scripts&quot;&gt;the Manual section in this guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Clone this project and run &lt;code&gt;make init&lt;/code&gt; to initialize submodules.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run with simulator&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://verilator.org/guide/latest/&quot;&gt;Verilator&lt;/a&gt;, the open-source Verilog simulator.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make emu&lt;/code&gt; to build the C++ simulator &lt;code&gt;./build/emu&lt;/code&gt; with Verilator.&lt;/li&gt; 
 &lt;li&gt;Refer to &lt;code&gt;./build/emu --help&lt;/code&gt; for run-time arguments of the simulator.&lt;/li&gt; 
 &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; and &lt;code&gt;verilator.mk&lt;/code&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make emu CONFIG=MinimalConfig EMU_THREADS=2 -j10
./build/emu -b 0 -e 0 -i ./ready-to-run/coremark-2-iteration.bin --diff ./ready-to-run/riscv64-nemu-interpreter-so
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run with xspdb&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://github.com/XS-MLVP/picker&quot;&gt;picker&lt;/a&gt;, a verifaction tool that supports high-level languages.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make pdb&lt;/code&gt; to build XiangShan Python binaries.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make pdb-run&lt;/code&gt; to run XiangShan binaries.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example output and interaction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make pdb-run
[Info] Set PMEM_BASE to 0x80000000 (Current: 0x80000000)
[Info] Set FIRST_INST_ADDRESS to 0x80000000 (Current: 0x80000000)
Using simulated 32768B flash
[Info] reset dut complete
&amp;gt; XiangShan/scripts/pdb-run.py(13)run()
-&amp;gt; while True:
(XiangShan) xload ready-to-run/microbench.bin   # Load binary (Tab-compatible)
(XiangShan) xwatch_commit_pc 0x80000004         # set watch point,  
(XiangShan) xistep 3                            # Step to next three instruction commit, it will stop at watch point 
[Info] Find break point (Inst commit), break (step 2107 cycles) at cycle: 2207 (0x89f)
[Info] Find break point (Inst commit, Target commit), break (step 2108 cycles) at cycle: 2208 (0x8a0)
(XiangShan) xpc                                 # print pc info
PC[0]: 0x80000000    Instr: 0x00000093
PC[1]: 0x80000004    Instr: 0x00000113
PC[2]: 0x0    Instr: 0x0
...
PC[7]: 0x0    Instr: 0x0
(XiangShan) xistep 1000000                      # Execute to binary end
[Info] Find break point (Inst commit), break (step 2037 cycles) at cycle: 2207 (0x89f)
[Info] Find break point (Inst commit), break (step 2180 cycles) at cycle: 2207 (0x89f)
...
HIT GOOD LOOP at pc = 0xf0001cb0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Troubleshooting Guide&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan/wiki/Troubleshooting-Guide&quot;&gt;Troubleshooting Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;The implementation of XiangShan is inspired by several key papers. We list these papers in XiangShan document, see: &lt;a href=&quot;https://docs.xiangshan.cc/zh-cn/latest/acknowledgments/&quot;&gt;Acknowledgements&lt;/a&gt;. We very much encourage and expect that more academic innovations can be realised based on XiangShan in the future.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;Copyright © 2020-2025 Institute of Computing Technology, Chinese Academy of Sciences.&lt;/p&gt; 
&lt;p&gt;Copyright © 2021-2025 Beijing Institute of Open Source Chip&lt;/p&gt; 
&lt;p&gt;Copyright © 2020-2022 by Peng Cheng Laboratory.&lt;/p&gt; 
&lt;p&gt;XiangShan is licensed under &lt;a href=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/LICENSE&quot;&gt;Mulan PSL v2&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lichess-org/lila</title>
      <link>https://github.com/lichess-org/lila</link>
      <description>&lt;p&gt;♞ lichess.org: the forever free, adless and open source chess server ♞&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&quot;https://lichess.org&quot;&gt;lichess.org&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/lichess-org/lila/actions/workflows/server.yml&quot;&gt;&lt;img src=&quot;https://github.com/lichess-org/lila/actions/workflows/server.yml/badge.svg?sanitize=true&quot; alt=&quot;Build server&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/lichess-org/lila/actions/workflows/assets.yml&quot;&gt;&lt;img src=&quot;https://github.com/lichess-org/lila/actions/workflows/assets.yml/badge.svg?sanitize=true&quot; alt=&quot;Build assets&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://crowdin.com/project/lichess&quot;&gt;&lt;img src=&quot;https://d322cqt584bo4o.cloudfront.net/lichess/localized.svg?sanitize=true&quot; alt=&quot;Crowdin&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://mastodon.online/@lichess&quot;&gt;&lt;img src=&quot;https://img.shields.io/mastodon/follow/109298525492334687?domain=mastodon.online&quot; alt=&quot;Mastodon&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://bsky.app/profile/lichess.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Bluesky-0285FF?logo=bluesky&amp;amp;logoColor=fff&quot; alt=&quot;Bluesky&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/lichess&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/280713822073913354?label=Discord&amp;amp;logo=discord&amp;amp;style=flat&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/lichess-org/lila/master/public/images/home-bicolor.png&quot; alt=&quot;Lichess homepage&quot; title=&quot;Lichess comes with light and dark theme, this screenshot shows both.&quot; /&gt; 
&lt;p&gt;Lila (li[chess in sca]la) is a free online chess game server focused on &lt;a href=&quot;https://lichess.org/games&quot;&gt;realtime&lt;/a&gt; gameplay and ease of use.&lt;/p&gt; 
&lt;p&gt;It features a &lt;a href=&quot;https://lichess.org/games/search&quot;&gt;search engine&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/ief49lif&quot;&gt;computer analysis&lt;/a&gt; distributed with &lt;a href=&quot;https://github.com/lichess-org/fishnet&quot;&gt;fishnet&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/tournament&quot;&gt;tournaments&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/simul&quot;&gt;simuls&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/forum&quot;&gt;forums&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/team&quot;&gt;teams&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/training&quot;&gt;tactic trainer&lt;/a&gt;, a &lt;a href=&quot;https://lichess.org/mobile&quot;&gt;mobile app&lt;/a&gt;, and a &lt;a href=&quot;https://lichess.org/study&quot;&gt;shared analysis board&lt;/a&gt;. The UI is available in more than &lt;a href=&quot;https://crowdin.com/project/lichess&quot;&gt;140 languages&lt;/a&gt; thanks to the community.&lt;/p&gt; 
&lt;p&gt;Lichess is written in &lt;a href=&quot;https://www.scala-lang.org/&quot;&gt;Scala 3&lt;/a&gt;, and relies on the &lt;a href=&quot;https://www.playframework.com/&quot;&gt;Play 2.8&lt;/a&gt; framework. &lt;a href=&quot;https://com-lihaoyi.github.io/scalatags/&quot;&gt;scalatags&lt;/a&gt; is used for templating. Pure chess logic is contained in the &lt;a href=&quot;https://github.com/lichess-org/scalachess&quot;&gt;scalachess&lt;/a&gt; submodule. The server is fully asynchronous, making heavy use of Scala Futures and &lt;a href=&quot;https://akka.io&quot;&gt;Akka streams&lt;/a&gt;. WebSocket connections are handled by a &lt;a href=&quot;https://github.com/lichess-org/lila-ws&quot;&gt;separate server&lt;/a&gt; that communicates using &lt;a href=&quot;https://redis.io/&quot;&gt;redis&lt;/a&gt;. Lichess talks to &lt;a href=&quot;https://stockfishchess.org/&quot;&gt;Stockfish&lt;/a&gt; deployed in an &lt;a href=&quot;https://github.com/lichess-org/fishnet&quot;&gt;AI cluster&lt;/a&gt; of donated servers. It uses &lt;a href=&quot;https://www.mongodb.com&quot;&gt;MongoDB&lt;/a&gt; to store more than 4.7 billion games, which are indexed by &lt;a href=&quot;https://github.com/elastic/elasticsearch&quot;&gt;elasticsearch&lt;/a&gt;. HTTP requests and WebSocket connections can be proxied by &lt;a href=&quot;https://nginx.org&quot;&gt;nginx&lt;/a&gt;. The web client is written in &lt;a href=&quot;https://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt; and &lt;a href=&quot;https://github.com/snabbdom/snabbdom&quot;&gt;snabbdom&lt;/a&gt;, using &lt;a href=&quot;https://sass-lang.com/&quot;&gt;Sass&lt;/a&gt; to generate CSS. All rated games are published in a &lt;a href=&quot;https://database.lichess.org&quot;&gt;free PGN database&lt;/a&gt;. Browser testing done with &lt;a href=&quot;https://www.browserstack.com&quot;&gt;Browserstack&lt;/a&gt;. Proxy detection done with &lt;a href=&quot;https://www.ip2location.com/database/ip2proxy&quot;&gt;IP2Proxy database&lt;/a&gt;. Please help us &lt;a href=&quot;https://crowdin.com/project/lichess&quot;&gt;translate Lichess with Crowdin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://lichess.org/source&quot;&gt;lichess.org/source&lt;/a&gt; for a list of repositories.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/lichess&quot;&gt;Join us on Discord&lt;/a&gt; for more info. Use &lt;a href=&quot;https://github.com/lichess-org/lila/issues&quot;&gt;GitHub issues&lt;/a&gt; for bug reports and feature requests.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;./lila.sh # thin wrapper around sbt
run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Wiki describes &lt;a href=&quot;https://github.com/lichess-org/lila/wiki/Lichess-Development-Onboarding&quot;&gt;how to setup a development environment&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;HTTP API&lt;/h2&gt; 
&lt;p&gt;Feel free to use the &lt;a href=&quot;https://lichess.org/api&quot;&gt;Lichess API&lt;/a&gt; in your applications and websites.&lt;/p&gt; 
&lt;h2&gt;Supported browsers&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chromium / Chrome&lt;/td&gt; 
   &lt;td&gt;last 10&lt;/td&gt; 
   &lt;td&gt;Full support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Firefox&lt;/td&gt; 
   &lt;td&gt;75+&lt;/td&gt; 
   &lt;td&gt;Full support (fastest local analysis since FF 79)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Edge&lt;/td&gt; 
   &lt;td&gt;91+&lt;/td&gt; 
   &lt;td&gt;Full support (reasonable support for 79+)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Opera&lt;/td&gt; 
   &lt;td&gt;66+&lt;/td&gt; 
   &lt;td&gt;Reasonable support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Safari&lt;/td&gt; 
   &lt;td&gt;11.1+&lt;/td&gt; 
   &lt;td&gt;Reasonable support&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Older browsers (including any version of Internet Explorer) will not work. For your own sake, please upgrade. Security and performance, think about it!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Lila is licensed under the GNU Affero General Public License 3 or any later version at your choice. See &lt;a href=&quot;https://github.com/lichess-org/lila/raw/master/COPYING.md&quot;&gt;copying&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Production architecture (as of July 2022)&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/lichess-org/lila/master/public/images/architecture.png&quot; alt=&quot;Lichess production server architecture diagram&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://lichess.org/thanks&quot;&gt;lichess.org/thanks&lt;/a&gt; and the contributors here:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/lichess-org/lila/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=lichess-org/lila&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Competence development program&lt;/h2&gt; 
&lt;p&gt;Lichess would like to support its contributors in their competence development by covering costs of relevant training materials and activities. This is a small way to further empower contributors who have given their time to Lichess and to enable or improve additional contributions to Lichess in the future. For more information, including how to apply, check &lt;a href=&quot;https://lichess.org/page/competence-development&quot;&gt;Competence Development for Lichess contributors&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>databricks/Spark-The-Definitive-Guide</title>
      <link>https://github.com/databricks/Spark-The-Definitive-Guide</link>
      <description>&lt;p&gt;Spark: The Definitive Guide&#39;s Code Repository&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
  </channel>
</rss>
