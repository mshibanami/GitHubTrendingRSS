<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub C++ Daily Trending</title>
    <description>Daily Trending of C++ in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:30:34 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>mavlink/qgroundcontrol</title>
      <link>https://github.com/mavlink/qgroundcontrol</link>
      <description>&lt;p&gt;Cross-platform ground control station for drones (Android, iOS, Mac OS, Linux, Windows)&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/Dronecode/UX-Design/35d8148a8a0559cd4bcf50bfa2c94614983cce91/QGC/Branding/Deliverables/QGC_RGB_Logo_Horizontal_Positive_PREFERRED/QGC_RGB_Logo_Horizontal_Positive_PREFERRED.svg?sanitize=true&quot; alt=&quot;QGroundControl Logo&quot; width=&quot;500&quot; /&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/mavlink/QGroundControl/releases&quot;&gt; &lt;img src=&quot;https://img.shields.io/github/release/mavlink/QGroundControl.svg?sanitize=true&quot; alt=&quot;Latest Release&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;&lt;em&gt;QGroundControl&lt;/em&gt; (QGC) is a highly intuitive and powerful Ground Control Station (GCS) designed for UAVs. Whether you&#39;re a first-time pilot or an experienced professional, QGC provides a seamless user experience for flight control and mission planning, making it the go-to solution for any &lt;em&gt;MAVLink-enabled drone&lt;/em&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üåü &lt;em&gt;Why Choose QGroundControl?&lt;/em&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;em&gt;üöÄ Ease of Use&lt;/em&gt;: A beginner-friendly interface designed for smooth operation without sacrificing advanced features for pros.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;‚úàÔ∏è Comprehensive Flight Control&lt;/em&gt;: Full flight control and mission management for &lt;em&gt;PX4&lt;/em&gt; and &lt;em&gt;ArduPilot&lt;/em&gt; powered UAVs.&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;üõ†Ô∏è Mission Planning&lt;/em&gt;: Easily plan complex missions with a simple drag-and-drop interface.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üîç For a deeper dive into using QGC, check out the &lt;a href=&quot;https://docs.qgroundcontrol.com/en/&quot;&gt;User Manual&lt;/a&gt; ‚Äì although, thanks to QGC&#39;s intuitive UI, you may not even need it!&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üöÅ &lt;em&gt;Key Features&lt;/em&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üïπÔ∏è &lt;em&gt;Full Flight Control&lt;/em&gt;: Supports all &lt;em&gt;MAVLink drones&lt;/em&gt;.&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;em&gt;Vehicle Setup&lt;/em&gt;: Tailored configuration for &lt;em&gt;PX4&lt;/em&gt; and &lt;em&gt;ArduPilot&lt;/em&gt; platforms.&lt;/li&gt; 
 &lt;li&gt;üîß &lt;em&gt;Fully Open Source&lt;/em&gt;: Customize and extend the software to suit your needs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;üéØ Check out the latest updates in our &lt;a href=&quot;https://github.com/mavlink/qgroundcontrol/raw/master/ChangeLog.md&quot;&gt;New Features and Release Notes&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üíª &lt;em&gt;Get Involved!&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;QGroundControl is &lt;em&gt;open-source&lt;/em&gt;, meaning you have the power to shape it! Whether you&#39;re fixing bugs, adding features, or customizing for your specific needs, QGC welcomes contributions from the community.&lt;/p&gt; 
&lt;p&gt;üõ†Ô∏è Start building today with our &lt;a href=&quot;https://dev.qgroundcontrol.com/en/&quot;&gt;Developer Guide&lt;/a&gt; and &lt;a href=&quot;https://dev.qgroundcontrol.com/en/getting_started/&quot;&gt;build instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;üîó &lt;em&gt;Useful Links&lt;/em&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üåê &lt;a href=&quot;http://qgroundcontrol.com&quot;&gt;Official Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìò &lt;a href=&quot;https://docs.qgroundcontrol.com/en/&quot;&gt;User Manual&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üõ†Ô∏è &lt;a href=&quot;https://dev.qgroundcontrol.com/en/&quot;&gt;Developer Guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üí¨ &lt;a href=&quot;https://docs.qgroundcontrol.com/en/Support/Support.html&quot;&gt;Discussion &amp;amp; Support&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ü§ù &lt;a href=&quot;https://dev.qgroundcontrol.com/en/contribute/&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;üìú &lt;a href=&quot;https://github.com/mavlink/qgroundcontrol/raw/master/.github/COPYING.md&quot;&gt;License Information&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;With QGroundControl, you&#39;re in full command of your UAV, ready to take your missions to the next level.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>IntelRealSense/librealsense</title>
      <link>https://github.com/IntelRealSense/librealsense</link>
      <description>&lt;p&gt;Intel¬Æ RealSense‚Ñ¢ SDK&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/img/realsense.png&quot; width=&quot;70%&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/IntelRealSense/librealsense.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/IntelRealSense/librealsense?sort=semver&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/compare/master...development&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/commits-since/IntelRealSense/librealsense/master/development?label=commits%20since&quot; alt=&quot;Commits&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/IntelRealSense/librealsense.svg?sanitize=true&quot; alt=&quot;Issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/actions/workflows/buildsCI.yaml&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/IntelRealSense/actions/workflows/buildsCI.yaml/badge.svg?branch=development&quot; alt=&quot;GitHub CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/IntelRealSense/librealsense.svg?sanitize=true&quot; alt=&quot;Forks&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Intel¬Æ RealSense‚Ñ¢ SDK 2.0&lt;/strong&gt; is a cross-platform library for Intel¬Æ RealSense‚Ñ¢ depth cameras.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span&gt;üìå&lt;/span&gt; For other Intel¬Æ RealSense‚Ñ¢ devices (F200, R200, LR200 and ZR300), please refer to the &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/tree/v1.12.1&quot;&gt;latest legacy release&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;The SDK allows depth and color streaming, and provides intrinsic and extrinsic calibration information. The library also offers synthetic streams (pointcloud, depth aligned to color and vise-versa), and a built-in support for &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/record-and-playback.md&quot;&gt;record and playback&lt;/a&gt; of streaming sessions.&lt;/p&gt; 
&lt;p&gt;Developer kits containing the necessary hardware to use this library are available for purchase at &lt;a href=&quot;https://store.intelrealsense.com/products.html&quot;&gt;store.intelrealsense.com&lt;/a&gt;. Information about the Intel¬Æ RealSense‚Ñ¢ technology at &lt;a href=&quot;https://www.intelrealsense.com/&quot;&gt;www.intelrealsense.com&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;span&gt;üìÇ&lt;/span&gt; Don&#39;t have access to a RealSense camera? Check-out &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/sample-data.md&quot;&gt;sample data&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Update on Recent Changes to the RealSense Product Line&lt;/h2&gt; 
&lt;p&gt;Please visit this link for product updates - &lt;a href=&quot;https://www.intelrealsense.com/message-to-customers/&quot;&gt;https://www.intelrealsense.com/message-to-customers/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Building librealsense - Using vcpkg&lt;/h2&gt; 
&lt;p&gt;You can download and install librealsense using the &lt;a href=&quot;https://github.com/Microsoft/vcpkg&quot;&gt;vcpkg&lt;/a&gt; dependency manager:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
./bootstrap-vcpkg.sh
./vcpkg integrate install
./vcpkg install realsense2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The librealsense port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&quot;https://github.com/Microsoft/vcpkg&quot;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; 
&lt;h2&gt;Download and Install&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download&lt;/strong&gt; - The latest releases including the Intel RealSense SDK, Viewer and Depth Quality tools are available at: &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/releases&quot;&gt;&lt;strong&gt;latest releases&lt;/strong&gt;&lt;/a&gt;. Please check the &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/wiki/Release-Notes&quot;&gt;&lt;strong&gt;release notes&lt;/strong&gt;&lt;/a&gt; for the supported platforms, new features and capabilities, known issues, how to upgrade the Firmware and more.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install&lt;/strong&gt; - You can also install or build from source the SDK (on &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/distribution_linux.md&quot;&gt;Linux&lt;/a&gt; \ &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/distribution_windows.md&quot;&gt;Windows&lt;/a&gt; \ &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/installation_osx.md&quot;&gt;Mac OS&lt;/a&gt; \ &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/android.md&quot;&gt;Android&lt;/a&gt; \ &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/scripts/Docker/readme.md&quot;&gt;Docker&lt;/a&gt;), connect your D400 depth camera and you are ready to start writing your first application.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Support &amp;amp; Issues&lt;/strong&gt;: If you need product support (e.g. ask a question about / are having problems with the device), please check the &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/wiki/Troubleshooting-Q%26A&quot;&gt;FAQ &amp;amp; Troubleshooting&lt;/a&gt; section. If not covered there, please search our &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aclosed&quot;&gt;Closed GitHub Issues&lt;/a&gt; page, &lt;a href=&quot;https://communities.intel.com/community/tech/realsense&quot;&gt;Community&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/support/emerging-technologies/intel-realsense-technology.html&quot;&gt;Support&lt;/a&gt; sites. If you still cannot find an answer to your question, please &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/issues/new&quot;&gt;open a new issue&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;What‚Äôs included in the SDK:&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;What&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Download link&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/tools/realsense-viewer&quot;&gt;Intel¬Æ RealSense‚Ñ¢ Viewer&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;With this application, you can quickly access your Intel¬Æ RealSense‚Ñ¢ Depth Camera to view the depth stream, visualize point clouds, record and playback streams, configure your camera settings, modify advanced controls, enable depth visualization and post processing and much more.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/IntelRealSense/librealsense/releases&quot;&gt;&lt;strong&gt;Intel.RealSense.Viewer.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/tools/depth-quality&quot;&gt;Depth Quality Tool&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This application allows you to test the camera‚Äôs depth quality, including: standard deviation from plane fit, normalized RMS ‚Äì the subpixel accuracy, distance accuracy and fill rate. You should be able to easily get and interpret several of the depth quality metrics and record and save the data for offline analysis.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/IntelRealSense/librealsense/releases&quot;&gt;&lt;strong&gt;Depth.Quality.Tool.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/tools/&quot;&gt;Debug Tools&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Device enumeration, FW logger, etc as can be seen at the tools directory&lt;/td&gt; 
   &lt;td&gt;Included in &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/releases&quot;&gt;&lt;strong&gt;Intel.RealSense.SDK.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples&quot;&gt;Code Samples&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;These simple examples demonstrate how to easily use the SDK to include code snippets that access the camera into your applications. Check some of the &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples&quot;&gt;&lt;strong&gt;C++ examples&lt;/strong&gt;&lt;/a&gt; including capture, pointcloud and more and basic &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples/C&quot;&gt;&lt;strong&gt;C examples&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Included in &lt;a href=&quot;https://github.com/IntelRealSense/librealsense/releases&quot;&gt;&lt;strong&gt;Intel.RealSense.SDK.exe&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/IntelRealSense/librealsense/tree/development/wrappers&quot;&gt;Wrappers&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/python&quot;&gt;Python&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/csharp&quot;&gt;C#/.NET&lt;/a&gt; API, as well as integration with the following 3rd-party technologies: &lt;a href=&quot;https://github.com/IntelRealSense/realsense-ros/tree/ros1-legacy&quot;&gt;ROS1&lt;/a&gt;, &lt;a href=&quot;https://github.com/IntelRealSense/realsense-ros/tree/ros2-master&quot;&gt;ROS2&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/labview&quot;&gt;LabVIEW&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/opencv&quot;&gt;OpenCV&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/pcl&quot;&gt;PCL&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/unity&quot;&gt;Unity&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/matlab&quot;&gt;Matlab&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/openni2&quot;&gt;OpenNI&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/wrappers/unrealengine4&quot;&gt;UnrealEngine4&lt;/a&gt; and more to come.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Ready to Hack!&lt;/h2&gt; 
&lt;p&gt;Our library offers a high level API for using Intel RealSense depth cameras (in addition to lower level ones). The following snippet shows how to start streaming frames and extracting the depth value of a pixel:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// Create a Pipeline - this serves as a top-level API for streaming and processing frames
rs2::pipeline p;

// Configure and start the pipeline
p.start();

while (true)
{
    // Block program until frames arrive
    rs2::frameset frames = p.wait_for_frames();

    // Try to get a frame of a depth image
    rs2::depth_frame depth = frames.get_depth_frame();

    // Get the depth frame&#39;s dimensions
    float width = depth.get_width();
    float height = depth.get_height();

    // Query the distance from the camera to the object in the center of the image
    float dist_to_center = depth.get_distance(width / 2, height / 2);

    // Print the distance
    std::cout &amp;lt;&amp;lt; &quot;The camera is facing an object &quot; &amp;lt;&amp;lt; dist_to_center &amp;lt;&amp;lt; &quot; meters away \r&quot;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information on the library, please follow our &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/examples&quot;&gt;examples&lt;/a&gt;, and read the &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc&quot;&gt;documentation&lt;/a&gt; to learn more.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;In order to contribute to Intel RealSense SDK, please follow our &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/CONTRIBUTING.md&quot;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/IntelRealSense/librealsense/master/LICENSE&quot;&gt;Apache License, Version 2.0&lt;/a&gt;. Copyright 2018 Intel Corporation&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Neargye/magic_enum</title>
      <link>https://github.com/Neargye/magic_enum</link>
      <description>&lt;p&gt;Static reflection for enums (to string, from string, iteration) for modern C++, work with any enum type without any macro or boilerplate code&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/Neargye/magic_enum/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/Neargye/magic_enum.svg?sanitize=true&quot; alt=&quot;Github releases&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://conan.io/center/recipes/magic_enum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Conan-package-blueviolet&quot; alt=&quot;Conan package&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/microsoft/vcpkg/tree/master/ports/magic-enum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Vcpkg-package-blueviolet&quot; alt=&quot;Vcpkg package&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.cppget.org/magic_enum?q=magic_enum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Build2-package-blueviolet&quot; alt=&quot;Build2 package&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/mesonbuild/wrapdb/raw/master/subprojects/magic_enum.wrap&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Meson-wrap-blueviolet&quot; alt=&quot;Meson wrap&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/Neargye/magic_enum.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://godbolt.org/z/feqcPa5G6&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/compiler_explorer-online-blue.svg?sanitize=true&quot; alt=&quot;Compiler explorer&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/Neargye/magic_enum&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/Neargye/magic_enum/badge&quot; alt=&quot;OpenSSF Scorecard&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://stand-with-ukraine.pp.ua&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/vshymanskyy/StandWithUkraine/main/badges/StandWithUkraine.svg?sanitize=true&quot; alt=&quot;Stand With Ukraine&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Magic Enum C++&lt;/h1&gt; 
&lt;p&gt;Header-only C++17 library provides static reflection for enums, work with any enum type without any macro or boilerplate code.&lt;/p&gt; 
&lt;p&gt;If you like this project, please consider donating to one of the funds that help victims of the war in Ukraine: &lt;a href=&quot;https://u24.gov.ua&quot;&gt;https://u24.gov.ua&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/reference.md&quot;&gt;Reference&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/limitations.md&quot;&gt;Limitations&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/#Integration&quot;&gt;Integration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/example/&quot;&gt;Features &amp;amp; Examples&lt;/a&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Basic&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;#include &amp;lt;magic_enum/magic_enum.hpp&amp;gt;
#include &amp;lt;iostream&amp;gt;

enum class Color { RED = -10, BLUE = 0, GREEN = 10 };

int main() {
  Color c1 = Color::RED;
  std::cout &amp;lt;&amp;lt; magic_enum::enum_name(c1) &amp;lt;&amp;lt; std::endl; // RED
  return 0;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum value to string&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Color color = Color::RED;
auto color_name = magic_enum::enum_name(color);
// color_name -&amp;gt; &quot;RED&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;String to enum value&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;std::string color_name{&quot;GREEN&quot;};
auto color = magic_enum::enum_cast&amp;lt;Color&amp;gt;(color_name);
if (color.has_value()) {
  // color.value() -&amp;gt; Color::GREEN
}

// case insensitive enum_cast
auto color = magic_enum::enum_cast&amp;lt;Color&amp;gt;(value, magic_enum::case_insensitive);

// enum_cast with BinaryPredicate
auto color = magic_enum::enum_cast&amp;lt;Color&amp;gt;(value, [](char lhs, char rhs) { return std::tolower(lhs) == std::tolower(rhs); }

// enum_cast with default
auto color_or_default = magic_enum::enum_cast&amp;lt;Color&amp;gt;(value).value_or(Color::NONE);
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Integer to enum value&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;int color_integer = 2;
auto color = magic_enum::enum_cast&amp;lt;Color&amp;gt;(color_integer);
if (color.has_value()) {
  // color.value() -&amp;gt; Color::BLUE
}

auto color_or_default = magic_enum::enum_cast&amp;lt;Color&amp;gt;(value).value_or(Color::NONE);
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Indexed access to enum value&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;std::size_t i = 0;
Color color = magic_enum::enum_value&amp;lt;Color&amp;gt;(i);
// color -&amp;gt; Color::RED
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum value sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr auto colors = magic_enum::enum_values&amp;lt;Color&amp;gt;();
// colors -&amp;gt; {Color::RED, Color::BLUE, Color::GREEN}
// colors[0] -&amp;gt; Color::RED
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Number of enum elements&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr std::size_t color_count = magic_enum::enum_count&amp;lt;Color&amp;gt;();
// color_count -&amp;gt; 3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum value to integer&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Color color = Color::RED;
auto color_integer = magic_enum::enum_integer(color); // or magic_enum::enum_underlying(color);
// color_integer -&amp;gt; 1
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum names sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr auto color_names = magic_enum::enum_names&amp;lt;Color&amp;gt;();
// color_names -&amp;gt; {&quot;RED&quot;, &quot;BLUE&quot;, &quot;GREEN&quot;}
// color_names[0] -&amp;gt; &quot;RED&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum entries sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr auto color_entries = magic_enum::enum_entries&amp;lt;Color&amp;gt;();
// color_entries -&amp;gt; {{Color::RED, &quot;RED&quot;}, {Color::BLUE, &quot;BLUE&quot;}, {Color::GREEN, &quot;GREEN&quot;}}
// color_entries[0].first -&amp;gt; Color::RED
// color_entries[0].second -&amp;gt; &quot;RED&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum fusion for multi-level switch/case statements&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;switch (magic_enum::enum_fuse(color, direction).value()) {
  case magic_enum::enum_fuse(Color::RED, Directions::Up).value(): // ...
  case magic_enum::enum_fuse(Color::BLUE, Directions::Down).value(): // ...
// ...
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum switch runtime value as constexpr constant&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Color color = Color::RED;
magic_enum::enum_switch([] (auto val) {
  constexpr Color c_color = val;
  // ...
}, color);
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum iterate for each enum as constexpr constant&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;magic_enum::enum_for_each&amp;lt;Color&amp;gt;([] (auto val) {
  constexpr Color c_color = val;
  // ...
});
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Check if enum contains&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;magic_enum::enum_contains(Color::GREEN); // -&amp;gt; true
magic_enum::enum_contains&amp;lt;Color&amp;gt;(2); // -&amp;gt; true
magic_enum::enum_contains&amp;lt;Color&amp;gt;(123); // -&amp;gt; false
magic_enum::enum_contains&amp;lt;Color&amp;gt;(&quot;GREEN&quot;); // -&amp;gt; true
magic_enum::enum_contains&amp;lt;Color&amp;gt;(&quot;fda&quot;); // -&amp;gt; false
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum index in sequence&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr auto color_index = magic_enum::enum_index(Color::BLUE);
// color_index.value() -&amp;gt; 1
// color_index.has_value() -&amp;gt; true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Functions for flags&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;enum Directions : std::uint64_t {
  Left = 1,
  Down = 2,
  Up = 4,
  Right = 8,
};
template &amp;lt;&amp;gt;
struct magic_enum::customize::enum_range&amp;lt;Directions&amp;gt; {
  static constexpr bool is_flags = true;
};

magic_enum::enum_flags_name(Directions::Up | Directions::Right); // -&amp;gt; &quot;Directions::Up|Directions::Right&quot;
magic_enum::enum_flags_contains(Directions::Up | Directions::Right); // -&amp;gt; true
magic_enum::enum_flags_cast(3); // -&amp;gt; &quot;Directions::Left|Directions::Down&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Enum type name&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;Color color = Color::RED;
auto type_name = magic_enum::enum_type_name&amp;lt;decltype(color)&amp;gt;();
// type_name -&amp;gt; &quot;Color&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;IOstream operator for enum&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;using magic_enum::iostream_operators::operator&amp;lt;&amp;lt;; // out-of-the-box ostream operators for enums.
Color color = Color::BLUE;
std::cout &amp;lt;&amp;lt; color &amp;lt;&amp;lt; std::endl; // &quot;BLUE&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;using magic_enum::iostream_operators::operator&amp;gt;&amp;gt;; // out-of-the-box istream operators for enums.
Color color;
std::cin &amp;gt;&amp;gt; color;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Bitwise operator for enum&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;enum class Flags { A = 1 &amp;lt;&amp;lt; 0, B = 1 &amp;lt;&amp;lt; 1, C = 1 &amp;lt;&amp;lt; 2, D = 1 &amp;lt;&amp;lt; 3 };
using namespace magic_enum::bitwise_operators; // out-of-the-box bitwise operators for enums.
// Support operators: ~, |, &amp;amp;, ^, |=, &amp;amp;=, ^=.
Flags flags = Flags::A | Flags::B &amp;amp; ~Flags::C;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Checks whether type is an &lt;a href=&quot;https://en.cppreference.com/w/cpp/language/enum#Unscoped_enumeration&quot;&gt;Unscoped enumeration&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;enum color { red, green, blue };
enum class direction { left, right };

magic_enum::is_unscoped_enum&amp;lt;color&amp;gt;::value -&amp;gt; true
magic_enum::is_unscoped_enum&amp;lt;direction&amp;gt;::value -&amp;gt; false
magic_enum::is_unscoped_enum&amp;lt;int&amp;gt;::value -&amp;gt; false

// Helper variable template.
magic_enum::is_unscoped_enum_v&amp;lt;color&amp;gt; -&amp;gt; true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Checks whether type is an &lt;a href=&quot;https://en.cppreference.com/w/cpp/language/enum#Scoped_enumerations&quot;&gt;Scoped enumeration&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;enum color { red, green, blue };
enum class direction { left, right };

magic_enum::is_scoped_enum&amp;lt;color&amp;gt;::value -&amp;gt; false
magic_enum::is_scoped_enum&amp;lt;direction&amp;gt;::value -&amp;gt; true
magic_enum::is_scoped_enum&amp;lt;int&amp;gt;::value -&amp;gt; false

// Helper variable template.
magic_enum::is_scoped_enum_v&amp;lt;direction&amp;gt; -&amp;gt; true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Static storage enum variable to string This version is much lighter on the compile times and is not restricted to the enum_range &lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/limitations.md&quot;&gt;limitation&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr Color color = Color::BLUE;
constexpr auto color_name = magic_enum::enum_name&amp;lt;color&amp;gt;();
// color_name -&amp;gt; &quot;BLUE&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;containers::array&lt;/code&gt; array container for enums.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;magic_enum::containers::array&amp;lt;Color, RGB&amp;gt; color_rgb_array {};
color_rgb_array[Color::RED] = {255, 0, 0};
color_rgb_array[Color::GREEN] = {0, 255, 0};
color_rgb_array[Color::BLUE] = {0, 0, 255};
magic_enum::containers::get&amp;lt;Color::BLUE&amp;gt;(color_rgb_array) // -&amp;gt; RGB{0, 0, 255}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;containers::bitset&lt;/code&gt; bitset container for enums.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;constexpr magic_enum::containers::bitset&amp;lt;Color&amp;gt; color_bitset_red_green {Color::RED|Color::GREEN};
bool all = color_bitset_red_green.all();
// all -&amp;gt; false
// Color::BLUE is missing
bool test = color_bitset_red_green.test(Color::RED);
// test -&amp;gt; true
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;containers::set&lt;/code&gt; set container for enums.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;auto color_set = magic_enum::containers::set&amp;lt;Color&amp;gt;();
bool empty = color_set.empty();
// empty -&amp;gt; true
color_set.insert(Color::GREEN);
color_set.insert(Color::BLUE);
color_set.insert(Color::RED);
std::size_t size = color_set.size();
// size -&amp;gt; 3
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Improved UB-free &quot;SFINAE-friendly&quot; &lt;a href=&quot;https://en.cppreference.com/w/cpp/types/underlying_type&quot;&gt;underlying_type&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;magic_enum::underlying_type&amp;lt;color&amp;gt;::type -&amp;gt; int

// Helper types.
magic_enum::underlying_type_t&amp;lt;Direction&amp;gt; -&amp;gt; int
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Remarks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;magic_enum&lt;/code&gt; does not pretend to be a silver bullet for reflection for enums, it was originally designed for small enum.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Before use, read the &lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/doc/limitations.md&quot;&gt;limitations&lt;/a&gt; of functionality.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Integration&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;You should add the required file &lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/include/magic_enum/magic_enum.hpp&quot;&gt;magic_enum.hpp&lt;/a&gt;, and optionally other headers from &lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/include/&quot;&gt;include dir&lt;/a&gt; or &lt;a href=&quot;https://github.com/Neargye/magic_enum/releases/latest&quot;&gt;release archive&lt;/a&gt;. Alternatively, you can build the library with CMake.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&quot;https://github.com/Microsoft/vcpkg/&quot;&gt;vcpkg&lt;/a&gt; on your project for external dependencies, then you can use the &lt;a href=&quot;https://github.com/microsoft/vcpkg/tree/master/ports/magic-enum&quot;&gt;magic-enum package&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&quot;https://www.conan.io/&quot;&gt;Conan&lt;/a&gt; to manage your dependencies, merely add &lt;code&gt;magic_enum/x.y.z&lt;/code&gt; to your conan&#39;s requires, where &lt;code&gt;x.y.z&lt;/code&gt; is the release version you want to use.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&quot;https://build2.org/&quot;&gt;Build2&lt;/a&gt; to build and manage your dependencies, add &lt;code&gt;depends: magic_enum ^x.y.z&lt;/code&gt; to the manifest file where &lt;code&gt;x.y.z&lt;/code&gt; is the release version you want to use. You can then import the target using &lt;code&gt;magic_enum%lib{magic_enum}&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Alternatively, you can use something like &lt;a href=&quot;https://github.com/TheLartians/CPM&quot;&gt;CPM&lt;/a&gt; which is based on CMake&#39;s &lt;code&gt;Fetch_Content&lt;/code&gt; module.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;CPMAddPackage(
    NAME magic_enum
    GITHUB_REPOSITORY Neargye/magic_enum
    GIT_TAG vx.y.z # Where `x.y.z` is the release version you want to use.
)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Bazel is also supported, simply add to your WORKSPACE file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http_archive(
    name = &quot;magic_enum&quot;,
    strip_prefix = &quot;magic_enum-&amp;lt;commit&amp;gt;&quot;,
    urls = [&quot;https://github.com/Neargye/magic_enum/archive/&amp;lt;commit&amp;gt;.zip&quot;],
)
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To use bazel inside the repository it&#39;s possible to do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bazel build //...
bazel test //...
bazel run //example
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Note that you must use a supported compiler or specify it with &lt;code&gt;export CC= &amp;lt;compiler&amp;gt;&lt;/code&gt;.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If you are using &lt;a href=&quot;https://www.ros.org/&quot;&gt;Ros&lt;/a&gt;, you can include this package by adding &lt;code&gt;&amp;lt;depend&amp;gt;magic_enum&amp;lt;/depend&amp;gt;&lt;/code&gt; to your package.xml and include this package in your workspace. In your CMakeLists.txt add the following:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;find_package(magic_enum CONFIG REQUIRED)
...
target_link_libraries(your_executable magic_enum::magic_enum)
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Compiler compatibility&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clang/LLVM &amp;gt;= 5&lt;/li&gt; 
 &lt;li&gt;MSVC++ &amp;gt;= 14.11 / Visual Studio &amp;gt;= 2017&lt;/li&gt; 
 &lt;li&gt;Xcode &amp;gt;= 10&lt;/li&gt; 
 &lt;li&gt;GCC &amp;gt;= 9&lt;/li&gt; 
 &lt;li&gt;MinGW &amp;gt;= 9&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/Neargye/magic_enum/master/LICENSE&quot;&gt;MIT License&lt;/a&gt;&lt;/h2&gt;</description>
    </item>
    
    <item>
      <title>google/benchmark</title>
      <link>https://github.com/google/benchmark</link>
      <description>&lt;p&gt;A microbenchmark support library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Benchmark&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google/benchmark/actions?query=workflow%3Abuild-and-test&quot;&gt;&lt;img src=&quot;https://github.com/google/benchmark/workflows/build-and-test/badge.svg?sanitize=true&quot; alt=&quot;build-and-test&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/google/benchmark/actions/workflows/bazel.yml&quot;&gt;&lt;img src=&quot;https://github.com/google/benchmark/actions/workflows/bazel.yml/badge.svg?sanitize=true&quot; alt=&quot;bazel&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/google/benchmark/actions?query=workflow%3Atest-bindings&quot;&gt;&lt;img src=&quot;https://github.com/google/benchmark/workflows/test-bindings/badge.svg?sanitize=true&quot; alt=&quot;test-bindings&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://coveralls.io/r/google/benchmark&quot;&gt;&lt;img src=&quot;https://coveralls.io/repos/google/benchmark/badge.svg?sanitize=true&quot; alt=&quot;Coverage Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/google/benchmark&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/google/benchmark/badge&quot; alt=&quot;OpenSSF Scorecard&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/cz7UX7wKC2&quot;&gt;&lt;img src=&quot;https://discordapp.com/api/guilds/1125694995928719494/widget.png?style=shield&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;A library to benchmark code snippets, similar to unit tests. Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;benchmark/benchmark.h&amp;gt;

static void BM_SomeFunction(benchmark::State&amp;amp; state) {
  // Perform setup here
  for (auto _ : state) {
    // This code gets timed
    SomeFunction();
  }
}
// Register the function as a benchmark
BENCHMARK(BM_SomeFunction);
// Run the benchmark
BENCHMARK_MAIN();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;To get started, see &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/#requirements&quot;&gt;Requirements&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/#installation&quot;&gt;Installation&lt;/a&gt;. See &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/#usage&quot;&gt;Usage&lt;/a&gt; for a full example and the &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/user_guide.md&quot;&gt;User Guide&lt;/a&gt; for a more comprehensive feature overview.&lt;/p&gt; 
&lt;p&gt;It may also help to read the &lt;a href=&quot;https://github.com/google/googletest/raw/main/docs/primer.md&quot;&gt;Google Test documentation&lt;/a&gt; as some of the structural aspects of the APIs are similar.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/forum/benchmark-discuss&quot;&gt;Discussion group&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;IRC channels:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://libera.chat&quot;&gt;libera&lt;/a&gt; #benchmark&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/tools.md&quot;&gt;Additional Tooling Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/AssemblyTests.md&quot;&gt;Assembly Testing Documentation&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/python_bindings.md&quot;&gt;Building and installing Python bindings&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;The library can be used with C++11. However, it requires C++17 to build, including compiler and standard library support.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/dependencies.md&quot;&gt;dependencies.md&lt;/a&gt; for more details regarding supported compilers and standards.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;If you have need for a particular compiler to be supported, patches are very welcome.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/platform_specific_build_instructions.md&quot;&gt;Platform-Specific Build Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;This describes the installation process using cmake. As pre-requisites, you&#39;ll need git and cmake installed.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/dependencies.md&quot;&gt;dependencies.md&lt;/a&gt; for more details regarding supported versions of build tools.&lt;/em&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Check out the library.
$ git clone https://github.com/google/benchmark.git
# Go to the library root directory
$ cd benchmark
# Make a build directory to place the build output.
$ cmake -E make_directory &quot;build&quot;
# Generate build system files with cmake, and download any dependencies.
$ cmake -E chdir &quot;build&quot; cmake -DBENCHMARK_DOWNLOAD_DEPENDENCIES=on -DCMAKE_BUILD_TYPE=Release ../
# or, starting with CMake 3.13, use a simpler form:
# cmake -DBENCHMARK_DOWNLOAD_DEPENDENCIES=on -DCMAKE_BUILD_TYPE=Release -S . -B &quot;build&quot;
# Build the library.
$ cmake --build &quot;build&quot; --config Release
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This builds the &lt;code&gt;benchmark&lt;/code&gt; and &lt;code&gt;benchmark_main&lt;/code&gt; libraries and tests. On a unix system, the build directory should now look something like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/benchmark
  /build
    /src
      /libbenchmark.a
      /libbenchmark_main.a
    /test
      ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, you can run the tests to check the build.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cmake -E chdir &quot;build&quot; ctest --build-config Release
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you want to install the library globally, also run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sudo cmake --build &quot;build&quot; --config Release --target install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that Google Benchmark requires Google Test to build and run the tests. This dependency can be provided two ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Checkout the Google Test sources into &lt;code&gt;benchmark/googletest&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Otherwise, if &lt;code&gt;-DBENCHMARK_DOWNLOAD_DEPENDENCIES=ON&lt;/code&gt; is specified during configuration as above, the library will automatically download and build any required dependencies.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you do not wish to build and run the tests, add &lt;code&gt;-DBENCHMARK_ENABLE_GTEST_TESTS=OFF&lt;/code&gt; to &lt;code&gt;CMAKE_ARGS&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Debug vs Release&lt;/h3&gt; 
&lt;p&gt;By default, benchmark builds as a debug library. You will see a warning in the output when this is the case. To build it as a release library instead, add &lt;code&gt;-DCMAKE_BUILD_TYPE=Release&lt;/code&gt; when generating the build system files, as shown above. The use of &lt;code&gt;--config Release&lt;/code&gt; in build commands is needed to properly support multi-configuration tools (like Visual Studio for example) and can be skipped for other build systems (like Makefile).&lt;/p&gt; 
&lt;p&gt;To enable link-time optimisation, also add &lt;code&gt;-DBENCHMARK_ENABLE_LTO=true&lt;/code&gt; when generating the build system files.&lt;/p&gt; 
&lt;p&gt;If you are using gcc, you might need to set &lt;code&gt;GCC_AR&lt;/code&gt; and &lt;code&gt;GCC_RANLIB&lt;/code&gt; cmake cache variables, if autodetection fails.&lt;/p&gt; 
&lt;p&gt;If you are using clang, you may need to set &lt;code&gt;LLVMAR_EXECUTABLE&lt;/code&gt;, &lt;code&gt;LLVMNM_EXECUTABLE&lt;/code&gt; and &lt;code&gt;LLVMRANLIB_EXECUTABLE&lt;/code&gt; cmake cache variables.&lt;/p&gt; 
&lt;p&gt;To enable sanitizer checks (eg., &lt;code&gt;asan&lt;/code&gt; and &lt;code&gt;tsan&lt;/code&gt;), add:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; -DCMAKE_C_FLAGS=&quot;-g -O2 -fno-omit-frame-pointer -fsanitize=address -fsanitize=thread -fno-sanitize-recover=all&quot;
 -DCMAKE_CXX_FLAGS=&quot;-g -O2 -fno-omit-frame-pointer -fsanitize=address -fsanitize=thread -fno-sanitize-recover=all &quot;  
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Stable and Experimental Library Versions&lt;/h3&gt; 
&lt;p&gt;The main branch contains the latest stable version of the benchmarking library; the API of which can be considered largely stable, with source breaking changes being made only upon the release of a new major version.&lt;/p&gt; 
&lt;p&gt;Newer, experimental, features are implemented and tested on the &lt;a href=&quot;https://github.com/google/benchmark/tree/v2&quot;&gt;&lt;code&gt;v2&lt;/code&gt; branch&lt;/a&gt;. Users who wish to use, test, and provide feedback on the new features are encouraged to try this branch. However, this branch provides no stability guarantees and reserves the right to change and break the API at any time.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;h3&gt;Basic usage&lt;/h3&gt; 
&lt;p&gt;Define a function that executes the code to measure, register it as a benchmark function using the &lt;code&gt;BENCHMARK&lt;/code&gt; macro, and ensure an appropriate &lt;code&gt;main&lt;/code&gt; function is available:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &amp;lt;benchmark/benchmark.h&amp;gt;

static void BM_StringCreation(benchmark::State&amp;amp; state) {
  for (auto _ : state)
    std::string empty_string;
}
// Register the function as a benchmark
BENCHMARK(BM_StringCreation);

// Define another benchmark
static void BM_StringCopy(benchmark::State&amp;amp; state) {
  std::string x = &quot;hello&quot;;
  for (auto _ : state)
    std::string copy(x);
}
BENCHMARK(BM_StringCopy);

BENCHMARK_MAIN();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run the benchmark, compile and link against the &lt;code&gt;benchmark&lt;/code&gt; library (libbenchmark.a/.so). If you followed the build steps above, this library will be under the build directory you created.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Example on linux after running the build steps above. Assumes the
# `benchmark` and `build` directories are under the current directory.
$ g++ mybenchmark.cc -std=c++11 -isystem benchmark/include \
  -Lbenchmark/build/src -lbenchmark -lpthread -o mybenchmark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, link against the &lt;code&gt;benchmark_main&lt;/code&gt; library and remove &lt;code&gt;BENCHMARK_MAIN();&lt;/code&gt; above to get the same behavior.&lt;/p&gt; 
&lt;p&gt;The compiled executable will run all benchmarks by default. Pass the &lt;code&gt;--help&lt;/code&gt; flag for option information or see the &lt;a href=&quot;https://raw.githubusercontent.com/google/benchmark/main/docs/user_guide.md&quot;&gt;User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Usage with CMake&lt;/h3&gt; 
&lt;p&gt;If using CMake, it is recommended to link against the project-provided &lt;code&gt;benchmark::benchmark&lt;/code&gt; and &lt;code&gt;benchmark::benchmark_main&lt;/code&gt; targets using &lt;code&gt;target_link_libraries&lt;/code&gt;. It is possible to use &lt;code&gt;find_package&lt;/code&gt; to import an installed version of the library.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;find_package(benchmark REQUIRED)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, &lt;code&gt;add_subdirectory&lt;/code&gt; will incorporate the library directly in to one&#39;s CMake project.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;add_subdirectory(benchmark)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Either way, link to the library as follows.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-cmake&quot;&gt;target_link_libraries(MyTarget benchmark::benchmark)
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>gabime/spdlog</title>
      <link>https://github.com/gabime/spdlog</link>
      <description>&lt;p&gt;Fast C++ logging library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;spdlog&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/gabime/spdlog/actions/workflows/linux.yml&quot;&gt;&lt;img src=&quot;https://github.com/gabime/spdlog/actions/workflows/linux.yml/badge.svg?sanitize=true&quot; alt=&quot;ci&quot; /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&quot;https://github.com/gabime/spdlog/actions/workflows/windows.yml&quot;&gt;&lt;img src=&quot;https://github.com/gabime/spdlog/actions/workflows/windows.yml/badge.svg?sanitize=true&quot; alt=&quot;ci&quot; /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&quot;https://github.com/gabime/spdlog/actions/workflows/macos.yml&quot;&gt;&lt;img src=&quot;https://github.com/gabime/spdlog/actions/workflows/macos.yml/badge.svg?sanitize=true&quot; alt=&quot;ci&quot; /&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&quot;https://ci.appveyor.com/project/gabime/spdlog&quot;&gt;&lt;img src=&quot;https://ci.appveyor.com/api/projects/status/d2jnxclg20vd0o50?svg=true&amp;amp;branch=v1.x&quot; alt=&quot;Build status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gabime/spdlog/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/gabime/spdlog.svg?sanitize=true&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Fast C++ logging library&lt;/p&gt; 
&lt;h2&gt;Install&lt;/h2&gt; 
&lt;h4&gt;Header-only version&lt;/h4&gt; 
&lt;p&gt;Copy the include &lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/include/spdlog&quot;&gt;folder&lt;/a&gt; to your build tree and use a C++11 compiler.&lt;/p&gt; 
&lt;h4&gt;Compiled version (recommended - much faster compile times)&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ git clone https://github.com/gabime/spdlog.git
$ cd spdlog &amp;amp;&amp;amp; mkdir build &amp;amp;&amp;amp; cd build
$ cmake .. &amp;amp;&amp;amp; cmake --build .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;see example &lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/example/CMakeLists.txt&quot;&gt;CMakeLists.txt&lt;/a&gt; on how to use.&lt;/p&gt; 
&lt;h2&gt;Platforms&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux, FreeBSD, OpenBSD, Solaris, AIX&lt;/li&gt; 
 &lt;li&gt;Windows (msvc 2013+, cygwin)&lt;/li&gt; 
 &lt;li&gt;macOS (clang 3.5+)&lt;/li&gt; 
 &lt;li&gt;Android&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Package managers:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Debian: &lt;code&gt;sudo apt install libspdlog-dev&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Homebrew: &lt;code&gt;brew install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;MacPorts: &lt;code&gt;sudo port install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;FreeBSD: &lt;code&gt;pkg install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Fedora: &lt;code&gt;dnf install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Gentoo: &lt;code&gt;emerge dev-libs/spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Arch Linux: &lt;code&gt;pacman -S spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;openSUSE: &lt;code&gt;sudo zypper in spdlog-devel&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ALT Linux: &lt;code&gt;apt-get install libspdlog-devel&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;vcpkg: &lt;code&gt;vcpkg install spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;conan: &lt;code&gt;conan install --requires=spdlog/[*]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;conda: &lt;code&gt;conda install -c conda-forge spdlog&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;build2: &lt;code&gt;depends: spdlog ^1.8.2&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Very fast (see &lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/#benchmarks&quot;&gt;benchmarks&lt;/a&gt; below).&lt;/li&gt; 
 &lt;li&gt;Headers only or compiled&lt;/li&gt; 
 &lt;li&gt;Feature-rich formatting, using the excellent &lt;a href=&quot;https://github.com/fmtlib/fmt&quot;&gt;fmt&lt;/a&gt; library.&lt;/li&gt; 
 &lt;li&gt;Asynchronous mode (optional)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gabime/spdlog/wiki/Custom-formatting&quot;&gt;Custom&lt;/a&gt; formatting.&lt;/li&gt; 
 &lt;li&gt;Multi/Single threaded loggers.&lt;/li&gt; 
 &lt;li&gt;Various log targets: 
  &lt;ul&gt; 
   &lt;li&gt;Rotating log files.&lt;/li&gt; 
   &lt;li&gt;Daily log files.&lt;/li&gt; 
   &lt;li&gt;Console logging (colors supported).&lt;/li&gt; 
   &lt;li&gt;syslog.&lt;/li&gt; 
   &lt;li&gt;Windows event log.&lt;/li&gt; 
   &lt;li&gt;Windows debugger (&lt;code&gt;OutputDebugString(..)&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;Log to Qt widgets (&lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/#log-to-qt-with-nice-colors&quot;&gt;example&lt;/a&gt;).&lt;/li&gt; 
   &lt;li&gt;Easily &lt;a href=&quot;https://github.com/gabime/spdlog/wiki/Sinks#implementing-your-own-sink&quot;&gt;extendable&lt;/a&gt; with custom log targets.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Log filtering - log levels can be modified at runtime as well as compile time.&lt;/li&gt; 
 &lt;li&gt;Support for loading log levels from argv or environment var.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/#backtrace-support&quot;&gt;Backtrace&lt;/a&gt; support - store debug messages in a ring buffer and display them later on demand.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage samples&lt;/h2&gt; 
&lt;h4&gt;Basic usage&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/spdlog.h&quot;

int main() 
{
    spdlog::info(&quot;Welcome to spdlog!&quot;);
    spdlog::error(&quot;Some error message with arg: {}&quot;, 1);
    
    spdlog::warn(&quot;Easy padding in numbers like {:08d}&quot;, 12);
    spdlog::critical(&quot;Support for int: {0:d};  hex: {0:x};  oct: {0:o}; bin: {0:b}&quot;, 42);
    spdlog::info(&quot;Support for floats {:03.2f}&quot;, 1.23456);
    spdlog::info(&quot;Positional args are {1} {0}..&quot;, &quot;too&quot;, &quot;supported&quot;);
    spdlog::info(&quot;{:&amp;lt;30}&quot;, &quot;left aligned&quot;);
    
    spdlog::set_level(spdlog::level::debug); // Set *global* log level to debug
    spdlog::debug(&quot;This message should be displayed..&quot;);    
    
    // change log pattern
    spdlog::set_pattern(&quot;[%H:%M:%S %z] [%n] [%^---%L---%$] [thread %t] %v&quot;);
    
    // Compile time log levels
    // Note that this does not change the current log level, it will only
    // remove (depending on SPDLOG_ACTIVE_LEVEL) the call on the release code.
    SPDLOG_TRACE(&quot;Some trace message with param {}&quot;, 42);
    SPDLOG_DEBUG(&quot;Some debug message&quot;);
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Create stdout/stderr logger object&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/spdlog.h&quot;
#include &quot;spdlog/sinks/stdout_color_sinks.h&quot;
void stdout_example()
{
    // create a color multi-threaded logger
    auto console = spdlog::stdout_color_mt(&quot;console&quot;);    
    auto err_logger = spdlog::stderr_color_mt(&quot;stderr&quot;);    
    spdlog::get(&quot;console&quot;)-&amp;gt;info(&quot;loggers can be retrieved from a global registry using the spdlog::get(logger_name)&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Basic file logger&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/basic_file_sink.h&quot;
void basic_logfile_example()
{
    try 
    {
        auto logger = spdlog::basic_logger_mt(&quot;basic_logger&quot;, &quot;logs/basic-log.txt&quot;);
    }
    catch (const spdlog::spdlog_ex &amp;amp;ex)
    {
        std::cout &amp;lt;&amp;lt; &quot;Log init failed: &quot; &amp;lt;&amp;lt; ex.what() &amp;lt;&amp;lt; std::endl;
    }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Rotating files&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/rotating_file_sink.h&quot;
void rotating_example()
{
    // Create a file rotating logger with 5 MB size max and 3 rotated files
    auto max_size = 1048576 * 5;
    auto max_files = 3;
    auto logger = spdlog::rotating_logger_mt(&quot;some_logger_name&quot;, &quot;logs/rotating.txt&quot;, max_size, max_files);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Daily files&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
#include &quot;spdlog/sinks/daily_file_sink.h&quot;
void daily_example()
{
    // Create a daily logger - a new file is created every day at 2:30 am
    auto logger = spdlog::daily_logger_mt(&quot;daily_logger&quot;, &quot;logs/daily.txt&quot;, 2, 30);
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Backtrace support&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Debug messages can be stored in a ring buffer instead of being logged immediately.
// This is useful to display debug logs only when needed (e.g. when an error happens).
// When needed, call dump_backtrace() to dump them to your log.

spdlog::enable_backtrace(32); // Store the latest 32 messages in a buffer. 
// or my_logger-&amp;gt;enable_backtrace(32)..
for(int i = 0; i &amp;lt; 100; i++)
{
  spdlog::debug(&quot;Backtrace message {}&quot;, i); // not logged yet..
}
// e.g. if some error happened:
spdlog::dump_backtrace(); // log them now! show the last 32 messages
// or my_logger-&amp;gt;dump_backtrace(32)..
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Periodic flush&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// periodically flush all *registered* loggers every 3 seconds:
// warning: only use if all your loggers are thread-safe (&quot;_mt&quot; loggers)
spdlog::flush_every(std::chrono::seconds(3));

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Stopwatch&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Stopwatch support for spdlog
#include &quot;spdlog/stopwatch.h&quot;
void stopwatch_example()
{
    spdlog::stopwatch sw;    
    spdlog::debug(&quot;Elapsed {}&quot;, sw);
    spdlog::debug(&quot;Elapsed {:.3}&quot;, sw);       
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Log binary data in hex&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// many types of std::container&amp;lt;char&amp;gt; types can be used.
// ranges are supported too.
// format flags:
// {:X} - print in uppercase.
// {:s} - don&#39;t separate each byte with space.
// {:p} - don&#39;t print the position on each line start.
// {:n} - don&#39;t split the output into lines.
// {:a} - show ASCII if :n is not set.

#include &quot;spdlog/fmt/bin_to_hex.h&quot;

void binary_example()
{
    auto console = spdlog::get(&quot;console&quot;);
    std::array&amp;lt;char, 80&amp;gt; buf;
    console-&amp;gt;info(&quot;Binary example: {}&quot;, spdlog::to_hex(buf));
    console-&amp;gt;info(&quot;Another binary example:{:n}&quot;, spdlog::to_hex(std::begin(buf), std::begin(buf) + 10));
    // more examples:
    // logger-&amp;gt;info(&quot;uppercase: {:X}&quot;, spdlog::to_hex(buf));
    // logger-&amp;gt;info(&quot;uppercase, no delimiters: {:Xs}&quot;, spdlog::to_hex(buf));
    // logger-&amp;gt;info(&quot;uppercase, no delimiters, no position info: {:Xsp}&quot;, spdlog::to_hex(buf));
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Logger with multi sinks - each with a different format and log level&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
// create a logger with 2 targets, with different log levels and formats.
// The console will show only warnings or errors, while the file will log all. 
void multi_sink_example()
{
    auto console_sink = std::make_shared&amp;lt;spdlog::sinks::stdout_color_sink_mt&amp;gt;();
    console_sink-&amp;gt;set_level(spdlog::level::warn);
    console_sink-&amp;gt;set_pattern(&quot;[multi_sink_example] [%^%l%$] %v&quot;);

    auto file_sink = std::make_shared&amp;lt;spdlog::sinks::basic_file_sink_mt&amp;gt;(&quot;logs/multisink.txt&quot;, true);
    file_sink-&amp;gt;set_level(spdlog::level::trace);

    spdlog::logger logger(&quot;multi_sink&quot;, {console_sink, file_sink});
    logger.set_level(spdlog::level::debug);
    logger.warn(&quot;this should appear in both console and file&quot;);
    logger.info(&quot;this message should not appear in the console, only in the file&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Register several loggers - change global level&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
// Creation of loggers. Set levels to all registered loggers. 
void set_level_example()
{
    auto logger1 = spdlog::basic_logger_mt(&quot;logger1&quot;, &quot;logs/logger1.txt&quot;);
    auto logger2 = spdlog::basic_logger_mt(&quot;logger2&quot;, &quot;logs/logger2.txt&quot;);

    spdlog::set_default_logger(logger2);
    spdlog::default_logger()-&amp;gt;set_level(spdlog::level::trace); // set level for the default logger (logger2) to trace

    spdlog::trace(&quot;trace message to the logger2 (specified as default)&quot;);

    spdlog::set_level(spdlog::level::off) // (sic!) set level for *all* registered loggers to off (disable)
  
    logger1.warn(&quot;warn message will not appear because the level set to off&quot;);
    logger2.warn(&quot;warn message will not appear because the level set to off&quot;);
    spdlog::warn(&quot;warn message will not appear because the level set to off&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;User-defined callbacks about log events&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
// create a logger with a lambda function callback, the callback will be called
// each time something is logged to the logger
void callback_example()
{
    auto callback_sink = std::make_shared&amp;lt;spdlog::sinks::callback_sink_mt&amp;gt;([](const spdlog::details::log_msg &amp;amp;msg) {
         // for example you can be notified by sending an email to yourself
    });
    callback_sink-&amp;gt;set_level(spdlog::level::err);

    auto console_sink = std::make_shared&amp;lt;spdlog::sinks::stdout_color_sink_mt&amp;gt;();
    spdlog::logger logger(&quot;custom_callback_logger&quot;, {console_sink, callback_sink});

    logger.info(&quot;some info log&quot;);
    logger.error(&quot;critical issue&quot;); // will notify you
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Asynchronous logging&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/async.h&quot;
#include &quot;spdlog/sinks/basic_file_sink.h&quot;
void async_example()
{
    // default thread pool settings can be modified *before* creating the async logger:
    // spdlog::init_thread_pool(8192, 1); // queue with 8k items and 1 backing thread.
    auto async_file = spdlog::basic_logger_mt&amp;lt;spdlog::async_factory&amp;gt;(&quot;async_file_logger&quot;, &quot;logs/async_log.txt&quot;);
    // alternatively:
    // auto async_file = spdlog::create_async&amp;lt;spdlog::sinks::basic_file_sink_mt&amp;gt;(&quot;async_file_logger&quot;, &quot;logs/async_log.txt&quot;);   
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Asynchronous logger with multi sinks&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/async.h&quot;
#include &quot;spdlog/sinks/stdout_color_sinks.h&quot;
#include &quot;spdlog/sinks/rotating_file_sink.h&quot;

void multi_sink_example2()
{
    spdlog::init_thread_pool(8192, 1);
    auto stdout_sink = std::make_shared&amp;lt;spdlog::sinks::stdout_color_sink_mt &amp;gt;();
    auto rotating_sink = std::make_shared&amp;lt;spdlog::sinks::rotating_file_sink_mt&amp;gt;(&quot;mylog.txt&quot;, 1024*1024*10, 3);
    std::vector&amp;lt;spdlog::sink_ptr&amp;gt; sinks {stdout_sink, rotating_sink};
    auto logger = std::make_shared&amp;lt;spdlog::async_logger&amp;gt;(&quot;loggername&quot;, sinks.begin(), sinks.end(), spdlog::thread_pool(), spdlog::async_overflow_policy::block);
    spdlog::register_logger(logger);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;User-defined types&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;template&amp;lt;&amp;gt;
struct fmt::formatter&amp;lt;my_type&amp;gt; : fmt::formatter&amp;lt;std::string&amp;gt;
{
    auto format(my_type my, format_context &amp;amp;ctx) const -&amp;gt; decltype(ctx.out())
    {
        return fmt::format_to(ctx.out(), &quot;[my_type i={}]&quot;, my.i);
    }
};

void user_defined_example()
{
    spdlog::info(&quot;user defined type: {}&quot;, my_type(14));
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;User-defined flags in the log pattern&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Log patterns can contain custom flags.
// the following example will add new flag &#39;%*&#39; - which will be bound to a &amp;lt;my_formatter_flag&amp;gt; instance.
#include &quot;spdlog/pattern_formatter.h&quot;
class my_formatter_flag : public spdlog::custom_flag_formatter
{
public:
    void format(const spdlog::details::log_msg &amp;amp;, const std::tm &amp;amp;, spdlog::memory_buf_t &amp;amp;dest) override
    {
        std::string some_txt = &quot;custom-flag&quot;;
        dest.append(some_txt.data(), some_txt.data() + some_txt.size());
    }

    std::unique_ptr&amp;lt;custom_flag_formatter&amp;gt; clone() const override
    {
        return spdlog::details::make_unique&amp;lt;my_formatter_flag&amp;gt;();
    }
};

void custom_flags_example()
{    
    auto formatter = std::make_unique&amp;lt;spdlog::pattern_formatter&amp;gt;();
    formatter-&amp;gt;add_flag&amp;lt;my_formatter_flag&amp;gt;(&#39;*&#39;).set_pattern(&quot;[%n] [%*] [%^%l%$] %v&quot;);
    spdlog::set_formatter(std::move(formatter));
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Custom error handler&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;void err_handler_example()
{
    // can be set globally or per logger(logger-&amp;gt;set_error_handler(..))
    spdlog::set_error_handler([](const std::string &amp;amp;msg) { spdlog::get(&quot;console&quot;)-&amp;gt;error(&quot;*** LOGGER ERROR ***: {}&quot;, msg); });
    spdlog::get(&quot;console&quot;)-&amp;gt;info(&quot;some invalid message to trigger an error {}{}{}{}&quot;, 3);
}

&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;syslog&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/syslog_sink.h&quot;
void syslog_example()
{
    std::string ident = &quot;spdlog-example&quot;;
    auto syslog_logger = spdlog::syslog_logger_mt(&quot;syslog&quot;, ident, LOG_PID);
    syslog_logger-&amp;gt;warn(&quot;This is warning that will end up in syslog.&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Android example&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/sinks/android_sink.h&quot;
void android_example()
{
    std::string tag = &quot;spdlog-android&quot;;
    auto android_logger = spdlog::android_logger_mt(&quot;android&quot;, tag);
    android_logger-&amp;gt;critical(&quot;Use \&quot;adb shell logcat\&quot; to view this message.&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Load log levels from the env variable or argv&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/cfg/env.h&quot;
int main (int argc, char *argv[])
{
    spdlog::cfg::load_env_levels();
    // or specify the env variable name:
    // MYAPP_LEVEL=info,mylogger=trace &amp;amp;&amp;amp; ./example
    // spdlog::cfg::load_env_levels(&quot;MYAPP_LEVEL&quot;);
    // or from the command line:
    // ./example SPDLOG_LEVEL=info,mylogger=trace
    // #include &quot;spdlog/cfg/argv.h&quot; // for loading levels from argv
    // spdlog::cfg::load_argv_levels(argc, argv);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So then you can:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ export SPDLOG_LEVEL=info,mylogger=trace
$ ./example
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Log file open/close event handlers&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// You can get callbacks from spdlog before/after a log file has been opened or closed. 
// This is useful for cleanup procedures or for adding something to the start/end of the log file.
void file_events_example()
{
    // pass the spdlog::file_event_handlers to file sinks for open/close log file notifications
    spdlog::file_event_handlers handlers;
    handlers.before_open = [](spdlog::filename_t filename) { spdlog::info(&quot;Before opening {}&quot;, filename); };
    handlers.after_open = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(&quot;After opening\n&quot;, fstream); };
    handlers.before_close = [](spdlog::filename_t filename, std::FILE *fstream) { fputs(&quot;Before closing\n&quot;, fstream); };
    handlers.after_close = [](spdlog::filename_t filename) { spdlog::info(&quot;After closing {}&quot;, filename); };
    auto my_logger = spdlog::basic_logger_st(&quot;some_logger&quot;, &quot;logs/events-sample.txt&quot;, true, handlers);        
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Replace the Default Logger&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;void replace_default_logger_example()
{
    auto new_logger = spdlog::basic_logger_mt(&quot;new_default_logger&quot;, &quot;logs/new-default-log.txt&quot;, true);
    spdlog::set_default_logger(new_logger);
    spdlog::info(&quot;new logger log message&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Log to Qt with nice colors&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;#include &quot;spdlog/spdlog.h&quot;
#include &quot;spdlog/sinks/qt_sinks.h&quot;
MainWindow::MainWindow(QWidget *parent) : QMainWindow(parent)
{
    setMinimumSize(640, 480);
    auto log_widget = new QTextEdit(this);
    setCentralWidget(log_widget);
    int max_lines = 500; // keep the text widget to max 500 lines. remove old lines if needed.
    auto logger = spdlog::qt_color_logger_mt(&quot;qt_logger&quot;, log_widget, max_lines);
    logger-&amp;gt;info(&quot;Some info message&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Mapped Diagnostic Context&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;// Mapped Diagnostic Context (MDC) is a map that stores key-value pairs (string values) in thread local storage.
// Each thread maintains its own MDC, which loggers use to append diagnostic information to log outputs.
// Note: it is not supported in asynchronous mode due to its reliance on thread-local storage.
#include &quot;spdlog/mdc.h&quot;
void mdc_example()
{
    spdlog::mdc::put(&quot;key1&quot;, &quot;value1&quot;);
    spdlog::mdc::put(&quot;key2&quot;, &quot;value2&quot;);
    // if not using the default format, use the %&amp;amp; formatter to print mdc data
    // spdlog::set_pattern(&quot;[%H:%M:%S %z] [%^%L%$] [%&amp;amp;] %v&quot;);
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Benchmarks&lt;/h2&gt; 
&lt;p&gt;Below are some &lt;a href=&quot;https://raw.githubusercontent.com/gabime/spdlog/v1.x/bench/bench.cpp&quot;&gt;benchmarks&lt;/a&gt; done in Ubuntu 64 bit, Intel i7-4770 CPU @ 3.40GHz&lt;/p&gt; 
&lt;h4&gt;Synchronous mode&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;[info] **************************************************************
[info] Single thread, 1,000,000 iterations
[info] **************************************************************
[info] basic_st         Elapsed: 0.17 secs        5,777,626/sec
[info] rotating_st      Elapsed: 0.18 secs        5,475,894/sec
[info] daily_st         Elapsed: 0.20 secs        5,062,659/sec
[info] empty_logger     Elapsed: 0.07 secs       14,127,300/sec
[info] **************************************************************
[info] C-string (400 bytes). Single thread, 1,000,000 iterations
[info] **************************************************************
[info] basic_st         Elapsed: 0.41 secs        2,412,483/sec
[info] rotating_st      Elapsed: 0.72 secs        1,389,196/sec
[info] daily_st         Elapsed: 0.42 secs        2,393,298/sec
[info] null_st          Elapsed: 0.04 secs       27,446,957/sec
[info] **************************************************************
[info] 10 threads, competing over the same logger object, 1,000,000 iterations
[info] **************************************************************
[info] basic_mt         Elapsed: 0.60 secs        1,659,613/sec
[info] rotating_mt      Elapsed: 0.62 secs        1,612,493/sec
[info] daily_mt         Elapsed: 0.61 secs        1,638,305/sec
[info] null_mt          Elapsed: 0.16 secs        6,272,758/sec
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Asynchronous mode&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;[info] -------------------------------------------------
[info] Messages     : 1,000,000
[info] Threads      : 10
[info] Queue        : 8,192 slots
[info] Queue memory : 8,192 x 272 = 2,176 KB 
[info] -------------------------------------------------
[info] 
[info] *********************************
[info] Queue Overflow Policy: block
[info] *********************************
[info] Elapsed: 1.70784 secs     585,535/sec
[info] Elapsed: 1.69805 secs     588,910/sec
[info] Elapsed: 1.7026 secs      587,337/sec
[info] 
[info] *********************************
[info] Queue Overflow Policy: overrun
[info] *********************************
[info] Elapsed: 0.372816 secs    2,682,285/sec
[info] Elapsed: 0.379758 secs    2,633,255/sec
[info] Elapsed: 0.373532 secs    2,677,147/sec

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Documentation can be found in the &lt;a href=&quot;https://github.com/gabime/spdlog/wiki&quot;&gt;wiki&lt;/a&gt; pages.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Powered by&lt;/h3&gt; 
&lt;a href=&quot;https://jb.gg/OpenSource&quot;&gt; &lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/jetbrains.svg?sanitize=true&quot; alt=&quot;JetBrains logo&quot; width=&quot;200&quot; /&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>78/xiaozhi-esp32</title>
      <link>https://github.com/78/xiaozhi-esp32</link>
      <description>&lt;p&gt;An MCP-based chatbot | ‰∏Ä‰∏™Âü∫‰∫éMCPÁöÑËÅäÂ§©Êú∫Âô®‰∫∫&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;An MCP-based Chatbot | ‰∏Ä‰∏™Âü∫‰∫é MCP ÁöÑËÅäÂ§©Êú∫Âô®‰∫∫&lt;/h1&gt; 
&lt;p&gt;Ôºà‰∏≠Êñá | &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/README_en.md&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/README_ja.md&quot;&gt;Êó•Êú¨Ë™û&lt;/a&gt;Ôºâ&lt;/p&gt; 
&lt;h2&gt;ËßÜÈ¢ë&lt;/h2&gt; 
&lt;p&gt;üëâ &lt;a href=&quot;https://www.bilibili.com/video/BV1bpjgzKEhd/&quot;&gt;‰∫∫Á±ªÔºöÁªô AI Ë£ÖÊëÑÂÉèÂ§¥ vs AIÔºöÂΩìÂú∫ÂèëÁé∞‰∏ª‰∫∫‰∏âÂ§©Ê≤°Ê¥óÂ§¥„Äêbilibili„Äë&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href=&quot;https://www.bilibili.com/video/BV1XnmFYLEJN/&quot;&gt;ÊâãÂ∑•ÊâìÈÄ†‰Ω†ÁöÑ AI Â•≥ÂèãÔºåÊñ∞ÊâãÂÖ•Èó®ÊïôÁ®ã„Äêbilibili„Äë&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;‰ªãÁªç&lt;/h2&gt; 
&lt;p&gt;ËøôÊòØ‰∏Ä‰∏™Áî±ËôæÂì•ÂºÄÊ∫êÁöÑ ESP32 È°πÁõÆÔºå‰ª• MIT ËÆ∏ÂèØËØÅÂèëÂ∏ÉÔºåÂÖÅËÆ∏‰ªª‰Ωï‰∫∫ÂÖçË¥π‰ΩøÁî®ÔºåÊàñÁî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ&lt;/p&gt; 
&lt;p&gt;Êàë‰ª¨Â∏åÊúõÈÄöËøáËøô‰∏™È°πÁõÆÔºåËÉΩÂ§üÂ∏ÆÂä©Â§ßÂÆ∂‰∫ÜËß£ AI Á°¨‰ª∂ÂºÄÂèëÔºåÂ∞ÜÂΩì‰∏ãÈ£ûÈÄüÂèëÂ±ïÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂ∫îÁî®Âà∞ÂÆûÈôÖÁöÑÁ°¨‰ª∂ËÆæÂ§á‰∏≠„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†Êúâ‰ªª‰ΩïÊÉ≥Ê≥ïÊàñÂª∫ËÆÆÔºåËØ∑ÈöèÊó∂ÊèêÂá∫ Issues ÊàñÂä†ÂÖ• QQ Áæ§Ôºö1011329060&lt;/p&gt; 
&lt;h3&gt;Âü∫‰∫é MCP ÊéßÂà∂‰∏áÁâ©&lt;/h3&gt; 
&lt;p&gt;Â∞èÊô∫ AI ËÅäÂ§©Êú∫Âô®‰∫∫‰Ωú‰∏∫‰∏Ä‰∏™ËØ≠Èü≥‰∫§‰∫íÂÖ•Âè£ÔºåÂà©Áî® Qwen / DeepSeek Á≠âÂ§ßÊ®°ÂûãÁöÑ AI ËÉΩÂäõÔºåÈÄöËøá MCP ÂçèËÆÆÂÆûÁé∞Â§öÁ´ØÊéßÂà∂„ÄÇ&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mcp-based-graph.jpg&quot; alt=&quot;ÈÄöËøáMCPÊéßÂà∂‰∏áÁâ©&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;Â∑≤ÂÆûÁé∞ÂäüËÉΩ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Wi-Fi / ML307 Cat.1 4G&lt;/li&gt; 
 &lt;li&gt;Á¶ªÁ∫øËØ≠Èü≥Âî§ÈÜí &lt;a href=&quot;https://github.com/espressif/esp-sr&quot;&gt;ESP-SR&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅ‰∏§ÁßçÈÄö‰ø°ÂçèËÆÆÔºà&lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/websocket.md&quot;&gt;Websocket&lt;/a&gt; Êàñ MQTT+UDPÔºâ&lt;/li&gt; 
 &lt;li&gt;ÈááÁî® OPUS Èü≥È¢ëÁºñËß£Á†Å&lt;/li&gt; 
 &lt;li&gt;Âü∫‰∫éÊµÅÂºè ASR + LLM + TTS Êû∂ÊûÑÁöÑËØ≠Èü≥‰∫§‰∫í&lt;/li&gt; 
 &lt;li&gt;Â£∞Á∫πËØÜÂà´ÔºåËØÜÂà´ÂΩìÂâçËØ¥ËØù‰∫∫ÁöÑË∫´‰ªΩ &lt;a href=&quot;https://github.com/modelscope/3D-Speaker&quot;&gt;3D Speaker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;OLED / LCD ÊòæÁ§∫Â±èÔºåÊîØÊåÅË°®ÊÉÖÊòæÁ§∫&lt;/li&gt; 
 &lt;li&gt;ÁîµÈáèÊòæÁ§∫‰∏éÁîµÊ∫êÁÆ°ÁêÜ&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅÂ§öËØ≠Ë®ÄÔºà‰∏≠Êñá„ÄÅËã±Êñá„ÄÅÊó•ÊñáÔºâ&lt;/li&gt; 
 &lt;li&gt;ÊîØÊåÅ ESP32-C3„ÄÅESP32-S3„ÄÅESP32-P4 ËäØÁâáÂπ≥Âè∞&lt;/li&gt; 
 &lt;li&gt;ÈÄöËøáËÆæÂ§áÁ´Ø MCP ÂÆûÁé∞ËÆæÂ§áÊéßÂà∂ÔºàÈü≥Èáè„ÄÅÁÅØÂÖâ„ÄÅÁîµÊú∫„ÄÅGPIO Á≠âÔºâ&lt;/li&gt; 
 &lt;li&gt;ÈÄöËøá‰∫ëÁ´Ø MCP Êâ©Â±ïÂ§ßÊ®°ÂûãËÉΩÂäõÔºàÊô∫ËÉΩÂÆ∂Â±ÖÊéßÂà∂„ÄÅPCÊ°åÈù¢Êìç‰Ωú„ÄÅÁü•ËØÜÊêúÁ¥¢„ÄÅÈÇÆ‰ª∂Êî∂ÂèëÁ≠âÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Á°¨‰ª∂&lt;/h2&gt; 
&lt;h3&gt;Èù¢ÂåÖÊùøÊâãÂ∑•Âà∂‰ΩúÂÆûË∑µ&lt;/h3&gt; 
&lt;p&gt;ËØ¶ËßÅÈ£û‰π¶ÊñáÊ°£ÊïôÁ®ãÔºö&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href=&quot;https://ccnphfhqs21z.feishu.cn/wiki/F5krwD16viZoF0kKkvDcrZNYnhb?from=from_copylink&quot;&gt;„ÄäÂ∞èÊô∫ AI ËÅäÂ§©Êú∫Âô®‰∫∫ÁôæÁßëÂÖ®‰π¶„Äã&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Èù¢ÂåÖÊùøÊïàÊûúÂõæÂ¶Ç‰∏ãÔºö&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/wiring2.jpg&quot; alt=&quot;Èù¢ÂåÖÊùøÊïàÊûúÂõæ&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;ÊîØÊåÅ 70 Â§ö‰∏™ÂºÄÊ∫êÁ°¨‰ª∂Ôºà‰ªÖÂ±ïÁ§∫ÈÉ®ÂàÜÔºâ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://oshwhub.com/li-chuang-kai-fa-ban/li-chuang-shi-zhan-pai-esp32-s3-kai-fa-ban&quot; target=&quot;_blank&quot; title=&quot;Á´ãÂàõ¬∑ÂÆûÊàòÊ¥æ ESP32-S3 ÂºÄÂèëÊùø&quot;&gt;Á´ãÂàõ¬∑ÂÆûÊàòÊ¥æ ESP32-S3 ÂºÄÂèëÊùø&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/espressif/esp-box&quot; target=&quot;_blank&quot; title=&quot;‰πêÈë´ ESP32-S3-BOX3&quot;&gt;‰πêÈë´ ESP32-S3-BOX3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.m5stack.com/zh_CN/core/CoreS3&quot; target=&quot;_blank&quot; title=&quot;M5Stack CoreS3&quot;&gt;M5Stack CoreS3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.m5stack.com/en/atom/Atomic%20Echo%20Base&quot; target=&quot;_blank&quot; title=&quot;AtomS3R + Echo Base&quot;&gt;M5Stack AtomS3R + Echo Base&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gf.bilibili.com/item/detail/1108782064&quot; target=&quot;_blank&quot; title=&quot;Á•ûÂ•áÊåâÈíÆ 2.4&quot;&gt;Á•ûÂ•áÊåâÈíÆ 2.4&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.waveshare.net/shop/ESP32-S3-Touch-AMOLED-1.8.htm&quot; target=&quot;_blank&quot; title=&quot;ÂæÆÈõ™ÁîµÂ≠ê ESP32-S3-Touch-AMOLED-1.8&quot;&gt;ÂæÆÈõ™ÁîµÂ≠ê ESP32-S3-Touch-AMOLED-1.8&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Xinyuan-LilyGO/T-Circle-S3&quot; target=&quot;_blank&quot; title=&quot;LILYGO T-Circle-S3&quot;&gt;LILYGO T-Circle-S3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://oshwhub.com/tenclass01/xmini_c3&quot; target=&quot;_blank&quot; title=&quot;ËôæÂì• Mini C3&quot;&gt;ËôæÂì• Mini C3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://oshwhub.com/movecall/cuican-ai-pendant-lights-up-y&quot; target=&quot;_blank&quot; title=&quot;Movecall CuiCan ESP32S3&quot;&gt;ÁíÄÁí®¬∑AI ÂêäÂù†&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/WMnologo/xingzhi-ai&quot; target=&quot;_blank&quot; title=&quot;Êó†ÂêçÁßëÊäÄNologo-ÊòüÊô∫-1.54&quot;&gt;Êó†ÂêçÁßëÊäÄ Nologo-ÊòüÊô∫-1.54TFT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.seeedstudio.com/SenseCAP-Watcher-W1-A-p-5979.html&quot; target=&quot;_blank&quot; title=&quot;SenseCAP Watcher&quot;&gt;SenseCAP Watcher&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1BHJtz6E2S/&quot; target=&quot;_blank&quot; title=&quot;ESP-HI Ë∂Ö‰ΩéÊàêÊú¨Êú∫Âô®Áãó&quot;&gt;ESP-HI Ë∂Ö‰ΩéÊàêÊú¨Êú∫Âô®Áãó&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div style=&quot;display: flex; justify-content: space-between;&quot;&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lichuang-s3.jpg&quot; target=&quot;_blank&quot; title=&quot;Á´ãÂàõ¬∑ÂÆûÊàòÊ¥æ ESP32-S3 ÂºÄÂèëÊùø&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lichuang-s3.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/espbox3.jpg&quot; target=&quot;_blank&quot; title=&quot;‰πêÈë´ ESP32-S3-BOX3&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/espbox3.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/m5cores3.jpg&quot; target=&quot;_blank&quot; title=&quot;M5Stack CoreS3&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/m5cores3.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/atoms3r.jpg&quot; target=&quot;_blank&quot; title=&quot;AtomS3R + Echo Base&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/atoms3r.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/magiclick.jpg&quot; target=&quot;_blank&quot; title=&quot;Á•ûÂ•áÊåâÈíÆ 2.4&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/magiclick.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/waveshare.jpg&quot; target=&quot;_blank&quot; title=&quot;ÂæÆÈõ™ÁîµÂ≠ê ESP32-S3-Touch-AMOLED-1.8&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/waveshare.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lilygo-t-circle-s3.jpg&quot; target=&quot;_blank&quot; title=&quot;LILYGO T-Circle-S3&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/lilygo-t-circle-s3.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/xmini-c3.jpg&quot; target=&quot;_blank&quot; title=&quot;ËôæÂì• Mini C3&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/xmini-c3.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/movecall-cuican-esp32s3.jpg&quot; target=&quot;_blank&quot; title=&quot;CuiCan&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/movecall-cuican-esp32s3.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/wmnologo_xingzhi_1.54.jpg&quot; target=&quot;_blank&quot; title=&quot;Êó†ÂêçÁßëÊäÄNologo-ÊòüÊô∫-1.54&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/wmnologo_xingzhi_1.54.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/sensecap_watcher.jpg&quot; target=&quot;_blank&quot; title=&quot;SenseCAP Watcher&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/sensecap_watcher.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/esp-hi.jpg&quot; target=&quot;_blank&quot; title=&quot;ESP-HI Ë∂Ö‰ΩéÊàêÊú¨Êú∫Âô®Áãó&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/v1/esp-hi.jpg&quot; width=&quot;240&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;ËΩØ‰ª∂&lt;/h2&gt; 
&lt;h3&gt;Âõ∫‰ª∂ÁÉßÂΩï&lt;/h3&gt; 
&lt;p&gt;Êñ∞ÊâãÁ¨¨‰∏ÄÊ¨°Êìç‰ΩúÂª∫ËÆÆÂÖà‰∏çË¶ÅÊê≠Âª∫ÂºÄÂèëÁéØÂ¢ÉÔºåÁõ¥Êé•‰ΩøÁî®ÂÖçÂºÄÂèëÁéØÂ¢ÉÁÉßÂΩïÁöÑÂõ∫‰ª∂„ÄÇ&lt;/p&gt; 
&lt;p&gt;Âõ∫‰ª∂ÈªòËÆ§Êé•ÂÖ• &lt;a href=&quot;https://xiaozhi.me&quot;&gt;xiaozhi.me&lt;/a&gt; ÂÆòÊñπÊúçÂä°Âô®Ôºå‰∏™‰∫∫Áî®Êà∑Ê≥®ÂÜåË¥¶Âè∑ÂèØ‰ª•ÂÖçË¥π‰ΩøÁî® Qwen ÂÆûÊó∂Ê®°Âûã„ÄÇ&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href=&quot;https://ccnphfhqs21z.feishu.cn/wiki/Zpz4wXBtdimBrLk25WdcXzxcnNS&quot;&gt;Êñ∞ÊâãÁÉßÂΩïÂõ∫‰ª∂ÊïôÁ®ã&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;ÂºÄÂèëÁéØÂ¢É&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cursor Êàñ VSCode&lt;/li&gt; 
 &lt;li&gt;ÂÆâË£Ö ESP-IDF Êèí‰ª∂ÔºåÈÄâÊã© SDK ÁâàÊú¨ 5.4 Êàñ‰ª•‰∏ä&lt;/li&gt; 
 &lt;li&gt;Linux ÊØî Windows Êõ¥Â•ΩÔºåÁºñËØëÈÄüÂ∫¶Âø´Ôºå‰πüÂÖçÂéªÈ©±Âä®ÈóÆÈ¢òÁöÑÂõ∞Êâ∞&lt;/li&gt; 
 &lt;li&gt;Êú¨È°πÁõÆ‰ΩøÁî® Google C++ ‰ª£Á†ÅÈ£éÊ†ºÔºåÊèê‰∫§‰ª£Á†ÅÊó∂ËØ∑Á°Æ‰øùÁ¨¶ÂêàËßÑËåÉ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂºÄÂèëËÄÖÊñáÊ°£&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/main/boards/README.md&quot;&gt;Ëá™ÂÆö‰πâÂºÄÂèëÊùøÊåáÂçó&lt;/a&gt; - Â≠¶‰π†Â¶Ç‰Ωï‰∏∫Â∞èÊô∫ AI ÂàõÂª∫Ëá™ÂÆö‰πâÂºÄÂèëÊùø&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mcp-usage.md&quot;&gt;MCP ÂçèËÆÆÁâ©ËÅîÁΩëÊéßÂà∂Áî®Ê≥ïËØ¥Êòé&lt;/a&gt; - ‰∫ÜËß£Â¶Ç‰ΩïÈÄöËøá MCP ÂçèËÆÆÊéßÂà∂Áâ©ËÅîÁΩëËÆæÂ§á&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mcp-protocol.md&quot;&gt;MCP ÂçèËÆÆ‰∫§‰∫íÊµÅÁ®ã&lt;/a&gt; - ËÆæÂ§áÁ´Ø MCP ÂçèËÆÆÁöÑÂÆûÁé∞ÊñπÂºè&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/mqtt-udp.md&quot;&gt;MQTT + UDP Ê∑∑ÂêàÈÄö‰ø°ÂçèËÆÆÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/78/xiaozhi-esp32/main/docs/websocket.md&quot;&gt;‰∏Ä‰ªΩËØ¶ÁªÜÁöÑ WebSocket ÈÄö‰ø°ÂçèËÆÆÊñáÊ°£&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Â§ßÊ®°ÂûãÈÖçÁΩÆ&lt;/h2&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†Â∑≤ÁªèÊã•Êúâ‰∏Ä‰∏™Â∞èÊô∫ AI ËÅäÂ§©Êú∫Âô®‰∫∫ËÆæÂ§áÔºåÂπ∂‰∏îÂ∑≤Êé•ÂÖ•ÂÆòÊñπÊúçÂä°Âô®ÔºåÂèØ‰ª•ÁôªÂΩï &lt;a href=&quot;https://xiaozhi.me&quot;&gt;xiaozhi.me&lt;/a&gt; ÊéßÂà∂Âè∞ËøõË°åÈÖçÁΩÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;üëâ &lt;a href=&quot;https://www.bilibili.com/video/BV1jUCUY2EKM/&quot;&gt;ÂêéÂè∞Êìç‰ΩúËßÜÈ¢ëÊïôÁ®ãÔºàÊóßÁâàÁïåÈù¢Ôºâ&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Áõ∏ÂÖ≥ÂºÄÊ∫êÈ°πÁõÆ&lt;/h2&gt; 
&lt;p&gt;Âú®‰∏™‰∫∫ÁîµËÑë‰∏äÈÉ®ÁΩ≤ÊúçÂä°Âô®ÔºåÂèØ‰ª•ÂèÇËÄÉ‰ª•‰∏ãÁ¨¨‰∏âÊñπÂºÄÊ∫êÁöÑÈ°πÁõÆÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server&quot;&gt;xinnan-tech/xiaozhi-esp32-server&lt;/a&gt; Python ÊúçÂä°Âô®&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joey-zhou/xiaozhi-esp32-server-java&quot;&gt;joey-zhou/xiaozhi-esp32-server-java&lt;/a&gt; Java ÊúçÂä°Âô®&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AnimeAIChat/xiaozhi-server-go&quot;&gt;AnimeAIChat/xiaozhi-server-go&lt;/a&gt; Golang ÊúçÂä°Âô®&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‰ΩøÁî®Â∞èÊô∫ÈÄö‰ø°ÂçèËÆÆÁöÑÁ¨¨‰∏âÊñπÂÆ¢Êà∑Á´ØÈ°πÁõÆÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huangjunsen0406/py-xiaozhi&quot;&gt;huangjunsen0406/py-xiaozhi&lt;/a&gt; Python ÂÆ¢Êà∑Á´Ø&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TOM88812/xiaozhi-android-client&quot;&gt;TOM88812/xiaozhi-android-client&lt;/a&gt; Android ÂÆ¢Êà∑Á´Ø&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://github.com/100askTeam/xiaozhi-linux&quot;&gt;100askTeam/xiaozhi-linux&lt;/a&gt; ÁôæÈóÆÁßëÊäÄÊèê‰æõÁöÑ Linux ÂÆ¢Êà∑Á´Ø&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/78/xiaozhi-sf32&quot;&gt;78/xiaozhi-sf32&lt;/a&gt; ÊÄùÊæàÁßëÊäÄÁöÑËìùÁâôËäØÁâáÂõ∫‰ª∂&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/QuecPython/solution-xiaozhiAI&quot;&gt;QuecPython/solution-xiaozhiAI&lt;/a&gt; ÁßªËøúÊèê‰æõÁöÑ QuecPython Âõ∫‰ª∂&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;a href=&quot;https://star-history.com/#78/xiaozhi-esp32&amp;amp;Date&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=78/xiaozhi-esp32&amp;amp;type=Date&amp;amp;theme=dark&quot; /&gt; 
  &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=78/xiaozhi-esp32&amp;amp;type=Date&quot; /&gt; 
  &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=78/xiaozhi-esp32&amp;amp;type=Date&quot; /&gt; 
 &lt;/picture&gt; &lt;/a&gt;</description>
    </item>
    
    <item>
      <title>grpc/grpc</title>
      <link>https://github.com/grpc/grpc</link>
      <description>&lt;p&gt;C++ based gRPC (C++, Python, Ruby, Objective-C, PHP, C#)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gRPC ‚Äì An RPC library and framework&lt;/h1&gt; 
&lt;p&gt;gRPC is a modern, open source, high-performance remote procedure call (RPC) framework that can run anywhere. gRPC enables client and server applications to communicate transparently, and simplifies the building of connected systems.&lt;/p&gt; 
&lt;table&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Homepage:&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://grpc.io/&quot;&gt;grpc.io&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;Mailing List:&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/grpc-io&quot;&gt;grpc-io@googlegroups.com&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;https://gitter.im/grpc/grpc?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/grpc/grpc.svg?sanitize=true&quot; alt=&quot;Join the chat at https://gitter.im/grpc/grpc&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;To start using gRPC&lt;/h2&gt; 
&lt;p&gt;To maximize usability, gRPC supports the standard method for adding dependencies to a user&#39;s chosen language (if there is one). In most languages, the gRPC runtime comes as a package available in a user&#39;s language package manager.&lt;/p&gt; 
&lt;p&gt;For instructions on how to use the language-specific gRPC runtime for a project, please refer to these documents&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/cpp&quot;&gt;C++&lt;/a&gt;: follow the instructions under the &lt;code&gt;src/cpp&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-dotnet&quot;&gt;C#/.NET&lt;/a&gt;: NuGet packages &lt;code&gt;Grpc.Net.Client&lt;/code&gt;, &lt;code&gt;Grpc.AspNetCore.Server&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-dart&quot;&gt;Dart&lt;/a&gt;: pub package &lt;code&gt;grpc&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-go&quot;&gt;Go&lt;/a&gt;: &lt;code&gt;go get google.golang.org/grpc&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-java&quot;&gt;Java&lt;/a&gt;: Use JARs from Maven Central Repository&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-kotlin&quot;&gt;Kotlin&lt;/a&gt;: Use JARs from Maven Central Repository&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-node&quot;&gt;Node&lt;/a&gt;: &lt;code&gt;npm install @grpc/grpc-js&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/objective-c&quot;&gt;Objective-C&lt;/a&gt;: Add &lt;code&gt;gRPC-ProtoRPC&lt;/code&gt; dependency to podspec&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/php&quot;&gt;PHP&lt;/a&gt;: &lt;code&gt;pecl install grpc&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/python/grpcio&quot;&gt;Python&lt;/a&gt;: &lt;code&gt;pip install grpcio&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/ruby&quot;&gt;Ruby&lt;/a&gt;: &lt;code&gt;gem install grpc&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/grpc/grpc-web&quot;&gt;WebJS&lt;/a&gt;: follow the grpc-web instructions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Per-language quickstart guides and tutorials can be found in the &lt;a href=&quot;https://grpc.io/docs/&quot;&gt;documentation section on the grpc.io website&lt;/a&gt;. Code examples are available in the &lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/examples&quot;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;p&gt;Precompiled bleeding-edge package builds of gRPC &lt;code&gt;master&lt;/code&gt; branch&#39;s &lt;code&gt;HEAD&lt;/code&gt; are uploaded daily to &lt;a href=&quot;https://packages.grpc.io&quot;&gt;packages.grpc.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;To start developing gRPC&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome!&lt;/p&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/CONTRIBUTING.md&quot;&gt;How to contribute&lt;/a&gt; which will guide you through the entire workflow of how to build the source code, how to run the tests, and how to contribute changes to the gRPC codebase. The &quot;How to contribute&quot; document also contains info on how the contribution process works and contains best practices for creating contributions.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;Sometimes things go wrong. Please check out the &lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/TROUBLESHOOTING.md&quot;&gt;Troubleshooting guide&lt;/a&gt; if you are experiencing issues with gRPC.&lt;/p&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://grafana-dot-grpc-testing.appspot.com/&quot;&gt;Performance dashboard&lt;/a&gt; for performance numbers of master branch daily builds.&lt;/p&gt; 
&lt;h2&gt;Concepts&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/CONCEPTS.md&quot;&gt;gRPC Concepts&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;About This Repository&lt;/h2&gt; 
&lt;p&gt;This repository contains source code for gRPC libraries implemented in multiple languages written on top of a shared C++ core library &lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/core&quot;&gt;src/core&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Libraries in different languages may be in various states of development. We are seeking contributions for all of these libraries:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Shared C++ [core library]&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/core&quot;&gt;src/core&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C++&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/cpp&quot;&gt;src/cpp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ruby&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/ruby&quot;&gt;src/ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/python&quot;&gt;src/python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/php&quot;&gt;src/php&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C# (core library based)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/csharp&quot;&gt;src/csharp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Objective-C&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/grpc/grpc/master/src/objective-c&quot;&gt;src/objective-c&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Source repo&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-java&quot;&gt;grpc-java&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kotlin&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-kotlin&quot;&gt;grpc-kotlin&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-go&quot;&gt;grpc-go&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NodeJS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-node&quot;&gt;grpc-node&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WebJS&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-web&quot;&gt;grpc-web&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dart&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-dart&quot;&gt;grpc-dart&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.NET (pure C# impl.)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-dotnet&quot;&gt;grpc-dotnet&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Swift&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/grpc/grpc-swift&quot;&gt;grpc-swift&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>apache/brpc</title>
      <link>https://github.com/apache/brpc</link>
      <description>&lt;p&gt;brpc is an Industrial-grade RPC framework using C++ Language, which is often used in high performance system such as Search, Storage, Machine learning, Advertisement, Recommendation etc. &quot;brpc&quot; means &quot;better RPC&quot;.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/README_cn.md&quot;&gt;‰∏≠ÊñáÁâà&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/apache/brpc/actions/workflows/ci-linux.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/brpc/actions/workflows/ci-linux.yml/badge.svg?sanitize=true&quot; alt=&quot;Linux Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/apache/brpc/actions/workflows/ci-macos.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/brpc/actions/workflows/ci-macos.yml/badge.svg?sanitize=true&quot; alt=&quot;MacOs Build Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/images/logo.png#gh-light-mode-only&quot; alt=&quot;brpc logo (light)&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/images/logo-white.png#gh-dark-mode-only&quot; alt=&quot;brpc logo (dark)&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://brpc.apache.org/&quot;&gt;bRPC&lt;/a&gt; is an Industrial-grade RPC framework using C++ Language, which is often used in high performance system such as Search, Storage, Machine learning, Advertisement, Recommendation etc.&lt;/p&gt; 
&lt;h3&gt;&quot;bRPC&quot; means &quot;better RPC&quot;.&lt;/h3&gt; 
&lt;p&gt;You can use it to:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Build a server that can talk in multiple protocols (&lt;strong&gt;on same port&lt;/strong&gt;), or access all sorts of services 
  &lt;ul&gt; 
   &lt;li&gt;restful http/https, &lt;a href=&quot;https://httpwg.org/specs/rfc9113.html&quot;&gt;h2&lt;/a&gt;/&lt;a href=&quot;https://grpc.io&quot;&gt;gRPC&lt;/a&gt;. using http/h2 in bRPC is much more friendly than &lt;a href=&quot;https://curl.haxx.se/libcurl/&quot;&gt;libcurl&lt;/a&gt;. Access protobuf-based protocols with HTTP/h2+json, probably from another language.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/redis_client.md&quot;&gt;redis&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/memcache_client.md&quot;&gt;memcached&lt;/a&gt;, thread-safe, more friendly and performant than the official clients.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/apache/brpc/raw/master/src/brpc/rtmp.h&quot;&gt;rtmp&lt;/a&gt;/&lt;a href=&quot;https://en.wikipedia.org/wiki/Flash_Video&quot;&gt;flv&lt;/a&gt;/&lt;a href=&quot;https://en.wikipedia.org/wiki/HTTP_Live_Streaming&quot;&gt;hls&lt;/a&gt;, for building &lt;a href=&quot;https://github.com/brpc/media-server&quot;&gt;streaming services&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;hadoop_rpc (may be opensourced)&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Remote_direct_memory_access&quot;&gt;rdma&lt;/a&gt; support&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/thrift.md&quot;&gt;thrift&lt;/a&gt; support, thread-safe, more friendly and performant than the official clients.&lt;/li&gt; 
   &lt;li&gt;all sorts of protocols used in Baidu: &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/baidu_std.md&quot;&gt;baidu_std&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/streaming_rpc.md&quot;&gt;streaming_rpc&lt;/a&gt;, hulu_pbrpc, &lt;a href=&quot;https://github.com/baidu/sofa-pbrpc&quot;&gt;sofa_pbrpc&lt;/a&gt;, nova_pbrpc, public_pbrpc, ubrpc and nshead-based ones.&lt;/li&gt; 
   &lt;li&gt;Build &lt;a href=&quot;https://en.wikipedia.org/wiki/High_availability&quot;&gt;HA&lt;/a&gt; distributed services using an industrial-grade implementation of &lt;a href=&quot;https://raft.github.io&quot;&gt;RAFT consensus algorithm&lt;/a&gt; which is opensourced at &lt;a href=&quot;https://github.com/brpc/braft&quot;&gt;braft&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Servers can handle requests &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/server.md&quot;&gt;synchronously&lt;/a&gt; or &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/server.md#asynchronous-service&quot;&gt;asynchronously&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Clients can access servers &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/client.md#synchronus-call&quot;&gt;synchronously&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/client.md#asynchronous-call&quot;&gt;asynchronously&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/client.md#semi-synchronous-call&quot;&gt;semi-synchronously&lt;/a&gt;, or use &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/combo_channel.md&quot;&gt;combo channels&lt;/a&gt; to simplify sharded or parallel accesses declaratively.&lt;/li&gt; 
 &lt;li&gt;Debug services &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/builtin_service.md&quot;&gt;via http&lt;/a&gt;, and run &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/cpu_profiler.md&quot;&gt;cpu&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/heap_profiler.md&quot;&gt;heap&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/contention_profiler.md&quot;&gt;contention&lt;/a&gt; profilers.&lt;/li&gt; 
 &lt;li&gt;Get &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/overview.md#better-latency-and-throughput&quot;&gt;better latency and throughput&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/new_protocol.md&quot;&gt;Extend bRPC&lt;/a&gt; with the protocols used in your organization quickly, or customize components, including &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/load_balancing.md#%E5%91%BD%E5%90%8D%E6%9C%8D%E5%8A%A1&quot;&gt;naming services&lt;/a&gt; (dns, zk, etcd), &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/load_balancing.md#%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1&quot;&gt;load balancers&lt;/a&gt; (rr, random, consistent hashing)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try it!&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/overview.md&quot;&gt;overview&lt;/a&gt; to know where bRPC can be used and its advantages.&lt;/li&gt; 
 &lt;li&gt;Read &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/getting_started.md&quot;&gt;getting started&lt;/a&gt; for building steps and play with &lt;a href=&quot;https://github.com/apache/brpc/tree/master/example/&quot;&gt;examples&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Docs: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/benchmark.md&quot;&gt;Performance benchmark&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/bvar.md&quot;&gt;bvar&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/bvar_c++.md&quot;&gt;bvar_c++&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/bthread.md&quot;&gt;bthread&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/bthread_or_not.md&quot;&gt;bthread or not&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/thread_local.md&quot;&gt;thread-local&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/execution_queue.md&quot;&gt;Execution Queue&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Client 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/client.md&quot;&gt;Basics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/error_code.md&quot;&gt;Error code&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/combo_channel.md&quot;&gt;Combo channels&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/http_client.md&quot;&gt;Access http/h2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/http_derivatives.md#h2grpc&quot;&gt;Access gRPC&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/thrift.md#client-accesses-thrift-server&quot;&gt;Access thrift&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/ub_client.md&quot;&gt;Access UB&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/streaming_rpc.md&quot;&gt;Streaming RPC&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/redis_client.md&quot;&gt;Access redis&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/memcache_client.md&quot;&gt;Access memcached&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/backup_request.md&quot;&gt;Backup request&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/dummy_server.md&quot;&gt;Dummy server&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Server 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/server.md&quot;&gt;Basics&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/http_service.md&quot;&gt;Serve http/h2&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/http_derivatives.md#h2grpc&quot;&gt;Serve gRPC&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/thrift.md#server-processes-thrift-requests&quot;&gt;Serve thrift&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/nshead_service.md&quot;&gt;Serve Nshead&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/server_debugging.md&quot;&gt;Debug server issues&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/server_push.md&quot;&gt;Server push&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/avalanche.md&quot;&gt;Avalanche&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/auto_concurrency_limiter.md&quot;&gt;Auto ConcurrencyLimiter&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://github.com/brpc/media-server&quot;&gt;Media Server&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/json2pb.md&quot;&gt;json2pb&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/builtin_service.md&quot;&gt;Builtin Services&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/status.md&quot;&gt;status&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/vars.md&quot;&gt;vars&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/connections.md&quot;&gt;connections&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/flags.md&quot;&gt;flags&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/rpcz.md&quot;&gt;rpcz&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/cpu_profiler.md&quot;&gt;cpu_profiler&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/heap_profiler.md&quot;&gt;heap_profiler&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/contention_profiler.md&quot;&gt;contention_profiler&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Tools 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/rpc_press.md&quot;&gt;rpc_press&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/rpc_replay.md&quot;&gt;rpc_replay&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/rpc_view.md&quot;&gt;rpc_view&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/benchmark_http.md&quot;&gt;benchmark_http&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/parallel_http.md&quot;&gt;parallel_http&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Others 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/iobuf.md&quot;&gt;IOBuf&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/streaming_log.md&quot;&gt;Streaming Log&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/flatmap.md&quot;&gt;FlatMap&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/brpc_intro.pptx&quot;&gt;bRPC introduction&lt;/a&gt;(training material)&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/tutorial_on_building_services.pptx&quot;&gt;A tutorial on building large-scale services&lt;/a&gt;(training material)&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/brpc_internal.pptx&quot;&gt;bRPC internal&lt;/a&gt;(training material)&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;RPC in depth 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/new_protocol.md&quot;&gt;New Protocol&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/atomic_instructions.md&quot;&gt;Atomic instructions&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/io.md&quot;&gt;IO&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/en/threading_overview.md&quot;&gt;Threading Overview&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/load_balancing.md&quot;&gt;Load Balancing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/lalb.md&quot;&gt;Locality-aware&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/consistent_hashing.md&quot;&gt;Consistent Hashing&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/memory_management.md&quot;&gt;Memory Management&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/timer_keeping.md&quot;&gt;Timer keeping&lt;/a&gt;&lt;/li&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/docs/cn/bthread_id.md&quot;&gt;bthread_id&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;Use cases 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/community/cases.md&quot;&gt;User cases&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contribute code&lt;/h1&gt; 
&lt;p&gt;Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/apache/brpc/master/CONTRIBUTING.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Feedback and Getting involved&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Report bugs, ask questions or give suggestions by &lt;a href=&quot;https://github.com/apache/brpc/issues&quot;&gt;Github Issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Subscribe to the mailing list(&lt;a href=&quot;mailto:dev-subscribe@brpc.apache.org&quot;&gt;dev-subscribe@brpc.apache.org&lt;/a&gt;) to get updated with the project&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Code of Conduct&lt;/h1&gt; 
&lt;p&gt;We follow the code of conduct from Apache Software Foundation, please refer it here &lt;a href=&quot;https://www.apache.org/foundation/policies/conduct&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>capnproto/capnproto</title>
      <link>https://github.com/capnproto/capnproto</link>
      <description>&lt;p&gt;Cap&#39;n Proto serialization/RPC system - core tools and C++ library&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;THIS IS THE V2 DEVELOPMENT BRANCH&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;On this branch, we may make breaking changes to the API at any time. Do not use this branch if you want stability. If you want &quot;1.0 plus bug fixes&quot;, use the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt; 
&lt;p&gt;For more, see the 1.0 release announcement:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://capnproto.org/news/2023-07-28-capnproto-1.0.html&quot;&gt;https://capnproto.org/news/2023-07-28-capnproto-1.0.html&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;img src=&quot;http://kentonv.github.io/capnproto/images/infinity-times-faster.png&quot; style=&quot;width:334px; height:306px; float: right;&quot; /&gt; 
&lt;p&gt;Cap&#39;n Proto is an insanely fast data interchange format and capability-based RPC system. Think JSON, except binary. Or think &lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;, except faster. In fact, in benchmarks, Cap&#39;n Proto is INFINITY TIMES faster than Protocol Buffers.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://kentonv.github.io/capnproto/&quot;&gt;Read more...&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/cutlass</title>
      <link>https://github.com/NVIDIA/cutlass</link>
      <description>&lt;p&gt;CUDA Templates for Linear Algebra Subroutines&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/images/gemm-hierarchy-with-epilogue-no-labels.png&quot; alt=&quot;ALT&quot; title=&quot;Complete CUDA GEMM decomposition&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Overview&lt;/h1&gt; 
&lt;h1&gt;CUTLASS 4.2.1&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;CUTLASS 4.2.1 - Sept 2025&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;CUTLASS is a collection of abstractions for implementing high-performance matrix-matrix multiplication (GEMM) and related computations at all levels and scales within CUDA. It incorporates strategies for hierarchical decomposition and data movement. CUTLASS decomposes these &quot;moving parts&quot; into reusable, modular software components and abstractions.&lt;/p&gt; 
&lt;p&gt;Primitives for different levels of a conceptual parallelization hierarchy can be specialized and tuned via custom tiling sizes, data types, and other algorithmic policy. The resulting flexibility simplifies their use as building blocks within custom kernels and applications.&lt;/p&gt; 
&lt;p&gt;CUTLASS has been providing CUDA C++ template abstractions for high-performance linear algebra since 2017 and these abstractions provide extensive support for a wide range of computations including mixed-precision computations, specialized data-movement (async copy) and multiply-accumulate abstractions for FP64, FP32, TF32, FP16, BF16, &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm&quot;&gt;FP32 emulation via tensor core instruction&lt;/a&gt;, 8b floating point types (e5m2 and e4m3), block scaled data types (NVIDIA NVFP4 and OCP standard MXFP4, MXFP6, MXFP8), narrow integer types (4 and 8b signed and unsigned integers), and binary 1b data types (where architectures allow for the native support of such data types) across NVIDIA&#39;s Volta, Turing, Ampere, Ada, Hopper, and Blackwell architectures.&lt;/p&gt; 
&lt;p&gt;To this rich ecosystem of C++ based kernel programming abstractions, CUTLASS 4 adds CUTLASS DSLs. These are Python native interfaces for writing high-performance CUDA kernels based on core CUTLASS and CuTe concepts without any performance compromises. This allows for a much smoother learning curve, orders of magnitude faster compile times, native integration with DL frameworks without writing glue code, and much more intuitive metaprogramming that does not require deep C++ expertise.&lt;/p&gt; 
&lt;p&gt;Overall we envision CUTLASS DSLs as a family of domain-specific languages (DSLs). With the release of 4.0, we are releasing the first of these in CuTe DSL. This is a low level programming model that is fully consistent with CuTe C++ abstractions -- exposing core concepts such as layouts, tensors, hardware atoms, and full control over the hardware thread and data hierarchy.&lt;/p&gt; 
&lt;p&gt;CuTe DSL demonstrates optimal matrix multiply and other linear algebra operations targeting the programmable, high-throughput &lt;em&gt;Tensor Cores&lt;/em&gt; implemented by NVIDIA&#39;s Ampere, Hopper, and Blackwell architectures.&lt;/p&gt; 
&lt;p&gt;We believe it will become an indispensable tool for students, researchers, and performance engineers alike -- flattening the learning curve of GPU programming, rapidly prototyping kernel designs, and bringing optimized solutions into production.&lt;/p&gt; 
&lt;p&gt;CuTe DSL is currently in public beta and will graduate out of beta by end of summer 2025.&lt;/p&gt; 
&lt;p&gt;To get started quickly - please refer :&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/quickstart.html&quot;&gt;CUTLASS C++ Quick Start Guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/pythonDSL/quick_start.html&quot;&gt;CuTe DSL Quick Start Guide&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;What&#39;s New in CUTLASS 4.2&lt;/h1&gt; 
&lt;h2&gt;CuTe DSL&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;More Python versions are now supported for both x86-64 and aarch64, including 
  &lt;ul&gt; 
   &lt;li&gt;Python 3.10, 3.11, 3.12, and 3.13&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Added new example and updated notebook to get started with CuTe DSL 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/python/CuTeDSL/ampere/call_bypass_dlpack.py&quot;&gt;Call kernels with dlpack bypassed&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Updates on &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/python/CuTeDSL/notebooks/tensorssa.ipynb&quot;&gt;TensorSSA demonstration&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;Added a section for introducing the broadcast&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;API updates 
  &lt;ul&gt; 
   &lt;li&gt;Please refer to &lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/pythonDSL/cute_dsl_api/changelog.html&quot;&gt;DSL API changelog&lt;/a&gt; for details&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Bug fixings and improvements 
  &lt;ul&gt; 
   &lt;li&gt;Fixed &lt;code&gt;cute.print_tensor&lt;/code&gt; for coordinate tensor&lt;/li&gt; 
   &lt;li&gt;Fixed &lt;code&gt;cute.print&lt;/code&gt; for tuple of layouts&lt;/li&gt; 
   &lt;li&gt;Fixed frozen object is not properly updated after fully assigned in dynamic control flow&lt;/li&gt; 
   &lt;li&gt;Fixed assign tuple/list element in a dynamic control flow may cause compilation failure&lt;/li&gt; 
   &lt;li&gt;Improved error message when CUDA context is not initialized&lt;/li&gt; 
   &lt;li&gt;Improved docstring of congruent and weakly_congruent&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;CUTLASS C++&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Support for Blackwell SM103 kernels for B300 GPUs. 
  &lt;ul&gt; 
   &lt;li&gt;Collective mainloop codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/collective/sm103_blockscaled_mma_warpspecialized.hpp&quot;&gt;Blockscaled datatypes with support for dense GEMM mainloop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;New &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/dispatch_policy.hpp&quot;&gt;GEMM&lt;/a&gt; and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/epilogue/dispatch_policy.hpp&quot;&gt;epilogue&lt;/a&gt; dispatch policies for collectives, kernel layers, and builders.&lt;/li&gt; 
   &lt;li&gt;Kernel codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/kernel/sm103_blockscaled_gemm_tma_warpspecialized.hpp&quot;&gt;Blockscaled datatypes with support for dense GEMM kernel&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Set of examples that demonstrate the usage of the 3.x API for targeting Blackwell SM103 architecture: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/89_sm103_fp4_ultra_gemm/&quot;&gt;Blockscaled ultra fp4 dense GEMM&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/90_sm103_fp4_ultra_grouped_gemm&quot;&gt;Blockscaled ultra fp4 dense grouped GEMM&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Set of unit tests that demonstrate the usage of Blackwell SM103 blockscaled GEMM 
  &lt;ul&gt; 
   &lt;li&gt;Unit test files with prefix name of &lt;code&gt;sm103_&lt;/code&gt; under &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/&quot;&gt;GEMM device unit tests&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Blackwell SM121 kernels for DGX Spark GPUs. 
  &lt;ul&gt; 
   &lt;li&gt;Share the major codes with Blackwell SM120 kernels.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Add support for heuristics-based kernel filtering and autotuning using &lt;code&gt;nvidia-matmul-heuristics&lt;/code&gt; to find the best kernels for a given scenario. 
  &lt;ul&gt; 
   &lt;li&gt;Details please refer to &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/media/docs/cpp/heuristics.md&quot;&gt;heuristics doc&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Further enhance Blackwell SM100 Attention kernels in &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/77_blackwell_fmha/&quot;&gt;example 77&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;Add fused reduction kernel support for cutlass MLA.&lt;/li&gt; 
   &lt;li&gt;Add softmax skip correction.&lt;/li&gt; 
   &lt;li&gt;Support for GQA in FMHA backward kernel.&lt;/li&gt; 
   &lt;li&gt;Fix an issue where &lt;code&gt;get_unmasked_trip_count&lt;/code&gt; may return a negative value.&lt;/li&gt; 
   &lt;li&gt;Fix an issue where mbarriers are initialized with a zero arrival count.&lt;/li&gt; 
   &lt;li&gt;Fix a corner case issue where the sequence length of q is not a multiple of tile_q.&lt;/li&gt; 
   &lt;li&gt;Remove tma padding for forward kernel inputs.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Add Blackwell SM100 kernels for MoEs (focusing on Low-Latency inference performance): &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/92_blackwell_moe_gemm/&quot;&gt;example 92&lt;/a&gt;. It uses TMA (for weights) and CPASYNC (for tokens) to load input matrices and allow only one problem dimension to vary across groups/experts, unlike general Grouped GEMMs. Note: further API simplifications and kernel improvements are upcoming. Any feedback on API is welcome.&lt;/li&gt; 
 &lt;li&gt;Further enhance blockwise and groupwise GEMMs on Hopper and Blackwell 
  &lt;ul&gt; 
   &lt;li&gt;On Blackwell SM120, a blockwise gemm kernel is added: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/87_blackwell_geforce_gemm_blockwise/&quot;&gt;example 87&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;On Hopper, add K major scale factor support for SM90 blockwise kernels.&lt;/li&gt; 
   &lt;li&gt;On Hopper, relax the restriction that the k dimension of the problem size has to be the multiple of the k dimension of the tile size.&lt;/li&gt; 
   &lt;li&gt;On Hopper, grouped version supports the case when k = 0.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Blackwell SM100 fp4 gemv kernels. 
  &lt;ul&gt; 
   &lt;li&gt;Kernel codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/kernel/gemv_blockscaled.h&quot;&gt;Gemv kernel&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Example codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/91_fp4_gemv/&quot;&gt;example 91&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Blackwell SM100 legacy mixed input GEMM kernels. 
  &lt;ul&gt; 
   &lt;li&gt;Collective mainloop codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/collective/sm100_mma_warpspecialized_mixed_input.hpp&quot;&gt;Mixed input mainloop&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Kernel codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/kernel/sm100_gemm_tma_warpspecialized_mixed_input_transform.hpp&quot;&gt;Mixed input kernel&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Example codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/86_blackwell_mixed_dtype_gemm/&quot;&gt;example 86&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for Blackwell SM100 cpasync kernel. 
  &lt;ul&gt; 
   &lt;li&gt;Collective mainloop codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/collective/sm100_mma_cpasync_warpspecialized.hpp&quot;&gt;cpasync mainloop&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Kernel codes: &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/include/cutlass/gemm/kernel/sm100_gemm_cpasync_warpspecialized.hpp&quot;&gt;cpasync kernel&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support Blackwell SM120 mixed input blockscaled grouped GEMM.&lt;/li&gt; 
 &lt;li&gt;Instantiating more Blackwell kernels in profiler. 
  &lt;ul&gt; 
   &lt;li&gt;Blackwell SM100 and SM103 kernels support &lt;code&gt;CUTLASS_LIBRARY_INSTANTIATION_LEVEL&lt;/code&gt; to instantiate all possible combinations.&lt;/li&gt; 
   &lt;li&gt;To use this feature, &lt;code&gt;CUTLASS_LIBRARY_KERNELS&lt;/code&gt; must be non-empty. Profiler will combine &lt;code&gt;CUTLASS_LIBRARY_KERNELS&lt;/code&gt; and &lt;code&gt;CUTLASS_LIBRARY_INSTANTIATION_LEVEL&lt;/code&gt; to instantiate specific kernels.&lt;/li&gt; 
   &lt;li&gt;Details please check &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/media/docs/cpp/profiler.md&quot;&gt;Profiler Doc&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Fix some profiler issues: 
  &lt;ul&gt; 
   &lt;li&gt;Modify default cluster callback values to none 0 to avoid profiler failure when these values are not set in command line.&lt;/li&gt; 
   &lt;li&gt;Fix some no output and timeout issues.&lt;/li&gt; 
   &lt;li&gt;Fix Pingpong Blockwise Hopper library generation.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;From CUDA 13.0, the Blackwell SM101 for Thor GPUs is renamed to SM110. 
  &lt;ul&gt; 
   &lt;li&gt;For CUDA toolkit version &amp;lt; 13.0, SM101 is still used for Thor GPUs.&lt;/li&gt; 
   &lt;li&gt;For CUDA toolkit version &amp;gt;= 13.0, SM110 is used for Thor GPUs and SM101 is no longer valid.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Rename legacy Python API package from &lt;code&gt;cutlass&lt;/code&gt; to &lt;code&gt;cutlass_cppgen&lt;/code&gt; and add Blackwell EVT support to legacy Python interface. 
  &lt;ul&gt; 
   &lt;li&gt;Restructuring the C++ Blackwell SM100 Collective Epilogue Builder to work with the Python interface&#39;s &lt;code&gt;EpilogueDescriptors&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Added Blackwell SM100 EVT Emitter on the Python side and routed most emission through Hopper SM90 Emitter.&lt;/li&gt; 
   &lt;li&gt;Added some support for running SM100 kernels via the Python interface.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;CuTe changes: 
  &lt;ul&gt; 
   &lt;li&gt;Fix inaccurate GridDim calculation under &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples/cute/tutorial/blackwell/&quot;&gt;CuTe tutorial&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Add &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-movmatrix&quot;&gt;movmatrix&lt;/a&gt; support.&lt;/li&gt; 
   &lt;li&gt;Fix smallest MMA-N allowed for Blackwell fp8 and fp16 gemm kernels.&lt;/li&gt; 
   &lt;li&gt;Support fp16 accmulator for sm89 fp8 mma.&lt;/li&gt; 
   &lt;li&gt;Shorten &lt;code&gt;nullspace&lt;/code&gt; implementation.&lt;/li&gt; 
   &lt;li&gt;Isolate and comment on &lt;code&gt;cosize&lt;/code&gt; hacks.&lt;/li&gt; 
   &lt;li&gt;Important documentation correction: &lt;code&gt;E&amp;lt;0,1&amp;gt; == 1@0@1&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Fix some kernel issues: 
  &lt;ul&gt; 
   &lt;li&gt;Fix Hopper SM90 group gemm kernel to only use the commit group and wait group instead of also waiting on mbarriers.&lt;/li&gt; 
   &lt;li&gt;Fix a tiny bug when K is large for Blackwell SM103 fp4 grouped GEMM kernel.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Add following unit tests: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/test/unit/cute/ampere/cooperative_gemm.cu&quot;&gt;fp16 accmulator for sm89 fp8 mma&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/test/unit/cute/turing/movm.cu&quot;&gt;movmatrix test&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/test/unit/gemm/device/sm100_tensorop_gemm/f16_f16_void_f32_narrow_mma_n.cu&quot;&gt;fp8 narrow mma n&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/test/unit/gemm/device/sm100_tensorop_gemm/f8_f8_void_bf16_narrow_mma_n.cu&quot;&gt;fp16 narrow mma n&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note: CUTLASS 4.x builds are known to be down on Windows platforms for all CUDA toolkits. CUTLASS team is working on a fix.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;See the &lt;a href=&quot;https://docs.nvidia.com/cutlass/CHANGELOG.html&quot;&gt;CHANGELOG&lt;/a&gt; for details of all past releases and updates.&lt;/strong&gt;&lt;/p&gt; 
&lt;h1&gt;Performance&lt;/h1&gt; 
&lt;p&gt;CUTLASS primitives are very efficient. When used to construct device-wide GEMM kernels, they exhibit nearly optimal utilization of peak theoretical throughput. The figure below shows CUTLASS 3.8&#39;s performance as a % of theoretical peak utilization on various input and output data types when run on NVIDIA Blackwell SM100 architecture GPU.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/images/cutlass-3.8-blackwell-gemm-peak-performance.svg?sanitize=true&quot; alt=&quot;ALT&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The two figures below show the continual CUTLASS performance improvements on an &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/h100/&quot;&gt;NVIDIA H100&lt;/a&gt; (NVIDIA Hopper architecture) since CUTLASS 3.1. CUTLASS 3.5.1 was compiled with the &lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot;&gt;CUDA 12.5u1 Toolkit&lt;/a&gt;. Tensor Core operations are implemented using CUDA&#39;s &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma&quot;&gt;mma&lt;/a&gt; and &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions&quot;&gt;wgmma&lt;/a&gt; instructions.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/images/cutlass-3.5.1-gemm-peak-performance.png&quot; alt=&quot;ALT&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/media/images/cutlass-3.5.1-gemm-peak-performance-fp8.png&quot; alt=&quot;ALT&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;CuTe&lt;/h1&gt; 
&lt;p&gt;CUTLASS 3.0 introduced a new core library, CuTe, to describe and manipulate tensors of threads and data. CuTe is a collection of C++ CUDA template abstractions for defining and operating on hierarchically multidimensional layouts of threads and data. CuTe provides &lt;code&gt;Layout&lt;/code&gt; and &lt;code&gt;Tensor&lt;/code&gt; objects that compactly package the type, shape, memory space, and layout of data, while performing the complicated indexing for the user. This lets programmers focus on the logical descriptions of their algorithms while CuTe does the mechanical bookkeeping for them. With these tools, we can quickly design, implement, and modify all dense linear algebra operations.&lt;/p&gt; 
&lt;p&gt;The core abstractions of CuTe are hierarchically multidimensional layouts which can be composed with data arrays to represent tensors. The representation of layouts is powerful enough to represent nearly everything we need to implement efficient dense linear algebra. Layouts can also be combined and manipulated via functional composition, on which we build a large set of common operations such as tiling and partitioning.&lt;/p&gt; 
&lt;p&gt;CUTLASS 3.0 and beyond adopts CuTe throughout the GEMM hierarchy in its templates. This greatly simplifies the design and improves code composability and readability. More documentation specific to CuTe can be found in its &lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/cute/00_quickstart.html&quot;&gt;dedicated documentation directory&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Compatibility&lt;/h1&gt; 
&lt;p&gt;Minimum requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Architecture: Volta (compute capability 7.0)&lt;/li&gt; 
 &lt;li&gt;Compiler: Must support at least C++17&lt;/li&gt; 
 &lt;li&gt;CUDA Toolkit version: 11.4&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;CUTLASS requires a C++17 host compiler and performs best when built with the &lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot;&gt;&lt;strong&gt;CUDA 12.8 Toolkit&lt;/strong&gt;&lt;/a&gt;. It is also compatible with CUDA 11.4, CUDA 11.5, CUDA 11.6, CUDA 11.7, CUDA 11.8, and all other CUDA 12.x versions.&lt;/p&gt; 
&lt;h2&gt;Operating Systems&lt;/h2&gt; 
&lt;p&gt;We have tested the following environments.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Operating System&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Compiler&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ubuntu 18.04&lt;/td&gt; 
   &lt;td&gt;GCC 7.5.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ubuntu 20.04&lt;/td&gt; 
   &lt;td&gt;GCC 10.3.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ubuntu 22.04&lt;/td&gt; 
   &lt;td&gt;GCC 11.2.0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Note: GCC 8.5.0 has known regressions regarding fold expressions and overloaded operators. Using GCC 7.5.0 or (preferred) GCC &amp;gt;= 9 is recommended.&lt;/p&gt; 
&lt;p&gt;Note: CUTLASS 3.x builds are known to be down on Windows platforms for all CUDA toolkits. CUTLASS team is working on a fix.&lt;/p&gt; 
&lt;h2&gt;Hardware&lt;/h2&gt; 
&lt;p&gt;CUTLASS runs successfully on the following NVIDIA GPUs, and it is expected to be efficient on Volta, Turing, Ampere, Ada, and Hopper architecture based NVIDIA GPUs.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;CUDA Compute Capability&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Minimum CUDA Toolkit Required by CUTLASS-3&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA V100 Tensor Core GPU&lt;/td&gt; 
   &lt;td&gt;7.0&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA TitanV&lt;/td&gt; 
   &lt;td&gt;7.0&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA GeForce RTX 20x0 series&lt;/td&gt; 
   &lt;td&gt;7.5&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA T4&lt;/td&gt; 
   &lt;td&gt;7.5&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA A100 Tensor Core GPU&lt;/td&gt; 
   &lt;td&gt;8.0&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA A10&lt;/td&gt; 
   &lt;td&gt;8.6&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA GeForce RTX 30x0 series&lt;/td&gt; 
   &lt;td&gt;8.6&lt;/td&gt; 
   &lt;td&gt;11.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA GeForce RTX 40x0 series&lt;/td&gt; 
   &lt;td&gt;8.9&lt;/td&gt; 
   &lt;td&gt;11.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA L40&lt;/td&gt; 
   &lt;td&gt;8.9&lt;/td&gt; 
   &lt;td&gt;11.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA H100 Tensor Core GPU&lt;/td&gt; 
   &lt;td&gt;9.0&lt;/td&gt; 
   &lt;td&gt;11.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA H200 Tensor Core GPU&lt;/td&gt; 
   &lt;td&gt;9.0&lt;/td&gt; 
   &lt;td&gt;11.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA B200 Tensor Core GPU&lt;/td&gt; 
   &lt;td&gt;10.0&lt;/td&gt; 
   &lt;td&gt;12.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA B300 Tensor Core GPU&lt;/td&gt; 
   &lt;td&gt;10.3&lt;/td&gt; 
   &lt;td&gt;13.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA DRIVE Thor&lt;/td&gt; 
   &lt;td&gt;11.0&lt;/td&gt; 
   &lt;td&gt;13.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA GeForce RTX 50x0 series&lt;/td&gt; 
   &lt;td&gt;12.0&lt;/td&gt; 
   &lt;td&gt;12.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;NVIDIA DGX Spark&lt;/td&gt; 
   &lt;td&gt;12.1&lt;/td&gt; 
   &lt;td&gt;13.0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Target Architecture&lt;/h2&gt; 
&lt;p&gt;In general, PTX code generated for one target architecture can be run on future architectures (i.e., it is forward compatible). However, CUDA 12.0 introduced the concept of &quot;architecture-accelerated features&quot; whose PTX does not have forward compatibility guarantees. Several Hopper and Blackwell PTX instructions fall under this category of architecture-accelerated features, and thus require a &lt;code&gt;sm_90a&lt;/code&gt; or &lt;code&gt;sm100a&lt;/code&gt; target architecture (note the &quot;a&quot; appended). For more details on this and other architecture-accelerated instructions, please refer to the &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#feature-availability&quot;&gt;CUDA Documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The target architecture information is passed on to CUTLASS via the cmake flag &lt;code&gt;CUTLASS_NVCC_ARCHS&lt;/code&gt;. In order to maximize performance on Hopper GH100, users are required to build CUTLASS with &lt;code&gt;90a&lt;/code&gt; as the target architecture. If a user accidentally builds a kernel which uses SM90a features (e.g. Hopper Tensor Core Instructions), using the SM90 target (note the lack of &quot;a&quot;), with either CUDA Toolkit 12 or 11.8, the kernel is expected to fail with a runtime error.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cmake .. -DCUTLASS_NVCC_ARCHS=&quot;90a&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cmake .. -DCUTLASS_NVCC_ARCHS=&quot;100a&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: The NVIDIA Blackwell SM100 architecture used in the datacenter products has a different compute capability than the one underpinning NVIDIA Blackwell GeForce RTX 50 series GPUs (SM120). As a result, kernels compiled for Blackwell SM100 architecture with arch conditional features (using &lt;code&gt;sm100a&lt;/code&gt;) are not compatible with RTX 50 series GPUs.&lt;/p&gt; 
&lt;p&gt;Please refer to the &lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/functionality.html&quot;&gt;functionality documentation&lt;/a&gt; for details on which kernels require which target architectures.&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;CUTLASS is described in the following documents and the accompanying &lt;a href=&quot;https://nvidia.github.io/cutlass&quot;&gt;Doxygen documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/quickstart.html&quot;&gt;Quick Start Guide&lt;/a&gt; - basics of building and running CUTLASS&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/functionality.html&quot;&gt;Functionality&lt;/a&gt; - summarizes functionality available in CUTLASS&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/efficient_gemm.html&quot;&gt;Efficient GEMM in CUDA&lt;/a&gt; - describes how GEMM kernels may be implemented efficiently in CUDA&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/cutlass_3x_design.html&quot;&gt;CUTLASS 3.x Design&lt;/a&gt; - describes the CUTLASS 3.x design, its benefits, and how CuTe enables us to write much more composable components&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/gemm_api_3x.html&quot;&gt;GEMM API 3.x&lt;/a&gt; - describes the CUTLASS 3.x GEMM model and C++ template concepts&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/gemm_api.html&quot;&gt;GEMM API 2.x&lt;/a&gt; - describes the CUTLASS 2.x GEMM model and C++ template concepts&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/implicit_gemm_convolution.html&quot;&gt;Implicit GEMM Convolution&lt;/a&gt; - describes 2-D and 3-D convolution in CUTLASS&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/code_organization.html&quot;&gt;Code Organization&lt;/a&gt; - describes the organization and contents of the CUTLASS project&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/terminology.html&quot;&gt;Terminology&lt;/a&gt; - describes terms used in the code&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/programming_guidelines.html&quot;&gt;Programming Guidelines&lt;/a&gt; - guidelines for writing efficient modern CUDA C++&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/fundamental_types.html&quot;&gt;Fundamental types&lt;/a&gt; - describes basic C++ classes used in CUTLASS to represent numeric quantities and arrays&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/layout.html&quot;&gt;Layouts&lt;/a&gt; - describes layouts of matrices and tensors in memory&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/tile_iterator_concept.html&quot;&gt;Tile Iterators&lt;/a&gt; - describes C++ concepts for iterating over tiles of matrices in memory&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/profiler.html&quot;&gt;CUTLASS Profiler&lt;/a&gt; - command-line driven profiling application&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/utilities.html&quot;&gt;CUTLASS Utilities&lt;/a&gt; - additional templates used to facilitate rapid development&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/dependent_kernel_launch.html&quot;&gt;Dependent kernel launch&lt;/a&gt; - describes a new feature in Hopper which allows overlapping dependent kernels in the same stream, and how it is used in CUTLASS.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Resources&lt;/h1&gt; 
&lt;p&gt;We have also described the structure of an efficient GEMM in our talk at the &lt;a href=&quot;http://on-demand.gputechconf.com/gtc/2018/presentation/s8854-cutlass-software-primitives-for-dense-linear-algebra-at-all-levels-and-scales-within-cuda.pdf&quot;&gt;GPU Technology Conference 2018&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8854/&quot;&gt;CUTLASS: Software Primitives for Dense Linear Algebra at All Levels and Scales within CUDA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/&quot;&gt;Developing CUDA Kernels to Push Tensor Cores to the Absolute Limit on NVIDIA A100&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31883/&quot;&gt;Accelerating Convolution with Tensor Cores in CUTLASS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41996/&quot;&gt;Accelerating Backward Data Gradient by Increasing Tensor Core Utilization in CUTLASS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41131/&quot;&gt;CUTLASS: Python API, Enhancements, and NVIDIA Hopper&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Building CUTLASS&lt;/h1&gt; 
&lt;p&gt;CUTLASS is a header-only template library and does not need to be built to be used by other projects. Client applications should target CUTLASS&#39;s &lt;code&gt;include/&lt;/code&gt; directory in their include paths.&lt;/p&gt; 
&lt;p&gt;CUTLASS unit tests, examples, and utilities can be build with CMake. The minimum version of CMake is given in the &lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/quickstart.html&quot;&gt;Quickstart guide&lt;/a&gt;. Make sure the &lt;code&gt;CUDACXX&lt;/code&gt; environment variable points to NVCC in the CUDA Toolkit installed on your system.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Create a build directory within the CUTLASS project, then run CMake. By default CUTLASS will build kernels for CUDA architecture versions 5.0, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6, 8.9, and 9.0. To reduce compile time you can specify the architectures to build CUTLASS for by changing the CMake configuration setting &lt;code&gt;CUTLASS_NVCC_ARCHS&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ mkdir build &amp;amp;&amp;amp; cd build

$ cmake .. -DCUTLASS_NVCC_ARCHS=80               # compiles for NVIDIA&#39;s Ampere Architecture
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;From the &lt;code&gt;build/&lt;/code&gt; directory, compile and run the CUTLASS unit tests by building the target &lt;code&gt;test_unit&lt;/code&gt; with make.&lt;/p&gt; 
&lt;p&gt;The unit tests are organized as several binaries mirroring the top-level namespaces of CUTLASS, and they may be executed in parallel via make&#39;s &lt;code&gt;-j&lt;/code&gt; command line argument.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make test_unit -j
...
...
...
[----------] Global test environment tear-down
[==========] 946 tests from 57 test cases ran. (10812 ms total)
[  PASSED  ] 946 tests.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All tests should pass on supported platforms, though the exact number of tests may vary over time.&lt;/p&gt; 
&lt;h1&gt;Project Structure&lt;/h1&gt; 
&lt;p&gt;CUTLASS is arranged as a header-only library along with Utilities, Tools, Examples, and unit tests. &lt;a href=&quot;https://nvidia.github.io/cutlass&quot;&gt;Doxygen documentation&lt;/a&gt; provides a complete list of files, classes, and template concepts defined in the CUTLASS project.&lt;/p&gt; 
&lt;p&gt;A detailed explanation of the source code organization may be found in the &lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/code_organization.html&quot;&gt;CUTLASS documentation&lt;/a&gt;, but several main components are summarized below.&lt;/p&gt; 
&lt;h2&gt;CUTLASS Template Library&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;include/                     # client applications should target this directory in their build&#39;s include paths

  cutlass/                   # CUDA Templates for Linear Algebra Subroutines and Solvers - headers only

    arch/                    # direct exposure of architecture features (including instruction-level GEMMs)

    conv/                    # code specialized for convolution

    epilogue/                # code specialized for the epilogue of gemm/convolution

    gemm/                    # code specialized for general matrix product computations

    layout/                  # layout definitions for matrices, tensors, and other mathematical objects in memory

    platform/                # CUDA-capable Standard Library components

    reduction/               # bandwidth-limited reduction kernels that do not fit the &quot;gemm&quot; model

    thread/                  # simt code that can be performed within a CUDA thread

    transform/               # code specialized for layout, type, and domain transformations

    *                        # core vocabulary types, containers, and basic numeric operations

  cute/                      # CuTe Layout, layout algebra, MMA/Copy atoms, tiled MMA/Copy

    algorithm/               # Definitions of core operations such as copy, gemm, and operations on cute::tuples

    arch/                    # Bare bones PTX wrapper structs for copy and math instructions

    atom/                    # Meta-information either link to or built from arch/ operators

      mma_atom.hpp           # cute::Mma_Atom and cute::TiledMma

      copy_atom.hpp          # cute::Copy_Atom and cute::TiledCopy

      *sm*.hpp               # Arch specific meta-information for copy and math operations

    *                        # Core library types such as Shape, Stride, Layout, Tensor, and associated operations

&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;CUTLASS SDK Examples&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/examples&quot;&gt;CUTLASS SDK examples&lt;/a&gt; apply CUTLASS templates to implement basic computations.&lt;/p&gt; 
&lt;h3&gt;Tools&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;tools/
  library/                   # CUTLASS Instance Library - contains instantiations of all supported CUTLASS templates
    include/
      cutlass/
        library/

  profiler/                  # CUTLASS Profiler         - command-line utility for executing operations in the
                             #                            CUTLASS Library

  util/                      # CUTLASS Utilities        - contains numerous helper classes for
    include/                 #                            managing tensors in device memory, reference
      cutlass/               #                            implementations for GEMM, random initialization
        util/                #                            of tensors, and I/O.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Test&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;test/unit/&lt;/code&gt; directory consist of unit tests implemented with Google Test that demonstrate basic usage of Core API components and complete tests of the CUTLASS GEMM computations.&lt;/p&gt; 
&lt;p&gt;Instructions for building and running the Unit tests are described in the &lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/quickstart.html&quot;&gt;Quickstart guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Performance Profiling&lt;/h1&gt; 
&lt;p&gt;The &lt;code&gt;tools/profiler/&lt;/code&gt; directory contains a command-line utility for launching each of the GEMM kernels. It can be built as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make cutlass_profiler -j16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Building all GEMM and Convolution kernels (&lt;em&gt;long&lt;/em&gt; build times)&lt;/h2&gt; 
&lt;p&gt;By default, only one tile size is instantiated for each data type, math instruction, and layout. To instantiate all, set the following environment variable when running CMake from an empty &lt;code&gt;build/&lt;/code&gt; directory. Beware, this results in &lt;em&gt;tens of thousands&lt;/em&gt; of kernels and long build times. This would also result in a large binary size and on some platforms linker to fail on building the library. Therefore, it&#39;s highly recommended to generate only a subset of kernels as demonstrated in the sub-section below.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_KERNELS=all
...
$ make cutlass_profiler -j16
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Building a subset of GEMM and Convolution kernels (&lt;em&gt;reduced&lt;/em&gt; build times)&lt;/h2&gt; 
&lt;p&gt;To compile strictly one kernel or a small set of kernels, a comma-delimited list of kernel names with wildcard characters may be used to reduce the set of kernels. The following examples show building exactly one or a subset of kernels for NVIDIA Ampere and Turing architecture:&lt;/p&gt; 
&lt;h3&gt;Building a subset Tensor Core GEMM kernels&lt;/h3&gt; 
&lt;p&gt;To compile a subset of Tensor Core GEMM kernels with FP32 accumulation and FP16 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cmake .. -DCUTLASS_NVCC_ARCHS=&#39;75;80&#39; -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*gemm_f16_*_nt_align8
...
$ make cutlass_profiler -j16
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example command line for profiling a subset of Tensor Core GEMM kernels is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*gemm_f16_*_nt_align8 --m=3456 --n=4096 --k=4096

...
=============================
  Problem ID: 1

        Provider: CUTLASS
   OperationKind: gemm
       Operation: cutlass_tensorop_s1688gemm_f16_256x128_32x2_nt_align8

          Status: Success
    Verification: ON
     Disposition: Passed

reference_device: Passed
          cuBLAS: Passed

       Arguments: --gemm_kind=universal --m=3456 --n=4096 --k=4096 --A=f16:column --B=f16:row --C=f32:column --alpha=1  \
                  --beta=0 --split_k_slices=1 --batch_count=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128  \
                  --cta_k=32 --stages=2 --warps_m=4 --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75  \
                  --max_cc=1024

           Bytes: 118489088  bytes
           FLOPs: 115992428544  flops

         Runtime: 1.55948  ms
          Memory: 70.7616 GiB/s

            Math: 74378.8 GFLOP/s



=============================
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building one CUDA Core GEMM kernel&lt;/h3&gt; 
&lt;p&gt;To compile one SGEMM kernel targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cmake .. -DCUTLASS_NVCC_ARCHS=&#39;75;80&#39; -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sgemm_128x128_8x2_nn_align1
...
$ make cutlass_profiler -j16
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example command line for profiling single SGEMM CUDA kernel is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=3456 --n=4096 --k=4096

=============================
  Problem ID: 1

        Provider: CUTLASS
   OperationKind: gemm
       Operation: cutlass_simt_sgemm_128x128_8x2_nn_align1

          Status: Success
    Verification: ON
     Disposition: Passed

          cuBLAS: Passed

       Arguments: --m=3456 --n=4096 --k=4096 --A=f32:column --B=f32:column --C=f32:column --alpha=1 --beta=0 --split_k_slices=1  \
                  --batch_count=1 --op_class=simt --accum=f32 --cta_m=128 --cta_n=128 --cta_k=8 --stages=2 --warps_m=4  \
                  --warps_n=2 --warps_k=1 --inst_m=1 --inst_n=1 --inst_k=1 --min_cc=50 --max_cc=1024

           Bytes: 180355072  bytes
           FLOPs: 115992428544  flops

         Runtime: 6.73655  ms
          Memory: 24.934 GiB/s

            Math: 17218.4 GFLOP/s

=============================
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building a subset of Tensor Core Convolution kernels&lt;/h3&gt; 
&lt;p&gt;To compile a subset of Tensor core convolution kernels implementing forward propagation (fprop) with FP32 accumulation and FP16 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cmake .. -DCUTLASS_NVCC_ARCHS=&#39;75;80&#39; -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*fprop_optimized_f16
...
$ make cutlass_profiler -j16
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example command line for profiling a subset of Tensor Core convolution kernels is as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*fprop_optimized_f16 --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3

...
=============================
  Problem ID: 1

        Provider: CUTLASS
   OperationKind: conv2d
       Operation: cutlass_tensorop_s16816fprop_optimized_f16_128x128_32x5_nhwc

          Status: Success
    Verification: ON
     Disposition: Passed

reference_device: Passed

       Arguments: --conv_kind=fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --p=224 --q=224 --pad_h=1 --pad_w=1  \
                  --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1 --Activation=f16:nhwc --Filter=f16:nhwc --Output=f32:nhwc  \
                  --conv_mode=cross --iterator_algorithm=optimized --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1  \
                  --eq_gemm_provider=none --op_class=tensorop --accum=f32 --cta_m=128 --cta_n=128 --cta_k=32 --stages=5  \
                  --warps_m=2 --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=16 --min_cc=80 --max_cc=1024

           Bytes: 1130659840  bytes
           FLOPs: 118482796544  flops

         Runtime: 0.711496  ms
          Memory: 1479.99 GiB/s

            Math: 166526 GFLOP/s

=============================
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Building one Convolution CUDA kernel&lt;/h3&gt; 
&lt;p&gt;To compile and run one CUDA Core convolution kernel implementing forward propagation (fprop) with F32 accumulation and FP32 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ cmake .. -DCUTLASS_NVCC_ARCHS=&#39;75;80&#39; -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc
...
$ make cutlass_profiler -j16
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Example command line for profiling one CUDA Core convolution kernel:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3


=============================
  Problem ID: 1

        Provider: CUTLASS
   OperationKind: conv2d
       Operation: cutlass_simt_sfprop_optimized_128x128_8x2_nhwc

          Status: Success
    Verification: ON
     Disposition: Passed

reference_device: Passed

       Arguments: --conv_kind=fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --p=224 --q=224 --pad_h=1 --pad_w=1  \
                  --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1 --Activation=f32:nhwc --Filter=f32:nhwc --Output=f32:nhwc  \
                  --conv_mode=cross --iterator_algorithm=optimized --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1  \
                  --eq_gemm_provider=none --op_class=simt --accum=f32 --cta_m=128 --cta_n=128 --cta_k=8 --stages=2 --warps_m=4  \
                  --warps_n=2 --warps_k=1 --inst_m=1 --inst_n=1 --inst_k=1 --min_cc=50 --max_cc=1024

           Bytes: 2055798784  bytes
           FLOPs: 118482796544  flops

         Runtime: 7.34266  ms
          Memory: 260.752 GiB/s

            Math: 16136.2 GFLOP/s


=============================

&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;More Details on Compiling CUTLASS Kernels and CUTLASS Profiler&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Please follow the links for more CMake examples on selectively compiling CUTLASS kernels: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/quickstart.html#gemm-cmake-examples&quot;&gt;GEMM CMake Examples&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/quickstart.html#convolution-cmake-examples&quot;&gt;Implicit GEMM convolution CMake Examples&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.nvidia.com/cutlass/media/docs/cpp/profiler.html&quot;&gt;Further details about the CUTLASS Profiler are described here.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;About&lt;/h1&gt; 
&lt;p&gt;CUTLASS is released by NVIDIA Corporation as Open Source software under the &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/LICENSE.txt&quot;&gt;3-clause &quot;New&quot; BSD license&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contributors&lt;/h1&gt; 
&lt;p&gt;The official list of CUTLASS developers and contributors is available here: &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/cutlass/main/CONTRIBUTORS.md&quot;&gt;CONTRIBUTORS&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Copyright&lt;/h1&gt; 
&lt;p&gt;Copyright (c) 2017 - 2025 NVIDIA CORPORATION &amp;amp; AFFILIATES. All rights reserved. SPDX-License-Identifier: BSD-3-Clause&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;  Redistribution and use in source and binary forms, with or without
  modification, are permitted provided that the following conditions are met:

  1. Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

  2. Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

  3. Neither the name of the copyright holder nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS &quot;AS IS&quot;
  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>google/googletest</title>
      <link>https://github.com/google/googletest</link>
      <description>&lt;p&gt;GoogleTest - Google Testing and Mocking Framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GoogleTest&lt;/h1&gt; 
&lt;h3&gt;Announcements&lt;/h3&gt; 
&lt;h4&gt;Documentation Updates&lt;/h4&gt; 
&lt;p&gt;Our documentation is now live on GitHub Pages at &lt;a href=&quot;https://google.github.io/googletest/&quot;&gt;https://google.github.io/googletest/&lt;/a&gt;. We recommend browsing the documentation on GitHub Pages rather than directly in the repository.&lt;/p&gt; 
&lt;h4&gt;Release 1.17.0&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google/googletest/releases/tag/v1.17.0&quot;&gt;Release 1.17.0&lt;/a&gt; is now available.&lt;/p&gt; 
&lt;p&gt;The 1.17.x branch &lt;a href=&quot;https://opensource.google/documentation/policies/cplusplus-support#c_language_standard&quot;&gt;requires at least C++17&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Continuous Integration&lt;/h4&gt; 
&lt;p&gt;We use Google&#39;s internal systems for continuous integration.&lt;/p&gt; 
&lt;h4&gt;Coming Soon&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;We are planning to take a dependency on &lt;a href=&quot;https://github.com/abseil/abseil-cpp&quot;&gt;Abseil&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Welcome to &lt;strong&gt;GoogleTest&lt;/strong&gt;, Google&#39;s C++ test framework!&lt;/h2&gt; 
&lt;p&gt;This repository is a merger of the formerly separate GoogleTest and GoogleMock projects. These were so closely related that it makes sense to maintain and release them together.&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://google.github.io/googletest/&quot;&gt;GoogleTest User&#39;s Guide&lt;/a&gt; for documentation. We recommend starting with the &lt;a href=&quot;https://google.github.io/googletest/primer.html&quot;&gt;GoogleTest Primer&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;More information about building GoogleTest can be found at &lt;a href=&quot;https://raw.githubusercontent.com/google/googletest/main/googletest/README.md&quot;&gt;googletest/README.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;xUnit test framework: &lt;br /&gt; Googletest is based on the &lt;a href=&quot;https://en.wikipedia.org/wiki/XUnit&quot;&gt;xUnit&lt;/a&gt; testing framework, a popular architecture for unit testing&lt;/li&gt; 
 &lt;li&gt;Test discovery: &lt;br /&gt; Googletest automatically discovers and runs your tests, eliminating the need to manually register your tests&lt;/li&gt; 
 &lt;li&gt;Rich set of assertions: &lt;br /&gt; Googletest provides a variety of assertions, such as equality, inequality, exceptions, and more, making it easy to test your code&lt;/li&gt; 
 &lt;li&gt;User-defined assertions: &lt;br /&gt; You can define your own assertions with Googletest, making it simple to write tests that are specific to your code&lt;/li&gt; 
 &lt;li&gt;Death tests: &lt;br /&gt; Googletest supports death tests, which verify that your code exits in a certain way, making it useful for testing error-handling code&lt;/li&gt; 
 &lt;li&gt;Fatal and non-fatal failures: &lt;br /&gt; You can specify whether a test failure should be treated as fatal or non-fatal with Googletest, allowing tests to continue running even if a failure occurs&lt;/li&gt; 
 &lt;li&gt;Value-parameterized tests: &lt;br /&gt; Googletest supports value-parameterized tests, which run multiple times with different input values, making it useful for testing functions that take different inputs&lt;/li&gt; 
 &lt;li&gt;Type-parameterized tests: &lt;br /&gt; Googletest also supports type-parameterized tests, which run with different data types, making it useful for testing functions that work with different data types&lt;/li&gt; 
 &lt;li&gt;Various options for running tests: &lt;br /&gt; Googletest provides many options for running tests including running individual tests, running tests in a specific order and running tests in parallel&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Supported Platforms&lt;/h2&gt; 
&lt;p&gt;GoogleTest follows Google&#39;s &lt;a href=&quot;https://opensource.google/documentation/policies/cplusplus-support&quot;&gt;Foundational C++ Support Policy&lt;/a&gt;. See &lt;a href=&quot;https://github.com/google/oss-policies-info/raw/main/foundational-cxx-support-matrix.md&quot;&gt;this table&lt;/a&gt; for a list of currently supported versions of compilers, platforms, and build tools.&lt;/p&gt; 
&lt;h2&gt;Who Is Using GoogleTest?&lt;/h2&gt; 
&lt;p&gt;In addition to many internal projects at Google, GoogleTest is also used by the following notable projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://www.chromium.org/&quot;&gt;Chromium projects&lt;/a&gt; (behind the Chrome browser and Chrome OS).&lt;/li&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://llvm.org/&quot;&gt;LLVM&lt;/a&gt; compiler.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/protobuf&quot;&gt;Protocol Buffers&lt;/a&gt;, Google&#39;s data interchange format.&lt;/li&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://opencv.org/&quot;&gt;OpenCV&lt;/a&gt; computer vision library.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related Open Source Projects&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/nholthaus/gtest-runner&quot;&gt;GTest Runner&lt;/a&gt; is a Qt5 based automated test-runner and Graphical User Interface with powerful features for Windows and Linux platforms.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ospector/gtest-gbar&quot;&gt;GoogleTest UI&lt;/a&gt; is a test runner that runs your test binary, allows you to track its progress via a progress bar, and displays a list of test failures. Clicking on one shows failure text. GoogleTest UI is written in C#.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/kinow/gtest-tap-listener&quot;&gt;GTest TAP Listener&lt;/a&gt; is an event listener for GoogleTest that implements the &lt;a href=&quot;https://en.wikipedia.org/wiki/Test_Anything_Protocol&quot;&gt;TAP protocol&lt;/a&gt; for test result output. If your test runner understands TAP, you may find it useful.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google/gtest-parallel&quot;&gt;gtest-parallel&lt;/a&gt; is a test runner that runs tests from your binary in parallel to provide significant speed-up.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=DavidSchuldenfrei.gtest-adapter&quot;&gt;GoogleTest Adapter&lt;/a&gt; is a VS Code extension allowing to view GoogleTest in a tree view and run/debug your tests.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/matepek/vscode-catch2-test-adapter&quot;&gt;C++ TestMate&lt;/a&gt; is a VS Code extension allowing to view GoogleTest in a tree view and run/debug your tests.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://pypi.org/project/cornichon/&quot;&gt;Cornichon&lt;/a&gt; is a small Gherkin DSL parser that generates stub code for GoogleTest.&lt;/p&gt; 
&lt;h2&gt;Contributing Changes&lt;/h2&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://github.com/google/googletest/raw/main/CONTRIBUTING.md&quot;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for details on how to contribute to this project.&lt;/p&gt; 
&lt;p&gt;Happy testing!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>LadybirdBrowser/ladybird</title>
      <link>https://github.com/LadybirdBrowser/ladybird</link>
      <description>&lt;p&gt;Truly independent web browser&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ladybird&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://ladybird.org&quot;&gt;Ladybird&lt;/a&gt; is a truly independent web browser, using a novel engine based on web standards.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] Ladybird is in a pre-alpha state, and only suitable for use by developers&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;We aim to build a complete, usable browser for the modern web.&lt;/p&gt; 
&lt;p&gt;Ladybird uses a multi-process architecture with a main UI process, several WebContent renderer processes, an ImageDecoder process, and a RequestServer process.&lt;/p&gt; 
&lt;p&gt;Image decoding and network connections are done out of process to be more robust against malicious content. Each tab has its own renderer process, which is sandboxed from the rest of the system.&lt;/p&gt; 
&lt;p&gt;At the moment, many core library support components are inherited from SerenityOS:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LibWeb: Web rendering engine&lt;/li&gt; 
 &lt;li&gt;LibJS: JavaScript engine&lt;/li&gt; 
 &lt;li&gt;LibWasm: WebAssembly implementation&lt;/li&gt; 
 &lt;li&gt;LibCrypto/LibTLS: Cryptography primitives and Transport Layer Security&lt;/li&gt; 
 &lt;li&gt;LibHTTP: HTTP/1.1 client&lt;/li&gt; 
 &lt;li&gt;LibGfx: 2D Graphics Library, Image Decoding and Rendering&lt;/li&gt; 
 &lt;li&gt;LibUnicode: Unicode and locale support&lt;/li&gt; 
 &lt;li&gt;LibMedia: Audio and video playback&lt;/li&gt; 
 &lt;li&gt;LibCore: Event loop, OS abstraction layer&lt;/li&gt; 
 &lt;li&gt;LibIPC: Inter-process communication&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How do I build and run this?&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/BuildInstructionsLadybird.md&quot;&gt;build instructions&lt;/a&gt; for information on how to build Ladybird.&lt;/p&gt; 
&lt;p&gt;Ladybird runs on Linux, macOS, Windows (with WSL2), and many other *Nixes.&lt;/p&gt; 
&lt;h2&gt;How do I read the documentation?&lt;/h2&gt; 
&lt;p&gt;Code-related documentation can be found in the &lt;a href=&quot;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/&quot;&gt;documentation&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h2&gt;Get in touch and participate!&lt;/h2&gt; 
&lt;p&gt;Join &lt;a href=&quot;https://discord.gg/nvfjVJ4Svh&quot;&gt;our Discord server&lt;/a&gt; to participate in development discussion.&lt;/p&gt; 
&lt;p&gt;Please read &lt;a href=&quot;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/Documentation/GettingStartedContributing.md&quot;&gt;Getting started contributing&lt;/a&gt; if you plan to contribute to Ladybird for the first time.&lt;/p&gt; 
&lt;p&gt;Before opening an issue, please see the &lt;a href=&quot;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md#issue-policy&quot;&gt;issue policy&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/ISSUES.md&quot;&gt;detailed issue-reporting guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The full contribution guidelines can be found in &lt;a href=&quot;https://raw.githubusercontent.com/LadybirdBrowser/ladybird/master/CONTRIBUTING.md&quot;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Ladybird is licensed under a 2-clause BSD license.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>electron/electron</title>
      <link>https://github.com/electron/electron</link>
      <description>&lt;p&gt;Build cross-platform desktop apps with JavaScript, HTML, and CSS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://electronjs.org&quot;&gt;&lt;img src=&quot;https://electronjs.org/images/electron-logo.svg?sanitize=true&quot; alt=&quot;Electron Logo&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/electron/electron/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/electron/electron/actions/workflows/build.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/electronjs&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/745037351163527189?color=%237289DA&amp;amp;label=chat&amp;amp;logo=discord&amp;amp;logoColor=white&quot; alt=&quot;Electron Discord Invite&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;span&gt;üìù&lt;/span&gt; Available Translations: üá®üá≥ üáßüá∑ üá™üá∏ üáØüáµ üá∑üá∫ üá´üá∑ üá∫üá∏ üá©üá™. View these docs in other languages on our &lt;a href=&quot;https://crowdin.com/project/electron&quot;&gt;Crowdin&lt;/a&gt; project.&lt;/p&gt; 
&lt;p&gt;The Electron framework lets you write cross-platform desktop applications using JavaScript, HTML and CSS. It is based on &lt;a href=&quot;https://nodejs.org/&quot;&gt;Node.js&lt;/a&gt; and &lt;a href=&quot;https://www.chromium.org&quot;&gt;Chromium&lt;/a&gt; and is used by the &lt;a href=&quot;https://github.com/Microsoft/vscode/&quot;&gt;Visual Studio Code&lt;/a&gt; and many other &lt;a href=&quot;https://electronjs.org/apps&quot;&gt;apps&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://twitter.com/electronjs&quot;&gt;@electronjs&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; 
&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&quot;https://github.com/electron/electron/tree/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&quot;mailto:coc@electronjs.org&quot;&gt;coc@electronjs.org&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;To install prebuilt Electron binaries, use &lt;a href=&quot;https://docs.npmjs.com/&quot;&gt;&lt;code&gt;npm&lt;/code&gt;&lt;/a&gt;. The preferred method is to install Electron as a development dependency in your app:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;npm install electron --save-dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more installation options and troubleshooting tips, see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/installation.md&quot;&gt;installation&lt;/a&gt;. For info on how to manage Electron versions in your apps, see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/docs/tutorial/electron-versioning.md&quot;&gt;Electron versioning&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Platform support&lt;/h2&gt; 
&lt;p&gt;Each Electron release provides binaries for macOS, Windows, and Linux.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;macOS (Big Sur and up): Electron provides 64-bit Intel and Apple Silicon / ARM binaries for macOS.&lt;/li&gt; 
 &lt;li&gt;Windows (Windows 10 and up): Electron provides &lt;code&gt;ia32&lt;/code&gt; (&lt;code&gt;x86&lt;/code&gt;), &lt;code&gt;x64&lt;/code&gt; (&lt;code&gt;amd64&lt;/code&gt;), and &lt;code&gt;arm64&lt;/code&gt; binaries for Windows. Windows on ARM support was added in Electron 5.0.8. Support for Windows 7, 8 and 8.1 was &lt;a href=&quot;https://www.electronjs.org/blog/windows-7-to-8-1-deprecation-notice&quot;&gt;removed in Electron 23, in line with Chromium&#39;s Windows deprecation policy&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Linux: The prebuilt binaries of Electron are built on Ubuntu 20.04. They have also been verified to work on: 
  &lt;ul&gt; 
   &lt;li&gt;Ubuntu 18.04 and newer&lt;/li&gt; 
   &lt;li&gt;Fedora 32 and newer&lt;/li&gt; 
   &lt;li&gt;Debian 10 and newer&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Electron Fiddle&lt;/h2&gt; 
&lt;p&gt;Use &lt;a href=&quot;https://github.com/electron/fiddle&quot;&gt;&lt;code&gt;Electron Fiddle&lt;/code&gt;&lt;/a&gt; to build, run, and package small Electron experiments, to see code examples for all of Electron&#39;s APIs, and to try out different versions of Electron. It&#39;s designed to make the start of your journey with Electron easier.&lt;/p&gt; 
&lt;h2&gt;Resources for learning Electron&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://electronjs.org/docs&quot;&gt;electronjs.org/docs&lt;/a&gt; - All of Electron&#39;s documentation&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/electron/fiddle&quot;&gt;electron/fiddle&lt;/a&gt; - A tool to build, run, and package small Electron experiments&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://electronjs.org/community#boilerplates&quot;&gt;electronjs.org/community#boilerplates&lt;/a&gt; - Sample starter apps created by the community&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Programmatic usage&lt;/h2&gt; 
&lt;p&gt;Most people use Electron from the command line, but if you require &lt;code&gt;electron&lt;/code&gt; inside your &lt;strong&gt;Node app&lt;/strong&gt; (not your Electron app) it will return the file path to the binary. Use this to spawn Electron from Node scripts:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-javascript&quot;&gt;const electron = require(&#39;electron&#39;)
const proc = require(&#39;node:child_process&#39;)

// will print something similar to /Users/maf/.../Electron
console.log(electron)

// spawn Electron
const child = proc.spawn(electron)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Mirrors&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://npmmirror.com/mirrors/electron/&quot;&gt;China&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://www.electronjs.org/docs/latest/tutorial/installation#mirror&quot;&gt;Advanced Installation Instructions&lt;/a&gt; to learn how to use a custom mirror.&lt;/p&gt; 
&lt;h2&gt;Documentation translations&lt;/h2&gt; 
&lt;p&gt;We crowdsource translations for our documentation via &lt;a href=&quot;https://crowdin.com/project/electron&quot;&gt;Crowdin&lt;/a&gt;. We currently accept translations for Chinese (Simplified), French, German, Japanese, Portuguese, Russian, and Spanish.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you are interested in reporting/fixing issues and contributing directly to the code base, please see &lt;a href=&quot;https://raw.githubusercontent.com/electron/electron/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more information on what we&#39;re looking for and how to get started.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;Info on reporting bugs, getting help, finding third-party tools and sample apps, and more can be found on the &lt;a href=&quot;https://www.electronjs.org/community&quot;&gt;Community page&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/electron/electron/raw/main/LICENSE&quot;&gt;MIT&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;When using Electron logos, make sure to follow &lt;a href=&quot;https://trademark-policy.openjsf.org/&quot;&gt;OpenJS Foundation Trademark Policy&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>NVIDIA/TensorRT-LLM</title>
      <link>https://github.com/NVIDIA/TensorRT-LLM</link>
      <description>&lt;p&gt;TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;h1&gt;TensorRT LLM&lt;/h1&gt; 
 &lt;h4&gt; A TensorRT Toolbox for Optimized Large Language Model Inference&lt;/h4&gt; 
 &lt;p&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat&quot; alt=&quot;Documentation&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-3123/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.12-green&quot; alt=&quot;python&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.python.org/downloads/release/python-31012/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/python-3.10-green&quot; alt=&quot;python&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://developer.nvidia.com/cuda-downloads&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/cuda-13.0.0-green&quot; alt=&quot;cuda&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://developer.nvidia.com/tensorrt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/TRT-10.13.2-green&quot; alt=&quot;trt&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/tensorrt_llm/version.py&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/release-1.1.0rc6-green&quot; alt=&quot;version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-blue&quot; alt=&quot;license&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/torch/arch_overview.md&quot;&gt;Architecture&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/performance/perf-overview.md&quot;&gt;Performance&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html&quot;&gt;Examples&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/&quot;&gt;Documentation&lt;/a&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;|&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://github.com/NVIDIA/TensorRT-LLM/issues?q=is%3Aissue%20state%3Aopen%20label%3Aroadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;div align=&quot;left&quot;&gt; 
  &lt;h2&gt;Tech Blogs&lt;/h2&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;[09/26] Inference Time Compute Implementation in TensorRT LLM ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog13_Inference_Time_Compute_Implementation_in_TensorRT-LLM.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[09/19] Combining Guided Decoding and Speculative Decoding: Making CPU and GPU Cooperate Seamlessly ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog12_Combining_Guided_Decoding_and_Speculative_Decoding.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[08/29] ADP Balance Strategy ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog10_ADP_Balance_Strategy.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[08/05] Running a High-Performance GPT-OSS-120B Inference Server with TensorRT LLM ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog9_Deploying_GPT_OSS_on_TRTLLM.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[08/01] Scaling Expert Parallelism in TensorRT LLM (Part 2: Performance Status and Optimization) ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog8_Scaling_Expert_Parallelism_in_TensorRT-LLM_part2.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[07/26] N-Gram‚ÄØSpeculative‚ÄØDecoding‚ÄØin TensorRT LLM ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog7_NGram_performance_Analysis_And_Auto_Enablement.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[06/19] Disaggregated Serving in TensorRT LLM ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog5_Disaggregated_Serving_in_TensorRT-LLM.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[06/05] Scaling Expert Parallelism in TensorRT LLM (Part 1: Design and Implementation of Large-scale EP) ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog4_Scaling_Expert_Parallelism_in_TensorRT-LLM.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[05/30] Optimizing DeepSeek R1 Throughput on NVIDIA Blackwell GPUs: A Deep Dive for Developers ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog3_Optimizing_DeepSeek_R1_Throughput_on_NVIDIA_Blackwell_GPUs.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[05/23] DeepSeek R1 MTP Implementation and Optimization ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog2_DeepSeek_R1_MTP_Implementation_and_Optimization.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[05/16] Pushing Latency Boundaries: Optimizing DeepSeek-R1 Performance on NVIDIA B200 GPUs ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/tech_blog/blog1_Pushing_Latency_Boundaries_Optimizing_DeepSeek-R1_Performance_on_NVIDIA_B200_GPUs.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Latest News&lt;/h2&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;[08/05] üåü TensorRT LLM delivers Day-0 support for OpenAI&#39;s latest open-weights models: GPT-OSS-120B &lt;a href=&quot;https://huggingface.co/openai/gpt-oss-120b&quot;&gt;‚û°Ô∏è link&lt;/a&gt; and GPT-OSS-20B &lt;a href=&quot;https://huggingface.co/openai/gpt-oss-20b&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[07/15] üåü TensorRT LLM delivers Day-0 support for LG AI Research&#39;s latest model, EXAONE 4.0 &lt;a href=&quot;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[06/17] Join NVIDIA and DeepInfra for a developer meetup on June 26 ‚ú® &lt;a href=&quot;https://events.nvidia.com/scaletheunscalablenextgenai&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[05/22] Blackwell Breaks the 1,000 TPS/User Barrier With Meta‚Äôs Llama 4 Maverick ‚ú® &lt;a href=&quot;https://developer.nvidia.com/blog/blackwell-breaks-the-1000-tps-user-barrier-with-metas-llama-4-maverick/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[04/10] TensorRT LLM DeepSeek R1 performance benchmarking best practices now published. ‚ú® &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/Best_perf_practice_on_DeepSeek-R1_in_TensorRT-LLM.md&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[04/05] TensorRT LLM can run Llama 4 at over 40,000 tokens per second on B200 GPUs!&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/media/l4_launch_perf.png&quot; alt=&quot;L4_perf&quot; /&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;[03/22] TensorRT LLM is now fully open-source, with developments moved to GitHub!&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[03/18] üöÄüöÄ NVIDIA Blackwell Delivers World-Record DeepSeek-R1 Inference Performance with TensorRT LLM &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-blackwell-delivers-world-record-deepseek-r1-inference-performance/&quot;&gt;‚û°Ô∏è Link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[02/28] üåü NAVER Place Optimizes SLM-Based Vertical Services with TensorRT LLM &lt;a href=&quot;https://developer.nvidia.com/blog/spotlight-naver-place-optimizes-slm-based-vertical-services-with-nvidia-tensorrt-llm/&quot;&gt;‚û°Ô∏è Link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[02/25] üåü DeepSeek-R1 performance now optimized for Blackwell &lt;a href=&quot;https://huggingface.co/nvidia/DeepSeek-R1-FP4&quot;&gt;‚û°Ô∏è Link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[02/20] Explore the complete guide to achieve great accuracy, high throughput, and low latency at the lowest cost for your business &lt;a href=&quot;https://www.nvidia.com/en-us/solutions/ai/inference/balancing-cost-latency-and-performance-ebook/?ncid=so-twit-348956&amp;amp;linkId=100000341423615&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[02/18] Unlock #LLM inference with auto-scaling on @AWS EKS ‚ú® &lt;a href=&quot;https://aws.amazon.com/blogs/hpc/scaling-your-llm-inference-workloads-multi-node-deployment-with-tensorrt-llm-and-triton-on-amazon-eks/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[02/12] ü¶∏‚ö° Automating GPU Kernel Generation with DeepSeek-R1 and Inference Time Scaling &lt;a href=&quot;https://developer.nvidia.com/blog/automating-gpu-kernel-generation-with-deepseek-r1-and-inference-time-scaling/?ncid=so-twit-997075&amp;amp;linkId=100000338909937&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;[02/12] üåü How Scaling Laws Drive Smarter, More Powerful AI &lt;a href=&quot;https://blogs.nvidia.com/blog/ai-scaling-laws/?ncid=so-link-889273&amp;amp;linkId=100000338837832&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;details close&gt; 
   &lt;summary&gt;Previous News&lt;/summary&gt; 
   &lt;ul&gt; 
    &lt;li&gt; &lt;p&gt;[2025/01/25] Nvidia moves AI focus to inference cost, efficiency &lt;a href=&quot;https://www.fierceelectronics.com/ai/nvidia-moves-ai-focus-inference-cost-efficiency?linkId=100000332985606&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2025/01/24] üèéÔ∏è Optimize AI Inference Performance with NVIDIA Full-Stack Solutions &lt;a href=&quot;https://developer.nvidia.com/blog/optimize-ai-inference-performance-with-nvidia-full-stack-solutions/?ncid=so-twit-400810&amp;amp;linkId=100000332621049&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2025/01/23] üöÄ Fast, Low-Cost Inference Offers Key to Profitable AI &lt;a href=&quot;https://blogs.nvidia.com/blog/ai-inference-platform/?ncid=so-twit-693236-vt04&amp;amp;linkId=100000332307804&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2025/01/16] Introducing New KV Cache Reuse Optimizations in TensorRT LLM &lt;a href=&quot;https://developer.nvidia.com/blog/introducing-new-kv-cache-reuse-optimizations-in-nvidia-tensorrt-llm/?ncid=so-twit-363876&amp;amp;linkId=100000330323229&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2025/01/14] üì£ Bing&#39;s Transition to LLM/SLM Models: Optimizing Search with TensorRT LLM &lt;a href=&quot;https://blogs.bing.com/search-quality-insights/December-2024/Bing-s-Transition-to-LLM-SLM-Models-Optimizing-Search-with-TensorRT-LLM&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2025/01/04] ‚ö°Boost Llama 3.3 70B Inference Throughput 3x with TensorRT LLM Speculative Decoding &lt;a href=&quot;https://developer.nvidia.com/blog/boost-llama-3-3-70b-inference-throughput-3x-with-nvidia-tensorrt-llm-speculative-decoding/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/12/10] ‚ö° Llama 3.3 70B from AI at Meta is accelerated by TensorRT-LLM. üåü State-of-the-art model on par with Llama 3.1 405B for reasoning, math, instruction following and tool use. Explore the preview &lt;a href=&quot;https://build.nvidia.com/meta/llama-3_3-70b-instruct&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/12/03] üåü Boost your AI inference throughput by up to 3.6x. We now support speculative decoding and tripling token throughput with our NVIDIA TensorRT-LLM. Perfect for your generative AI apps. ‚ö°Learn how in this technical deep dive &lt;a href=&quot;https://nvda.ws/3ZCZTzD&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/12/02] Working on deploying ONNX models for performance-critical applications? Try our NVIDIA Nsight Deep Learning Designer ‚ö° A user-friendly GUI and tight integration with NVIDIA TensorRT that offers: ‚úÖ Intuitive visualization of ONNX model graphs ‚úÖ Quick tweaking of model architecture and parameters ‚úÖ Detailed performance profiling with either ORT or TensorRT ‚úÖ Easy building of TensorRT engines &lt;a href=&quot;https://developer.nvidia.com/nsight-dl-designer?ncid=so-link-485689&amp;amp;linkId=100000315016072&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/11/26] üì£ Introducing TensorRT LLM for Jetson AGX Orin, making it even easier to deploy on Jetson AGX Orin with initial support in JetPack 6.1 via the v0.12.0-jetson branch of the TensorRT LLM repo. ‚úÖ Pre-compiled TensorRT LLM wheels &amp;amp; containers for easy integration ‚úÖ Comprehensive guides &amp;amp; docs to get you started &lt;a href=&quot;https://forums.developer.nvidia.com/t/tensorrt-llm-for-jetson/313227?linkId=100000312718869&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/11/21] NVIDIA TensorRT LLM Multiblock Attention Boosts Throughput by More Than 3x for Long Sequence Lengths on NVIDIA HGX H200 &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-tensorrt-llm-multiblock-attention-boosts-throughput-by-more-than-3x-for-long-sequence-lengths-on-nvidia-hgx-h200/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/11/19] Llama 3.2 Full-Stack Optimizations Unlock High Performance on NVIDIA GPUs &lt;a href=&quot;https://developer.nvidia.com/blog/llama-3-2-full-stack-optimizations-unlock-high-performance-on-nvidia-gpus/?ncid=so-link-721194&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/11/09] üöÄüöÄüöÄ 3x Faster AllReduce with NVSwitch and TensorRT LLM MultiShot &lt;a href=&quot;https://developer.nvidia.com/blog/3x-faster-allreduce-with-nvswitch-and-tensorrt-llm-multishot/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/11/09] ‚ú® NVIDIA advances the AI ecosystem with the AI model of LG AI Research üôå &lt;a href=&quot;https://blogs.nvidia.co.kr/blog/nvidia-lg-ai-research/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/11/02] üåüüåüüåü NVIDIA and LlamaIndex Developer Contest üôå Enter for a chance to win prizes including an NVIDIA¬Æ GeForce RTX‚Ñ¢ 4080 SUPER GPU, DLI credits, and moreüôå &lt;a href=&quot;https://developer.nvidia.com/llamaindex-developer-contest&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/10/28] üèéÔ∏èüèéÔ∏èüèéÔ∏è NVIDIA GH200 Superchip Accelerates Inference by 2x in Multiturn Interactions with Llama Models &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-gh200-superchip-accelerates-inference-by-2x-in-multiturn-interactions-with-llama-models/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/10/22] New üìù Step-by-step instructions on how to ‚úÖ Optimize LLMs with NVIDIA TensorRT-LLM, ‚úÖ Deploy the optimized models with Triton Inference Server, ‚úÖ Autoscale LLMs deployment in a Kubernetes environment. üôå Technical Deep Dive: &lt;a href=&quot;https://nvda.ws/3YgI8UT&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/10/07] üöÄüöÄüöÄOptimizing Microsoft Bing Visual Search with NVIDIA Accelerated Libraries &lt;a href=&quot;https://developer.nvidia.com/blog/optimizing-microsoft-bing-visual-search-with-nvidia-accelerated-libraries/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/09/29] üåü AI at Meta PyTorch + TensorRT v2.4 üåü ‚ö°TensorRT 10.1 ‚ö°PyTorch 2.4 ‚ö°CUDA 12.4 ‚ö°Python 3.12 &lt;a href=&quot;https://github.com/pytorch/TensorRT/releases/tag/v2.4.0&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/09/17] ‚ú® NVIDIA TensorRT LLM Meetup &lt;a href=&quot;https://drive.google.com/file/d/1RR8GqC-QbuaKuHj82rZcXb3MS20SWo6F/view?usp=share_link&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/09/17] ‚ú® Accelerating LLM Inference at Databricks with TensorRT-LLM &lt;a href=&quot;https://drive.google.com/file/d/1NeSmrLaWRJAY1rxD9lJmzpB9rzr38j8j/view?usp=sharing&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/09/17] ‚ú® TensorRT LLM @ Baseten &lt;a href=&quot;https://drive.google.com/file/d/1Y7L2jqW-aRmt31mCdqhwvGMmCSOzBUjG/view?usp=share_link&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/09/04] üèéÔ∏èüèéÔ∏èüèéÔ∏è Best Practices for Tuning TensorRT LLM for Optimal Serving with BentoML &lt;a href=&quot;https://www.bentoml.com/blog/tuning-tensor-rt-llm-for-optimal-serving-with-bentoml&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/08/20] üèéÔ∏èSDXL with #TensorRT Model Optimizer ‚è±Ô∏è‚ö° üèÅ cache diffusion üèÅ quantization aware training üèÅ QLoRA üèÅ #Python 3.12 &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-tensorrt-model-optimizer-v0-15-boosts-inference-performance-and-expands-model-support/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/08/13] üêç DIY Code Completion with #Mamba ‚ö° #TensorRT #LLM for speed ü§ñ NIM for ease ‚òÅÔ∏è deploy anywhere &lt;a href=&quot;https://developer.nvidia.com/blog/revolutionizing-code-completion-with-codestral-mamba-the-next-gen-coding-llm/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/08/06] üó´ Multilingual Challenge Accepted üó´ ü§ñ #TensorRT #LLM boosts low-resource languages like Hebrew, Indonesian and Vietnamese ‚ö°&lt;a href=&quot;https://developer.nvidia.com/blog/accelerating-hebrew-llm-performance-with-nvidia-tensorrt-llm/?linkId=100000278659647&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/07/30] Introducingüçä @SliceXAI ELM Turbo ü§ñ train ELM once ‚ö° #TensorRT #LLM optimize ‚òÅÔ∏è deploy anywhere &lt;a href=&quot;https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/07/23] üëÄ @AIatMeta Llama 3.1 405B trained on 16K NVIDIA H100s - inference is #TensorRT #LLM optimized ‚ö° ü¶ô 400 tok/s - per node ü¶ô 37 tok/s - per user ü¶ô 1 node inference &lt;a href=&quot;https://developer.nvidia.com/blog/supercharging-llama-3-1-across-nvidia-platforms&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/07/09] Checklist to maximize multi-language performance of @meta #Llama3 with #TensorRT #LLM inference: ‚úÖ MultiLingual ‚úÖ NIM ‚úÖ LoRA tuned adaptors&lt;a href=&quot;https://developer.nvidia.com/blog/deploy-multilingual-llms-with-nvidia-nim/&quot;&gt;‚û°Ô∏è Tech blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/07/02] Let the @MistralAI MoE tokens fly üìà üöÄ #Mixtral 8x7B with NVIDIA #TensorRT #LLM on #H100. &lt;a href=&quot;https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm?ncid=so-twit-928467&quot;&gt;‚û°Ô∏è Tech blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/06/24] Enhanced with NVIDIA #TensorRT #LLM, @upstage.ai‚Äôs solar-10.7B-instruct is ready to power your developer projects through our API catalog üèéÔ∏è. ‚ú®&lt;a href=&quot;https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/06/18] CYMI: ü§© Stable Diffusion 3 dropped last week üéä üèéÔ∏è Speed up your SD3 with #TensorRT INT8 Quantization&lt;a href=&quot;https://build.nvidia.com/upstage/solar-10_7b-instruct?snippet_tab=Try&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/06/18] üß∞Deploying ComfyUI with TensorRT? Here‚Äôs your setup guide &lt;a href=&quot;https://github.com/comfyanonymous/ComfyUI_TensorRT&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/06/11] ‚ú®#TensorRT Weight-Stripped Engines ‚ú® Technical Deep Dive for serious coders ‚úÖ+99% compression ‚úÖ1 set of weights ‚Üí ** GPUs ‚úÖ0 performance loss ‚úÖ** models‚Ä¶LLM, CNN, etc.&lt;a href=&quot;https://developer.nvidia.com/blog/maximum-performance-and-minimum-footprint-for-ai-apps-with-nvidia-tensorrt-weight-stripped-engines/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/06/04] ‚ú® #TensorRT and GeForce #RTX unlock ComfyUI SD superhero powers ü¶∏‚ö° üé• Demo: &lt;a href=&quot;https://youtu.be/64QEVfbPHyg&quot;&gt;‚û°Ô∏è link&lt;/a&gt; üìó DIY notebook: &lt;a href=&quot;https://console.brev.dev/launchable/deploy?userID=2x2sil999&amp;amp;orgID=ktj33l4xj&amp;amp;name=ComfyUI_TensorRT&amp;amp;instance=L4%40g2-standard-4%3Anvidia-l4%3A1&amp;amp;diskStorage=500&amp;amp;cloudID=GCP&amp;amp;baseImage=docker.io%2Fpytorch%2Fpytorch%3A2.2.0-cuda12.1-cudnn8-runtime&amp;amp;ports=ComfUI%3A8188&amp;amp;file=https%3A%2F%2Fgithub.com%2Fbrevdev%2Fnotebooks%2Fblob%2Fmain%2Ftensorrt-comfyui.ipynb&amp;amp;launchableID=env-2hQX3n7ae5mq3NjNZ32DfAG0tJf&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/05/28] ‚ú®#TensorRT weight stripping for ResNet-50 ‚ú® ‚úÖ+99% compression ‚úÖ1 set of weights ‚Üí ** GPUs\ ‚úÖ0 performance loss ‚úÖ** models‚Ä¶LLM, CNN, etc üëÄ üìö DIY &lt;a href=&quot;https://console.brev.dev/launchable/deploy?userID=2x2sil999&amp;amp;orgID=ktj33l4xj&amp;amp;launchableID=env-2h6bym7h5GFNho3vpWQQeUYMwTM&amp;amp;instance=L4%40g6.xlarge&amp;amp;diskStorage=500&amp;amp;cloudID=devplane-brev-1&amp;amp;baseImage=nvcr.io%2Fnvidia%2Ftensorrt%3A24.05-py3&amp;amp;file=https%3A%2F%2Fgithub.com%2FNVIDIA%2FTensorRT%2Fblob%2Frelease%2F10.0%2Fsamples%2Fpython%2Fsample_weight_stripping%2Fnotebooks%2Fweight_stripping.ipynb&amp;amp;name=tensorrt_weight_stripping_resnet50&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/05/21] ‚ú®@modal_labs has the codes for serverless @AIatMeta Llama 3 on #TensorRT #LLM ‚ú®üëÄ üìö Marvelous Modal Manual: Serverless TensorRT LLM (LLaMA 3 8B) | Modal Docs &lt;a href=&quot;https://modal.com/docs/examples/trtllm_llama&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/05/08] NVIDIA TensorRT Model Optimizer -- the newest member of the #TensorRT ecosystem is a library of post-training and training-in-the-loop model optimization techniques ‚úÖquantization ‚úÖsparsity ‚úÖQAT &lt;a href=&quot;https://developer.nvidia.com/blog/accelerate-generative-ai-inference-performance-with-nvidia-tensorrt-model-optimizer-now-publicly-available/&quot;&gt;‚û°Ô∏è blog&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/05/07] ü¶ôü¶ôü¶ô 24,000 tokens per second üõ´Meta Llama 3 takes off with #TensorRT #LLM üìö&lt;a href=&quot;https://blogs.nvidia.com/blog/meta-llama3-inference-acceleration/&quot;&gt;‚û°Ô∏è link&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/02/06] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/quantization-in-TRT-LLM.md&quot;&gt;üöÄ Speed up inference with SOTA quantization techniques in TRT-LLM&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2024/01/30] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/XQA-kernel.md&quot;&gt; New XQA-kernel provides 2.4x more Llama-70B throughput within the same latency budget&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2023/12/04] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/Falcon180B-H200.md&quot;&gt;Falcon-180B on a single H200 GPU with INT4 AWQ, and 6.7x faster Llama-70B over A100&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2023/11/27] &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/boost-inference-performance-for-llms-with-new-amazon-sagemaker-containers/&quot;&gt;SageMaker LMI now supports TensorRT LLM - improves throughput by 60%, compared to previous version&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2023/11/13] &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/docs/source/blogs/H200launch.md&quot;&gt;H200 achieves nearly 12,000 tok/sec on Llama2-13B&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2023/10/22] &lt;a href=&quot;https://github.com/NVIDIA/trt-llm-rag-windows#readme&quot;&gt;üöÄ RAG on Windows using TensorRT LLM and LlamaIndex ü¶ô&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2023/10/19] Getting Started Guide - &lt;a href=&quot;https://developer.nvidia.com/blog/optimizing-inference-on-llms-with-tensorrt-llm-now-publicly-available/&quot;&gt;Optimizing Inference on Large Language Models with NVIDIA TensorRT-LLM, Now Publicly Available &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
    &lt;li&gt; &lt;p&gt;[2023/10/17] &lt;a href=&quot;https://blogs.nvidia.com/blog/2023/10/17/tensorrt-llm-windows-stable-diffusion-rtx/&quot;&gt;Large Language Models up to 4x Faster on RTX With TensorRT LLM for Windows &lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;/ul&gt; 
  &lt;/details&gt; 
  &lt;h2&gt;TensorRT LLM Overview&lt;/h2&gt; 
  &lt;p&gt;TensorRT LLM is an open-sourced library for optimizing Large Language Model (LLM) inference. It provides state-of-the-art optimizations, including custom attention kernels, inflight batching, paged KV caching, quantization (FP8, &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/&quot;&gt;FP4&lt;/a&gt;, INT4 &lt;a href=&quot;https://arxiv.org/abs/2306.00978&quot;&gt;AWQ&lt;/a&gt;, INT8 &lt;a href=&quot;https://arxiv.org/abs/2211.10438&quot;&gt;SmoothQuant&lt;/a&gt;, ...), speculative decoding, and much more, to perform inference efficiently on NVIDIA GPUs.&lt;/p&gt; 
  &lt;p&gt;&lt;a href=&quot;https://github.com/NVIDIA/TensorRT-LLM/raw/main/docs/source/torch/arch_overview.md&quot;&gt;Architected on PyTorch&lt;/a&gt;, TensorRT LLM provides a high-level Python &lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html#llm-api&quot;&gt;LLM API&lt;/a&gt; that supports a wide range of inference setups - from single-GPU to multi-GPU or multi-node deployments. It includes built-in support for various parallelism strategies and advanced features. The LLM API integrates seamlessly with the broader inference ecosystem, including NVIDIA &lt;a href=&quot;https://github.com/ai-dynamo/dynamo&quot;&gt;Dynamo&lt;/a&gt; and the &lt;a href=&quot;https://github.com/triton-inference-server/server&quot;&gt;Triton Inference Server&lt;/a&gt;.&lt;/p&gt; 
  &lt;p&gt;TensorRT LLM is designed to be modular and easy to modify. Its PyTorch-native architecture allows developers to experiment with the runtime or extend functionality. Several popular models are also pre-defined and can be customized using &lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/tensorrt_llm/_torch/models/modeling_deepseekv3.py&quot;&gt;native PyTorch code&lt;/a&gt;, making it easy to adapt the system to specific needs.&lt;/p&gt; 
  &lt;h2&gt;Getting Started&lt;/h2&gt; 
  &lt;p&gt;To get started with TensorRT-LLM, visit our documentation:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/quick-start-guide.html&quot;&gt;Quick Start Guide&lt;/a&gt; 
    &lt;ul&gt; 
     &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/NVIDIA/TensorRT-LLM/main/examples/models/core/deepseek_v3&quot;&gt;Running DeepSeek&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/installation/linux.html&quot;&gt;Installation Guide for Linux&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/installation/grace-hopper.html&quot;&gt;Installation Guide for Grace Hopper&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/reference/support-matrix.html&quot;&gt;Supported Hardware, Models, and other Software&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/performance/performance-tuning-guide/benchmarking-default-performance.html#benchmarking-with-trtllm-bench&quot;&gt;Benchmarking Performance&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/release-notes.html&quot;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Deprecation Policy&lt;/h2&gt; 
  &lt;p&gt;Deprecation is used to inform developers that some APIs and tools are no longer recommended for use. Beginning with version 1.0, TensorRT LLM has the following deprecation policy:&lt;/p&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Communication of Deprecation&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Deprecation notices are documented in the Release Notes.&lt;/li&gt; 
   &lt;li&gt;Deprecated APIs, methods, classes, or parameters include a statement in the source code indicating when they were deprecated.&lt;/li&gt; 
   &lt;li&gt;If used, deprecated methods, classes, or parameters issue runtime deprecation warnings.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol start=&quot;2&quot;&gt; 
   &lt;li&gt;Migration Period&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;TensorRT LLM provides a 3-month migration period after deprecation.&lt;/li&gt; 
   &lt;li&gt;During this period, deprecated APIs, tools, or parameters continue to work but trigger warnings.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol start=&quot;3&quot;&gt; 
   &lt;li&gt;Scope of Deprecation&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Full API/Method/Class Deprecation: The entire API/method/class is marked for removal.&lt;/li&gt; 
   &lt;li&gt;Partial Deprecation: If only specific parameters of an API/method are deprecated (e.g., param1 in LLM.generate(param1, param2)), the method itself remains functional, but the deprecated parameters will be removed in a future release.&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol start=&quot;4&quot;&gt; 
   &lt;li&gt;Removal After Migration Period&lt;/li&gt; 
  &lt;/ol&gt; 
  &lt;ul&gt; 
   &lt;li&gt;After the 3-month migration period ends, deprecated APIs, tools, or parameters are removed in a manner consistent with semantic versioning (major version changes may include breaking removals).&lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;h2&gt;Useful Links&lt;/h2&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://huggingface.co/collections/nvidia/model-optimizer-66aa84f7966b3150262481a4&quot;&gt;Quantized models on Hugging Face&lt;/a&gt;: A growing collection of quantized (e.g., FP8, FP4) and optimized LLMs, including &lt;a href=&quot;https://huggingface.co/nvidia/DeepSeek-R1-FP4&quot;&gt;DeepSeek FP4&lt;/a&gt;, ready for fast inference with TensorRT LLM.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/ai-dynamo/dynamo&quot;&gt;NVIDIA Dynamo&lt;/a&gt;: A datacenter scale distributed inference serving framework that works seamlessly with TensorRT LLM.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://nvidia.github.io/TensorRT-LLM/torch/auto_deploy/auto-deploy.html&quot;&gt;AutoDeploy&lt;/a&gt;: A prototype backend for TensorRT LLM to simplify and accelerate the deployment of PyTorch models.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIA/TensorRT-LLM/issues/5359&quot;&gt;WeChat Discussion Group&lt;/a&gt;: A real-time channel for TensorRT LLM Q&amp;amp;A and news.&lt;/li&gt; 
  &lt;/ul&gt; 
 &lt;/div&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>dalathegreat/Battery-Emulator</title>
      <link>https://github.com/dalathegreat/Battery-Emulator</link>
      <description>&lt;p&gt;This revolutionary software enables EV battery packs to be easily reused for stationary storage in combination with solar inverters&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Battery-Emulator ‚ö°üîã&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/dalathegreat/BYD-Battery-Emulator-For-Gen24?color=%23008000&quot; alt=&quot;GitHub release (with filter)&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/stars/dalathegreat/Battery-Emulator?style=flat&amp;amp;color=%23128512&quot; alt=&quot;GitHub Repo stars&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/forks/dalathegreat/Battery-Emulator?style=flat&amp;amp;color=%23128512&quot; alt=&quot;GitHub forks&quot; /&gt; &lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/dalathegreat/BYD-Battery-Emulator-For-Gen24/compile-common-image-lilygo-TCAN.yml?color=0E810E&quot; alt=&quot;GitHub actions&quot; /&gt; &lt;img src=&quot;https://img.shields.io/badge/made-with_love-blue?color=%23008000&quot; alt=&quot;Static Badge&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;What is Battery Emulator?&lt;/h2&gt; 
&lt;p&gt;Many manufacturers sell home battery systems to enable homeowners to store power collected from the grid, or renewable sources, to use at times when electricity demand is higher. However almost all of these home battery systems charge a high cost for every kilowatt hour (kWh) of capacity you buy.&lt;/p&gt; 
&lt;p&gt;At the same time, EV manufacturers have been putting high capacity battery packs into their cars, with no firm plan for what should happen to those batteries if the car is damaged in a crash, or reaches the end of its life as a vehicle.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Battery Emulator&lt;/strong&gt; enables EV battery packs to be repurposed for stationary storage. It acts as a translation layer between the EV battery and the home inverter. This makes it extremely cheap and easy to use large EV batteries in a true plug&#39;n&#39;play fashion!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION] Working with high voltage is dangerous. Always follow local laws and regulations regarding high voltage work. If you are unsure about the rules in your country, consult a licensed electrician for more information.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Quickstart guide üìú&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pick a &lt;a href=&quot;https://github.com/dalathegreat/Battery-Emulator/wiki#supported-inverters-list&quot;&gt;supported inverter&lt;/a&gt; (solar panels optional) &lt;span&gt;üåû&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;Pick a &lt;a href=&quot;https://github.com/dalathegreat/Battery-Emulator/wiki#supported-batteries-list&quot;&gt;supported battery&lt;/a&gt; &lt;span&gt;üîã&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;Order the Battery-Emulator &lt;a href=&quot;https://github.com/dalathegreat/Battery-Emulator/wiki#where-do-i-get-the-hardware-needed&quot;&gt;compatible hardware&lt;/a&gt; &lt;span&gt;ü§ñ&lt;/span&gt;&lt;/li&gt; 
 &lt;li&gt;Follow the &lt;a href=&quot;https://github.com/dalathegreat/Battery-Emulator/wiki/Installation-guidelines&quot;&gt;installation guidelines&lt;/a&gt; section for how to install and commission your battery properly &lt;span&gt;üìì&lt;/span&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation basics ü™õ&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Connect your Battery Emulator hardware to your EV battery&lt;/li&gt; 
 &lt;li&gt;Connect your Battery Emulator hardware to your inverter&lt;/li&gt; 
 &lt;li&gt;Wire up high voltage cable between the inverter and the battery&lt;/li&gt; 
 &lt;li&gt;Add a low voltage power supply to your Battery Emulator hardware&lt;/li&gt; 
 &lt;li&gt;Configure any additional requirements to allow Battery Emulator to switch on your EV battery (also referred to as &#39;closing contactors&#39;)&lt;/li&gt; 
 &lt;li&gt;Enjoy a big cheap grid connected battery!&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For examples showing wiring, see each battery type&#39;s own Wiki page. For instance the &lt;a href=&quot;https://github.com/dalathegreat/Battery-Emulator/wiki/Battery:-Nissan-LEAF---e%E2%80%90NV200&quot;&gt;Nissan LEAF page&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;How to install the software üíª&lt;/h2&gt; 
&lt;p&gt;Start by watching this &lt;a href=&quot;https://www.youtube.com/watch?v=sR3t7j0R9Z0&quot;&gt;quickstart guide&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=sR3t7j0R9Z0&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/sR3t7j0R9Z0/0.jpg&quot; alt=&quot;IMAGE ALT TEXT HERE&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open the &lt;a href=&quot;https://dalathegreat.github.io/BE-Web-Installer/&quot;&gt;webinstaller page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Follow the instructions on that page to install the software&lt;/li&gt; 
 &lt;li&gt;After successful installation, connect to the wireless network (Battery-Emulator , password: 123456789)&lt;/li&gt; 
 &lt;li&gt;Go to setup page and configure component selection&lt;/li&gt; 
 &lt;li&gt;(OPTIONAL, connect the board to your home Wifi)&lt;/li&gt; 
 &lt;li&gt;Connect your battery and inverter to the board and you are done! üîã‚ö°&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Dependencies üìñ&lt;/h2&gt; 
&lt;p&gt;This code uses the following excellent libraries:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/adafruit/Adafruit_NeoPixel&quot;&gt;adafruit/Adafruit_NeoPixel&lt;/a&gt; LGPL-3.0 license&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ayushsharma82/ElegantOTA&quot;&gt;ayushsharma82/ElegantOTA&lt;/a&gt; AGPL-3.0 license&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bblanchon/ArduinoJson&quot;&gt;bblanchon/ArduinoJson&lt;/a&gt; MIT-License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eModbus/eModbus&quot;&gt;eModbus/eModbus&lt;/a&gt; MIT-License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ESP32Async/AsyncTCP&quot;&gt;ESP32Async/AsyncTCP&lt;/a&gt; LGPL-3.0 license&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ESP32Async/ESPAsyncWebServer&quot;&gt;ESP32Async/ESPAsyncWebServer&lt;/a&gt; LGPL-3.0 license&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pierremolinaro/acan-esp32&quot;&gt;pierremolinaro/acan-esp32&lt;/a&gt; MIT-License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pierremolinaro/acan2515&quot;&gt;pierremolinaro/acan2515&lt;/a&gt; MIT-License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pierremolinaro/acan2517FD&quot;&gt;pierremolinaro/acan2517FD&lt;/a&gt; MIT-License&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It is also based on the information found in the following excellent repositories/websites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gitlab.com/pelle8/inverter_resources&quot;&gt;https://gitlab.com/pelle8/inverter_resources&lt;/a&gt; //new url&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/burra/byd_battery&quot;&gt;https://github.com/burra/byd_battery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/flodorn/TeslaBMSV2&quot;&gt;https://github.com/flodorn/TeslaBMSV2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SunshadeCorp/can-service&quot;&gt;https://github.com/SunshadeCorp/can-service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/openvehicles/Open-Vehicle-Monitoring-System-3&quot;&gt;https://github.com/openvehicles/Open-Vehicle-Monitoring-System-3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dalathegreat/leaf_can_bus_messages&quot;&gt;https://github.com/dalathegreat/leaf_can_bus_messages&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rand12345/solax_can_bus&quot;&gt;https://github.com/rand12345/solax_can_bus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Tom-evnut/BMWI3BMS/&quot;&gt;https://github.com/Tom-evnut/BMWI3BMS/&lt;/a&gt; SMA-CAN&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/FozzieUK/FoxESS-Canbus-Protocol&quot;&gt;https://github.com/FozzieUK/FoxESS-Canbus-Protocol&lt;/a&gt; FoxESS-CAN&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maciek16c/hyundai-santa-fe-phev-battery&quot;&gt;https://github.com/maciek16c/hyundai-santa-fe-phev-battery&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ljames28/Renault-Zoe-PH2-ZE50-Canbus-LBC-Information&quot;&gt;https://github.com/ljames28/Renault-Zoe-PH2-ZE50-Canbus-LBC-Information&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Renault Zoe CAN Matrix &lt;a href=&quot;https://docs.google.com/spreadsheets/u/0/d/1Qnk-yzzcPiMArO-QDzO4a8ptAS2Sa4HhVu441zBzlpM/edit?pli=1#gid=0&quot;&gt;https://docs.google.com/spreadsheets/u/0/d/1Qnk-yzzcPiMArO-QDzO4a8ptAS2Sa4HhVu441zBzlpM/edit?pli=1#gid=0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Pylon hacking &lt;a href=&quot;https://www.eevblog.com/forum/programming/pylontech-sc0500-protocol-hacking/&quot;&gt;https://www.eevblog.com/forum/programming/pylontech-sc0500-protocol-hacking/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Like this project? üíñ&lt;/h2&gt; 
&lt;p&gt;Leave a ‚≠ê If you think this project is useful. Consider hopping onto my Patreon to encourage more open-source projects! As a bonus, you will get access to the Discord server, where we hangout, develop, support, share, discuss etc. all things related to DIY EV storage solutions. See you on the server? ;)&lt;/p&gt; 
&lt;a href=&quot;https://www.patreon.com/dala&quot;&gt; &lt;img src=&quot;https://c5.patreon.com/external/logo/become_a_patron_button@2x.png&quot; width=&quot;160&quot; /&gt; &lt;/a&gt; &amp;lt;------ Click here to learn more! 
&lt;p&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/66b8e967-7f5e-409d-91ec-d012489a86d2&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>protocolbuffers/protobuf</title>
      <link>https://github.com/protocolbuffers/protobuf</link>
      <description>&lt;p&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Protocol Buffers - Google&#39;s data interchange format&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/protocolbuffers/protobuf&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/protocolbuffers/protobuf/badge&quot; alt=&quot;OpenSSF Scorecard&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2023 Google LLC&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Protocol Buffers (a.k.a., protobuf) are Google&#39;s language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can learn more about it in &lt;a href=&quot;https://protobuf.dev&quot;&gt;protobuf&#39;s documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.&lt;/p&gt; 
&lt;h2&gt;Working With Protobuf Source Code&lt;/h2&gt; 
&lt;p&gt;Most users will find working from &lt;a href=&quot;https://github.com/protocolbuffers/protobuf/releases&quot;&gt;supported releases&lt;/a&gt; to be the easiest path.&lt;/p&gt; 
&lt;p&gt;If you choose to work from the head revision of the main branch your build will occasionally be broken by source-incompatible changes and insufficiently-tested (and therefore broken) behavior.&lt;/p&gt; 
&lt;p&gt;If you are using C++ or otherwise need to build protobuf from source as a part of your project, you should pin to a release commit on a release branch.&lt;/p&gt; 
&lt;p&gt;This is because even release branches can experience some instability in between release commits.&lt;/p&gt; 
&lt;h3&gt;Bazel with Bzlmod&lt;/h3&gt; 
&lt;p&gt;Protobuf supports &lt;a href=&quot;https://bazel.build/external/module&quot;&gt;Bzlmod&lt;/a&gt; with Bazel 7 +. Users should specify a dependency on protobuf in their MODULE.bazel file as follows.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bazel_dep(name = &quot;protobuf&quot;, version = &amp;lt;VERSION&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Users can optionally override the repo name, such as for compatibility with WORKSPACE.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bazel_dep(name = &quot;protobuf&quot;, version = &amp;lt;VERSION&amp;gt;, repo_name = &quot;com_google_protobuf&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Bazel with WORKSPACE&lt;/h3&gt; 
&lt;p&gt;Users can also add the following to their legacy &lt;a href=&quot;https://bazel.build/external/overview#workspace-system&quot;&gt;WORKSPACE&lt;/a&gt; file.&lt;/p&gt; 
&lt;p&gt;Note that with the release of 30.x there are a few more load statements to properly set up rules_java and rules_python.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;http_archive(
    name = &quot;com_google_protobuf&quot;,
    strip_prefix = &quot;protobuf-VERSION&quot;,
    sha256 = ...,
    url = ...,
)

load(&quot;@com_google_protobuf//:protobuf_deps.bzl&quot;, &quot;protobuf_deps&quot;)

protobuf_deps()

load(&quot;@rules_java//java:rules_java_deps.bzl&quot;, &quot;rules_java_dependencies&quot;)

rules_java_dependencies()

load(&quot;@rules_java//java:repositories.bzl&quot;, &quot;rules_java_toolchains&quot;)

rules_java_toolchains()

load(&quot;@rules_python//python:repositories.bzl&quot;, &quot;py_repositories&quot;)

py_repositories()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Protobuf Compiler Installation&lt;/h2&gt; 
&lt;p&gt;The protobuf compiler is written in C++. If you are using C++, please follow the &lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&quot;&gt;C++ Installation Instructions&lt;/a&gt; to install protoc along with the C++ runtime.&lt;/p&gt; 
&lt;p&gt;For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our &lt;a href=&quot;https://github.com/protocolbuffers/protobuf/releases&quot;&gt;GitHub release page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In the downloads section of each release, you can find pre-built binaries in zip packages: &lt;code&gt;protoc-$VERSION-$PLATFORM.zip&lt;/code&gt;. It contains the protoc binary as well as a set of standard &lt;code&gt;.proto&lt;/code&gt; files distributed along with protobuf.&lt;/p&gt; 
&lt;p&gt;If you are looking for an old version that is not available in the release page, check out the &lt;a href=&quot;https://repo1.maven.org/maven2/com/google/protobuf/protoc/&quot;&gt;Maven repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it&#39;s recommended to build your own protoc binary from source.&lt;/p&gt; 
&lt;p&gt;If you would like to build protoc binary from source, see the &lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md&quot;&gt;C++ Installation Instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Protobuf Runtime Installation&lt;/h2&gt; 
&lt;p&gt;Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Language&lt;/th&gt; 
   &lt;th&gt;Source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C++ (include C++ runtime and protoc)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src&quot;&gt;src&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Java&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/java&quot;&gt;java&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Python&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/python&quot;&gt;python&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Objective-C&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/objectivec&quot;&gt;objectivec&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;C#&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/csharp&quot;&gt;csharp&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ruby&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/ruby&quot;&gt;ruby&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Go&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/protocolbuffers/protobuf-go&quot;&gt;protocolbuffers/protobuf-go&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;PHP&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/php&quot;&gt;php&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dart&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/dart-lang/protobuf&quot;&gt;dart-lang/protobuf&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;JavaScript&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/protocolbuffers/protobuf-javascript&quot;&gt;protocolbuffers/protobuf-javascript&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The best way to learn how to use protobuf is to follow the &lt;a href=&quot;https://protobuf.dev/getting-started&quot;&gt;tutorials in our developer guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to learn from code examples, take a look at the examples in the &lt;a href=&quot;https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples&quot;&gt;examples&lt;/a&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The complete documentation is available at the &lt;a href=&quot;https://protobuf.dev&quot;&gt;Protocol Buffers doc site&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support Policy&lt;/h2&gt; 
&lt;p&gt;Read about our &lt;a href=&quot;https://protobuf.dev/version-support/&quot;&gt;version support policy&lt;/a&gt; to stay current on support timeframes for the language libraries.&lt;/p&gt; 
&lt;h2&gt;Developer Community&lt;/h2&gt; 
&lt;p&gt;To be alerted to upcoming changes in Protocol Buffers and connect with protobuf developers and users, &lt;a href=&quot;https://groups.google.com/g/protobuf&quot;&gt;join the Google Group&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ocornut/imgui</title>
      <link>https://github.com/ocornut/imgui</link>
      <description>&lt;p&gt;Dear ImGui: Bloat-free Graphical User interface for C++ with minimal dependencies&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dear ImGui&lt;/h1&gt; 
&lt;center&gt;
 &lt;b&gt;&lt;i&gt;&quot;Give someone state and they&#39;ll have a bug one day, but teach them how to represent state in two separate locations that have to be kept in sync and they&#39;ll have bugs for a lifetime.&quot;&lt;/i&gt;&lt;/b&gt;
&lt;/center&gt; 
&lt;a href=&quot;https://twitter.com/rygorous/status/1507178315886444544&quot;&gt;-ryg&lt;/a&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ocornut/imgui/actions?workflow=build&quot;&gt;&lt;img src=&quot;https://github.com/ocornut/imgui/workflows/build/badge.svg?sanitize=true&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ocornut/imgui/actions?workflow=static-analysis&quot;&gt;&lt;img src=&quot;https://github.com/ocornut/imgui/workflows/static-analysis/badge.svg?sanitize=true&quot; alt=&quot;Static Analysis Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ocornut/imgui_test_engine/actions?workflow=tests&quot;&gt;&lt;img src=&quot;https://github.com/ocornut/imgui_test_engine/workflows/tests/badge.svg?sanitize=true&quot; alt=&quot;Tests Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;sub&gt;(This library is available under a free and permissive license, but needs financial support to sustain its continued improvements. In addition to maintenance and stability there are many desirable features yet to be added. If your company is using Dear ImGui, please consider reaching out.)&lt;/sub&gt;&lt;/p&gt; 
&lt;p&gt;Businesses: support continued development and maintenance via invoiced sponsoring/support contracts: &lt;br /&gt;&amp;nbsp;&amp;nbsp;&lt;em&gt;E-mail: contact @ dearimgui dot com&lt;/em&gt; &lt;br /&gt;Individuals: support continued development and maintenance &lt;a href=&quot;https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=WGHNC6MBFLZ2S&quot;&gt;here&lt;/a&gt;. Also see &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Funding&quot;&gt;Funding&lt;/a&gt; page.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#the-pitch&quot;&gt;The Pitch&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#usage&quot;&gt;Usage&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#how-it-works&quot;&gt;How it works&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#releases--changelogs&quot;&gt;Releases &amp;amp; Changelogs&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#demo&quot;&gt;Demo&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#getting-started--integration&quot;&gt;Getting Started &amp;amp; Integration&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#gallery&quot;&gt;Gallery&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#support-frequently-asked-questions-faq&quot;&gt;Support, FAQ&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#how-to-help&quot;&gt;How to help&lt;/a&gt; - &lt;strong&gt;&lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Funding&quot;&gt;Funding &amp;amp; Sponsors&lt;/a&gt;&lt;/strong&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#credits&quot;&gt;Credits&lt;/a&gt; - &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#license&quot;&gt;License&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ocornut/imgui/wiki&quot;&gt;Wiki&lt;/a&gt; - &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Useful-Extensions&quot;&gt;Extensions&lt;/a&gt; - &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Bindings&quot;&gt;Language bindings &amp;amp; framework backends&lt;/a&gt; - &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui&quot;&gt;Software using Dear ImGui&lt;/a&gt; - &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Quotes&quot;&gt;User quotes&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;The Pitch&lt;/h3&gt; 
&lt;p&gt;Dear ImGui is a &lt;strong&gt;bloat-free graphical user interface library for C++&lt;/strong&gt;. It outputs optimized vertex buffers that you can render anytime in your 3D-pipeline-enabled application. It is fast, portable, renderer agnostic, and self-contained (no external dependencies).&lt;/p&gt; 
&lt;p&gt;Dear ImGui is designed to &lt;strong&gt;enable fast iterations&lt;/strong&gt; and to &lt;strong&gt;empower programmers&lt;/strong&gt; to create &lt;strong&gt;content creation tools and visualization / debug tools&lt;/strong&gt; (as opposed to UI for the average end-user). It favors simplicity and productivity toward this goal and lacks certain features commonly found in more high-level libraries. Among other things, full internationalization (right-to-left text, bidirectional text, text shaping etc.) and accessibility features are not supported.&lt;/p&gt; 
&lt;p&gt;Dear ImGui is particularly suited to integration in game engines (for tooling), real-time 3D applications, fullscreen applications, embedded applications, or any applications on console platforms where operating system features are non-standard.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Minimize state synchronization.&lt;/li&gt; 
 &lt;li&gt;Minimize UI-related state storage on user side.&lt;/li&gt; 
 &lt;li&gt;Minimize setup and maintenance.&lt;/li&gt; 
 &lt;li&gt;Easy to use to create dynamic UI which are the reflection of a dynamic data set.&lt;/li&gt; 
 &lt;li&gt;Easy to use to create code-driven and data-driven tools.&lt;/li&gt; 
 &lt;li&gt;Easy to use to create ad hoc short-lived tools and long-lived, more elaborate tools.&lt;/li&gt; 
 &lt;li&gt;Easy to hack and improve.&lt;/li&gt; 
 &lt;li&gt;Portable, minimize dependencies, run on target (consoles, phones, etc.).&lt;/li&gt; 
 &lt;li&gt;Efficient runtime and memory consumption.&lt;/li&gt; 
 &lt;li&gt;Battle-tested, used by &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui&quot;&gt;many major actors in the game industry&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The core of Dear ImGui is self-contained within a few platform-agnostic files&lt;/strong&gt; which you can easily compile in your application/engine. They are all the files in the root folder of the repository (imgui*.cpp, imgui*.h). &lt;strong&gt;No specific build process is required&lt;/strong&gt;. You can add the .cpp files into your existing project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Backends for a variety of graphics API and rendering platforms&lt;/strong&gt; are provided in the &lt;a href=&quot;https://github.com/ocornut/imgui/tree/master/backends&quot;&gt;backends/&lt;/a&gt; folder, along with example applications in the &lt;a href=&quot;https://github.com/ocornut/imgui/tree/master/examples&quot;&gt;examples/&lt;/a&gt; folder. You may also create your own backend. Anywhere where you can render textured triangles, you can render Dear ImGui.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/ocornut/imgui/master/#getting-started--integration&quot;&gt;Getting Started &amp;amp; Integration&lt;/a&gt; section of this document for more details.&lt;/p&gt; 
&lt;p&gt;After Dear ImGui is set up in your application, you can use it from _anywhere_ in your program loop:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;ImGui::Text(&quot;Hello, world %d&quot;, 123);
if (ImGui::Button(&quot;Save&quot;))
    MySaveFunction();
ImGui::InputText(&quot;string&quot;, buf, IM_ARRAYSIZE(buf));
ImGui::SliderFloat(&quot;float&quot;, &amp;amp;f, 0.0f, 1.0f);
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/8225057/191050833-b7ecf528-bfae-4a9f-ac1b-f3d83437a2f4.png&quot; alt=&quot;sample code output (dark, segoeui font, freetype)&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/8225057/191050838-8742efd4-504d-4334-a9a2-e756d15bc2ab.png&quot; alt=&quot;sample code output (light, segoeui font, freetype)&quot; /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-cpp&quot;&gt;// Create a window called &quot;My First Tool&quot;, with a menu bar.
ImGui::Begin(&quot;My First Tool&quot;, &amp;amp;my_tool_active, ImGuiWindowFlags_MenuBar);
if (ImGui::BeginMenuBar())
{
    if (ImGui::BeginMenu(&quot;File&quot;))
    {
        if (ImGui::MenuItem(&quot;Open..&quot;, &quot;Ctrl+O&quot;)) { /* Do stuff */ }
        if (ImGui::MenuItem(&quot;Save&quot;, &quot;Ctrl+S&quot;))   { /* Do stuff */ }
        if (ImGui::MenuItem(&quot;Close&quot;, &quot;Ctrl+W&quot;))  { my_tool_active = false; }
        ImGui::EndMenu();
    }
    ImGui::EndMenuBar();
}

// Edit a color stored as 4 floats
ImGui::ColorEdit4(&quot;Color&quot;, my_color);

// Generate samples and plot them
float samples[100];
for (int n = 0; n &amp;lt; 100; n++)
    samples[n] = sinf(n * 0.2f + ImGui::GetTime() * 1.5f);
ImGui::PlotLines(&quot;Samples&quot;, samples, 100);

// Display contents in a scrolling region
ImGui::TextColored(ImVec4(1,1,0,1), &quot;Important Stuff&quot;);
ImGui::BeginChild(&quot;Scrolling&quot;);
for (int n = 0; n &amp;lt; 50; n++)
    ImGui::Text(&quot;%04d: Some text&quot;, n);
ImGui::EndChild();
ImGui::End();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/8225057/191055698-690a5651-458f-4856-b5a9-e8cc95c543e2.gif&quot; alt=&quot;my_first_tool_v188&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;Dear ImGui allows you to &lt;strong&gt;create elaborate tools&lt;/strong&gt; as well as very short-lived ones. On the extreme side of short-livedness: using the Edit&amp;amp;Continue (hot code reload) feature of modern compilers you can add a few widgets to tweak variables while your application is running, and remove the code a minute later! Dear ImGui is not just for tweaking values. You can use it to trace a running algorithm by just emitting text commands. You can use it along with your own reflection data to browse your dataset live. You can use it to expose the internals of a subsystem in your engine, to create a logger, an inspection tool, a profiler, a debugger, an entire game-making editor/framework, etc.&lt;/p&gt; 
&lt;h3&gt;How it works&lt;/h3&gt; 
&lt;p&gt;The IMGUI paradigm through its API tries to minimize superfluous state duplication, state synchronization, and state retention from the user&#39;s point of view. It is less error-prone (less code and fewer bugs) than traditional retained-mode interfaces, and lends itself to creating dynamic user interfaces. Check out the Wiki&#39;s &lt;a href=&quot;https://github.com/ocornut/imgui/wiki#about-the-imgui-paradigm&quot;&gt;About the IMGUI paradigm&lt;/a&gt; section for more details.&lt;/p&gt; 
&lt;p&gt;Dear ImGui outputs vertex buffers and command lists that you can easily render in your application. The number of draw calls and state changes required to render them is fairly small. Because Dear ImGui doesn&#39;t know or touch graphics state directly, you can call its functions anywhere in your code (e.g. in the middle of a running algorithm, or in the middle of your own rendering process). Refer to the sample applications in the examples/ folder for instructions on how to integrate Dear ImGui with your existing codebase.&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;A common misunderstanding is to mistake immediate mode GUI for immediate mode rendering, which usually implies hammering your driver/GPU with a bunch of inefficient draw calls and state changes as the GUI functions are called. This is NOT what Dear ImGui does. Dear ImGui outputs vertex buffers and a small list of draw calls batches. It never touches your GPU directly. The draw call batches are decently optimal and you can render them later, in your app or even remotely.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Releases &amp;amp; Changelogs&lt;/h3&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/ocornut/imgui/releases&quot;&gt;Releases&lt;/a&gt; page for decorated Changelogs. Reading the changelogs is a good way to keep up to date with the things Dear ImGui has to offer, and maybe will give you ideas of some features that you&#39;ve been ignoring until now!&lt;/p&gt; 
&lt;h3&gt;Demo&lt;/h3&gt; 
&lt;p&gt;Calling the &lt;code&gt;ImGui::ShowDemoWindow()&lt;/code&gt; function will create a demo window showcasing a variety of features and examples. The code is always available for reference in &lt;code&gt;imgui_demo.cpp&lt;/code&gt;. &lt;a href=&quot;https://raw.githubusercontent.com/wiki/ocornut/imgui/web/v167/v167-misc.png&quot;&gt;Here&#39;s how the demo looks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You should be able to build the examples from sources. If you don&#39;t, let us know! If you want to have a quick look at some Dear ImGui features, you can download Windows binaries of the demo app here:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.dearimgui.com/binaries/imgui-demo-binaries-20250625.zip&quot;&gt;imgui-demo-binaries-20250625.zip&lt;/a&gt; (Windows, 1.92.0, built 2025/06/25, master) or &lt;a href=&quot;https://www.dearimgui.com/binaries&quot;&gt;older binaries&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The demo applications are not DPI aware so expect some blurriness on a 4K screen. For DPI awareness in your application, you can load/reload your font at a different scale and scale your style with &lt;code&gt;style.ScaleAllSizes()&lt;/code&gt; (see &lt;a href=&quot;https://www.dearimgui.com/faq&quot;&gt;FAQ&lt;/a&gt;).&lt;/p&gt; 
&lt;h3&gt;Getting Started &amp;amp; Integration&lt;/h3&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Getting-Started&quot;&gt;Getting Started&lt;/a&gt; guide for details.&lt;/p&gt; 
&lt;p&gt;On most platforms and when using C++, &lt;strong&gt;you should be able to use a combination of the &lt;a href=&quot;https://github.com/ocornut/imgui/tree/master/backends&quot;&gt;imgui_impl_xxxx&lt;/a&gt; backends without modification&lt;/strong&gt; (e.g. &lt;code&gt;imgui_impl_win32.cpp&lt;/code&gt; + &lt;code&gt;imgui_impl_dx11.cpp&lt;/code&gt;). If your engine supports multiple platforms, consider using more imgui_impl_xxxx files instead of rewriting them: this will be less work for you, and you can get Dear ImGui running immediately. You can &lt;em&gt;later&lt;/em&gt; decide to rewrite a custom backend using your custom engine functions if you wish so.&lt;/p&gt; 
&lt;p&gt;Integrating Dear ImGui within your custom engine is a matter of 1) wiring mouse/keyboard/gamepad inputs 2) uploading a texture to your GPU/render engine 3) providing a render function that can bind textures and render textured triangles, which is essentially what Backends are doing. The &lt;a href=&quot;https://github.com/ocornut/imgui/tree/master/examples&quot;&gt;examples/&lt;/a&gt; folder is populated with applications doing just that: setting up a window and using backends. If you follow the &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Getting-Started&quot;&gt;Getting Started&lt;/a&gt; guide it should in theory take you less than an hour to integrate Dear ImGui. &lt;strong&gt;Make sure to spend time reading the &lt;a href=&quot;https://www.dearimgui.com/faq&quot;&gt;FAQ&lt;/a&gt;, comments, and the examples applications!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Officially maintained backends/bindings (in repository):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Renderers: DirectX9, DirectX10, DirectX11, DirectX12, Metal, OpenGL/ES/ES2, SDL_GPU, SDL_Renderer2/3, Vulkan, WebGPU.&lt;/li&gt; 
 &lt;li&gt;Platforms: GLFW, SDL2/SDL3, Win32, Glut, OSX, Android.&lt;/li&gt; 
 &lt;li&gt;Frameworks: Allegro5, Emscripten.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Bindings&quot;&gt;Third-party backends/bindings&lt;/a&gt; wiki page:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Languages: C, C# and: Beef, ChaiScript, CovScript, Crystal, D, Go, Haskell, Haxe/hxcpp, Java, JavaScript, Julia, Kotlin, Lobster, Lua, Nim, Odin, Pascal, PureBasic, Python, ReaScript, Ruby, Rust, Swift, Zig...&lt;/li&gt; 
 &lt;li&gt;Frameworks: AGS/Adventure Game Studio, Amethyst, Blender, bsf, Cinder, Cocos2d-x, Defold, Diligent Engine, Ebiten, Flexium, GML/Game Maker Studio, GLEQ, Godot, GTK3, Irrlicht Engine, JUCE, L√ñVE+LUA, Mach Engine, Magnum, Marmalade, Monogame, NanoRT, nCine, Nim Game Lib, Nintendo 3DS/Switch/WiiU (homebrew), Ogre, openFrameworks, OSG/OpenSceneGraph, Orx, Photoshop, px_render, Qt/QtDirect3D, raylib, SFML, Sokol, Unity, Unreal Engine 4/5, UWP, vtk, VulkanHpp, VulkanSceneGraph, Win32 GDI, WxWidgets.&lt;/li&gt; 
 &lt;li&gt;Many bindings are auto-generated (by good old &lt;a href=&quot;https://github.com/cimgui/cimgui&quot;&gt;cimgui&lt;/a&gt; or newer/experimental &lt;a href=&quot;https://github.com/dearimgui/dear_bindings&quot;&gt;dear_bindings&lt;/a&gt;), you can use their metadata output to generate bindings for other languages.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Useful-Extensions&quot;&gt;Useful Extensions/Widgets&lt;/a&gt; wiki page:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Automation/testing, Text editors, node editors, timeline editors, plotting, software renderers, remote network access, memory editors, gizmos, etc. Notable and well supported extensions include &lt;a href=&quot;https://github.com/epezent/implot&quot;&gt;ImPlot&lt;/a&gt; and &lt;a href=&quot;https://github.com/ocornut/imgui_test_engine&quot;&gt;Dear ImGui Test Engine&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also see &lt;a href=&quot;https://github.com/ocornut/imgui/wiki&quot;&gt;Wiki&lt;/a&gt; for more links and ideas.&lt;/p&gt; 
&lt;h3&gt;Gallery&lt;/h3&gt; 
&lt;p&gt;Examples projects using Dear ImGui: &lt;a href=&quot;https://github.com/wolfpld/tracy&quot;&gt;Tracy&lt;/a&gt; (profiler), &lt;a href=&quot;https://github.com/WerWolv/ImHex&quot;&gt;ImHex&lt;/a&gt; (hex editor/data analysis), &lt;a href=&quot;https://remedybg.itch.io/remedybg&quot;&gt;RemedyBG&lt;/a&gt; (debugger) and &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Software-using-Dear-ImGui&quot;&gt;hundreds of others&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more user-submitted screenshots of projects using Dear ImGui, check out the &lt;a href=&quot;https://github.com/ocornut/imgui/issues?q=label%3Agallery&quot;&gt;Gallery Threads&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;For a list of third-party widgets and extensions, check out the &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Useful-Extensions&quot;&gt;Useful Extensions/Widgets&lt;/a&gt; wiki page.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Custom engine &lt;a href=&quot;https://github.com/tksuoran/erhe&quot;&gt;erhe&lt;/a&gt; (docking branch)&lt;br /&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/994606/147875067-a848991e-2ad2-4fd3-bf71-4aeb8a547bcf.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/8225057/190203358-6988b846-0686-480e-8663-1311fbd18abd.jpg&quot; alt=&quot;erhe&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Custom engine for &lt;a href=&quot;http://www.TheDragonsTrap.com&quot;&gt;Wonder Boy: The Dragon&#39;s Trap&lt;/a&gt; (2017)&lt;br /&gt;&lt;a href=&quot;https://cloud.githubusercontent.com/assets/8225057/20628927/33e14cac-b329-11e6-80f6-9524e93b048a.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/8225057/190203379-57fcb80e-4aec-4fec-959e-17ddd3cd71e5.jpg&quot; alt=&quot;the dragon&#39;s trap&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Custom engine (untitled)&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wiki/ocornut/imgui/web/v160/editor_white.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/8225057/190203393-c5ac9f22-b900-4d1e-bfeb-6027c63e3d92.jpg&quot; alt=&quot;editor white&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tracy Profiler (&lt;a href=&quot;https://github.com/wolfpld/tracy&quot;&gt;github&lt;/a&gt;)&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wiki/ocornut/imgui/web/v176/tracy_profiler.png&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/8225057/190203401-7b595f6e-607c-44d3-97ea-4c2673244dfb.jpg&quot; alt=&quot;tracy profiler&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Support, Frequently Asked Questions (FAQ)&lt;/h3&gt; 
&lt;p&gt;See: &lt;a href=&quot;https://github.com/ocornut/imgui/raw/master/docs/FAQ.md&quot;&gt;Frequently Asked Questions (FAQ)&lt;/a&gt; where common questions are answered.&lt;/p&gt; 
&lt;p&gt;See: &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Getting-Started&quot;&gt;Getting Started&lt;/a&gt; and &lt;a href=&quot;https://github.com/ocornut/imgui/wiki&quot;&gt;Wiki&lt;/a&gt; for many links, references, articles.&lt;/p&gt; 
&lt;p&gt;See: &lt;a href=&quot;https://github.com/ocornut/imgui/wiki#about-the-imgui-paradigm&quot;&gt;Articles about the IMGUI paradigm&lt;/a&gt; to read/learn about the Immediate Mode GUI paradigm.&lt;/p&gt; 
&lt;p&gt;See: &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Upcoming-Changes&quot;&gt;Upcoming Changes&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See: &lt;a href=&quot;https://github.com/ocornut/imgui_test_engine&quot;&gt;Dear ImGui Test Engine + Test Suite&lt;/a&gt; for Automation &amp;amp; Testing.&lt;/p&gt; 
&lt;p&gt;For the purposes of getting search engines to crawl the wiki, here&#39;s a link to the &lt;a href=&quot;https://github-wiki-see.page/m/ocornut/imgui/wiki&quot;&gt;Crawlable Wiki&lt;/a&gt; (not for humans, &lt;a href=&quot;https://github-wiki-see.page/&quot;&gt;here&#39;s why&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Getting started? For first-time users having issues compiling/linking/running or issues loading fonts, please use &lt;a href=&quot;https://github.com/ocornut/imgui/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;. For ANY other questions, bug reports, requests, feedback, please post on &lt;a href=&quot;https://github.com/ocornut/imgui/issues&quot;&gt;GitHub Issues&lt;/a&gt;. Please read and fill the New Issue template carefully.&lt;/p&gt; 
&lt;p&gt;Private support is available for paying business customers (E-mail: &lt;em&gt;contact @ dearimgui dot com&lt;/em&gt;).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Which version should I get?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We occasionally tag &lt;a href=&quot;https://github.com/ocornut/imgui/releases&quot;&gt;Releases&lt;/a&gt; (with nice releases notes) but it is generally safe and recommended to sync to latest &lt;code&gt;master&lt;/code&gt; or &lt;code&gt;docking&lt;/code&gt; branch. The library is fairly stable and regressions tend to be fixed fast when reported. Advanced users may want to use the &lt;code&gt;docking&lt;/code&gt; branch with &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Multi-Viewports&quot;&gt;Multi-Viewport&lt;/a&gt; and &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Docking&quot;&gt;Docking&lt;/a&gt; features. This branch is kept in sync with master regularly.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Who uses Dear ImGui?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Quotes&quot;&gt;Quotes&lt;/a&gt;, &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Funding&quot;&gt;Funding &amp;amp; Sponsors&lt;/a&gt;, and &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Software-using-dear-imgui&quot;&gt;Software using Dear ImGui&lt;/a&gt; Wiki pages for an idea of who is using Dear ImGui. Please add your game/software if you can! Also, see the &lt;a href=&quot;https://github.com/ocornut/imgui/issues?q=label%3Agallery&quot;&gt;Gallery Threads&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;How to help&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;How can I help?&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://github.com/ocornut/imgui/issues&quot;&gt;GitHub Forum/Issues&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;You may help with development and submit pull requests! Please understand that by submitting a PR you are also submitting a request for the maintainer to review your code and then take over its maintenance forever. PR should be crafted both in the interest of the end-users and also to ease the maintainer into understanding and accepting it.&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Help-Wanted&quot;&gt;Help wanted&lt;/a&gt; on the &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/&quot;&gt;Wiki&lt;/a&gt; for some more ideas.&lt;/li&gt; 
 &lt;li&gt;Be a &lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Funding&quot;&gt;Funding Supporter&lt;/a&gt;! Have your company financially support this project via invoiced sponsors/maintenance or by buying a license for &lt;a href=&quot;https://github.com/ocornut/imgui_test_engine&quot;&gt;Dear ImGui Test Engine&lt;/a&gt; (please reach out: contact AT dearimgui DOT com).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Sponsors&lt;/h2&gt; 
&lt;p&gt;Ongoing Dear ImGui development is and has been financially supported by users and private sponsors. &lt;br /&gt;Please see the &lt;strong&gt;&lt;a href=&quot;https://github.com/ocornut/imgui/wiki/Funding&quot;&gt;detailed list of current and past Dear ImGui funding supporters and sponsors&lt;/a&gt;&lt;/strong&gt; for details. &lt;br /&gt;From November 2014 to December 2019, ongoing development has also been financially supported by its users on Patreon and through individual donations.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;THANK YOU to all past and present supporters for helping to keep this project alive and thriving!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Dear ImGui is using software and services provided free of charge for open source projects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pvs-studio.com/en/pvs-studio/?utm_source=website&amp;amp;utm_medium=github&amp;amp;utm_campaign=open_source&quot;&gt;PVS-Studio&lt;/a&gt; for static analysis (supports C/C++/C#/Java).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/features/actions&quot;&gt;GitHub actions&lt;/a&gt; for continuous integration systems.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenCppCoverage/OpenCppCoverage&quot;&gt;OpenCppCoverage&lt;/a&gt; for code coverage analysis.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;Developed by &lt;a href=&quot;https://www.miracleworld.net&quot;&gt;Omar Cornut&lt;/a&gt; and every direct or indirect &lt;a href=&quot;https://github.com/ocornut/imgui/graphs/contributors&quot;&gt;contributors&lt;/a&gt; to the GitHub. The early version of this library was developed with the support of &lt;a href=&quot;https://www.mediamolecule.com&quot;&gt;Media Molecule&lt;/a&gt; and first used internally on the game &lt;a href=&quot;https://tearaway.mediamolecule.com&quot;&gt;Tearaway&lt;/a&gt; (PS Vita).&lt;/p&gt; 
&lt;p&gt;Recurring contributors include Rokas Kupstys &lt;a href=&quot;https://github.com/rokups&quot;&gt;@rokups&lt;/a&gt; (2020-2022): a good portion of work on automation system and regression tests now available in &lt;a href=&quot;https://github.com/ocornut/imgui_test_engine&quot;&gt;Dear ImGui Test Engine&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Maintenance/support contracts, sponsoring invoices and other B2B transactions are hosted and handled by &lt;a href=&quot;https://www.discohello.com&quot;&gt;Disco Hello&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Omar: &quot;I first discovered the IMGUI paradigm at &lt;a href=&quot;https://www.q-games.com&quot;&gt;Q-Games&lt;/a&gt; where Atman Binstock had dropped his own simple implementation in the codebase, which I spent quite some time improving and thinking about. It turned out that Atman was exposed to the concept directly by working with Casey. When I moved to Media Molecule I rewrote a new library trying to overcome the flaws and limitations of the first one I&#39;ve worked with. It became this library and since then I have spent an unreasonable amount of time iterating and improving it.&quot;&lt;/p&gt; 
&lt;p&gt;Embeds &lt;a href=&quot;https://www.proggyfonts.net&quot;&gt;ProggyClean.ttf&lt;/a&gt; font by Tristan Grimmer (MIT license). &lt;br /&gt;Embeds &lt;a href=&quot;https://github.com/nothings/stb/&quot;&gt;stb_textedit.h, stb_truetype.h, stb_rect_pack.h&lt;/a&gt; by Sean Barrett (public domain).&lt;/p&gt; 
&lt;p&gt;Inspiration, feedback, and testing for early versions: Casey Muratori, Atman Binstock, Mikko Mononen, Emmanuel Briney, Stefan Kamoda, Anton Mikhailov, Matt Willis. Special thanks to Alex Evans, Patrick Doane, Marco Koegler for kindly helping. Also thank you to everyone posting feedback, questions and patches on GitHub.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Dear ImGui is licensed under the MIT License, see &lt;a href=&quot;https://github.com/ocornut/imgui/raw/master/LICENSE.txt&quot;&gt;LICENSE.txt&lt;/a&gt; for more information.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AutoHotkey/AutoHotkey</title>
      <link>https://github.com/AutoHotkey/AutoHotkey</link>
      <description>&lt;p&gt;AutoHotkey - macro-creation and automation-oriented scripting utility for Windows.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoHotkey&lt;/h1&gt; 
&lt;p&gt;AutoHotkey is a free, open source macro-creation and automation software utility that allows users to automate repetitive tasks. It is driven by a custom scripting language that has special provision for defining keyboard shortcuts, otherwise known as hotkeys.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.autohotkey.com/&quot;&gt;https://www.autohotkey.com/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://www.autohotkey.com/boards/&quot;&gt;AutoHotkey Community forum&lt;/a&gt; is the primary source of support for AutoHotkey.&lt;/p&gt; 
&lt;p&gt;AutoHotkey v1 is not being maintained, but support is provided by community members.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Scripts not working&lt;/strong&gt;: For assistance getting code AutoHotkey scripts to work the way you want, start a topic in the &lt;a href=&quot;https://www.autohotkey.com/boards/viewforum.php?f=82&quot;&gt;Ask for Help (v2)&lt;/a&gt; or &lt;a href=&quot;https://www.autohotkey.com/boards/viewforum.php?f=76&quot;&gt;Ask for Help (v1)&lt;/a&gt; subforum, depending on your AutoHotkey version.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bug reporting&lt;/strong&gt;: If in doubt about the nature of your issue, please post in &lt;a href=&quot;https://www.autohotkey.com/boards/viewforum.php?f=82&quot;&gt;Ask for Help (v2)&lt;/a&gt; for confirmation. Otherwise, bugs should be reported in the &lt;a href=&quot;https://www.autohotkey.com/boards/viewforum.php?f=14&quot;&gt;Bug Reports&lt;/a&gt; subforum.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;False positives&lt;/strong&gt;: If you notice any AutoHotkey files (downloaded from official sources) are being flagged as suspicious or a virus, they are likely false positives. Please refer to our page on how to resolve or report these &lt;a href=&quot;https://www.autohotkey.com/download/safe.htm&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Other development topics&lt;/strong&gt;: For any other development related enquiries, please utilize the &lt;a href=&quot;https://www.autohotkey.com/boards/viewforum.php?f=37&quot;&gt;AutoHotkey Development&lt;/a&gt; subforum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;How to Compile&lt;/h2&gt; 
&lt;p&gt;AutoHotkey is developed with &lt;a href=&quot;https://www.visualstudio.com/products/visual-studio-community-vs&quot;&gt;Microsoft Visual Studio Community 2022&lt;/a&gt;, which is a free download from Microsoft.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Get the source code.&lt;/li&gt; 
 &lt;li&gt;Open AutoHotkeyx.sln in Visual Studio.&lt;/li&gt; 
 &lt;li&gt;Select the appropriate Build and Platform.&lt;/li&gt; 
 &lt;li&gt;Build.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The project is configured in a way that allows building with Visual Studio 2012 or later, but only the 2022 toolset is regularly tested. Some newer C++ language features are used and therefore a later version of the compiler might be required.&lt;/p&gt; 
&lt;h2&gt;Developing in VS Code&lt;/h2&gt; 
&lt;p&gt;AutoHotkey v2 can also be built and debugged in VS Code.&lt;/p&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=ms-vscode.cpptools&quot;&gt;C/C++ for Visual Studio Code&lt;/a&gt;. VS Code might prompt you to install this if you open a .cpp file.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/vs/17/release/vs_BuildTools.exe&quot;&gt;Build Tools for Visual Studio 2022&lt;/a&gt; with the &quot;Desktop development with C++&quot; workload, or similar (some older or newer versions and different products should work).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Build Configurations&lt;/h2&gt; 
&lt;p&gt;AutoHotkeyx.vcxproj contains several combinations of build configurations. The main configurations are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Debug&lt;/strong&gt;: AutoHotkey.exe in debug mode.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Release&lt;/strong&gt;: AutoHotkey.exe for general use.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Self-contained&lt;/strong&gt;: AutoHotkeySC.bin, used for compiled scripts.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Secondary configurations are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;(mbcs)&lt;/strong&gt;: ANSI (multi-byte character set). Configurations without this suffix are Unicode.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;.dll&lt;/strong&gt;: Builds an experimental dll for use hosting the interpreter, such as to enable the use of v1 libraries in a v2 script. See &lt;a href=&quot;https://raw.githubusercontent.com/AutoHotkey/AutoHotkey/alpha/README-LIB.md&quot;&gt;README-LIB.md&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Platforms&lt;/h2&gt; 
&lt;p&gt;AutoHotkeyx.vcxproj includes the following Platforms:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Win32&lt;/strong&gt;: for Windows 32-bit.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;x64&lt;/strong&gt;: for Windows x64.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;AutoHotkey supports Windows XP with or without service packs and Windows 2000 via an asm patch (win2kcompat.asm). Support may be removed if maintaining it becomes non-trivial. Older versions are not supported.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ggml-org/llama.cpp</title>
      <link>https://github.com/ggml-org/llama.cpp</link>
      <description>&lt;p&gt;LLM inference in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png&quot; alt=&quot;llama&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;License: MIT&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/ggml-org/llama.cpp&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml&quot;&gt;&lt;img src=&quot;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg?sanitize=true&quot; alt=&quot;Server&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/205&quot;&gt;Manifesto&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/ggml&quot;&gt;ggml&lt;/a&gt; / &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/raw/master/docs/ops.md&quot;&gt;ops&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;LLM inference in C/C++&lt;/p&gt; 
&lt;h2&gt;Recent API changes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues/9289&quot;&gt;Changelog for &lt;code&gt;libllama&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues/9291&quot;&gt;Changelog for &lt;code&gt;llama-server&lt;/code&gt; REST API&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Hot topics&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/15396&quot;&gt;guide : running gpt-oss with llama.cpp&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/15313&quot;&gt;[FEEDBACK] Better packaging for llama.cpp to support downstream consumers ü§ó&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Support for the &lt;code&gt;gpt-oss&lt;/code&gt; model with native MXFP4 format has been added | &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/15091&quot;&gt;PR&lt;/a&gt; | &lt;a href=&quot;https://blogs.nvidia.com/blog/rtx-ai-garage-openai-oss&quot;&gt;Collaboration with NVIDIA&lt;/a&gt; | &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/15095&quot;&gt;Comment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hot PRs: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pulls?q=is%3Apr+label%3Ahot+&quot;&gt;All&lt;/a&gt; | &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pulls?q=is%3Apr+label%3Ahot+is%3Aopen&quot;&gt;Open&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Multimodal support arrived in &lt;code&gt;llama-server&lt;/code&gt;: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/12898&quot;&gt;#12898&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/multimodal.md&quot;&gt;documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;VS Code extension for FIM completions: &lt;a href=&quot;https://github.com/ggml-org/llama.vscode&quot;&gt;https://github.com/ggml-org/llama.vscode&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Vim/Neovim plugin for FIM completions: &lt;a href=&quot;https://github.com/ggml-org/llama.vim&quot;&gt;https://github.com/ggml-org/llama.vim&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Introducing GGUF-my-LoRA &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/10123&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face Inference Endpoints now support GGUF out of the box! &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9669&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Hugging Face GGUF editor: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9268&quot;&gt;discussion&lt;/a&gt; | &lt;a href=&quot;https://huggingface.co/spaces/CISCai/gguf-editor&quot;&gt;tool&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quick start&lt;/h2&gt; 
&lt;p&gt;Getting started with llama.cpp is straightforward. Here are several ways to install it on your machine:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;code&gt;llama.cpp&lt;/code&gt; using &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/install.md&quot;&gt;brew, nix or winget&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Run with Docker - see our &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&quot;&gt;Docker documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Download pre-built binaries from the &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/releases&quot;&gt;releases page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Build from source by cloning this repository - check out &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&quot;&gt;our build guide&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Once installed, you&#39;ll need a model to work with. Head to the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/#obtaining-and-quantizing-models&quot;&gt;Obtaining and quantizing models&lt;/a&gt; section to learn more.&lt;/p&gt; 
&lt;p&gt;Example command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;# Use a local model file
llama-cli -m my_model.gguf

# Or download and run a model directly from Hugging Face
llama-cli -hf ggml-org/gemma-3-1b-it-GGUF

# Launch OpenAI-compatible API server
llama-server -hf ggml-org/gemma-3-1b-it-GGUF
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Plain C/C++ implementation without any dependencies&lt;/li&gt; 
 &lt;li&gt;Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks&lt;/li&gt; 
 &lt;li&gt;AVX, AVX2, AVX512 and AMX support for x86 architectures&lt;/li&gt; 
 &lt;li&gt;1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use&lt;/li&gt; 
 &lt;li&gt;Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads GPUs via MUSA)&lt;/li&gt; 
 &lt;li&gt;Vulkan and SYCL backend support&lt;/li&gt; 
 &lt;li&gt;CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The &lt;code&gt;llama.cpp&lt;/code&gt; project is the main playground for developing new features for the &lt;a href=&quot;https://github.com/ggml-org/ggml&quot;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Models&lt;/summary&gt; 
 &lt;p&gt;Typically finetunes of the base models below are supported as well.&lt;/p&gt; 
 &lt;p&gt;Instructions for adding support for new models: &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/HOWTO-add-model.md&quot;&gt;HOWTO-add-model.md&lt;/a&gt;&lt;/p&gt; 
 &lt;h4&gt;Text-only&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; LLaMA ü¶ô&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; LLaMA 2 ü¶ôü¶ô&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; LLaMA 3 ü¶ôü¶ôü¶ô&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/mistralai/Mistral-7B-v0.1&quot;&gt;Mistral 7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=mistral-ai/Mixtral&quot;&gt;Mixtral MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/databricks/dbrx-instruct&quot;&gt;DBRX&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=tiiuae/falcon&quot;&gt;Falcon&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca&quot;&gt;Chinese LLaMA / Alpaca&lt;/a&gt; and &lt;a href=&quot;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&quot;&gt;Chinese LLaMA-2 / Alpaca-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/bofenghuang/vigogne&quot;&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/5423&quot;&gt;BERT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://bair.berkeley.edu/blog/2023/04/03/koala/&quot;&gt;Koala&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=baichuan-inc/Baichuan&quot;&gt;Baichuan 1 &amp;amp; 2&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/hiyouga/baichuan-7b-sft&quot;&gt;derivations&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=BAAI/Aquila&quot;&gt;Aquila 1 &amp;amp; 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3187&quot;&gt;Starcoder models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/smallcloudai/Refact-1_6B-fim&quot;&gt;Refact&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3417&quot;&gt;MPT&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3553&quot;&gt;Bloom&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=01-ai/Yi&quot;&gt;Yi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/stabilityai&quot;&gt;StableLM models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=deepseek-ai/deepseek&quot;&gt;Deepseek models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Qwen/Qwen&quot;&gt;Qwen models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/3557&quot;&gt;PLaMo-13B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=microsoft/phi&quot;&gt;Phi models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/11003&quot;&gt;PhiMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/gpt2&quot;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/5118&quot;&gt;Orion 14B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=internlm2&quot;&gt;InternLM2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/WisdomShell/codeshell&quot;&gt;CodeShell&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://ai.google.dev/gemma&quot;&gt;Gemma&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/state-spaces/mamba&quot;&gt;Mamba&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/keyfan/grok-1-hf&quot;&gt;Grok-1&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=xverse&quot;&gt;Xverse&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=CohereForAI/c4ai-command-r&quot;&gt;Command-R models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=sea-lion&quot;&gt;SEA-LION&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/GritLM/GritLM-7B&quot;&gt;GritLM-7B&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/GritLM/GritLM-8x7B&quot;&gt;GritLM-8x7B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://allenai.org/olmo&quot;&gt;OLMo&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://allenai.org/olmo&quot;&gt;OLMo 2&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/allenai/OLMoE-1B-7B-0924&quot;&gt;OLMoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330&quot;&gt;Granite models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/EleutherAI/gpt-neox&quot;&gt;GPT-NeoX&lt;/a&gt; + &lt;a href=&quot;https://github.com/EleutherAI/pythia&quot;&gt;Pythia&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520&quot;&gt;Snowflake-Arctic MoE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Smaug&quot;&gt;Smaug&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/LumiOpen/Poro-34B&quot;&gt;Poro 34B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/1bitLLM&quot;&gt;Bitnet b1.58 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=flan-t5&quot;&gt;Flan T5&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca&quot;&gt;Open Elm models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/THUDM/chatglm3-6b&quot;&gt;ChatGLM3-6b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-4-9b&quot;&gt;ChatGLM4-9b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-edge-1.5b-chat&quot;&gt;GLMEdge-1.5b&lt;/a&gt; + &lt;a href=&quot;https://huggingface.co/THUDM/glm-edge-4b-chat&quot;&gt;GLMEdge-4b&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&quot;&gt;GLM-4-0414&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966&quot;&gt;SmolLM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct&quot;&gt;EXAONE-3.0-7.8B-Instruct&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a&quot;&gt;FalconMamba Models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/inceptionai/jais-13b-chat&quot;&gt;Jais&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a&quot;&gt;Bielik-11B-v2.3&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/BlinkDL/RWKV-LM&quot;&gt;RWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1&quot;&gt;QRWKV-6&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct&quot;&gt;GigaChat-20B-A3B&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/trillionlabs/Trillion-7B-preview&quot;&gt;Trillion-7B-preview&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&quot;&gt;Ling models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38&quot;&gt;LFM2 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/tencent/hunyuan-dense-model-6890632cda26b19119c9c5e7&quot;&gt;Hunyuan models&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h4&gt;Multimodal&lt;/h4&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e&quot;&gt;LLaVA 1.5 models&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2&quot;&gt;LLaVA 1.6 models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=SkunkworksAI/Bakllava&quot;&gt;BakLLaVA&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/NousResearch/Obsidian-3B-V0.5&quot;&gt;Obsidian&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Lin-Chen/ShareGPT4V&quot;&gt;ShareGPT4V&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=mobileVLM&quot;&gt;MobileVLM 1.7B/3B models&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=Yi-VL&quot;&gt;Yi-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=MiniCPM&quot;&gt;Mini CPM&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/vikhyatk/moondream2&quot;&gt;Moondream&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://github.com/BAAI-DCAI/Bunny&quot;&gt;Bunny&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/models?search=glm-edge&quot;&gt;GLM-EDGE&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&quot;&gt;Qwen2-VL&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa&quot;&gt;LFM2-VL&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Bindings&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Python: &lt;a href=&quot;https://github.com/ddh0/easy-llama&quot;&gt;ddh0/easy-llama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Python: &lt;a href=&quot;https://github.com/abetlen/llama-cpp-python&quot;&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Go: &lt;a href=&quot;https://github.com/go-skynet/go-llama.cpp&quot;&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Node.js: &lt;a href=&quot;https://github.com/withcatai/node-llama-cpp&quot;&gt;withcatai/node-llama-cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (llama.cpp server client): &lt;a href=&quot;https://modelfusion.dev/integration/model-provider/llamacpp&quot;&gt;lgrammel/modelfusion&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JS/TS (Programmable Prompt Engine CLI): &lt;a href=&quot;https://github.com/offline-ai/cli&quot;&gt;offline-ai/cli&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;JavaScript/Wasm (works in browser): &lt;a href=&quot;https://github.com/tangledgroup/llama-cpp-wasm&quot;&gt;tangledgroup/llama-cpp-wasm&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Typescript/Wasm (nicer API, available on npm): &lt;a href=&quot;https://github.com/ngxson/wllama&quot;&gt;ngxson/wllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Ruby: &lt;a href=&quot;https://github.com/yoshoku/llama_cpp.rb&quot;&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more features): &lt;a href=&quot;https://github.com/edgenai/llama_cpp-rs&quot;&gt;edgenai/llama_cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (nicer API): &lt;a href=&quot;https://github.com/mdrokz/rust-llama.cpp&quot;&gt;mdrokz/rust-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (more direct bindings): &lt;a href=&quot;https://github.com/utilityai/llama-cpp-rs&quot;&gt;utilityai/llama-cpp-rs&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Rust (automated build from crates.io): &lt;a href=&quot;https://github.com/ShelbyJenkins/llm_client&quot;&gt;ShelbyJenkins/llm_client&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/.NET: &lt;a href=&quot;https://github.com/SciSharp/LLamaSharp&quot;&gt;SciSharp/LLamaSharp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;C#/VB.NET (more features - community license): &lt;a href=&quot;https://docs.lm-kit.com/lm-kit-net/index.html&quot;&gt;LM-Kit.NET&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Scala 3: &lt;a href=&quot;https://github.com/donderom/llm4s&quot;&gt;donderom/llm4s&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Clojure: &lt;a href=&quot;https://github.com/phronmophobic/llama.clj&quot;&gt;phronmophobic/llama.clj&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;React Native: &lt;a href=&quot;https://github.com/mybigday/llama.rn&quot;&gt;mybigday/llama.rn&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Java: &lt;a href=&quot;https://github.com/kherud/java-llama.cpp&quot;&gt;kherud/java-llama.cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Java: &lt;a href=&quot;https://github.com/QuasarByte/llama-cpp-jna&quot;&gt;QuasarByte/llama-cpp-jna&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Zig: &lt;a href=&quot;https://github.com/Deins/llama.cpp.zig&quot;&gt;deins/llama.cpp.zig&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter/Dart: &lt;a href=&quot;https://github.com/netdur/llama_cpp_dart&quot;&gt;netdur/llama_cpp_dart&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Flutter: &lt;a href=&quot;https://github.com/xuegao-tzx/Fllama&quot;&gt;xuegao-tzx/Fllama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;PHP (API bindings and features built on top of llama.cpp): &lt;a href=&quot;https://github.com/distantmagic/resonance&quot;&gt;distantmagic/resonance&lt;/a&gt; &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/pull/6326&quot;&gt;(more info)&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Guile Scheme: &lt;a href=&quot;https://savannah.nongnu.org/projects/guile-llama-cpp&quot;&gt;guile_llama_cpp&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href=&quot;https://github.com/srgtuszy/llama-cpp-swift&quot;&gt;srgtuszy/llama-cpp-swift&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Swift &lt;a href=&quot;https://github.com/ShenghaiWang/SwiftLlama&quot;&gt;ShenghaiWang/SwiftLlama&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;Delphi &lt;a href=&quot;https://github.com/Embarcadero/llama-cpp-delphi&quot;&gt;Embarcadero/llama-cpp-delphi&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;UIs&lt;/summary&gt; 
 &lt;p&gt;&lt;em&gt;(to have a project listed here, it should clearly state that it depends on &lt;code&gt;llama.cpp&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&quot;&gt;AI Sublime Text plugin&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/cztomsik/ava&quot;&gt;cztomsik/ava&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/alexpinel/Dot&quot;&gt;Dot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ylsdamxssjxxdd/eva&quot;&gt;eva&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/iohub/coLLaMA&quot;&gt;iohub/collama&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/janhq/jan&quot;&gt;janhq/jan&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/johnbean393/Sidekick&quot;&gt;johnbean393/Sidekick&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/zhouwg/kantv?tab=readme-ov-file&quot;&gt;KanTV&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/firatkiral/kodibot&quot;&gt;KodiBot&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.vim&quot;&gt;llama.vim&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/abgulati/LARS&quot;&gt;LARS&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/vietanhdev/llama-assistant&quot;&gt;Llama Assistant&lt;/a&gt; (GPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/guinmoon/LLMFarm?tab=readme-ov-file&quot;&gt;LLMFarm&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/undreamai/LLMUnity&quot;&gt;LLMUnity&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://lmstudio.ai/&quot;&gt;LMStudio&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/mudler/LocalAI&quot;&gt;LocalAI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/LostRuins/koboldcpp&quot;&gt;LostRuins/koboldcpp&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://mindmac.app&quot;&gt;MindMac&lt;/a&gt; (proprietary)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/MindWorkAI/AI-Studio&quot;&gt;MindWorkAI/AI-Studio&lt;/a&gt; (FSL-1.1-MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Mobile-Artificial-Intelligence/maid&quot;&gt;Mobile-Artificial-Intelligence/maid&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/Mozilla-Ocho/llamafile&quot;&gt;Mozilla-Ocho/llamafile&lt;/a&gt; (Apache-2.0)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/nat/openplayground&quot;&gt;nat/openplayground&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/nomic-ai/gpt4all&quot;&gt;nomic-ai/gpt4all&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ollama/ollama&quot;&gt;ollama/ollama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/oobabooga/text-generation-webui&quot;&gt;oobabooga/text-generation-webui&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/a-ghorbani/pocketpal-ai&quot;&gt;PocketPal AI&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/psugihara/FreeChat&quot;&gt;psugihara/FreeChat&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/ptsochantaris/emeltal&quot;&gt;ptsochantaris/emeltal&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/pythops/tenere&quot;&gt;pythops/tenere&lt;/a&gt; (AGPL)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/containers/ramalama&quot;&gt;ramalama&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/semperai/amica&quot;&gt;semperai/amica&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/withcatai/catai&quot;&gt;withcatai/catai&lt;/a&gt; (MIT)&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/blackhole89/autopen&quot;&gt;Autopen&lt;/a&gt; (GPL)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Tools&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/akx/ggify&quot;&gt;akx/ggify&lt;/a&gt; ‚Äì download PyTorch models from HuggingFace Hub and convert them to GGML&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/akx/ollama-dl&quot;&gt;akx/ollama-dl&lt;/a&gt; ‚Äì download models from the Ollama library to be used directly with llama.cpp&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/crashr/gppm&quot;&gt;crashr/gppm&lt;/a&gt; ‚Äì launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser&quot;&gt;gpustack/gguf-parser&lt;/a&gt; - review/check the GGUF file and estimate the memory usage&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902&quot;&gt;Styled Lines&lt;/a&gt; (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Infrastructure&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/intentee/paddler&quot;&gt;Paddler&lt;/a&gt; - Open-source LLMOps platform for hosting and scaling AI in your own infrastructure&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/gpustack/gpustack&quot;&gt;GPUStack&lt;/a&gt; - Manage GPU clusters for running LLMs&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/onicai/llama_cpp_canister&quot;&gt;llama_cpp_canister&lt;/a&gt; - llama.cpp as a smart contract on the Internet Computer, using WebAssembly&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/mostlygeek/llama-swap&quot;&gt;llama-swap&lt;/a&gt; - transparent proxy that adds automatic model switching with llama-server&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/kalavai-net/kalavai-client&quot;&gt;Kalavai&lt;/a&gt; - Crowdsource end to end LLM deployment at any scale&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/InftyAI/llmaz&quot;&gt;llmaz&lt;/a&gt; - ‚ò∏Ô∏è Easy, advanced inference platform for large language models on Kubernetes.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Games&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://github.com/MorganRO8/Lucys_Labyrinth&quot;&gt;Lucy&#39;s Labyrinth&lt;/a&gt; - A simple maze game where agents controlled by an AI model will try to trick you.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Supported backends&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Backend&lt;/th&gt; 
   &lt;th&gt;Target devices&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#metal-build&quot;&gt;Metal&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Apple Silicon&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#blas-build&quot;&gt;BLAS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/BLIS.md&quot;&gt;BLIS&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/SYCL.md&quot;&gt;SYCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Intel and Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#musa&quot;&gt;MUSA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Moore Threads GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cuda&quot;&gt;CUDA&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Nvidia GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#hip&quot;&gt;HIP&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;AMD GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#vulkan&quot;&gt;Vulkan&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cann&quot;&gt;CANN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ascend NPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md&quot;&gt;OpenCL&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Adreno GPU&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/zDNN.md&quot;&gt;IBM zDNN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;IBM Z &amp;amp; LinuxONE&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#webgpu&quot;&gt;WebGPU [In Progress]&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc&quot;&gt;RPC&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;All&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Obtaining and quantizing models&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://huggingface.co&quot;&gt;Hugging Face&lt;/a&gt; platform hosts a &lt;a href=&quot;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&quot;&gt;number of LLMs&lt;/a&gt; compatible with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&quot;&gt;Trending&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/models?sort=trending&amp;amp;search=llama+gguf&quot;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can either manually download the GGUF file or directly use any &lt;code&gt;llama.cpp&lt;/code&gt;-compatible models from &lt;a href=&quot;https://huggingface.co/&quot;&gt;Hugging Face&lt;/a&gt; or other model hosting sites, such as &lt;a href=&quot;https://modelscope.cn/&quot;&gt;ModelScope&lt;/a&gt;, by using this CLI argument: &lt;code&gt;-hf &amp;lt;user&amp;gt;/&amp;lt;model&amp;gt;[:quant]&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;llama-cli -hf ggml-org/gemma-3-1b-it-GGUF
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, the CLI would download from Hugging Face, you can switch to other options with the environment variable &lt;code&gt;MODEL_ENDPOINT&lt;/code&gt;. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. &lt;code&gt;MODEL_ENDPOINT=https://www.modelscope.cn/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;After downloading a model, use the CLI tools to run it locally - see below.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires the model to be stored in the &lt;a href=&quot;https://github.com/ggml-org/ggml/raw/master/docs/gguf.md&quot;&gt;GGUF&lt;/a&gt; file format. Models in other data formats can be converted to GGUF using the &lt;code&gt;convert_*.py&lt;/code&gt; Python scripts in this repo.&lt;/p&gt; 
&lt;p&gt;The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/ggml-org/gguf-my-repo&quot;&gt;GGUF-my-repo space&lt;/a&gt; to convert to GGUF format and quantize model weights to smaller sizes&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/ggml-org/gguf-my-lora&quot;&gt;GGUF-my-LoRA space&lt;/a&gt; to convert LoRA adapters to GGUF format (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/10123&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://huggingface.co/spaces/CISCai/gguf-editor&quot;&gt;GGUF-editor space&lt;/a&gt; to edit GGUF meta data in the browser (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9268&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9268&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Use the &lt;a href=&quot;https://ui.endpoints.huggingface.co/&quot;&gt;Inference Endpoints&lt;/a&gt; to directly host &lt;code&gt;llama.cpp&lt;/code&gt; in the cloud (more info: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/9669&quot;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To learn more about model quantization, &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/quantize/README.md&quot;&gt;read this documentation&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main&quot;&gt;&lt;code&gt;llama-cli&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A CLI tool for accessing and experimenting with most of &lt;code&gt;llama.cpp&lt;/code&gt;&#39;s functionality.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run in conversation mode&lt;/summary&gt; 
   &lt;p&gt;Models with a built-in chat template will automatically activate conversation mode. If this doesn&#39;t occur, you can manually enable it by adding &lt;code&gt;-cnv&lt;/code&gt; and specifying a suitable chat template with &lt;code&gt;--chat-template NAME&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf

# &amp;gt; hi, who are you?
# Hi there! I&#39;m your helpful assistant! I&#39;m an AI-powered chatbot designed to assist and provide information to users like you. I&#39;m here to help answer your questions, provide guidance, and offer support on a wide range of topics. I&#39;m a friendly and knowledgeable AI, and I&#39;m always happy to help with anything you need. What&#39;s on your mind, and how can I assist you today?
#
# &amp;gt; what is 1+1?
# Easy peasy! The answer to 1+1 is... 2!
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run in conversation mode with custom chat template&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the &quot;chatml&quot; template (use -h to see the list of supported templates)
llama-cli -m model.gguf -cnv --chat-template chatml

# use a custom template
llama-cli -m model.gguf -cnv --in-prefix &#39;User: &#39; --reverse-prompt &#39;User:&#39;
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run simple text completion&lt;/summary&gt; 
   &lt;p&gt;To disable conversation mode explicitly, use &lt;code&gt;-no-cnv&lt;/code&gt;&lt;/p&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf -p &quot;I believe the meaning of life is&quot; -n 128 -no-cnv

# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don&#39;t align with societal expectations. I think that&#39;s what I love about yoga ‚Äì it&#39;s not just a physical practice, but a spiritual one too. It&#39;s about connecting with yourself, listening to your inner voice, and honoring your own unique journey.
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain the output with a custom grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p &#39;Request: schedule a call at 8pm; Command:&#39;

# {&quot;appointmentTime&quot;: &quot;8pm&quot;, &quot;appointmentDetails&quot;: &quot;schedule a a call&quot;}
&lt;/code&gt;&lt;/pre&gt; 
   &lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/&quot;&gt;grammars/&lt;/a&gt; folder contains a handful of sample grammars. To write your own, check out the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&quot;&gt;GBNF Guide&lt;/a&gt;.&lt;/p&gt; 
   &lt;p&gt;For authoring more complex JSON grammars, check out &lt;a href=&quot;https://grammar.intrinsiclabs.ai/&quot;&gt;https://grammar.intrinsiclabs.ai/&lt;/a&gt;&lt;/p&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server&quot;&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A lightweight, &lt;a href=&quot;https://github.com/openai/openai-openapi&quot;&gt;OpenAI API&lt;/a&gt; compatible, HTTP server for serving LLMs.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Start a local HTTP server with default configuration on port 8080&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-server -m model.gguf --port 8080

# Basic web UI can be accessed via browser: http://localhost:8080
# Chat completion endpoint: http://localhost:8080/v1/chat/completions
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Support multiple-users and parallel decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# up to 4 concurrent requests, each with 4096 max context
llama-server -m model.gguf -c 16384 -np 4
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Enable speculative decoding&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# the draft.gguf model should be a small variant of the target model.gguf
llama-server -m model.gguf -md draft.gguf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve an embedding model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the /embedding endpoint
llama-server -m model.gguf --embedding --pooling cls -ub 8192
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Serve a reranking model&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# use the /reranking endpoint
llama-server -m model.gguf --reranking
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Constrain all outputs with a grammar&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# custom grammar
llama-server -m model.gguf --grammar-file grammar.gbnf

# JSON
llama-server -m model.gguf --grammar-file grammars/json.gbnf
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity&quot;&gt;&lt;code&gt;llama-perplexity&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A tool for measuring the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity/README.md&quot;&gt;perplexity&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/%5Bhttps://huggingface.co/docs/transformers/perplexity%5D(https://huggingface.co/docs/transformers/perplexity)&quot;&gt;^1&lt;/a&gt; (and other quality metrics) of a model over a given text.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Measure the perplexity over a text file&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-perplexity -m model.gguf -f file.txt

# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...
# Final estimate: PPL = 5.4007 +/- 0.67339
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Measure KL divergence&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# TODO
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/llama-bench&quot;&gt;&lt;code&gt;llama-bench&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;Benchmark the performance of the inference for various parameters.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details open&gt; 
   &lt;summary&gt;Run default benchmark&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-bench -m model.gguf

# Output:
# | model               |       size |     params | backend    | threads |          test |                  t/s |
# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ¬± 20.55 |
# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ¬± 0.81 |
#
# build: 3e0ba0e60 (4229)
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run&quot;&gt;&lt;code&gt;llama-run&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A comprehensive example for running &lt;code&gt;llama.cpp&lt;/code&gt; models. Useful for inferencing. Used with RamaLama &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/%5BRamaLama%5D(https://github.com/containers/ramalama)&quot;&gt;^3&lt;/a&gt;.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Run a model with a specific prompt (by default it&#39;s pulled from Ollama registry)&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-run granite-code
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/simple&quot;&gt;&lt;code&gt;llama-simple&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;h4&gt;A minimal example for implementing apps with &lt;code&gt;llama.cpp&lt;/code&gt;. Useful for developers.&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; 
  &lt;details&gt; 
   &lt;summary&gt;Basic text completion&lt;/summary&gt; 
   &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;llama-simple -m model.gguf

# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called &quot;The Art of
&lt;/code&gt;&lt;/pre&gt; 
  &lt;/details&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Contributors can open PRs&lt;/li&gt; 
 &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; 
 &lt;li&gt;Maintainers can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; 
 &lt;li&gt;Any help with managing issues, PRs and projects is very appreciated!&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&quot;&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more information&lt;/li&gt; 
 &lt;li&gt;Make sure to read this: &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/discussions/205&quot;&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A bit of backstory for those who are interested: &lt;a href=&quot;https://changelog.com/podcast/532&quot;&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main/README.md&quot;&gt;main (cli)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md&quot;&gt;server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&quot;&gt;GBNF grammars&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Development documentation&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&quot;&gt;How to build&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&quot;&gt;Running on Docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/android.md&quot;&gt;Build on Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/token_generation_performance_tips.md&quot;&gt;Performance troubleshooting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks&quot;&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Seminal papers and background on the models&lt;/h4&gt; 
&lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;LLaMA: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;GPT-3.5 / InstructGPT / ChatGPT: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://openai.com/research/instruction-following&quot;&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;XCFramework&lt;/h2&gt; 
&lt;p&gt;The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-swift&quot;&gt;// swift-tools-version: 5.10
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: &quot;MyLlamaPackage&quot;,
    targets: [
        .executableTarget(
            name: &quot;MyLlamaPackage&quot;,
            dependencies: [
                &quot;LlamaFramework&quot;
            ]),
        .binaryTarget(
            name: &quot;LlamaFramework&quot;,
            url: &quot;https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip&quot;,
            checksum: &quot;c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab&quot;
        )
    ]
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The above example is using an intermediate build &lt;code&gt;b5046&lt;/code&gt; of the library. This can be modified to use a different version by changing the URL and checksum.&lt;/p&gt; 
&lt;h2&gt;Completions&lt;/h2&gt; 
&lt;p&gt;Command-line completion is available for some environments.&lt;/p&gt; 
&lt;h4&gt;Bash Completion&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ build/bin/llama-cli --completion-bash &amp;gt; ~/.llama-completion.bash
$ source ~/.llama-completion.bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optionally this can be added to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; to load it automatically. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;$ echo &quot;source ~/.llama-completion.bash&quot; &amp;gt;&amp;gt; ~/.bashrc
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhirose/cpp-httplib&quot;&gt;yhirose/cpp-httplib&lt;/a&gt; - Single-header HTTP server, used by &lt;code&gt;llama-server&lt;/code&gt; - MIT license&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nothings/stb&quot;&gt;stb-image&lt;/a&gt; - Single-header image format decoder, used by multimodal subsystem - Public domain&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nlohmann/json&quot;&gt;nlohmann/json&lt;/a&gt; - Single-header JSON library, used by various tools/examples - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/minja&quot;&gt;minja&lt;/a&gt; - Minimal Jinja parser in C++, used by various tools/examples - MIT License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run/linenoise.cpp/linenoise.cpp&quot;&gt;linenoise.cpp&lt;/a&gt; - C++ library that provides readline-like line editing capabilities, used by &lt;code&gt;llama-run&lt;/code&gt; - BSD 2-Clause License&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://curl.se/&quot;&gt;curl&lt;/a&gt; - Client-side URL transfer library, used by various tools/examples - &lt;a href=&quot;https://curl.se/docs/copyright.html&quot;&gt;CURL License&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mackron/miniaudio&quot;&gt;miniaudio.h&lt;/a&gt; - Single-header audio format decoder, used by multimodal subsystem - Public domain&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>k2-fsa/sherpa-onnx</title>
      <link>https://github.com/k2-fsa/sherpa-onnx</link>
      <description>&lt;p&gt;Speech-to-text, text-to-speech, speaker diarization, speech enhancement, source separation, and VAD using next-gen Kaldi with onnxruntime without Internet connection. Support embedded systems, Android, iOS, HarmonyOS, Raspberry Pi, RISC-V, x86_64 servers, websocket server/client, support 12 programming languages&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Supported functions&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Speech recognition&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/tts/all-in-one.html&quot;&gt;Speech synthesis&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/source-separation/index.html&quot;&gt;Source separation&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Speaker identification&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-diarization/index.html&quot;&gt;Speaker diarization&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;Speaker verification&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/spoken-language-identification/index.html&quot;&gt;Spoken Language identification&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/index.html&quot;&gt;Audio tagging&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/index.html&quot;&gt;Voice activity detection&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/kws/index.html&quot;&gt;Keyword spotting&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/punctuation/index.html&quot;&gt;Add punctuation&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speech-enhancment/index.html&quot;&gt;Speech enhancement&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Supported platforms&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Architecture&lt;/th&gt; 
   &lt;th&gt;Android&lt;/th&gt; 
   &lt;th&gt;iOS&lt;/th&gt; 
   &lt;th&gt;Windows&lt;/th&gt; 
   &lt;th&gt;macOS&lt;/th&gt; 
   &lt;th&gt;linux&lt;/th&gt; 
   &lt;th&gt;HarmonyOS&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;x64&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;x86&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;arm64&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;arm32&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;riscv64&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Supported programming languages&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;1. C++&lt;/th&gt; 
   &lt;th&gt;2. C&lt;/th&gt; 
   &lt;th&gt;3. Python&lt;/th&gt; 
   &lt;th&gt;4. JavaScript&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;5. Java&lt;/th&gt; 
   &lt;th&gt;6. C#&lt;/th&gt; 
   &lt;th&gt;7. Kotlin&lt;/th&gt; 
   &lt;th&gt;8. Swift&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;9. Go&lt;/th&gt; 
   &lt;th&gt;10. Dart&lt;/th&gt; 
   &lt;th&gt;11. Rust&lt;/th&gt; 
   &lt;th&gt;12. Pascal&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
   &lt;td&gt;‚úîÔ∏è&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;For Rust support, please see &lt;a href=&quot;https://github.com/thewh1teagle/sherpa-rs&quot;&gt;sherpa-rs&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;It also supports WebAssembly.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/fJdxzg2VbG&quot;&gt;Join our discord&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;This repository supports running the following functions &lt;strong&gt;locally&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Speech-to-text (i.e., ASR); both streaming and non-streaming are supported&lt;/li&gt; 
 &lt;li&gt;Text-to-speech (i.e., TTS)&lt;/li&gt; 
 &lt;li&gt;Speaker diarization&lt;/li&gt; 
 &lt;li&gt;Speaker identification&lt;/li&gt; 
 &lt;li&gt;Speaker verification&lt;/li&gt; 
 &lt;li&gt;Spoken language identification&lt;/li&gt; 
 &lt;li&gt;Audio tagging&lt;/li&gt; 
 &lt;li&gt;VAD (e.g., &lt;a href=&quot;https://github.com/snakers4/silero-vad&quot;&gt;silero-vad&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Speech enhancement (e.g., &lt;a href=&quot;https://github.com/Xiaobin-Rong/gtcrn&quot;&gt;gtcrn&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Keyword spotting&lt;/li&gt; 
 &lt;li&gt;Source separation (e.g., &lt;a href=&quot;https://github.com/deezer/spleeter&quot;&gt;spleeter&lt;/a&gt;, &lt;a href=&quot;https://github.com/Anjok07/ultimatevocalremovergui&quot;&gt;UVR&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;on the following platforms and operating systems:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;x86, &lt;code&gt;x86_64&lt;/code&gt;, 32-bit ARM, 64-bit ARM (arm64, aarch64), RISC-V (riscv64), &lt;strong&gt;RK NPU&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Linux, macOS, Windows, openKylin&lt;/li&gt; 
 &lt;li&gt;Android, WearOS&lt;/li&gt; 
 &lt;li&gt;iOS&lt;/li&gt; 
 &lt;li&gt;HarmonyOS&lt;/li&gt; 
 &lt;li&gt;NodeJS&lt;/li&gt; 
 &lt;li&gt;WebAssembly&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developer.download.nvidia.com/assets/embedded/secure/jetson/orin_nx/docs/Jetson_Orin_NX_DS-10712-001_v0.5.pdf?RCPGu9Q6OVAOv7a7vgtwc9-BLScXRIWq6cSLuditMALECJ_dOj27DgnqAPGVnT2VpiNpQan9SyFy-9zRykR58CokzbXwjSA7Gj819e91AXPrWkGZR3oS1VLxiDEpJa_Y0lr7UT-N4GnXtb8NlUkP4GkCkkF_FQivGPrAucCUywL481GH_WpP_p7ziHU1Wg==&amp;amp;t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczovL3d3dy5nb29nbGUuY29tLmhrLyJ9&quot;&gt;NVIDIA Jetson Orin NX&lt;/a&gt; (Support running on both CPU and GPU)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.seeedstudio.com/blog/2020/01/16/new-revision-of-jetson-nano-dev-kit-now-supports-new-jetson-nano-module/&quot;&gt;NVIDIA Jetson Nano B01&lt;/a&gt; (Support running on both CPU and GPU)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.raspberrypi.com/&quot;&gt;Raspberry Pi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.rock-chips.com/uploads/pdf/2022.8.26/191/RV1126%20Brief%20Datasheet.pdf&quot;&gt;RV1126&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://sipeed.com/licheepi4a&quot;&gt;LicheePi4A&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.starfivetech.com/en/site/boards&quot;&gt;VisionFive 2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://developer.horizon.ai/api/v1/fileData/documents_pi/index.html&quot;&gt;Êó≠Êó•X3Ê¥æ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://wiki.sipeed.com/hardware/zh/maixIII/ax-pi/axpi.html&quot;&gt;Áà±ËäØÊ¥æ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.rock-chips.com/uploads/pdf/2022.8.26/192/RK3588%20Brief%20Datasheet.pdf&quot;&gt;RK3588&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;etc&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;with the following APIs&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;C++, C, Python, Go, &lt;code&gt;C#&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Java, Kotlin, JavaScript&lt;/li&gt; 
 &lt;li&gt;Swift, Rust&lt;/li&gt; 
 &lt;li&gt;Dart, Object Pascal&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Links for Huggingface Spaces&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can visit the following Huggingface spaces to try sherpa-onnx without installing anything. All you need is a browser.&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;‰∏≠ÂõΩÈïúÂÉè&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker diarization&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/speaker-diarization&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/speaker-diarization&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/automatic-speech-recognition&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/automatic-speech-recognition&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech recognition with &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/automatic-speech-recognition-with-whisper&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/automatic-speech-recognition-with-whisper&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech synthesis&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/text-to-speech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/text-to-speech&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Generate subtitles&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/generate-subtitles-for-videos&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/generate-subtitles-for-videos&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/audio-tagging&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/audio-tagging&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Source separation&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/source-separation&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/source-separation&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spoken language identification with &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/spoken-language-identification&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://hf.qhduan.com/spaces/k2-fsa/spoken-language-identification&quot;&gt;ÈïúÂÉè&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;p&gt;We also have spaces built using WebAssembly. They are listed below:&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;Huggingface space&lt;/th&gt; 
    &lt;th&gt;ModelScope space&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Voice activity detection with &lt;a href=&quot;https://github.com/snakers4/silero-vad&quot;&gt;silero-vad&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-sherpa-onnx&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/csukuangfj/web-assembly-vad-sherpa-onnx&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (Chinese + English) with Zipformer&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (Chinese + English) with Paraformer&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (Chinese + English + Cantonese) with &lt;a href=&quot;https://www.modelscope.cn/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/summary&quot;&gt;Paraformer-large&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-zh-cantonese-en-paraformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-asr-sherpa-onnx-zh-cantonese-en-paraformer&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Real-time speech recognition (English)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-asr-sherpa-onnx-en&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-asr-sherpa-onnx-en&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese) with &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/icefall/zipformer.html#sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03-chinese&quot;&gt;Zipformer CTC&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-zipformer-ctc&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-zh-zipformer-ctc/summary&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese + English + Korean + Japanese + Cantonese) with &lt;a href=&quot;https://github.com/FunAudioLLM/SenseVoice&quot;&gt;SenseVoice&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-ja-ko-cantonese-sense-voice&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-zh-en-jp-ko-cantonese-sense-voice&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English) with &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt; tiny.en&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-whisper-tiny&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-en-whisper-tiny&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English) with &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-moonshine-tiny-en-int8.tar.bz2&quot;&gt;Moonshine tiny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-moonshine-tiny&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-en-moonshine-tiny&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English) with Zipformer trained with &lt;a href=&quot;https://github.com/SpeechColab/GigaSpeech&quot;&gt;GigaSpeech&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-zipformer-gigaspeech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-en-zipformer-gigaspeech&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese) with Zipformer trained with &lt;a href=&quot;https://github.com/wenet-e2e/WenetSpeech&quot;&gt;WenetSpeech&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-zipformer-wenetspeech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-zipformer-wenetspeech&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Japanese) with Zipformer trained with &lt;a href=&quot;https://research.reazon.jp/_static/reazonspeech_nlp2023.pdf&quot;&gt;ReazonSpeech&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-ja-zipformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-ja-zipformer&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Thai) with Zipformer trained with &lt;a href=&quot;https://github.com/speechcolab/gigaspeech2&quot;&gt;GigaSpeech2&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-th-zipformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-th-zipformer&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Chinese Â§öÁßçÊñπË®Ä) with a &lt;a href=&quot;https://github.com/tele-ai/telespeech-asr&quot;&gt;TeleSpeech-ASR&lt;/a&gt; CTC model&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-telespeech&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-telespeech&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English + Chinese, ÂèäÂ§öÁßç‰∏≠ÊñáÊñπË®Ä) with Paraformer-large&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (English + Chinese, ÂèäÂ§öÁßç‰∏≠ÊñáÊñπË®Ä) with Paraformer-small&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer-small&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/k2-fsa/web-assembly-vad-asr-sherpa-onnx-zh-en-paraformer-small&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + speech recognition (Â§öËØ≠ÁßçÂèäÂ§öÁßç‰∏≠ÊñáÊñπË®Ä) with &lt;a href=&quot;https://github.com/dataoceanai/dolphin&quot;&gt;Dolphin&lt;/a&gt;-base&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-vad-asr-sherpa-onnx-multi-lang-dophin-ctc&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/csukuangfj/web-assembly-vad-asr-sherpa-onnx-multi-lang-dophin-ctc&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech synthesis (English)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-en&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-en&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech synthesis (German)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-tts-sherpa-onnx-de&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://modelscope.cn/studios/k2-fsa/web-assembly-tts-sherpa-onnx-de&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker diarization&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/k2-fsa/web-assembly-speaker-diarization-sherpa-onnx&quot;&gt;Click me&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/csukuangfj/web-assembly-speaker-diarization-sherpa-onnx&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-built Android APKs&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can find pre-built Android APKs for this repository in the following table&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;‰∏≠ÂõΩÁî®Êà∑&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker diarization&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-diarization/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-diarization/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Simulated-streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-simulate-streaming-asr.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-simulate-streaming-asr-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Text-to-speech&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/tts/apk-engine.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/tts/apk-engine-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Voice activity detection (VAD)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD + non-streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk-asr.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/vad/apk-asr-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Two-pass speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-2pass.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/android/apk-2pass-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging (WearOS)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk-wearos.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/audio-tagging/apk-wearos-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker identification&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-identification/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/speaker-identification/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spoken language identification&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/spoken-language-identification/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/spoken-language-identification/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Keyword spotting&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/kws/apk.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/kws/apk-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-built Flutter APPs&lt;/h3&gt; 
&lt;details&gt; 
 &lt;h4&gt;Real-time speech recognition&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;‰∏≠ÂõΩÁî®Êà∑&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Streaming speech recognition&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/asr/app.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/asr/app-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;h4&gt;Text-to-speech&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;‰∏≠ÂõΩÁî®Êà∑&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Android (arm64-v8a, armeabi-v7a, x86_64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-android.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-android-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Linux (x64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-linux.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-linux-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;macOS (x64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-x64.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-x64-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;macOS (arm64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-arm64.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-macos-arm64-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Windows (x64)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-win.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/flutter/tts-win-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Note: You need to build from source for iOS.&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-built Lazarus APPs&lt;/h3&gt; 
&lt;details&gt; 
 &lt;h4&gt;Generating subtitles&lt;/h4&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
    &lt;th&gt;‰∏≠ÂõΩÁî®Êà∑&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Generate subtitles (ÁîüÊàêÂ≠óÂπï)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/lazarus/download-generated-subtitles.html&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/lazarus/download-generated-subtitles-cn.html&quot;&gt;ÁÇπÊ≠§&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Links for pre-trained models&lt;/h3&gt; 
&lt;details&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
    &lt;th&gt;URL&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech recognition (speech to text, ASR)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Text-to-speech (TTS)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/tts-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;VAD&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/silero_vad.onnx&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Keyword spotting&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/kws-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Audio tagging&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/audio-tagging-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker identification (Speaker ID)&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/speaker-recongition-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Spoken language identification (Language ID)&lt;/td&gt; 
    &lt;td&gt;See multi-lingual &lt;a href=&quot;https://github.com/openai/whisper&quot;&gt;Whisper&lt;/a&gt; ASR models from &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models&quot;&gt;Speech recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Punctuation&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/punctuation-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speaker segmentation&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/speaker-segmentation-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Speech enhancement&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/speech-enhancement-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Source separation&lt;/td&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/tag/source-separation-models&quot;&gt;Address&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Some pre-trained ASR models (Streaming)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;p&gt;Please see&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-paraformer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-paraformer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-ctc/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-ctc/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;for more models. The following table lists only &lt;strong&gt;SOME&lt;/strong&gt; of them.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name&lt;/th&gt; 
    &lt;th&gt;Supported Languages&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#csukuangfj-sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20-bilingual-chinese-english&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-small-bilingual-zh-en-2023-02-16-bilingual-chinese-english&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;Suitable for Cortex A7 CPU. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-zh-14m-2023-02-23&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-en-20M-2023-02-17.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-en-20M-2023-02-17&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;Suitable for Cortex A7 CPU. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-en-20m-2023-02-17&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-korean-2024-06-16.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-korean-2024-06-16&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Korean&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#sherpa-onnx-streaming-zipformer-korean-2024-06-16-korean&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-fr-2023-04-14.tar.bz2&quot;&gt;sherpa-onnx-streaming-zipformer-fr-2023-04-14&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;French&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/online-transducer/zipformer-transducer-models.html#shaojieli-sherpa-onnx-streaming-zipformer-fr-2023-04-14-french&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h4&gt;Some pre-trained ASR models (Non-Streaming)&lt;/h4&gt; 
&lt;details&gt; 
 &lt;p&gt;Please see&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/telespeech/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/telespeech/index.html&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/index.html&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/index.html&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;for more models. The following table lists only &lt;strong&gt;SOME&lt;/strong&gt; of them.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Name&lt;/th&gt; 
    &lt;th&gt;Supported Languages&lt;/th&gt; 
    &lt;th&gt;Description&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-parakeet-tdt-0-6b-v2-int8-english&quot;&gt;sherpa-onnx-nemo-parakeet-tdt-0.6b-v2-int8&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;It is converted from &lt;a href=&quot;https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2&quot;&gt;https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-whisper-tiny.en.tar.bz2&quot;&gt;Whisper tiny.en&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/tiny.en.html&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-moonshine-tiny-en-int8.tar.bz2&quot;&gt;Moonshine tiny&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;English&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://github.com/usefulsensors/moonshine&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/icefall/zipformer.html#sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03-chinese&quot;&gt;sherpa-onnx-zipformer-ctc-zh-int8-2025-07-03&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;A Zipformer CTC model&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17.tar.bz2&quot;&gt;sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, Cantonese, English, Korean, Japanese&lt;/td&gt; 
    &lt;td&gt;ÊîØÊåÅÂ§öÁßç‰∏≠ÊñáÊñπË®Ä. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/sense-voice/index.html&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-paraformer-zh-2024-03-09.tar.bz2&quot;&gt;sherpa-onnx-paraformer-zh-2024-03-09&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese, English&lt;/td&gt; 
    &lt;td&gt;‰πüÊîØÊåÅÂ§öÁßç‰∏≠ÊñáÊñπË®Ä. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-paraformer/paraformer-models.html#csukuangfj-sherpa-onnx-paraformer-zh-2024-03-09-chinese-english&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01.tar.bz2&quot;&gt;sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Japanese&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-ja-reazonspeech-2024-08-01-japanese&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-nemo-transducer-giga-am-russian-2024-10-24.tar.bz2&quot;&gt;sherpa-onnx-nemo-transducer-giga-am-russian-2024-10-24&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Russian&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/nemo-transducer-models.html#sherpa-onnx-nemo-transducer-giga-am-russian-2024-10-24-russian&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24.tar.bz2&quot;&gt;sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Russian&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-ctc/nemo/russian.html#sherpa-onnx-nemo-ctc-giga-am-russian-2024-10-24&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-ru-2024-09-18.tar.bz2&quot;&gt;sherpa-onnx-zipformer-ru-2024-09-18&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Russian&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-ru-2024-09-18-russian&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-korean-2024-06-24.tar.bz2&quot;&gt;sherpa-onnx-zipformer-korean-2024-06-24&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Korean&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-korean-2024-06-24-korean&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-thai-2024-06-20.tar.bz2&quot;&gt;sherpa-onnx-zipformer-thai-2024-06-20&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Thai&lt;/td&gt; 
    &lt;td&gt;See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/offline-transducer/zipformer-transducer-models.html#sherpa-onnx-zipformer-thai-2024-06-20-thai&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;&lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-telespeech-ctc-int8-zh-2024-06-04.tar.bz2&quot;&gt;sherpa-onnx-telespeech-ctc-int8-zh-2024-06-04&lt;/a&gt;&lt;/td&gt; 
    &lt;td&gt;Chinese&lt;/td&gt; 
    &lt;td&gt;ÊîØÊåÅÂ§öÁßçÊñπË®Ä. See &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/pretrained_models/telespeech/models.html#sherpa-onnx-telespeech-ctc-int8-zh-2024-06-04&quot;&gt;also&lt;/a&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;Useful links&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation: &lt;a href=&quot;https://k2-fsa.github.io/sherpa/onnx/&quot;&gt;https://k2-fsa.github.io/sherpa/onnx/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Bilibili ÊºîÁ§∫ËßÜÈ¢ë: &lt;a href=&quot;https://search.bilibili.com/all?keyword=%E6%96%B0%E4%B8%80%E4%BB%A3Kaldi&quot;&gt;https://search.bilibili.com/all?keyword=%E6%96%B0%E4%B8%80%E4%BB%A3Kaldi&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;How to reach us&lt;/h3&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://k2-fsa.github.io/sherpa/social-groups.html&quot;&gt;https://k2-fsa.github.io/sherpa/social-groups.html&lt;/a&gt; for Êñ∞‰∏Ä‰ª£ Kaldi &lt;strong&gt;ÂæÆ‰ø°‰∫§ÊµÅÁæ§&lt;/strong&gt; and &lt;strong&gt;QQ ‰∫§ÊµÅÁæ§&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;Projects using sherpa-onnx&lt;/h2&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/mtkresearch/BreezeApp&quot;&gt;BreezeApp&lt;/a&gt; from &lt;a href=&quot;https://github.com/mtkresearch&quot;&gt;MediaTek Research&lt;/a&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;BreezeAPP is a mobile AI application developed for both Android and iOS platforms. Users can download it directly from the App Store and enjoy a variety of features offline, including speech-to-text, text-to-speech, text-based chatbot interactions, and image question-answering&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://huggingface.co/MediaTek-Research/BreezeApp/resolve/main/BreezeApp.apk&quot;&gt;Download APK for BreezeAPP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://hf-mirror.com/MediaTek-Research/BreezeApp/blob/main/BreezeApp.apk&quot;&gt;APK ‰∏≠ÂõΩÈïúÂÉè&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;1&lt;/th&gt; 
   &lt;th&gt;2&lt;/th&gt; 
   &lt;th&gt;3&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/1cdbc057-b893-4de6-9e9c-f1d7dfd1d992&quot; alt=&quot;&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/d77cd98e-b057-442f-860d-d5befd5c769b&quot; alt=&quot;&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/57e546bf-3d39-45b9-b392-b48ca4fb3c58&quot; alt=&quot;&quot; /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/t41372/Open-LLM-VTuber&quot;&gt;Open-LLM-VTuber&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Talk to any LLM with hands-free voice interaction, voice interruption, and Live2D taking face running locally across platforms&lt;/p&gt; 
&lt;p&gt;See also &lt;a href=&quot;https://github.com/t41372/Open-LLM-VTuber/pull/50&quot;&gt;https://github.com/t41372/Open-LLM-VTuber/pull/50&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/ruzhila/voiceapi&quot;&gt;voiceapi&lt;/a&gt;&lt;/h3&gt; 
&lt;details&gt; 
 &lt;summary&gt;Streaming ASR and TTS based on FastAPI&lt;/summary&gt; 
 &lt;p&gt;It shows how to use the ASR and TTS Python APIs with FastAPI.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/jxlpzqc/TMSpeech&quot;&gt;ËÖæËÆØ‰ºöËÆÆÊë∏È±ºÂ∑•ÂÖ∑ TMSpeech&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Uses streaming ASR in C# with graphical user interface.&lt;/p&gt; 
&lt;p&gt;Video demo in Chinese: &lt;a href=&quot;https://www.bilibili.com/video/BV1rX4y1p7Nx&quot;&gt;„ÄêÂºÄÊ∫ê„ÄëWindowsÂÆûÊó∂Â≠óÂπïËΩØ‰ª∂ÔºàÁΩëËØæ/ÂºÄ‰ºöÂøÖÂ§áÔºâ&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/l1veIn/lol-wom-electron&quot;&gt;lol‰∫íÂä®Âä©Êâã&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It uses the JavaScript API of sherpa-onnx along with &lt;a href=&quot;https://electronjs.org/&quot;&gt;Electron&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Video demo in Chinese: &lt;a href=&quot;https://www.bilibili.com/video/BV142tje9E74&quot;&gt;ÁàÜ‰∫ÜÔºÅÁÇ´Á•ûÊïô‰Ω†ÂºÄÊâìÂ≠óÊåÇÔºÅÁúüÊ≠£ÂΩ±ÂìçËÉúÁéáÁöÑËã±ÈõÑËÅîÁõüÂ∑•ÂÖ∑ÔºÅËã±ÈõÑËÅîÁõüÁöÑÊúÄÂêé‰∏ÄÂùóÊãºÂõæÔºÅÂíåÊ∏∏Êàè‰∏≠ÁöÑÊØè‰∏™‰∫∫Êó†ÈöúÁ¢çÊ≤üÈÄöÔºÅ&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/hfyydd/sherpa-onnx-server&quot;&gt;Sherpa-ONNX ËØ≠Èü≥ËØÜÂà´ÊúçÂä°Âô®&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;A server based on nodejs providing Restful API for speech recognition.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/xinhecuican/QSmartAssistant&quot;&gt;QSmartAssistant&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;‰∏Ä‰∏™Ê®°ÂùóÂåñÔºåÂÖ®ËøáÁ®ãÂèØÁ¶ªÁ∫øÔºå‰ΩéÂç†Áî®ÁéáÁöÑÂØπËØùÊú∫Âô®‰∫∫/Êô∫ËÉΩÈü≥ÁÆ±&lt;/p&gt; 
&lt;p&gt;It uses QT. Both &lt;a href=&quot;https://github.com/xinhecuican/QSmartAssistant/raw/master/doc/%E5%AE%89%E8%A3%85.md#asr&quot;&gt;ASR&lt;/a&gt; and &lt;a href=&quot;https://github.com/xinhecuican/QSmartAssistant/raw/master/doc/%E5%AE%89%E8%A3%85.md#tts&quot;&gt;TTS&lt;/a&gt; are used.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/Jason-chen-coder/Flutter-EasySpeechRecognition&quot;&gt;Flutter-EasySpeechRecognition&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It extends &lt;a href=&quot;https://raw.githubusercontent.com/k2-fsa/sherpa-onnx/master/flutter-examples/streaming_asr&quot;&gt;./flutter-examples/streaming_asr&lt;/a&gt; by downloading models inside the app to reduce the size of the app.&lt;/p&gt; 
&lt;p&gt;Note: &lt;a href=&quot;https://github.com/umgc/spring2025/pull/82&quot;&gt;[Team B] Sherpa AI backend&lt;/a&gt; also uses sherpa-onnx in a Flutter APP.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/xue-fei/sherpa-onnx-unity&quot;&gt;sherpa-onnx-unity&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;sherpa-onnx in Unity. See also &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/issues/1695&quot;&gt;#1695&lt;/a&gt;, &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/issues/1892&quot;&gt;#1892&lt;/a&gt;, and &lt;a href=&quot;https://github.com/k2-fsa/sherpa-onnx/issues/1859&quot;&gt;#1859&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server&quot;&gt;xiaozhi-esp32-server&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Êú¨È°πÁõÆ‰∏∫xiaozhi-esp32Êèê‰æõÂêéÁ´ØÊúçÂä°ÔºåÂ∏ÆÂä©ÊÇ®Âø´ÈÄüÊê≠Âª∫ESP32ËÆæÂ§áÊéßÂà∂ÊúçÂä°Âô® Backend service for xiaozhi-esp32, helps you quickly build an ESP32 device control server.&lt;/p&gt; 
&lt;p&gt;See also&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server/issues/315&quot;&gt;ASRÊñ∞Â¢ûËΩªÈáèÁ∫ßsherpa-onnx-asr&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinnan-tech/xiaozhi-esp32-server/pull/379&quot;&gt;feat: ASRÂ¢ûÂä†sherpa-onnxÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/EternityForest/KaithemAutomation&quot;&gt;KaithemAutomation&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Pure Python, GUI-focused home automation/consumer grade SCADA.&lt;/p&gt; 
&lt;p&gt;It uses TTS from sherpa-onnx. See also &lt;a href=&quot;https://github.com/EternityForest/KaithemAutomation/commit/8e64d2b138725e426532f7d66bb69dd0b4f53693&quot;&gt;‚ú® Speak command that uses the new globally configured TTS model.&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/idootop/open-xiaoai-kws&quot;&gt;Open-XiaoAI KWS&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Enable custom wake word for XiaoAi Speakers. ËÆ©Â∞èÁà±Èü≥ÁÆ±ÊîØÊåÅËá™ÂÆö‰πâÂî§ÈÜíËØç„ÄÇ&lt;/p&gt; 
&lt;p&gt;Video demo in Chinese: &lt;a href=&quot;https://www.bilibili.com/video/BV1YfVUz5EMj&quot;&gt;Â∞èÁà±ÂêåÂ≠¶ÂêØÂä®ÔΩûÀ∂‚ïπÍá¥‚ïπÀ∂ÔºÅ&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/mawwalker/stt-server&quot;&gt;C++ WebSocket ASR Server&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It provides a WebSocket server based on C++ for ASR using sherpa-onnx.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/bbeyondllove/asr_server&quot;&gt;Go WebSocket Server&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It provides a WebSocket server based on the Go programming language for sherpa-onnx.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KxPKkwxGWZs&quot;&gt;Making robot Paimon, Ep10 &quot;The AI Part 1&quot;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;It is a &lt;a href=&quot;https://www.youtube.com/watch?v=KxPKkwxGWZs&quot;&gt;YouTube video&lt;/a&gt;, showing how the author tried to use AI so he can have a conversation with Paimon.&lt;/p&gt; 
&lt;p&gt;It uses sherpa-onnx for speech-to-text and text-to-speech.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;1&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://github.com/user-attachments/assets/f6eea2d5-1807-42cb-9160-be8da2971e1f&quot; alt=&quot;&quot; /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/ys-pro-duction/TtsReader&quot;&gt;TtsReader - Desktop application&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;A desktop text-to-speech application built using Kotlin Multiplatform.&lt;/p&gt; 
&lt;h3&gt;&lt;a href=&quot;https://github.com/Mentra-Community/MentraOS&quot;&gt;MentraOS&lt;/a&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Smart glasses OS, with dozens of built-in apps. Users get AI assistant, notifications, translation, screen mirror, captions, and more. Devs get to write 1 app that runs on any pair of smart glasses.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;It uses sherpa-onnx for real-time speech recognition on iOS and Android devices. See also &lt;a href=&quot;https://github.com/Mentra-Community/MentraOS/pull/861&quot;&gt;https://github.com/Mentra-Community/MentraOS/pull/861&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;It uses Swift for iOS and Java for Android.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>envoyproxy/envoy</title>
      <link>https://github.com/envoyproxy/envoy</link>
      <description>&lt;p&gt;Cloud-native high-performance edge/middle/service proxy&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://github.com/envoyproxy/artwork/raw/main/PNG/Envoy_Logo_Final_PANTONE.png&quot; alt=&quot;Envoy Logo&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.envoyproxy.io/&quot;&gt;Cloud-native high-performance edge/middle/service proxy&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Envoy is hosted by the &lt;a href=&quot;https://cncf.io&quot;&gt;Cloud Native Computing Foundation&lt;/a&gt; (CNCF). If you are a company that wants to help shape the evolution of technologies that are container-packaged, dynamically-scheduled and microservices-oriented, consider joining the CNCF. For details about who&#39;s involved and how Envoy plays a role, read the CNCF &lt;a href=&quot;https://www.cncf.io/blog/2017/09/13/cncf-hosts-envoy/&quot;&gt;announcement&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://bestpractices.coreinfrastructure.org/projects/1266&quot;&gt;&lt;img src=&quot;https://bestpractices.coreinfrastructure.org/projects/1266/badge&quot; alt=&quot;CII Best Practices&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://securityscorecards.dev/viewer/?uri=github.com/envoyproxy/envoy&quot;&gt;&lt;img src=&quot;https://api.securityscorecards.dev/projects/github.com/envoyproxy/envoy/badge&quot; alt=&quot;OpenSSF Scorecard&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://clomonitor.io/projects/cncf/envoy&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://clomonitor.io/api/projects/cncf/envoy/badge&quot; alt=&quot;CLOMonitor&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://dev.azure.com/cncf/envoy/_build/latest?definitionId=11&amp;amp;branchName=main&quot;&gt;&lt;img src=&quot;https://dev.azure.com/cncf/envoy/_apis/build/status/11?branchName=main&quot; alt=&quot;Azure Pipelines&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&amp;amp;can=1&amp;amp;q=proj:envoy&quot;&gt;&lt;img src=&quot;https://oss-fuzz-build-logs.storage.googleapis.com/badges/envoy.svg?sanitize=true&quot; alt=&quot;Fuzzing Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://powerci.osuosl.org/job/build-envoy-static-master/&quot;&gt;&lt;img src=&quot;https://powerci.osuosl.org/buildStatus/icon?job=build-envoy-static-master&amp;amp;subject=ppc64le%20build&quot; alt=&quot;Jenkins&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://ibmz-ci.osuosl.org/job/Envoy_IBMZ_CI/&quot;&gt;&lt;img src=&quot;https://ibmz-ci.osuosl.org/buildStatus/icon?job=Envoy_IBMZ_CI&amp;amp;subject=s390x%20build&quot; alt=&quot;Jenkins&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.envoyproxy.io/&quot;&gt;Official documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.envoyproxy.io/docs/envoy/latest/faq/overview&quot;&gt;FAQ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/envoyproxy/examples/&quot;&gt;Example documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@mattklein123/envoy-threading-model-a8d44b922310&quot;&gt;Blog&lt;/a&gt; about the threading model&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@mattklein123/envoy-hot-restart-1d16b14555b5&quot;&gt;Blog&lt;/a&gt; about hot restart&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@mattklein123/envoy-stats-b65c7f363342&quot;&gt;Blog&lt;/a&gt; about stats architecture&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@mattklein123/the-universal-data-plane-api-d15cec7a&quot;&gt;Blog&lt;/a&gt; about universal data plane API&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/@mattklein123/lyfts-envoy-dashboards-5c91738816b1&quot;&gt;Blog&lt;/a&gt; on Lyft&#39;s Envoy dashboards&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/envoyproxy/data-plane-api&quot;&gt;data-plane-api&lt;/a&gt;: v2 API definitions as a standalone repository. This is a read-only mirror of &lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/api/&quot;&gt;api&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/envoyproxy/envoy-perf&quot;&gt;envoy-perf&lt;/a&gt;: Performance testing framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/envoyproxy/envoy-filter-example&quot;&gt;envoy-filter-example&lt;/a&gt;: Example of how to add new filters and link to the main repository.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/envoy-announce&quot;&gt;envoy-announce&lt;/a&gt;: Low frequency mailing list where we will email announcements only.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/envoy-security-announce&quot;&gt;envoy-security-announce&lt;/a&gt;: Low frequency mailing list where we will email security related announcements only.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/envoy-users&quot;&gt;envoy-users&lt;/a&gt;: General user discussion.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/envoy-dev&quot;&gt;envoy-dev&lt;/a&gt;: Envoy developer discussion (APIs, feature design, etc.).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://groups.google.com/forum/#!forum/envoy-maintainers&quot;&gt;envoy-maintainers&lt;/a&gt;: Use this list to reach all core Envoy maintainers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://twitter.com/EnvoyProxy/&quot;&gt;Twitter&lt;/a&gt;: Follow along on Twitter!&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://envoyproxy.slack.com/&quot;&gt;Slack&lt;/a&gt;: Slack, to get invited go &lt;a href=&quot;https://communityinviter.com/apps/envoyproxy/envoy&quot;&gt;here&lt;/a&gt;. 
  &lt;ul&gt; 
   &lt;li&gt;NOTE: Response to user questions is best effort on Slack. For a &quot;guaranteed&quot; response please email envoy-users@ per the guidance in the following linked thread.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://groups.google.com/forum/#!topic/envoy-announce/l9zjYsnS3TY&quot;&gt;this&lt;/a&gt; email thread for information on email list usage.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributing to Envoy is fun and modern C++ is a lot less scary than you might think if you don&#39;t have prior experience. To get started:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/CONTRIBUTING.md&quot;&gt;Contributing guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/envoyproxy/envoy/issues?q=is%3Aopen+is%3Aissue+label%3Abeginner&quot;&gt;Beginner issues&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/ci#building-and-running-tests-as-a-developer&quot;&gt;Build/test quick start using docker&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/DEVELOPER.md&quot;&gt;Developer guide&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Consider installing the Envoy &lt;a href=&quot;https://github.com/envoyproxy/envoy/raw/main/support/README.md&quot;&gt;development support toolchain&lt;/a&gt;, which helps automate parts of the development process, particularly those involving code review.&lt;/li&gt; 
 &lt;li&gt;Please make sure that you let us know if you are working on an issue so we don&#39;t duplicate work!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Community Meeting&lt;/h2&gt; 
&lt;p&gt;The Envoy team has a scheduled meeting time twice per month on Tuesday at 9am PT. The public Google calendar is &lt;a href=&quot;https://goo.gl/PkDijT&quot;&gt;here&lt;/a&gt;. The meeting will only be held if there are agenda items listed in the &lt;a href=&quot;https://goo.gl/5Cergb&quot;&gt;meeting minutes&lt;/a&gt;. Any member of the community should be able to propose agenda items by adding to the minutes. The maintainers will either confirm the additions to the agenda, or will cancel the meeting within 24 hours of the scheduled date if there is no confirmed agenda.&lt;/p&gt; 
&lt;h2&gt;Security&lt;/h2&gt; 
&lt;h3&gt;Security Audit&lt;/h3&gt; 
&lt;p&gt;There has been several third party engagements focused on Envoy security:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In 2018 Cure53 performed a security audit, &lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/docs/security/audit_cure53_2018.pdf&quot;&gt;full report&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;In 2021 Ada Logics performed an audit on our fuzzing infrastructure with recommendations for improvements, &lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/docs/security/audit_fuzzer_adalogics_2021.pdf&quot;&gt;full report&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Reporting security vulnerabilities&lt;/h3&gt; 
&lt;p&gt;If you&#39;ve found a vulnerability or a potential vulnerability in Envoy please let us know at &lt;a href=&quot;mailto:envoy-security@googlegroups.com&quot;&gt;envoy-security&lt;/a&gt;. We&#39;ll send a confirmation email to acknowledge your report, and we&#39;ll send an additional email when we&#39;ve identified the issue positively or negatively.&lt;/p&gt; 
&lt;p&gt;For further details please see our complete &lt;a href=&quot;https://raw.githubusercontent.com/envoyproxy/envoy/main/SECURITY.md&quot;&gt;security release process&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;ppc64le builds&lt;/h3&gt; 
&lt;p&gt;Builds for the ppc64le architecture or using aws-lc are not covered by the envoy security policy. The ppc64le architecture is currently best-effort and not maintained by the Envoy maintainers.&lt;/p&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;For further details please see our &lt;a href=&quot;https://github.com/envoyproxy/envoy/raw/main/RELEASES.md&quot;&gt;release process&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
