<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Daily Trending</title>
    <description>Daily Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:33:19 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>maxim5/cs229-2018-autumn</title>
      <link>https://github.com/maxim5/cs229-2018-autumn</link>
      <description>&lt;p&gt;All notes and materials for the CS229: Machine Learning course by Stanford University&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS229 Autumn 2018&lt;/h1&gt; 
&lt;p&gt;All lecture notes, slides and assignments for &lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;CS229: Machine Learning&lt;/a&gt; course by Stanford University.&lt;/p&gt; 
&lt;p&gt;The videos of all lectures are available &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Useful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maxim5/cs229-2019-summer&quot;&gt;CS229 Summer 2019 edition&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/cookbook</title>
      <link>https://github.com/google-gemini/cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the Gemini API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemini API Cookbook&lt;/h1&gt; 
&lt;p&gt;This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For comprehensive API documentation, visit &lt;a href=&quot;https://ai.google.dev/gemini-api/docs&quot;&gt;ai.google.dev&lt;/a&gt;.&lt;/strong&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Navigating the Cookbook&lt;/h2&gt; 
&lt;p&gt;This cookbook is organized into two main categories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;Quick Starts&lt;/a&gt;:&lt;/strong&gt; Step-by-step guides covering both introductory topics (&quot;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;Get Started&lt;/a&gt;&quot;) and specific API features.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/examples/&quot;&gt;Examples&lt;/a&gt;:&lt;/strong&gt; Practical use cases demonstrating how to combine multiple features.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We also showcase &lt;strong&gt;Demos&lt;/strong&gt; in separate repositories, illustrating end-to-end applications of the Gemini API. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;What&#39;s New?&lt;/h2&gt; 
&lt;p&gt;Here are the recent additions and updates to the Gemini API and the Cookbook:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 2.5 models:&lt;/strong&gt; Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;Get Started Guide&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_thinking.ipynb&quot;&gt;thinking guide&lt;/a&gt; as they&#39;ll all be thinking ones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Imagen and Veo&lt;/strong&gt;: Get started with our media generation model with this brand new &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_Veo.ipynb&quot;&gt;Veo guide&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb&quot;&gt;Imagen guide&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini Robotics-ER 1.5&lt;/strong&gt;: Learn about this new Gemini model specifically for spatial understanding and reasoning for &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/gemini-robotics-er.ipynb&quot;&gt;robotics applications&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lyria and TTS&lt;/strong&gt;: Get started with podcast and music generation with the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_TTS.ipynb&quot;&gt;TTS&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LyriaRealTime.ipynb&quot;&gt;Lyria RealTime&lt;/a&gt; models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LiveAPI&lt;/strong&gt;: Get started with the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb&quot;&gt;multimodal Live API&lt;/a&gt; and unlock new interactivity with Gemini.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Recently Added Guides:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb&quot;&gt;Image-out&lt;/a&gt;: Use Gemini&#39;s native image generation capabilities to edit images with high consistency or generate visual stories.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Grounding.ipynb&quot;&gt;Grounding&lt;/a&gt;: Discover different ways to ground Gemini&#39;s answer using different tools, from Google Search to Youtube and the new &lt;strong&gt;url context&lt;/strong&gt; tool.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Batch_mode.ipynb&quot;&gt;Batch-mode&lt;/a&gt;: Use Batch-mode to send large volume of non-time-sensitive requests to the model and get a 50% discount.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;1. Quick Starts&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;quickstarts section&lt;/a&gt; contains step-by-step tutorials to get you started with Gemini and learn about its specific features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To begin, you&#39;ll need:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A Google account.&lt;/li&gt; 
 &lt;li&gt;An API key (create one in &lt;a href=&quot;https://aistudio.google.com/app/apikey&quot;&gt;Google AI Studio&lt;/a&gt;). &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We recommend starting with the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Authentication.ipynb&quot;&gt;Authentication&lt;/a&gt;: Set up your API key for access.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;&lt;strong&gt;Get started&lt;/strong&gt;&lt;/a&gt;: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, explore the other quickstarts tutorials to learn about individual features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb&quot;&gt;Get started with Live API&lt;/a&gt;: Get started with the live API with this comprehensive overview of its capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_Veo.ipynb&quot;&gt;Get started with Veo&lt;/a&gt;: Get started with our video generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb&quot;&gt;Get started with Imagen&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb&quot;&gt;Image-out&lt;/a&gt;: Get started with our image generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Search_Grounding.ipynb&quot;&gt;Grounding&lt;/a&gt;: use Google Search for grounded responses&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Code_Execution.ipynb&quot;&gt;Code execution&lt;/a&gt;: Generating and running Python code to solve complex tasks and even ouput graphs&lt;/li&gt; 
 &lt;li&gt;And &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;many more&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;2. Examples (Practical Use Cases)&lt;/h2&gt; 
&lt;p&gt;These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Browser_as_a_tool.ipynb&quot;&gt;Browser as a tool&lt;/a&gt;: Use a web browser for live and internal (intranet) web interactions&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Book_illustration.ipynb&quot;&gt;Illustrate a book&lt;/a&gt;: Use Gemini and Imagen to create illustration for an open-source book&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Animated_Story_Video_Generation_gemini.ipynb&quot;&gt;Animated Story Generation&lt;/a&gt;: Create animated videos by combining Gemini&#39;s story generation, Imagen, and audio synthesis&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/LiveAPI_plotting_and_mapping.ipynb&quot;&gt;Plotting and mapping Live&lt;/a&gt;: Mix &lt;em&gt;Live API&lt;/em&gt; and &lt;em&gt;Code execution&lt;/em&gt; to solve complex tasks live&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Spatial_understanding_3d.ipynb&quot;&gt;3D Spatial understanding&lt;/a&gt;: Use Gemini &lt;em&gt;3D spatial&lt;/em&gt; abilities to understand 3D scenes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/gradio_audio.py&quot;&gt;Gradio and live API&lt;/a&gt;: Use gradio to deploy your own instance of the &lt;em&gt;Live API&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;And &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/examples/&quot;&gt;many many more&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;3. Demos (End-to-End Applications)&lt;/h2&gt; 
&lt;p&gt;These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/gemini-api-quickstart&quot;&gt;Gemini API quickstart&lt;/a&gt;: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini&#39;s multi-modal capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/multimodal-live-api-web-console&quot;&gt;Multimodal Live API Web Console&lt;/a&gt;: React-based starter app for using the Multimodal Live API over a websocket&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/starter-applets&quot;&gt;Google AI Studio Starter Applets&lt;/a&gt;: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Official SDKs&lt;/h2&gt; 
&lt;p&gt;The Gemini API is a REST API. You can call it directly using tools like &lt;code&gt;curl&lt;/code&gt; (see &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/rest/&quot;&gt;REST examples&lt;/a&gt; or the great &lt;a href=&quot;https://www.postman.com/ai-on-postman/google-gemini-apis/overview&quot;&gt;Postman workspace&lt;/a&gt;), or use one of our official SDKs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/googleapis/python-genai&quot;&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-go&quot;&gt;Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-js&quot;&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-dart&quot;&gt;Dart (Flutter)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-android&quot;&gt;Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-swift&quot;&gt;Swift&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Important: Migration&lt;/h2&gt; 
&lt;p&gt;With Gemini 2 we are offering a &lt;a href=&quot;https://github.com/googleapis/python-genai&quot;&gt;new SDK&lt;/a&gt; (&lt;code&gt;&lt;a href=&quot;https://pypi.org/project/google-genai/&quot;&gt;google-genai&lt;/a&gt;&lt;/code&gt;, &lt;code&gt;v1.0&lt;/code&gt;). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the &lt;a href=&quot;https://aistudio.google.com/live&quot;&gt;live API&lt;/a&gt; (audio + video streaming), improved tool usage ( &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/code-execution?lang=python&quot;&gt;code execution&lt;/a&gt;, &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python&quot;&gt;function calling&lt;/a&gt; and integrated &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/grounding?lang=python&quot;&gt;Google search grounding&lt;/a&gt;), and media generation (&lt;a href=&quot;https://ai.google.dev/gemini-api/docs/imagen&quot;&gt;Imagen&lt;/a&gt; and &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/video&quot;&gt;Veo&lt;/a&gt;). This SDK allows you to connect to the Gemini API through either &lt;a href=&quot;https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash&quot;&gt;Google AI Studio&lt;/a&gt; or &lt;a href=&quot;https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2&quot;&gt;Vertex AI&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;&lt;a href=&quot;https://pypi.org/project/google-generativeai&quot;&gt;google-generativeai&lt;/a&gt;&lt;/code&gt; package will continue to support the original Gemini models. It &lt;em&gt;can&lt;/em&gt; also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/migrate&quot;&gt;migration guide&lt;/a&gt; for details. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Get Help&lt;/h2&gt; 
&lt;p&gt;Ask a question on the &lt;a href=&quot;https://discuss.ai.google.dev/&quot;&gt;Google AI Developer Forum&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;The Gemini API on Google Cloud Vertex AI&lt;/h2&gt; 
&lt;p&gt;For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See &lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai&quot;&gt;this repo&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Thank you for developing with the Gemini API! We&#39;re excited to see what you create.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/generative-ai-for-beginners</title>
      <link>https://github.com/microsoft/generative-ai-for-beginners</link>
      <description>&lt;p&gt;21 Lessons, Get Started Building with Generative AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/images/repo-thumbnailv4-fixed.png?WT.mc_id=academic-105485-koreyst&quot; alt=&quot;Generative AI For Beginners&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;21 Lessons teaching everything you need to know to start building Generative AI applications&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-For-Beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üåê Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/lt/README.md&quot;&gt;Lithuanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Generative AI for Beginners (Version 3) - A Course&lt;/h1&gt; 
&lt;p&gt;Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.&lt;/p&gt; 
&lt;h2&gt;üå± Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has 21 lessons. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;Lessons are labeled either &quot;Learn&quot; lessons explaining a Generative AI concept or &quot;Build&quot; lessons that explain a concept and code examples in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt; when possible.&lt;/p&gt; 
&lt;p&gt;For .NET Developers checkout &lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners (.NET Edition)&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Each lesson also includes a &quot;Keep Learning&quot; section with additional learning tools.&lt;/p&gt; 
&lt;h2&gt;What You Need&lt;/h2&gt; 
&lt;h3&gt;To run the code of this course, you can use either:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/azure-open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;Azure OpenAI Service&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;aoai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/gh-models?WT.mc_id=academic-105485-koreyst&quot;&gt;GitHub Marketplace Model Catalog&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;githubmodels&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;OpenAI API&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;oai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Basic knowledge of Python or TypeScript is helpful - *For absolute beginners check out these &lt;a href=&quot;https://aka.ms/genai-beginners/python?WT.mc_id=academic-105485-koreyst&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://aka.ms/genai-beginners/typescript?WT.mc_id=academic-105485-koreyst&quot;&gt;TypeScript&lt;/a&gt; courses&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A GitHub account to &lt;a href=&quot;https://aka.ms/genai-beginners/github?WT.mc_id=academic-105485-koreyst&quot;&gt;fork this entire repo&lt;/a&gt; to your own GitHub account&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have created a &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/strong&gt; lesson to help you with setting up your development environment.&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to &lt;a href=&quot;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&quot;&gt;star (üåü) this repo&lt;/a&gt; to find it easier later.&lt;/p&gt; 
&lt;h2&gt;üß† Ready to Deploy?&lt;/h2&gt; 
&lt;p&gt;If you are looking for more advanced code samples, check out our &lt;a href=&quot;https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst&quot;&gt;collection of Generative AI Code Samples&lt;/a&gt; in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;üó£Ô∏è Meet Other Learners, Get Support&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;official Azure AI Foundry Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;p&gt;Ask questions or share product feedback in our &lt;a href=&quot;https://aka.ms/azureaifoundry/forum&quot;&gt;Azure AI Foundry Developer Forum&lt;/a&gt; on Github.&lt;/p&gt; 
&lt;h2&gt;üöÄ Building a Startup?&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href=&quot;https://www.microsoft.com/startups&quot;&gt;Microsoft for Startups&lt;/a&gt; to find out how to get started building with Azure credits today.&lt;/p&gt; 
&lt;h2&gt;üôè Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst&quot;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&quot;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìÇ Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A short video introduction to the topic&lt;/li&gt; 
 &lt;li&gt;A written lesson located in the README&lt;/li&gt; 
 &lt;li&gt;Python and TypeScript code samples supporting Azure OpenAI and OpenAI API&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üóÉÔ∏è Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson Link&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to Setup Your Development Environment&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Introduction to Generative AI and LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Understanding what Generative AI is and how Large Language Models (LLMs) work.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Exploring and comparing different LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to select the right model for your use case&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Using Generative AI Responsibly&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to build Generative AI Applications responsibly&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Understanding Prompt Engineering Fundamentals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Hands-on Prompt Engineering Best Practices&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Creating Advanced Prompts&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply prompt engineering techniques that improve the outcome of your prompts.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Text Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A text generation app using Azure OpenAI / OpenAI API&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Chat Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; Techniques for efficiently building and integrating chat applications.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Search Apps Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A search application that uses Embeddings to search for data.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Image Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An image generation application&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Low Code AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A Generative AI application using Low Code tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Integrating External Applications with Function Calling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; What is function calling and its use cases for applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Designing UX for AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply UX design principles when developing Generative AI Applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Securing Your Generative AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The threats and risks to AI systems and methods to secure these systems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;The Generative AI Application Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The tools and metrics to manage the LLM Lifecycle and LLMOps&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Retrieval Augmented Generation (RAG) and Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using a RAG Framework to retrieve embeddings from a Vector Databases&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Open Source Models and Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using open source models available on Hugging Face&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;AI Agents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using an AI Agent Framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Fine-Tuning LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The what, why and how of fine-tuning LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/19-slm/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with SLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The benefits of building with Small Language Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/20-mistral/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Mistral Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Mistral Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/21-meta/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Meta Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Meta Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üåü Special thanks&lt;/h3&gt; 
&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/john0isaac/&quot;&gt;&lt;strong&gt;John Aziz&lt;/strong&gt;&lt;/a&gt; for creating all of the GitHub Actions and workflows&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/bernhard-merkle-738b73/&quot;&gt;&lt;strong&gt;Bernhard Merkle&lt;/strong&gt;&lt;/a&gt; for making key contributions to each lesson to improve the learner and code experience.&lt;/p&gt; 
&lt;h2&gt;üéí Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mcp-for-beginners&quot;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners&quot;&gt;AI Agents for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-js-course&quot;&gt;Generative AI for Beginners using JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genaijava&quot;&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>roboflow/notebooks</title>
      <link>https://github.com/roboflow/notebooks</link>
      <description>&lt;p&gt;A collection of tutorials on state-of-the-art computer vision models and techniques. Explore everything from foundational architectures like ResNet to cutting-edge models like YOLO11, RT-DETR, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5VL.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt; &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt; &lt;img width=&quot;850&quot; src=&quot;https://raw.githubusercontent.com/roboflow/notebooks/main/assets/roboflow-notebooks-banner.png&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/roboflow/notebooks&quot;&gt;notebooks&lt;/a&gt; | &lt;a href=&quot;https://github.com/roboflow/inference&quot;&gt;inference&lt;/a&gt; | &lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;autodistill&lt;/a&gt; | &lt;a href=&quot;https://github.com/roboflow/rf-detr&quot;&gt;RF-DETR&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://docs.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&quot; width=&quot;3%&quot; /&gt; &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt;
  &lt;a href=&quot;https://blog.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt;  
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;üëã hello&lt;/h2&gt; 
&lt;p&gt;This repository offers a growing collection of computer vision tutorials. Learn to use SOTA models like YOLOv11, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5-VL for tasks ranging from object detection, segmentation, and pose estimation to data extraction and OCR. Dive in and explore the exciting world of computer vision!&lt;/p&gt; 
&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; 
&lt;!--
   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
--&gt; 
&lt;h2&gt;üöÄ model tutorials (51 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-google-gamini-2-5.ipynb&quot;&gt;Zero-Shot Object Detection and Segmentation with Google Gemini 2.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-and-segmentation-with-google-gamini-2-5.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-google-gamini-2-5.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/gemini-2-5-object-detection-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2507.06261&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2507.06261-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb&quot;&gt;Fine-Tune RF-DETR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/rf-detr&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/yHW0ip-2i54&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/rf-detr&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;Zero-Shot Object Detection and Segmentation with YOLOE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yoloe-zero-shot-object-detection-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=eHAnIehnCt4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/THU-MIG/yoloe&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2503.07465&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2503.07465-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;Fine-Tune YOLOv12 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov12-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/sunsmarterjie/yolov12&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.12524&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;Zero-Shot Object Detection with Qwen2.5-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/qwen2-5-vl-zero-shot-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=xEfh0IR8Fvo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;Fine-Tune Qwen2.5-VL for JSON Data Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xEfh0IR8Fvo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma2 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;Fine-Tune PaliGemma2 for JSON Data Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma2 for LaTeX OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;Fine-Tune SAM-2.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-sam-2-1/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=QnCGcFHZy9s&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/sam2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;Fine-Tune GPT-4o on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/gpt-4o-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=6Q6TieCBA4E&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO11 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov11-how-to-train-custom-data/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=jE_s4tVgPHA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO11 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolo11-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=jE_s4tVgPHA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;Segment Images with SAM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/Dv003fTyO-Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;Segment Videos with SAM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/Dv003fTyO-Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;Fine-Tune RT-DETR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-rt-detr-custom-dataset-transformers/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/lyuwenyu/RT-DETR&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.08069&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.08069-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;Fine-Tune Florence-2 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-florence-2-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=i3KjYgxNH6w&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06242&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;Run Different Vision Tasks with Florence-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/florence-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=hj_ybcRdk5Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06242&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-fine-tune-paligemma/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=OMBmVInx68M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2407.07726&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2407.07726-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv10 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov10-how-to-train/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/THU-MIG/yolov10&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2405.14458&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;Zero-Shot Object Detection with YOLO-World&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-yolo-world/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=X7gKBGVz4vs&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/AILab-CVC/YOLO-World&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2401.17270&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2401.17270-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv9 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov9-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/XHT2c8jT3Bc&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov9&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2402.13616&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2402.13616-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune RTMDet on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-rtmdet-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/5kgWyo6Sg4E&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2212.07784&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2212.07784-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;Segment Images with FastSAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-fastsam&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/yHNPyqazYYU&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/CASIA-IVA-Lab/FastSAM&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2306.12156&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2306.12156-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO-NAS on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/V-H3eoPUnA8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Deci-AI/super-gradients/raw/master/YOLONAS.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;Segment Images with Segment Anything Model (SAM)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-segment-anything-model-sam&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.02643&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;Zero-Shot Object Detection with Grounding DINO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/grounding-dino-zero-shot-object-detection&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/cMa77r3YrDk&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;Fine-Tune DETR Transformer on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/AM8D4j9KoaU&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/AM8D4j9KoaU&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/detr&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2005.12872-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&quot;&gt;Classify Images with DINOv2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-classify-images-with-dinov2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/wuZtUMEiKWY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;Fine-Tune YOLOv8 on Pose Estimation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-a-custom-yolov8-pose-estimation-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;Fine-Tune YOLOv8 on Oriented Bounding Boxes (OBB) Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https:/blog.roboflow.com/train-yolov8-obb-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov8-instance-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/pFiGSrRtaU4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-a-yolov8-classification-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv7 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=5nsmXLyDaU4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv7 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov7-instance-segmentation-on-custom-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vFGxM2KLs10&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune MT-YOLOv6 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=fFCWrMFH2UY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/meituan/YOLOv6&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2209.02976&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2209.02976-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/watch?v=x0ThXHbtqCQ&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov5-classification-custom-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=DPjp9Kq4qn8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov5-instance-segmentation-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vKzfvtEtiYo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune Faster RCNN on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-detectron2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/e8LPflX0nwQ&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1703.06870v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1703.06870v3-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune SegFormer on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset-with-pytorch-lightning&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=4HNkBMfw-2o&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/NVlabs/SegFormer&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.15203v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2105.15203v3-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;Fine-Tune ViT on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-vision-transformer&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=8yRE2Pa-8_I&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/lucidrains/vit-pytorch&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2010.11929-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune Scaled-YOLOv4 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-scaled-yolov4&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=rEbpKxZbvIo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/ScaledYOLOv4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.10934&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2004.10934-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOS on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolos-transformer-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=N0V0xxSi6Xc&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.00666&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2106.00666-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolor-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=sZ5DiXDOHEM&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/yolor&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1506.02640-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOX on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=q3RbFbaQQGw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Megvii-BaseDetection/YOLOX&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.08430&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2107.08430-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;Fine-Tune ResNet34 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-a-custom-resnet34-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=93kXzUOiYY4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;Image Classification with OpenAI Clip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-openai-clip&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=8o701AEoZ8I&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv4-tiny Darknet on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.ai/train-yolov4-tiny-on-custom-data-lighting-fast-detection&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=NTnZgLsk_DA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/darknet&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.04244&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2011.04244-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;Train a YOLOv8 Classification Model with No Labeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-classification-model-no-labeling/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üìç tracker tutorials (2 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-sort-tracker.ipynb&quot;&gt;How to Track Objects with SORT Tracker&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-objects-with-sort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-sort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/trackers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1602.00763&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1602.00763-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-deepsort-tracker.ipynb&quot;&gt;How to Track Objects with DeepSORT Tracker&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-objects-with-deepsort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-deepsort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/trackers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1703.07402&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1703.07402-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üõ†Ô∏è computer vision skills (24 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb&quot;&gt;Basketball AI: Make or Miss - Jumpshot Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb&quot;&gt;Basketball AI: Detect NBA 3 Second Violation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/detect-3-second-violation-ai-basketball&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&quot;&gt;Basketball AI: How to Detect Track and Identify Basketball Players&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/football-ai.ipynb&quot;&gt;Football AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/football-ai.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/football-ai.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/camera-calibration-sports-computer-vision/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/aBVGKoNZQUw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/sports&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;Auto-Annotate Dataset with GroundedSAM 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;Run YOLOv7 Object Detection with OpenVINO + TorchORT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/accelerate-pytorch-openvino-torch-ort&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;Estimate Vehicle Speed with YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/estimate-speed-computer-vision/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/uWP6UjDeZvY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision/tree/develop/examples/speed_estimation&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;Detect and Count Objects in Polygon Zone with YOLOv5 / YOLOv8 / Detectron2 + Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-count-objects-in-a-zone&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/l_kf9CfZ_8M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;Track and Count Vehicles with YOLOv8 + ByteTRACK + Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov8-tracking-and-counting/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/OS5qI9YBkfk&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.06864&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;Football Players Tracking with YOLOv5 + ByteTRACK&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/track-football-players&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/QCG8QMhga9k&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ifzhang/ByteTrack&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.06864&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;Auto Train YOLOv8 Model with Autodistill&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/autodistill&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/gKTYMfwPo4M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;Image Embeddings Analysis - Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/YxJkE6FvGF4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO and SAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/enhance-image-annotation-with-grounding-dino-and-sam/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/C4NqaRBz_Kw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;Roboflow Video Inference with Custom Annotators&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/custom-annotator-video-inference&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/inference&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;DINO-GPT-4V Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/dino-gpt-4v/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;Train a Segmentation Model with No Labeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-a-segmentation-model-no-labeling/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;DINOv2 Image Retrieval&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;Vector Analysis with Scikit-learn and Bokeh&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/vector-analysis&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;RF100 Object Detection Model Benchmarking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/roboflow-100&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/jIgZMr-PBMo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/roboflow-100-benchmark&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2211.13523&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2211.13523-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;Create Segmentation Masks with Roboflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-create-segmentation-masks-with-roboflow&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;How to Use PolygonZone and Roboflow Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/polygonzone/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;Train a Package Detector With Two Labeled Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/package-detector/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill-seggpt&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;Image-to-Image Search with CLIP and faiss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/clip-image-search-faiss/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; 
&lt;h2&gt;üé¨ videos&lt;/h2&gt; 
&lt;p&gt;Almost every week we create tutorials showing you the hottest models in Computer Vision. üî• &lt;a href=&quot;https://www.youtube.com/@Roboflow&quot;&gt;Subscribe&lt;/a&gt;, and stay up to date with our latest YouTube videos!&lt;/p&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/CilXrt3S-ws&quot; title=&quot;How to Choose the Best Computer Vision Model for Your Project&quot;&gt;&lt;img src=&quot;https://github.com/roboflow/notebooks/assets/26109316/73a01d3b-cf70-40c3-a5e4-e4bc5be38d42&quot; alt=&quot;How to Choose the Best Computer Vision Model for Your Project&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/CilXrt3S-ws&quot; title=&quot;How to Choose the Best Computer Vision Model for Your Project&quot;&gt;&lt;strong&gt;How to Choose the Best Computer Vision Model for Your Project&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 26 May 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 26 May 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;br /&gt; In this video, we will dive into the complexity of choosing the right computer vision model for your unique project. From the importance of high-quality datasets to hardware considerations, interoperability, benchmarking, and licensing issues, this video covers it all... 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot; title=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/ae1ca38e-40b7-4b35-8582-e8ea5de3806e&quot; alt=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot; title=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot;&gt;&lt;strong&gt;Accelerate Image Annotation with SAM and Grounding DINO&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 20 Apr 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 20 Apr 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;br /&gt; Discover how to speed up your image annotation process using Grounding DINO and Segment Anything Model (SAM). Learn how to convert object detection datasets into instance segmentation datasets, and see the potential of using these models to automatically annotate your datasets for real-time detectors like YOLOv8... 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot; title=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/6913ff11-53c6-4341-8d90-eaff3023c3fd&quot; alt=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot; title=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot;&gt;&lt;strong&gt;SAM - Segment Anything Model by Meta AI: Complete Guide&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 11 Apr 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 11 Apr 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;br /&gt; Discover the incredible potential of Meta AI&#39;s Segment Anything Model (SAM)! We dive into SAM, an efficient and promptable model for image segmentation, which has revolutionized computer vision tasks. With over 1 billion masks on 11M licensed and privacy-respecting images, SAM&#39;s zero-shot performance is often superior to prior fully supervised results... &lt;/p&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;üíª run locally&lt;/h2&gt; 
&lt;p&gt;We try to make it as easy as possible to run Roboflow Notebooks in Colab and Kaggle, but if you still want to run them locally, below you will find instructions on how to do it. Remember don&#39;t install your dependencies globally, use &lt;a href=&quot;https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/&quot;&gt;venv&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;# clone repository and navigate to root directory
git clone git@github.com:roboflow-ai/notebooks.git
cd notebooks

# setup python environment and activate it
python3 -m venv venv
source venv/bin/activate

# install and run jupyter notebook
pip install notebook
jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;‚òÅÔ∏è run in sagemaker studio lab&lt;/h2&gt; 
&lt;p&gt;You can now open our tutorial notebooks in &lt;a href=&quot;https://aws.amazon.com/sagemaker/studio-lab/&quot;&gt;Amazon SageMaker Studio Lab&lt;/a&gt; - a free machine learning development environment that provides the compute, storage, and security‚Äîall at no cost‚Äîfor anyone to learn and experiment with ML.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Stable Diffusion Image Generation&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;YOLOv5 Custom Dataset Training&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;YOLOv7 Custom Dataset Training&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/stable-diffusion-image-generation.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov5-custom-training.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov7-custom-training.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üêû bugs &amp;amp; ü¶∏ contribution&lt;/h2&gt; 
&lt;p&gt;Computer Vision moves fast! Sometimes our notebooks lag a tad behind the ever-pushing forward libraries. If you notice that any of the notebooks is not working properly, create a &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=bug%2Ctriage&amp;amp;template=bug-report.yml&quot;&gt;bug report&lt;/a&gt; and let us know.&lt;/p&gt; 
&lt;p&gt;If you have an idea for a new tutorial we should do, create a &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=feature-request.yml&quot;&gt;feature request&lt;/a&gt;. We are constantly looking for new ideas. If you feel up to the task and want to create a tutorial yourself, please take a peek at our &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/CONTRIBUTING.md&quot;&gt;contribution guide&lt;/a&gt;. There you can find all the information you need.&lt;/p&gt; 
&lt;p&gt;We are here for you, so don&#39;t hesitate to &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/discussions&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pyannote/pyannote-audio</title>
      <link>https://github.com/pyannote/pyannote-audio</link>
      <description>&lt;p&gt;Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Using &lt;code&gt;pyannote.audio&lt;/code&gt; open-source toolkit in production?&lt;br /&gt; Consider switching to &lt;a href=&quot;https://www.pyannote.ai&quot;&gt;pyannoteAI&lt;/a&gt; for better and faster options.&lt;/p&gt; 
&lt;h1&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; speaker diarization toolkit&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; is an open-source toolkit written in Python for speaker diarization. Based on &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/pytorch.org&quot;&gt;PyTorch&lt;/a&gt; machine learning framework, it comes with state-of-the-art &lt;a href=&quot;https://hf.co/pyannote&quot;&gt;pretrained models and pipelines&lt;/a&gt;, that can be further finetuned to your own data for even better performance.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=37R_R82lfwA&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/37R_R82lfwA/0.jpg&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;TL;DR&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://github.com/pyannote/pyannote-audio&quot;&gt;&lt;code&gt;pyannote.audio&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;pip install pyannote.audio&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Accept &lt;a href=&quot;https://hf.co/pyannote/segmentation-3.0&quot;&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/a&gt; user conditions&lt;/li&gt; 
 &lt;li&gt;Accept &lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;&lt;code&gt;pyannote/speaker-diarization-3.1&lt;/code&gt;&lt;/a&gt; user conditions&lt;/li&gt; 
 &lt;li&gt;Create access token at &lt;a href=&quot;https://hf.co/settings/tokens&quot;&gt;&lt;code&gt;hf.co/settings/tokens&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained(
    &quot;pyannote/speaker-diarization-3.1&quot;,
    use_auth_token=&quot;HUGGINGFACE_ACCESS_TOKEN_GOES_HERE&quot;)

# send pipeline to GPU (when available)
import torch
pipeline.to(torch.device(&quot;cuda&quot;))

# apply pretrained pipeline
diarization = pipeline(&quot;audio.wav&quot;)

# print the result
for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f&quot;start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}&quot;)
# start=0.2s stop=1.5s speaker_0
# start=1.8s stop=3.9s speaker_1
# start=4.2s stop=5.7s speaker_0
# ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;ü§ó&lt;/span&gt; pretrained &lt;a href=&quot;https://hf.co/models?other=pyannote-audio-pipeline&quot;&gt;pipelines&lt;/a&gt; (and &lt;a href=&quot;https://hf.co/models?other=pyannote-audio-model&quot;&gt;models&lt;/a&gt;) on &lt;a href=&quot;https://huggingface.co/pyannote&quot;&gt;&lt;span&gt;ü§ó&lt;/span&gt; model hub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ü§Ø&lt;/span&gt; state-of-the-art performance (see &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/#benchmark&quot;&gt;Benchmark&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;üêç&lt;/span&gt; Python-first API&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;‚ö°&lt;/span&gt; multi-GPU training with &lt;a href=&quot;https://pytorchlightning.ai/&quot;&gt;pytorch-lightning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/CHANGELOG.md&quot;&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/FAQ.md&quot;&gt;Frequently asked questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Models 
  &lt;ul&gt; 
   &lt;li&gt;Available tasks explained&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/applying_a_model.ipynb&quot;&gt;Applying a pretrained model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/training_a_model.ipynb&quot;&gt;Training, fine-tuning, and transfer learning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Pipelines 
  &lt;ul&gt; 
   &lt;li&gt;Available pipelines explained&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/applying_a_pipeline.ipynb&quot;&gt;Applying a pretrained pipeline&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/adapting_pretrained_pipeline.ipynb&quot;&gt;Adapting a pretrained pipeline to your own data&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/voice_activity_detection.ipynb&quot;&gt;Training a pipeline&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Contributing 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/add_your_own_model.ipynb&quot;&gt;Adding a new model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/add_your_own_task.ipynb&quot;&gt;Adding a new task&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Adding a new pipeline&lt;/li&gt; 
   &lt;li&gt;Sharing pretrained models and pipelines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Blog 
  &lt;ul&gt; 
   &lt;li&gt;2022-12-02 &amp;gt; &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/adapting_pretrained_pipeline.ipynb&quot;&gt;&quot;How I reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022 speaker diarization challenges&quot;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2022-10-23 &amp;gt; &lt;a href=&quot;https://herve.niderb.fr/fastpages/2022/10/23/One-speaker-segmentation-model-to-rule-them-all&quot;&gt;&quot;One speaker segmentation model to rule them all&quot;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2021-08-05 &amp;gt; &lt;a href=&quot;https://herve.niderb.fr/fastpages/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html&quot;&gt;&quot;Streaming voice activity detection with pyannote.audio&quot;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Videos 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://umotion.univ-lemans.fr/video/9513-speech-segmentation-and-speaker-diarization/&quot;&gt;Introduction to speaker diarization&lt;/a&gt; / JSALT 2023 summer school / 90 min&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wDH2rvkjymY&quot;&gt;Speaker segmentation model&lt;/a&gt; / Interspeech 2021 / 3 min&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=37R_R82lfwA&quot;&gt;First release of pyannote.audio&lt;/a&gt; / ICASSP 2020 / 8 min&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Community contributions (not maintained by the core team) 
  &lt;ul&gt; 
   &lt;li&gt;2024-04-05 &amp;gt; &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/community/offline_usage_speaker_diarization.ipynb&quot;&gt;Offline speaker diarization (speaker-diarization-3.1)&lt;/a&gt; by &lt;a href=&quot;https://github.com/simonottenhauskenbun&quot;&gt;Simon Ottenhaus&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;Out of the box, &lt;code&gt;pyannote.audio&lt;/code&gt; speaker diarization &lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;pipeline&lt;/a&gt; v3.1 is expected to be much better (and faster) than v2.x. Those numbers are diarization error rates (in %):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-2.1&quot;&gt;v2.1&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;v3.1&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://www.pyannote.ai&quot;&gt;pyannoteAI&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.03603&quot;&gt;AISHELL-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14.1&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
   &lt;td&gt;11.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.openslr.org/119/&quot;&gt;AliMeeting&lt;/a&gt; (channel 1)&lt;/td&gt; 
   &lt;td&gt;27.4&lt;/td&gt; 
   &lt;td&gt;24.4&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;AMI&lt;/a&gt; (IHM)&lt;/td&gt; 
   &lt;td&gt;18.9&lt;/td&gt; 
   &lt;td&gt;18.8&lt;/td&gt; 
   &lt;td&gt;16.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;AMI&lt;/a&gt; (SDM)&lt;/td&gt; 
   &lt;td&gt;27.1&lt;/td&gt; 
   &lt;td&gt;22.4&lt;/td&gt; 
   &lt;td&gt;20.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.14448&quot;&gt;AVA-AVD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;66.3&lt;/td&gt; 
   &lt;td&gt;50.0&lt;/td&gt; 
   &lt;td&gt;39.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2001S97&quot;&gt;CALLHOME&lt;/a&gt; (&lt;a href=&quot;https://github.com/BUTSpeechFIT/CALLHOME_sublists/issues/1&quot;&gt;part 2&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;31.6&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
   &lt;td&gt;22.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2022S14&quot;&gt;DIHARD 3&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2012.01477&quot;&gt;full&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;26.9&lt;/td&gt; 
   &lt;td&gt;21.7&lt;/td&gt; 
   &lt;td&gt;17.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/revdotcom/speech-datasets&quot;&gt;Earnings21&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;17.0&lt;/td&gt; 
   &lt;td&gt;9.4&lt;/td&gt; 
   &lt;td&gt;9.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.07058&quot;&gt;Ego4D&lt;/a&gt; (dev.)&lt;/td&gt; 
   &lt;td&gt;61.5&lt;/td&gt; 
   &lt;td&gt;51.2&lt;/td&gt; 
   &lt;td&gt;43.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/X-LANCE/MSDWILD&quot;&gt;MSDWild&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;32.8&lt;/td&gt; 
   &lt;td&gt;25.3&lt;/td&gt; 
   &lt;td&gt;19.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.openslr.org/123/&quot;&gt;RAMC&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
   &lt;td&gt;22.2&lt;/td&gt; 
   &lt;td&gt;18.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.islrn.org/resources/360-758-359-485-0/&quot;&gt;REPERE&lt;/a&gt; (phase2)&lt;/td&gt; 
   &lt;td&gt;8.2&lt;/td&gt; 
   &lt;td&gt;7.8&lt;/td&gt; 
   &lt;td&gt;7.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/joonson/voxconverse&quot;&gt;VoxConverse&lt;/a&gt; (v0.3)&lt;/td&gt; 
   &lt;td&gt;11.2&lt;/td&gt; 
   &lt;td&gt;11.3&lt;/td&gt; 
   &lt;td&gt;9.4&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;http://pyannote.github.io/pyannote-metrics/reference.html#diarization&quot;&gt;Diarization error rate&lt;/a&gt; (in %)&lt;/p&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;p&gt;If you use &lt;code&gt;pyannote.audio&lt;/code&gt; please use the following citations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Plaquet23,
  author={Alexis Plaquet and Herv√© Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Bredin23,
  author={Herv√© Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;The commands below will setup pre-commit hooks and packages needed for developing the &lt;code&gt;pyannote.audio&lt;/code&gt; library.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e .[dev,testing]
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Test&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pytest
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;ü¶úüîó Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot; /&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot; /&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot; /&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pepy/dt/langchain&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in Github Codespace&quot; title=&quot;Open in Github Codespace&quot; width=&quot;150&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codspeed.io/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development ‚Äî all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If you‚Äôre looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain‚Äôs vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your application‚Äôs needs. As the industry frontier evolves, adapt quickly ‚Äî LangChain‚Äôs abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChain‚Äôs ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows ‚Äî and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.langchain.com/langgraph-platform&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://forum.langchain.com/&quot;&gt;LangChain Forum&lt;/a&gt;: Connect with the community and share all of your technical questions, ideas, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://chat.langchain.com/&quot;&gt;Chat LangChain&lt;/a&gt;: Ask questions &amp;amp; chat with our documentation.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/happy-llm</title>
      <link>https://github.com/datawhalechina/happy-llm</link>
      <description>&lt;p&gt;üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/head.jpg&quot; alt=&quot;alt text&quot; width=&quot;100%&quot; /&gt; 
 &lt;h1&gt;Happy-LLM&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub stars&quot; /&gt; 
 &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub forks&quot; /&gt; 
 &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot; /&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub Project&quot; /&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://swanlab.cn/@kmno4/Happy-LLM/overview&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg?sanitize=true&quot; alt=&quot;SwanLab&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://trendshift.io/repositories/14175&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14175&quot; alt=&quot;datawhalechina%2Fhappy-llm | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://datawhalechina.github.io/happy-llm/&quot;&gt;üìö Âú®Á∫øÈòÖËØªÂú∞ÂùÄ&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;Ê∑±ÂÖ•ÁêÜËß£ LLM Ê†∏ÂøÉÂéüÁêÜÔºåÂä®ÊâãÂÆûÁé∞‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Â§ßÊ®°Âûã&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ È°πÁõÆ‰ªãÁªç&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;ÂæàÂ§öÂ∞è‰ºô‰º¥Âú®ÁúãÂÆå DatawhaleÂºÄÊ∫êÈ°πÁõÆÔºö &lt;a href=&quot;https://github.com/datawhalechina/self-llm&quot;&gt;self-llm ÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó&lt;/a&gt; ÂêéÔºåÊÑüËßâÊÑèÁäπÊú™Â∞ΩÔºåÊÉ≥Ë¶ÅÊ∑±ÂÖ•‰∫ÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁêÜÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇ‰∫éÊòØÊàë‰ª¨ÔºàDatawhaleÔºâÂÜ≥ÂÆöÊé®Âá∫„ÄäHappy-LLM„ÄãÈ°πÁõÆÔºåÊó®Âú®Â∏ÆÂä©Â§ßÂÆ∂Ê∑±ÂÖ•ÁêÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁêÜÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇ&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÊòØ‰∏Ä‰∏™&lt;strong&gt;Á≥ªÁªüÊÄßÁöÑ LLM Â≠¶‰π†ÊïôÁ®ã&lt;/strong&gt;ÔºåÂ∞Ü‰ªé NLP ÁöÑÂü∫Êú¨Á†îÁ©∂ÊñπÊ≥ïÂá∫ÂèëÔºåÊ†πÊçÆ LLM ÁöÑÊÄùË∑ØÂèäÂéüÁêÜÈÄêÂ±ÇÊ∑±ÂÖ•Ôºå‰æùÊ¨°‰∏∫ËØªËÄÖÂâñÊûê LLM ÁöÑÊû∂ÊûÑÂü∫Á°ÄÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨‰ºöÁªìÂêàÁõÆÂâç LLM È¢ÜÂüüÊúÄ‰∏ªÊµÅÁöÑ‰ª£Á†ÅÊ°ÜÊû∂ÔºåÊºîÁªÉÂ¶Ç‰Ωï‰∫≤ÊâãÊê≠Âª∫„ÄÅËÆ≠ÁªÉ‰∏Ä‰∏™ LLMÔºåÊúü‰ª•ÂÆûÁé∞Êéà‰πã‰ª•È±ºÔºåÊõ¥Êéà‰πã‰ª•Ê∏î„ÄÇÂ∏åÊúõÂ§ßÂÆ∂ËÉΩ‰ªéËøôÊú¨‰π¶ÂºÄÂßãËµ∞ÂÖ• LLM ÁöÑÊµ©ÁÄö‰∏ñÁïåÔºåÊé¢Á¥¢ LLM ÁöÑÊó†Â∞ΩÂèØËÉΩ„ÄÇ&lt;/p&gt; 
&lt;h3&gt;‚ú® ‰Ω†Â∞ÜÊî∂Ëé∑‰ªÄ‰πàÔºü&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Datawhale ÂºÄÊ∫êÂÖçË¥π&lt;/strong&gt; ÂÆåÂÖ®ÂÖçË¥πÁöÑÂ≠¶‰π†Êú¨È°πÁõÆÊâÄÊúâÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Ê∑±ÂÖ•ÁêÜËß£&lt;/strong&gt; Transformer Êû∂ÊûÑÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;ÊéåÊè°&lt;/strong&gt; È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Êú¨ÂéüÁêÜ&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;‰∫ÜËß£&lt;/strong&gt; Áé∞ÊúâÂ§ßÊ®°ÂûãÁöÑÂü∫Êú¨ÁªìÊûÑ&lt;/li&gt; 
 &lt;li&gt;üèóÔ∏è &lt;strong&gt;Âä®ÊâãÂÆûÁé∞&lt;/strong&gt; ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ LLaMA2 Ê®°Âûã&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;ÊéåÊè°ËÆ≠ÁªÉ&lt;/strong&gt; ‰ªéÈ¢ÑËÆ≠ÁªÉÂà∞ÂæÆË∞ÉÁöÑÂÖ®ÊµÅÁ®ã&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;ÂÆûÊàòÂ∫îÁî®&lt;/strong&gt; RAG„ÄÅAgent Á≠âÂâçÊ≤øÊäÄÊúØ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ ÂÜÖÂÆπÂØºËà™&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Á´†ËäÇ&lt;/th&gt; 
   &lt;th&gt;ÂÖ≥ÈîÆÂÜÖÂÆπ&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/%E5%89%8D%E8%A8%80.md&quot;&gt;ÂâçË®Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Êú¨È°πÁõÆÁöÑÁºòËµ∑„ÄÅËÉåÊôØÂèäËØªËÄÖÂª∫ËÆÆ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20NLP%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md&quot;&gt;Á¨¨‰∏ÄÁ´† NLP Âü∫Á°ÄÊ¶ÇÂøµ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªÄ‰πàÊòØ NLP„ÄÅÂèëÂ±ïÂéÜÁ®ã„ÄÅ‰ªªÂä°ÂàÜÁ±ª„ÄÅÊñáÊú¨Ë°®Á§∫ÊºîËøõ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Transformer%E6%9E%B6%E6%9E%84.md&quot;&gt;Á¨¨‰∫åÁ´† Transformer Êû∂ÊûÑ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÅEncoder-Decoder„ÄÅÊâãÊääÊâãÊê≠Âª∫ Transformer&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&quot;&gt;Á¨¨‰∏âÁ´† È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Encoder-only„ÄÅEncoder-Decoder„ÄÅDecoder-Only Ê®°ÂûãÂØπÊØî&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&quot;&gt;Á¨¨ÂõõÁ´† Â§ßËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;LLM ÂÆö‰πâ„ÄÅËÆ≠ÁªÉÁ≠ñÁï•„ÄÅÊ∂åÁé∞ËÉΩÂäõÂàÜÊûê&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B.md&quot;&gt;Á¨¨‰∫îÁ´† Âä®ÊâãÊê≠Âª∫Â§ßÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ÂÆûÁé∞ LLaMA2„ÄÅËÆ≠ÁªÉ Tokenizer„ÄÅÈ¢ÑËÆ≠ÁªÉÂ∞èÂûã LLM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5.md&quot;&gt;Á¨¨ÂÖ≠Á´† Â§ßÊ®°ÂûãËÆ≠ÁªÉÂÆûË∑µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;È¢ÑËÆ≠ÁªÉ„ÄÅÊúâÁõëÁù£ÂæÆË∞É„ÄÅLoRA/QLoRA È´òÊïàÂæÆË∞É&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8.md&quot;&gt;Á¨¨‰∏ÉÁ´† Â§ßÊ®°ÂûãÂ∫îÁî®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê®°ÂûãËØÑÊµã„ÄÅRAG Ê£ÄÁ¥¢Â¢ûÂº∫„ÄÅAgent Êô∫ËÉΩ‰Ωì&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&quot;&gt;Extra Chapter LLM Blog&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ºòÁßÄÁöÑÂ§ßÊ®°Âûã Â≠¶‰π†Á¨îËÆ∞/Blog ÔºåÊ¨¢ËøéÂ§ßÂÆ∂Êù• PR ÔºÅ&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Extra Chapter LLM Blog&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/why-fine-tune-small-large-language-models/readme.md&quot;&gt;Â§ßÊ®°ÂûãÈÉΩËøô‰πàÂéâÂÆ≥‰∫ÜÔºåÂæÆË∞É0.6BÁöÑÂ∞èÊ®°ÂûãÊúâ‰ªÄ‰πàÊÑè‰πâÔºü&lt;/a&gt; @&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;‰∏çË¶ÅËë±ÂßúËíú&lt;/a&gt; 2025-7-11&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/transformer-architecture/&quot;&gt;Transformer Êï¥‰ΩìÊ®°ÂùóËÆæËÆ°Ëß£ËØª&lt;/a&gt; @&lt;a href=&quot;https://github.com/ditingdapeng&quot;&gt;ditingdapeng&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/text-data-processing/readme.md&quot;&gt;ÊñáÊú¨Êï∞ÊçÆÂ§ÑÁêÜËØ¶Ëß£&lt;/a&gt; @&lt;a href=&quot;https://github.com/xinala-781&quot;&gt;Ëî°ÈãÜÊç∑&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/vlm-concatenation-finetune/README.md&quot;&gt;Qwen3-&quot;VL&quot;‚Äî‚ÄîË∂ÖÂ∞è‰∏≠ÊñáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑ‚ÄúÊãºÊé•ÂæÆË∞É‚Äù‰πãË∑Ø&lt;/a&gt; @&lt;a href=&quot;https://github.com/ShaohonChen&quot;&gt;ShaohonChen&lt;/a&gt; 2025-7-30&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/s1-vllm-thinking-budget/readme.md&quot;&gt;S1: Thinking Budget with vLLM&lt;/a&gt; @&lt;a href=&quot;https://github.com/kmno4-zx&quot;&gt;kmno4-zx&lt;/a&gt; 2025-8-03&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/CDDRS/readme.md&quot;&gt;CDDRS: ‰ΩøÁî®ÁªÜÁ≤íÂ∫¶ËØ≠‰πâ‰ø°ÊÅØÊåáÂØºÂ¢ûÂº∫ÁöÑRAGÊ£ÄÁ¥¢ÊñπÊ≥ï&lt;/a&gt; @&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;Hongru0306&lt;/a&gt; 2025-8-21&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;Â¶ÇÊûúÂ§ßÂÆ∂Âú®Â≠¶‰π† Happy-LLM È°πÁõÆÊàñ LLM Áõ∏ÂÖ≥Áü•ËØÜ‰∏≠ÊúâËá™Â∑±Áã¨Âà∞ÁöÑËßÅËß£„ÄÅËÆ§Áü•„ÄÅÂÆûË∑µÔºåÊ¨¢ËøéÂ§ßÂÆ∂ PR Âú® &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&quot;&gt;Extra Chapter LLM Blog&lt;/a&gt; ‰∏≠„ÄÇËØ∑ÈÅµÂÆà Extra Chapter LLM Blog ÁöÑ &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/Readme.md&quot;&gt;PR ËßÑËåÉ&lt;/a&gt;ÔºåÊàë‰ª¨‰ºöËßÜ PR ÂÜÖÂÆπÁöÑË¥®ÈáèÂíå‰ª∑ÂÄºÊù•ÂÜ≥ÂÆöÊòØÂê¶ÂêàÂπ∂ÊàñË°•ÂÖÖÂà∞ Happy-LLM Ê≠£Êñá‰∏≠Êù•„ÄÇ&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
   &lt;th&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-Base-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-base&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-SFT-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-sft&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;ModelScope ÂàõÁ©∫Èó¥‰ΩìÈ™åÂú∞ÂùÄÔºö&lt;a href=&quot;https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft&quot;&gt;ü§ñ ÂàõÁ©∫Èó¥&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;PDF ÁâàÊú¨‰∏ãËΩΩ&lt;/h3&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;&lt;strong&gt;Êú¨ Happy-LLM PDF ÊïôÁ®ãÂÆåÂÖ®ÂºÄÊ∫êÂÖçË¥π„ÄÇ‰∏∫Èò≤Ê≠¢ÂêÑÁ±ªËê•ÈîÄÂè∑Âä†Ê∞¥Âç∞ÂêéË¥©ÂçñÁªôÂ§ßÊ®°ÂûãÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨ÁâπÂú∞Âú® PDF Êñá‰ª∂‰∏≠È¢ÑÂÖàÊ∑ªÂä†‰∫Ü‰∏çÂΩ±ÂìçÈòÖËØªÁöÑ Datawhale ÂºÄÊ∫êÊ†áÂøóÊ∞¥Âç∞ÔºåÊï¨ËØ∑Ë∞ÖËß£ÔΩû&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Happy-LLM PDF : &lt;a href=&quot;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&quot;&gt;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üí° Â¶Ç‰ΩïÂ≠¶‰π†&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÈÄÇÂêàÂ§ßÂ≠¶Áîü„ÄÅÁ†îÁ©∂‰∫∫Âëò„ÄÅLLM Áà±Â•ΩËÄÖ„ÄÇÂú®Â≠¶‰π†Êú¨È°πÁõÆ‰πãÂâçÔºåÂª∫ËÆÆÂÖ∑Â§á‰∏ÄÂÆöÁöÑÁºñÁ®ãÁªèÈ™åÔºåÂ∞§ÂÖ∂ÊòØË¶ÅÂØπ Python ÁºñÁ®ãËØ≠Ë®ÄÊúâ‰∏ÄÂÆöÁöÑ‰∫ÜËß£„ÄÇÊúÄÂ•ΩÂÖ∑Â§áÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÔºåÂπ∂‰∫ÜËß£ NLP È¢ÜÂüüÁöÑÁõ∏ÂÖ≥Ê¶ÇÂøµÂíåÊúØËØ≠Ôºå‰ª•‰æøÊõ¥ËΩªÊùæÂú∞Â≠¶‰π†Êú¨È°πÁõÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜ‚Äî‚ÄîÂü∫Á°ÄÁü•ËØÜ‰∏éÂÆûÊàòÂ∫îÁî®„ÄÇÁ¨¨1Á´†ÔΩûÁ¨¨4Á´†ÊòØÂü∫Á°ÄÁü•ËØÜÈÉ®ÂàÜÔºå‰ªéÊµÖÂÖ•Ê∑±‰ªãÁªç LLM ÁöÑÂü∫Êú¨ÂéüÁêÜ„ÄÇÂÖ∂‰∏≠ÔºåÁ¨¨1Á´†ÁÆÄÂçï‰ªãÁªç NLP ÁöÑÂü∫Êú¨‰ªªÂä°ÂíåÂèëÂ±ïÔºå‰∏∫Èùû NLP È¢ÜÂüüÁ†îÁ©∂ËÄÖÊèê‰æõÂèÇËÄÉÔºõÁ¨¨2Á´†‰ªãÁªç LLM ÁöÑÂü∫Êú¨Êû∂ÊûÑ‚Äî‚ÄîTransformerÔºåÂåÖÊã¨ÂéüÁêÜ‰ªãÁªçÂèä‰ª£Á†ÅÂÆûÁé∞Ôºå‰Ωú‰∏∫ LLM ÊúÄÈáçË¶ÅÁöÑÁêÜËÆ∫Âü∫Á°ÄÔºõÁ¨¨3Á´†Êï¥‰Ωì‰ªãÁªçÁªèÂÖ∏ÁöÑ PLMÔºåÂåÖÊã¨ Encoder-Only„ÄÅEncoder-Decoder Âíå Decoder-Only ‰∏âÁßçÊû∂ÊûÑÔºå‰πüÂêåÊó∂‰ªãÁªç‰∫ÜÂΩìÂâç‰∏Ä‰∫õ‰∏ªÊµÅ LLM ÁöÑÊû∂ÊûÑÂíåÊÄùÊÉ≥ÔºõÁ¨¨4Á´†ÂàôÊ≠£ÂºèËøõÂÖ• LLM ÈÉ®ÂàÜÔºåËØ¶ÁªÜ‰ªãÁªç LLM ÁöÑÁâπÁÇπ„ÄÅËÉΩÂäõÂíåÊï¥‰ΩìËÆ≠ÁªÉËøáÁ®ã„ÄÇÁ¨¨5Á´†ÔΩûÁ¨¨7Á´†ÊòØÂÆûÊàòÂ∫îÁî®ÈÉ®ÂàÜÔºåÂ∞ÜÈÄêÊ≠•Â∏¶È¢ÜÂ§ßÂÆ∂Ê∑±ÂÖ• LLM ÁöÑÂ∫ïÂ±ÇÁªÜËäÇ„ÄÇÂÖ∂‰∏≠ÔºåÁ¨¨5Á´†Â∞ÜÂ∏¶È¢ÜÂ§ßÂÆ∂ËÄÖÂü∫‰∫é PyTorch Â±Ç‰∫≤ÊâãÊê≠Âª∫‰∏Ä‰∏™ LLMÔºåÂπ∂ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊúâÁõëÁù£ÂæÆË∞ÉÁöÑÂÖ®ÊµÅÁ®ãÔºõÁ¨¨6Á´†Â∞ÜÂºïÂÖ•ÁõÆÂâç‰∏öÁïå‰∏ªÊµÅÁöÑ LLM ËÆ≠ÁªÉÊ°ÜÊû∂ TransformersÔºåÂ∏¶È¢ÜÂ≠¶‰π†ËÄÖÂü∫‰∫éËØ•Ê°ÜÊû∂Âø´ÈÄü„ÄÅÈ´òÊïàÂú∞ÂÆûÁé∞ LLM ËÆ≠ÁªÉËøáÁ®ãÔºõÁ¨¨7Á´†ÂàôÂ∞Ü‰ªãÁªç Âü∫‰∫é LLM ÁöÑÂêÑÁßçÂ∫îÁî®ÔºåË°•ÂÖ®Â≠¶‰π†ËÄÖÂØπ LLM ‰ΩìÁ≥ªÁöÑËÆ§Áü•ÔºåÂåÖÊã¨ LLM ÁöÑËØÑÊµã„ÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRetrieval-Augmented GenerationÔºåRAGÔºâ„ÄÅÊô∫ËÉΩ‰ΩìÔºàAgentÔºâÁöÑÊÄùÊÉ≥ÂíåÁÆÄÂçïÂÆûÁé∞„ÄÇ‰Ω†ÂèØ‰ª•Ê†πÊçÆ‰∏™‰∫∫ÂÖ¥Ë∂£ÂíåÈúÄÊ±ÇÔºåÈÄâÊã©ÊÄßÂú∞ÈòÖËØªÁõ∏ÂÖ≥Á´†ËäÇ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÂú®ÈòÖËØªÊú¨‰π¶ÁöÑËøáÁ®ã‰∏≠ÔºåÂª∫ËÆÆ‰Ω†Â∞ÜÁêÜËÆ∫ÂíåÂÆûÈôÖÁõ∏ÁªìÂêà„ÄÇLLM ÊòØ‰∏Ä‰∏™Âø´ÈÄüÂèëÂ±ï„ÄÅÊ≥®ÈáçÂÆûË∑µÁöÑÈ¢ÜÂüüÔºåÊàë‰ª¨Âª∫ËÆÆ‰Ω†Â§öÊäïÂÖ•ÂÆûÊàòÔºåÂ§çÁé∞Êú¨‰π¶Êèê‰æõÁöÑÂêÑÁßç‰ª£Á†ÅÔºåÂêåÊó∂ÁßØÊûÅÂèÇÂä† LLM Áõ∏ÂÖ≥ÁöÑÈ°πÁõÆ‰∏éÊØîËµõÔºåÁúüÊ≠£ÊäïÂÖ•Âà∞ LLM ÂºÄÂèëÁöÑÊµ™ÊΩÆ‰∏≠„ÄÇÊàë‰ª¨ÈºìÂä±‰Ω†ÂÖ≥Ê≥® Datawhale ÂèäÂÖ∂‰ªñ LLM Áõ∏ÂÖ≥ÂºÄÊ∫êÁ§æÂå∫ÔºåÂΩìÈÅáÂà∞ÈóÆÈ¢òÊó∂Ôºå‰Ω†ÂèØ‰ª•ÈöèÊó∂Âú®Êú¨È°πÁõÆÁöÑ issue Âå∫ÊèêÈóÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊúÄÂêéÔºåÊ¨¢ËøéÊØè‰∏Ä‰ΩçËØªËÄÖÂú®Â≠¶‰π†ÂÆåÊú¨È°πÁõÆÂêéÂä†ÂÖ•Âà∞ LLM ÂºÄÂèëËÄÖÁöÑË°åÂàó„ÄÇ‰Ωú‰∏∫ÂõΩÂÜÖ AI ÂºÄÊ∫êÁ§æÂå∫ÔºåÊàë‰ª¨Â∏åÊúõÂÖÖÂàÜËÅöÈõÜÂÖ±ÂàõËÄÖÔºå‰∏ÄËµ∑‰∏∞ÂØåËøô‰∏™ÂºÄÊ∫ê LLM ÁöÑ‰∏ñÁïåÔºåÊâìÈÄ†Êõ¥Â§ö„ÄÅÊõ¥ÂÖ®Èù¢ÁâπËâ≤ LLM ÁöÑÊïôÁ®ã„ÄÇÊòüÁÅ´ÁÇπÁÇπÔºåÊ±áËÅöÊàêÊµ∑„ÄÇÊàë‰ª¨Â∏åÊúõÊàê‰∏∫ LLM ‰∏éÊôÆÁΩóÂ§ß‰ºóÁöÑÈò∂Ê¢ØÔºå‰ª•Ëá™Áî±„ÄÅÂπ≥Á≠âÁöÑÂºÄÊ∫êÁ≤æÁ•ûÔºåÊã•Êä±Êõ¥ÊÅ¢ÂºòËÄåËæΩÈòîÁöÑ LLM ‰∏ñÁïå„ÄÇ&lt;/p&gt; 
&lt;h2&gt;ü§ù Â¶Ç‰ΩïË¥°ÁåÆ&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨Ê¨¢Ëøé‰ªª‰ΩïÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Êä•Âëä Bug&lt;/strong&gt; - ÂèëÁé∞ÈóÆÈ¢òËØ∑Êèê‰∫§ Issue&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;ÂäüËÉΩÂª∫ËÆÆ&lt;/strong&gt; - ÊúâÂ•ΩÊÉ≥Ê≥ïÂ∞±ÂëäËØâÊàë‰ª¨&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;ÂÜÖÂÆπÂÆåÂñÑ&lt;/strong&gt; - Â∏ÆÂä©ÊîπËøõÊïôÁ®ãÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;‰ª£Á†Å‰ºòÂåñ&lt;/strong&gt; - Êèê‰∫§ Pull Request&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;h3&gt;Ê†∏ÂøÉË¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;ÂÆãÂøóÂ≠¶-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (DatawhaleÊàêÂëò-‰∏≠ÂõΩÁüø‰∏öÂ§ßÂ≠¶(Âåó‰∫¨))&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;ÈÇπÈõ®Ë°°-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (DatawhaleÊàêÂëò-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://xinzhongzhu.github.io/&quot;&gt;Êú±‰ø°Âø†-ÊåáÂØº‰∏ìÂÆ∂&lt;/a&gt;ÔºàDatawhaleÈ¶ñÂ∏≠ÁßëÂ≠¶ÂÆ∂-ÊµôÊ±üÂ∏àËåÉÂ§ßÂ≠¶Êù≠Â∑û‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Èô¢ÊïôÊéàÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter Ë¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ditingdapeng&quot;&gt;ditingdapeng&lt;/a&gt;ÔºàÂÜÖÂÆπË¥°ÁåÆËÄÖ-‰∫ëÂéüÁîüÂü∫Á°ÄÊû∂ÊûÑÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinala-781&quot;&gt;Ëî°ÈãÜÊç∑&lt;/a&gt;ÔºàÂÜÖÂÆπË¥°ÁåÆËÄÖ-Á¶èÂ∑ûÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ShaohonChen&quot;&gt;ShaohonChen&lt;/a&gt; ÔºàÊÉÖÊÑüÊú∫Âô®ÂÆûÈ™åÂÆ§Á†îÁ©∂Âëò-Ë•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶Âú®ËØªÁ°ïÂ£´Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;ËÇñÈ∏øÂÑí, Â∫ÑÂÅ•Áê®&lt;/a&gt; (ÂÜÖÂÆπË¥°ÁåÆËÄÖ-ÂêåÊµéÂ§ßÂ≠¶)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÁâπÂà´ÊÑüË∞¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊÑüË∞¢ &lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt; ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖ‰ª¨ ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/happy-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/happy-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/star-history-2025710.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot; /&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;‚≠ê Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ÂÖ≥‰∫é Datawhale&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot; /&gt; 
 &lt;p&gt;Êâ´Êèè‰∫åÁª¥Á†ÅÂÖ≥Ê≥® Datawhale ÂÖ¨‰ºóÂè∑ÔºåËé∑ÂèñÊõ¥Â§ö‰ºòË¥®ÂºÄÊ∫êÂÜÖÂÆπ&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìú ÂºÄÊ∫êÂçèËÆÆ&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ΩúÂìÅÈááÁî®&lt;a href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ&lt;/a&gt;ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jakevdp/PythonDataScienceHandbook</title>
      <link>https://github.com/jakevdp/PythonDataScienceHandbook</link>
      <description>&lt;p&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Data Science Handbook&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge.svg?sanitize=true&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository contains the entire &lt;a href=&quot;http://shop.oreilly.com/product/0636920034919.do&quot;&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/PDSH-cover.png&quot; alt=&quot;cover image&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;How to Use this Book&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Read the book in its entirety online at &lt;a href=&quot;https://jakevdp.github.io/PythonDataScienceHandbook/&quot;&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the code using the Jupyter notebooks available in this repository&#39;s &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks&quot;&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch executable versions of these notebooks using &lt;a href=&quot;http://colab.research.google.com&quot;&gt;Google Colab&lt;/a&gt;: &lt;a href=&quot;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href=&quot;https://beta.mybinder.org/&quot;&gt;binder&lt;/a&gt;: &lt;a href=&quot;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge.svg?sanitize=true&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Buy the printed book through &lt;a href=&quot;http://shop.oreilly.com/product/0636920034919.do&quot;&gt;O&#39;Reilly Media&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt; 
&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href=&quot;http://ipython.org&quot;&gt;IPython&lt;/a&gt;, &lt;a href=&quot;http://numpy.org&quot;&gt;NumPy&lt;/a&gt;, &lt;a href=&quot;http://pandas.pydata.org&quot;&gt;Pandas&lt;/a&gt;, &lt;a href=&quot;http://matplotlib.org&quot;&gt;Matplotlib&lt;/a&gt;, &lt;a href=&quot;http://scikit-learn.org&quot;&gt;Scikit-Learn&lt;/a&gt;, and related packages. Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project, &lt;a href=&quot;https://github.com/jakevdp/WhirlwindTourOfPython&quot;&gt;A Whirlwind Tour of Python&lt;/a&gt;: it&#39;s a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&quot;&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt; 
&lt;h2&gt;Software&lt;/h2&gt; 
&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt; 
&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/requirements.txt&quot;&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use). To install the requirements using &lt;a href=&quot;http://conda.pydata.org&quot;&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can read more about using conda environments in the &lt;a href=&quot;http://conda.pydata.org/docs/using/envs.html&quot;&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;h3&gt;Code&lt;/h3&gt; 
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-CODE&quot;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Text&lt;/h3&gt; 
&lt;p&gt;The text content of the book is released under the &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-TEXT&quot;&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode&quot;&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>onnx/models</title>
      <link>https://github.com/onnx/models</link>
      <description>&lt;p&gt;A collection of pre-trained, state-of-the-art models in the ONNX format&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Deprecation Notice&lt;/strong&gt;: We sincerely thank the community for participating in the ONNX Model Zoo effort. As the machine learning ecosystem has evolved, much of the novel model sharing has successfully transitioned to Hugging Face, which maintains a vibrant and healthy state. We are preserving the ONNX Model Zoo repository for historical purposes only. Please note that models will no longer be available for LFS download starting July 1st, 2025. You can still get access to the models that were originally available on this repository by going to &lt;a href=&quot;https://huggingface.co/onnxmodelzoo&quot;&gt;https://huggingface.co/onnxmodelzoo&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;ONNX Model Zoo&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to the ONNX Model Zoo! The Open Neural Network Exchange (ONNX) is an open standard format created to represent machine learning models. Supported by a robust community of partners, ONNX defines a common set of operators and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.&lt;/p&gt; 
&lt;p&gt;This repository is a curated collection of pre-trained, state-of-the-art models in the ONNX format. These models are sourced from prominent open-source repositories and have been contributed by a diverse group of community members. Our aim is to facilitate the spread and usage of machine learning models among a wider audience of developers, researchers, and enthusiasts.&lt;/p&gt; 
&lt;p&gt;To handle ONNX model files, which can be large, we use Git LFS (Large File Storage).&lt;/p&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;Currently, we are expanding the ONNX Model Zoo by incorporating additional models from the following categories. As we are rigorously validating the new models for accuracy, refer to the &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#validated-models&quot;&gt;validated models&lt;/a&gt; below that have been successfully validated for accuracy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Computer Vision&lt;/li&gt; 
 &lt;li&gt;Natural Language Processing (NLP)&lt;/li&gt; 
 &lt;li&gt;Generative AI&lt;/li&gt; 
 &lt;li&gt;Graph Machine Learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These models are sourced from prominent open-source repositories such as &lt;a href=&quot;https://github.com/huggingface/pytorch-image-models&quot;&gt;timm&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision&quot;&gt;torchvision&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/hub/&quot;&gt;torch_hub&lt;/a&gt;, and &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformers&lt;/a&gt;, and exported into the ONNX format using the open-source &lt;a href=&quot;https://github.com/onnx/turnkeyml&quot;&gt;TurnkeyML toolchain&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Validated Models&lt;/h2&gt; 
&lt;h4&gt;Vision&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#image_classification&quot;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#object_detection&quot;&gt;Object Detection &amp;amp; Image Segmentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#body_analysis&quot;&gt;Body, Face &amp;amp; Gesture Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#image_manipulation&quot;&gt;Image Manipulation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Language&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#machine_comprehension&quot;&gt;Machine Comprehension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#machine_translation&quot;&gt;Machine Translation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#language_modelling&quot;&gt;Language Modelling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Other&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#visual_qna&quot;&gt;Visual Question Answering &amp;amp; Dialog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#speech&quot;&gt;Speech &amp;amp; Audio Processing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#others&quot;&gt;Other interesting models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read the &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#usage-&quot;&gt;Usage&lt;/a&gt; section below for more details on the file formats in the ONNX Model Zoo (.onnx, .pb, .npz), downloading multiple ONNX models through &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#gitlfs-&quot;&gt;Git LFS command line&lt;/a&gt;, and starter Python code for validating your ONNX model using test data.&lt;/p&gt; 
&lt;p&gt;INT8 models are generated by &lt;a href=&quot;https://github.com/intel/neural-compressor&quot;&gt;Intel¬Æ Neural Compressor&lt;/a&gt;. &lt;a href=&quot;https://github.com/intel/neural-compressor&quot;&gt;Intel¬Æ Neural Compressor&lt;/a&gt; is an open-source Python library which supports automatic accuracy-driven tuning strategies to help user quickly find out the best quantized model. It implements dynamic and static quantization for ONNX models and can represent quantized ONNX models with operator oriented as well as tensor oriented (QDQ) ways. Users can use web-based UI service or python code to do quantization. Read the &lt;a href=&quot;https://github.com/intel/neural-compressor/raw/master/README.md&quot;&gt;Introduction&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Image Classification &lt;a name=&quot;image_classification&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This collection of models take images as input, then classifies the major objects in the images into 1000 object categories such as keyboard, mouse, pencil, and many animals.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Huggingface Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/mobilenet&quot;&gt;MobileNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;Sandler et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light-weight deep neural network best suited for mobile and embedded vision applications. &lt;br /&gt;Top-5 error from paper - ~10%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/resnet&quot;&gt;ResNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;He et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN model (up to 152 layers). Uses shortcut connections to achieve higher accuracy when classifying images. &lt;br /&gt; Top-5 error from paper - ~3.6%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ResNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/squeezenet&quot;&gt;SqueezeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;Iandola et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A light-weight CNN model providing AlexNet level accuracy with 50x fewer parameters. &lt;br /&gt;Top-5 error from paper - ~20%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/SqueezeNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/vgg&quot;&gt;VGG&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;Simonyan et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model(up to 19 layers). Similar to AlexNet but uses multiple smaller kernel-sized filters that provides more accuracy when classifying images. &lt;br /&gt;Top-5 error from paper - ~8%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/VGG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/alexnet&quot;&gt;AlexNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Deep CNN model (up to 8 layers) where the input is an image and the output is a vector of 1000 numbers. &lt;br /&gt; Top-5 error from paper - ~15%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/AlexNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/inception_and_googlenet/googlenet&quot;&gt;GoogleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.4842.pdf&quot;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model(up to 22 layers). Comparatively smaller and faster than VGG and more accurate in detailing than AlexNet. &lt;br /&gt; Top-5 error from paper - ~6.7%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/GoogleNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/caffenet&quot;&gt;CaffeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf&quot;&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN variation of AlexNet for Image Classification in Caffe where the max pooling precedes the local response normalization (LRN) so that the LRN takes less compute and memory.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/CaffeNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/rcnn_ilsvrc13&quot;&gt;RCNN_ILSVRC13&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot;&gt;Girshick et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Pure Caffe implementation of R-CNN for image classification. This model uses localization of regions to classify and extract features from images.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/densenet-121&quot;&gt;DenseNet-121&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;Huang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model that has every layer connected to every other layer and passes on its own feature providing strong gradient flow and more diversified features.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/DenseNet-121&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/inception_and_googlenet/inception_v1&quot;&gt;Inception_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.4842&quot;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model is same as GoogLeNet, implemented through Caffe2 that has improved utilization of the computing resources inside the network and helps with the vanishing gradient problem. &lt;br /&gt; Top-5 error from paper - ~6.7%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/Inception_v1&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/inception_and_googlenet/inception_v2&quot;&gt;Inception_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model for Image Classification as an adaptation to Inception v1 with batch normalization. This model has reduced computational cost and improved image resolution compared to Inception v1. &lt;br /&gt; Top-5 error from paper ~4.82%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/shufflenet&quot;&gt;ShuffleNet_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.01083&quot;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This model greatly reduces the computational cost and provides a ~13x speedup over AlexNet on ARM-based mobile devices. Compared to MobileNet, ShuffleNet achieves superior performance by a significant margin due to it&#39;s efficient structure. &lt;br /&gt; Top-1 error from paper - ~32.6%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/shufflenet&quot;&gt;ShuffleNet_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This network architecture design considers direct metric such as speed, instead of indirect metric like FLOP. &lt;br /&gt; Top-1 error from paper - ~30.6%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/zfnet-512&quot;&gt;ZFNet-512&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Zeiler et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model (up to 8 layers) that increased the number of features that the network is capable of detecting that helps to pick image features at a finer level of resolution. &lt;br /&gt; Top-5 error from paper - ~14.3%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ZFNet-512&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/efficientnet-lite4&quot;&gt;EfficientNet-Lite4&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;Tan et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CNN model with an order of magnitude of few computations and parameters, while still acheiving state-of-the-art accuracy and better efficiency than previous ConvNets. &lt;br /&gt; Top-5 error from paper - ~2.9%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/EfficientNet-Lite4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Domain-based Image Classification &lt;a name=&quot;domain_based_image&quot;&gt;&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;This subset of models classify images for specific domains and datasets.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/mnist&quot;&gt;MNIST-Handwritten Digit Recognition&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Microsoft/CNTK/raw/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb&quot;&gt;Convolutional Neural Network with MNIST&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model for handwritten digit identification&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Object Detection &amp;amp; Image Segmentation &lt;a name=&quot;object_detection&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Object detection models detect the presence of multiple objects in an image and segment out areas of the image where the objects are detected. Semantic segmentation models partition an input image by labeling each pixel into a set of pre-defined categories.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/tiny-yolov2&quot;&gt;Tiny YOLOv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.08242.pdf&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time CNN for object detection that detects 20 different classes. A smaller version of the more complex full YOLOv2 network.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/ssd&quot;&gt;SSD&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;Liu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Single Stage Detector: real-time CNN for object detection that detects 80 different classes.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1&quot;&gt;SSD-MobileNetV1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;&gt;Howard et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A variant of MobileNet that uses the Single Shot Detector (SSD) model framework. The model detects 80 different object classes and locates up to 10 objects in an image.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/faster-rcnn&quot;&gt;Faster-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot;&gt;Ren et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Increases efficiency from R-CNN by connecting a RPN with a CNN to create a single, unified network for object detection that detects 80 different classes.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/faster-rcnn&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/mask-rcnn&quot;&gt;Mask-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;&gt;He et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time neural network for object instance segmentation that detects 80 different classes. Extends Faster R-CNN as each of the 300 elected ROIs go through 3 parallel branches of the network: label prediction, bounding box prediction and mask prediction.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/mask-rcnn&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/retinanet&quot;&gt;RetinaNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;Lin et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time dense detector network for object detection that addresses class imbalance through Focal Loss. RetinaNet is able to match the speed of previous one-stage detectors and defines the state-of-the-art in two-stage detectors (surpassing R-CNN).&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/yolov2-coco&quot;&gt;YOLO v2-coco&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.08242&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN model for real-time object detection system that can detect over 9000 object categories. It uses a single network evaluation, enabling it to be more than 1000x faster than R-CNN and 100x faster than Faster R-CNN. This model is trained with COCO dataset and contains 80 classes.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/yolov3&quot;&gt;YOLO v3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.02767.pdf&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A deep CNN model for real-time object detection that detects 80 different classes. A little bigger than YOLOv2 but still very fast. As accurate as SSD but 3 times faster.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/tiny-yolov3&quot;&gt;Tiny YOLOv3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.02767.pdf&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A smaller version of YOLOv3 model.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/yolov4&quot;&gt;YOLOv4&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.10934&quot;&gt;Bochkovskiy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Optimizes the speed and accuracy of object detection. Two times faster than EfficientDet. It improves YOLOv3&#39;s AP and FPS by 10% and 12%, respectively, with mAP50 of 52.32 on the COCO 2017 dataset and FPS of 41.7 on a Tesla V100.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/yolov4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/duc&quot;&gt;DUC&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.08502&quot;&gt;Wang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN based pixel-wise semantic segmentation model with &amp;gt;80% &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/models/semantic_segmentation/DUC/README.md/#metric&quot;&gt;mIOU&lt;/a&gt; (mean Intersection Over Union). Trained on cityscapes dataset, which can be effectively implemented in self driving vehicle systems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/DUC&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/fcn&quot;&gt;FCN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&quot;&gt;Long et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN based segmentation model trained end-to-end, pixel-to-pixel that produces efficient inference and learning. Built off of AlexNet, VGG net, GoogLeNet classification methods. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/FCN&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Body, Face &amp;amp; Gesture Analysis &lt;a name=&quot;body_analysis&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Face detection models identify and/or recognize human faces and emotions in given images. Body and Gesture Analysis models identify gender and age in given image.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/arcface&quot;&gt;ArcFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.07698&quot;&gt;Deng et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ArcFace&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/ultraface&quot;&gt;UltraFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&quot;&gt;Ultra-lightweight face detection model&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model is a lightweight facedetection model designed for edge computing devices.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ultraface&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/emotion_ferplus&quot;&gt;Emotion FerPlus&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.01041&quot;&gt;Barsoum et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN for emotion recognition trained on images of faces.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/age_gender&quot;&gt;Age and Gender Classification using Convolutional Neural Networks&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://data.vision.ee.ethz.ch/cvl/publications/papers/proceedings/eth_biwi_01229.pdf&quot;&gt;Rothe et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model accurately classifies gender and age even the amount of learning data is limited.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Image Manipulation &lt;a name=&quot;image_manipulation&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Image manipulation models use neural networks to transform input images to modified output images. Some popular models in this category involve style transfer or enhancing images by increasing resolution.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Unpaired Image to Image Translation using Cycle consistent Adversarial Network&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot;&gt;Zhu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The model uses learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/super_resolution/sub_pixel_cnn_2016&quot;&gt;Super Resolution with sub-pixel CNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Shi et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A deep CNN that uses sub-pixel convolution layers to upscale the input image.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/sub_pixel_cnn_2016&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/style_transfer/fast_neural_style&quot;&gt;Fast Neural Style Transfer&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.08155&quot;&gt;Johnson et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This method uses a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Speech &amp;amp; Audio Processing &lt;a name=&quot;speech&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This class of models uses audio data to train models that can identify voice, generate music, or even read text out loud.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech recognition with deep recurrent neural networks&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf&quot;&gt;Graves et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A RNN model for sequential data for speech recognition. Labels problems where the input-output alignment is unknown&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deep voice: Real time neural text to speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.07825&quot;&gt;Arik et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A DNN model that performs end-to-end neural speech synthesis. Requires fewer parameters and it is faster than other systems. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sound Generative models&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.03499&quot;&gt;WaveNet: A Generative Model for Raw Audio &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN model that generates raw audio waveforms. Has predictive distribution for each audio sample. Generates realistic music fragments. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Machine Comprehension &lt;a name=&quot;machine_comprehension&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This subset of natural language processing models that answer questions about a given context paragraph.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/bidirectional_attention_flow&quot;&gt;Bidirectional Attention Flow&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.01603&quot;&gt;Seo et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A model that answers a query about a given context paragraph.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/BiDAF&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/bert-squad&quot;&gt;BERT-Squad&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;Devlin et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model answers questions based on the context of the given input paragraph.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/BERT-Squad&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/roberta&quot;&gt;RoBERTa&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1907.11692.pdf&quot;&gt;Liu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A large transformer-based model that predicts sentiment based on given input text.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/RoBERTa&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/gpt-2&quot;&gt;GPT-2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;Radford et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A large transformer-based language model that given a sequence of words within some text, predicts the next word.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/GPT-2&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/t5&quot;&gt;T5&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;Raffel et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A large transformer-based language model trained on multiple tasks at once to achieve better semantic understanding of the prompt, capable of sentiment-analysis, question-answering, similarity-detection, translation, summarization, etc.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/T5&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Machine Translation &lt;a name=&quot;machine_translation&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This class of natural language processing models learns how to translate input text to another language.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Machine Translation by jointly learning to align and translate&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bahdanau et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Aims to build a single neural network that can be jointly tuned to maximize the translation performance. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google&#39;s Neural Machine Translation System&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.08144&quot;&gt;Wu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model helps to improve issues faced by the Neural Machine Translation (NMT) systems like parallelism that helps accelerate the final translation speed.&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Language Modelling &lt;a name=&quot;language_modelling&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This subset of natural language processing models learns representations of language from large corpuses of text.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deep Neural Network Language Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/a177/45f1d7045636577bcd5d513620df5860e9e5.pdf&quot;&gt;Arisoy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A DNN acoustic model. Used in many natural language technologies. Represents a probability distribution over all possible word strings in a language. &lt;br /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Visual Question Answering &amp;amp; Dialog &lt;a name=&quot;visual_qna&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This subset of natural language processing models uses input images to answer questions about those images.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VQA: Visual Question Answering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.00468v6.pdf&quot;&gt;Agrawal et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A model that takes an image and a free-form, open-ended natural language question about the image and outputs a natural-language answer. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Yin and Yang: Balancing and Answering Binary Visual Questions&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.05099.pdf&quot;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Addresses VQA by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image. Next, if the concept can be found in the image, it provides a ‚Äúyes‚Äù or ‚Äúno‚Äù answer. Its performance matches the traditional VQA approach on unbalanced dataset, and outperforms it on the balanced dataset. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Making the V in VQA Matter&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.00837.pdf&quot;&gt;Goyal et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Balances the VQA dataset by collecting complementary images such that every question is associated with a pair of similar images that result in two different answers to the question, providing a unique interpretable model that provides a counter-example based explanation. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Visual Dialog&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.08669&quot;&gt;Das et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An AI agent that holds a meaningful dialog with humans in natural, conversational language about visual content. Curates a large-scale Visual Dialog dataset (VisDial). &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Other interesting models &lt;a name=&quot;others&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;There are many interesting deep learning models that do not fit into the categories described above. The ONNX team would like to highly encourage users and researchers to &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt; their models to the growing model zoo.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text to Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.05396&quot;&gt;Generative Adversarial Text to image Synthesis &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Effectively bridges the advances in text and image modeling, translating visual concepts from characters to pixels. Generates plausible images of birds and flowers from detailed text descriptions. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Time Series Forecasting&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.07015.pdf&quot;&gt;Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The model extracts short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It helps to predict solar plant energy output, electricity consumption, and traffic jam situations. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Recommender systems&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf&quot;&gt;DropoutNet: Addressing Cold Start in Recommender Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A collaborative filtering method that makes predictions about an individual‚Äôs preference based on preference information from other users.&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Collaborative filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05031.pdf&quot;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A DNN model based on the interaction between user and item features using matrix factorization. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Autoencoders&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01057&quot;&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An LSTM (long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs.&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage &lt;a name=&quot;usage-&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Every ONNX backend should support running the models out of the box. After downloading and extracting the tarball of each model, you will find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A protobuf file &lt;code&gt;model.onnx&lt;/code&gt; that represents the serialized ONNX model.&lt;/li&gt; 
 &lt;li&gt;Test data (in the form of serialized protobuf TensorProto files or serialized NumPy archives).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage - Test data starter code&lt;/h3&gt; 
&lt;p&gt;The test data files can be used to validate ONNX models from the Model Zoo. We have provided the following interface examples for you to get started. Please replace &lt;code&gt;onnx_backend&lt;/code&gt; in your code with the appropriate framework of your choice that provides ONNX inferencing support, and likewise replace &lt;code&gt;backend.run_model&lt;/code&gt; with the framework&#39;s model evaluation logic.&lt;/p&gt; 
&lt;p&gt;There are two different formats for the test data files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serialized protobuf TensorProtos (.pb), stored in folders with the naming convention &lt;code&gt;test_data_set_*&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
import onnx
import os
import glob
import onnx_backend as backend

from onnx import numpy_helper

model = onnx.load(&#39;model.onnx&#39;)
test_data_dir = &#39;test_data_set_0&#39;

# Load inputs
inputs = []
inputs_num = len(glob.glob(os.path.join(test_data_dir, &#39;input_*.pb&#39;)))
for i in range(inputs_num):
    input_file = os.path.join(test_data_dir, &#39;input_{}.pb&#39;.format(i))
    tensor = onnx.TensorProto()
    with open(input_file, &#39;rb&#39;) as f:
        tensor.ParseFromString(f.read())
    inputs.append(numpy_helper.to_array(tensor))

# Load reference outputs
ref_outputs = []
ref_outputs_num = len(glob.glob(os.path.join(test_data_dir, &#39;output_*.pb&#39;)))
for i in range(ref_outputs_num):
    output_file = os.path.join(test_data_dir, &#39;output_{}.pb&#39;.format(i))
    tensor = onnx.TensorProto()
    with open(output_file, &#39;rb&#39;) as f:
        tensor.ParseFromString(f.read())
    ref_outputs.append(numpy_helper.to_array(tensor))

# Run the model on the backend
outputs = list(backend.run_model(model, inputs))

# Compare the results with reference outputs.
for ref_o, o in zip(ref_outputs, outputs):
    np.testing.assert_almost_equal(ref_o, o)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serialized Numpy archives, stored in files with the naming convention &lt;code&gt;test_data_*.npz&lt;/code&gt;. Each file contains one set of test inputs and outputs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
import onnx
import onnx_backend as backend

# Load the model and sample inputs and outputs
model = onnx.load(model_pb_path)
sample = np.load(npz_path, encoding=&#39;bytes&#39;)
inputs = list(sample[&#39;inputs&#39;])
outputs = list(sample[&#39;outputs&#39;])

# Run the model with an onnx backend and verify the results
np.testing.assert_almost_equal(outputs, backend.run_model(model, inputs))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage - Model quantization&lt;/h3&gt; 
&lt;p&gt;You can get quantized ONNX models by using &lt;a href=&quot;https://github.com/intel/neural-compressor&quot;&gt;Intel¬Æ Neural Compressor&lt;/a&gt;. It provides web-based UI service to make quantization easier and supports code-based usage for more abundant quantization settings. Refer to &lt;a href=&quot;https://github.com/intel/neural-compressor/raw/master/docs/bench.md&quot;&gt;bench document&lt;/a&gt; for how to use web-based UI service and &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/resource/docs/INC_code.md&quot;&gt;example document&lt;/a&gt; for a simple code-based demo. &lt;img src=&quot;https://raw.githubusercontent.com/onnx/models/main/resource/images/INC_GUI.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;There are multiple ways to access the ONNX Model Zoo:&lt;/p&gt; 
&lt;h3&gt;Git Clone (Not Recommended)&lt;/h3&gt; 
&lt;p&gt;Cloning the repository using git won&#39;t automatically download the ONNX models due to their size. To manage these files, first, install Git LFS by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git-lfs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To download a specific model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git lfs pull --include=&quot;[path to model].onnx&quot; --exclude=&quot;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To download all models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git lfs pull --include=&quot;*&quot; --exclude=&quot;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;GitHub UI&lt;/h3&gt; 
&lt;p&gt;Alternatively, you can download models directly from GitHub. Navigate to the model&#39;s page and click the &quot;Download&quot; button on the top right corner.&lt;/p&gt; 
&lt;h2&gt;Model Visualization&lt;/h2&gt; 
&lt;p&gt;For a graphical representation of each model&#39;s architecture, we recommend using &lt;a href=&quot;https://github.com/lutzroeder/netron&quot;&gt;Netron&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Contributions to the ONNX Model Zoo are welcome! Please check our &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribution guidelines&lt;/a&gt; for more information on how you can contribute to the growth and improvement of this resource.&lt;/p&gt; 
&lt;p&gt;Thank you for your interest in the ONNX Model Zoo, and we look forward to your participation in our community!&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/LICENSE&quot;&gt;Apache License v2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/Data-Science-For-Beginners</title>
      <link>https://github.com/microsoft/Data-Science-For-Beginners</link>
      <description>&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=main&amp;amp;repo=344191198&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/foundry/forum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Azure_AI_Foundry_Developer_Forum-blue?style=for-the-badge&amp;amp;logo=github&amp;amp;color=000000&amp;amp;logoColor=fff&quot; alt=&quot;Azure AI Foundry Developer Forum&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;üôè Special thanks üôè to our &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&quot;https://github.com/AdityaGarg00&quot;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/alondra-sanchez-molina/&quot;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/ankitasingh007&quot;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/anupam--mishra/&quot;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/arpitadas01/&quot;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&quot;https://www.linkedin.com/in/dibrinsofor&quot;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/dishita-bhasin-7065281bb&quot;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/majd-s/&quot;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/max-blum-6036a1186/&quot;&gt;Max Blum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/miguelmque/&quot;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/iftu119&quot;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/nawrin-tabassum&quot;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/raymond-wp/&quot;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/rty2423&quot;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&quot;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&quot;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/sheena-narua-n/&quot;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/tauqeerahmad5201/&quot;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&quot;https://www.linkedin.com/in/vidushi-gupta07/&quot;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jasleen-sondhi/&quot;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&quot; alt=&quot;Sketchnote by @sketchthedocs https://sketchthedocs.dev&quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;üåê Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href=&quot;https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Join Our Community&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/ds4beginners/discord&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/kzRShWzttr&quot; alt=&quot;Azure AI Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We have a Discord learn with AI series ongoing, learn more and join us at &lt;a href=&quot;https://aka.ms/learnwithai/discord&quot;&gt;Learn with AI Series&lt;/a&gt; from 18 - 30 September, 2025. You will get tips and tricks of using GitHub Copilot for Data Science.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/images/1.jpg&quot; alt=&quot;Learn with AI series&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Are you a student?&lt;/h1&gt; 
&lt;p&gt;Get started with the following resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-gb/learn/student-hub?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Student Hub page&lt;/a&gt; In this page, you will find beginner resources, Student packs and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://studentambassadors.microsoft.com?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn Student Ambassadors&lt;/a&gt; Join a global community of student ambassadors, this could be your way into Microsoft.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&quot;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/discussions&quot;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://aka.ms/student-page&quot;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&quot;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Meet the Team&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://youtu.be/8mzavjQSMM4&quot; title=&quot;Promo video&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&quot; alt=&quot;Promo video&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/in/mohitjaisal&quot;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;üé• Click the image above for a video about the project the folks who created it!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Pedagogy&lt;/h2&gt; 
&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; 
&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Find our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&quot;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&quot;&gt;Contributing&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&quot;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optional sketchnote&lt;/li&gt; 
 &lt;li&gt;Optional supplemental video&lt;/li&gt; 
 &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; 
 &lt;li&gt;Written lesson&lt;/li&gt; 
 &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; 
 &lt;li&gt;Knowledge checks&lt;/li&gt; 
 &lt;li&gt;A challenge&lt;/li&gt; 
 &lt;li&gt;Supplemental reading&lt;/li&gt; 
 &lt;li&gt;Assignment&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ff-quizzes.netlify.app/en/&quot;&gt;Post-lesson quiz&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-App folder, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&quot; alt=&quot; Sketchnote by @sketchthedocs https://sketchthedocs.dev&quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Number&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Topic&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Grouping&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Learning Objectives&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Linked Lesson&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Author&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data Science&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn the basic concepts behind data science and how it‚Äôs related to artificial intelligence, machine learning, and big data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/beZ7Mb_oz9I&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science Ethics&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;How data is classified and its common sources.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/Z5Zy85g4Yjw&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Relational Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced ‚Äúsee-quell‚Äù).&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with NoSQL Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Python&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/dZjWOGbsN4Y&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Preparation&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Quantities&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn how to use Matplotlib to visualize bird data ü¶Ü&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Distributions of Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Proportions&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Relationships&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Meaningful Visualizations&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Analyzing&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Communication&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Training models using Low Code tools.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Wild&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&quot;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data science driven projects in the real world.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;GitHub Codespaces&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this sample in a Codespace:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click the Code drop-down menu and select the Open with Codespaces option.&lt;/li&gt; 
 &lt;li&gt;Select + New codespace at the bottom on the pane. For more info, check out the &lt;a href=&quot;https://docs.github.com/en/codespaces/developing-in-codespaces/creating-a-codespace-for-a-repository#creating-a-codespace&quot;&gt;GitHub documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;VSCode Remote - Containers&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this repo in a container using your local machine and VSCode using the VS Code Remote - Containers extension:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in &lt;a href=&quot;https://code.visualstudio.com/docs/devcontainers/containers#_getting-started&quot;&gt;the getting started documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use this repository, you can either open the repository in an isolated Docker volume:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Under the hood, this will use the Remote-Containers: &lt;strong&gt;Clone Repository in Container Volume...&lt;/strong&gt; command to clone the source code in a Docker volume instead of the local filesystem. &lt;a href=&quot;https://docs.docker.com/storage/volumes/&quot;&gt;Volumes&lt;/a&gt; are the preferred mechanism for persisting container data.&lt;/p&gt; 
&lt;p&gt;Or open a locally cloned or downloaded version of the repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clone this repository to your local filesystem.&lt;/li&gt; 
 &lt;li&gt;Press F1 and select the &lt;strong&gt;Remote-Containers: Open Folder in Container...&lt;/strong&gt; command.&lt;/li&gt; 
 &lt;li&gt;Select the cloned copy of this folder, wait for the container to start, and try things out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Offline access&lt;/h2&gt; 
&lt;p&gt;You can run this documentation offline by using &lt;a href=&quot;https://docsify.js.org/#/&quot;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&quot;https://docsify.js.org/#/quickstart&quot;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genaijava&quot;&gt;Generative AI with Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/bash-for-beginners&quot;&gt;Bash for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/xr-dev-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/segment-anything</title>
      <link>https://github.com/facebookresearch/segment-anything</link>
      <description>&lt;p&gt;The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Latest updates -- SAM 2: Segment Anything in Images and Videos&lt;/h2&gt; 
&lt;p&gt;Please check out our new release on &lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SAM 2 code: &lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;https://github.com/facebookresearch/segment-anything-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SAM 2 demo: &lt;a href=&quot;https://sam2.metademolab.com/&quot;&gt;https://sam2.metademolab.com/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SAM 2 paper: &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;https://arxiv.org/abs/2408.00714&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/facebookresearch/segment-anything-2/raw/main/assets/model_diagram.png?raw=true&quot; alt=&quot;SAM 2 architecture&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt; is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model-in-the-loop data engine, which improves model and data via user interaction, to collect &lt;a href=&quot;https://ai.meta.com/datasets/segment-anything-video&quot;&gt;&lt;strong&gt;our SA-V dataset&lt;/strong&gt;&lt;/a&gt;, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains.&lt;/p&gt; 
&lt;h1&gt;Segment Anything&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ai.facebook.com/research/&quot;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://alexander-kirillov.github.io/&quot;&gt;Alexander Kirillov&lt;/a&gt;, &lt;a href=&quot;https://ericmintun.github.io/&quot;&gt;Eric Mintun&lt;/a&gt;, &lt;a href=&quot;https://nikhilaravi.com/&quot;&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href=&quot;https://hanzimao.me/&quot;&gt;Hanzi Mao&lt;/a&gt;, Chloe Rolland, Laura Gustafson, &lt;a href=&quot;https://tetexiao.com&quot;&gt;Tete Xiao&lt;/a&gt;, &lt;a href=&quot;https://www.spencerwhitehead.com/&quot;&gt;Spencer Whitehead&lt;/a&gt;, Alex Berg, Wan-Yen Lo, &lt;a href=&quot;https://pdollar.github.io/&quot;&gt;Piotr Dollar&lt;/a&gt;, &lt;a href=&quot;https://www.rossgirshick.info/&quot;&gt;Ross Girshick&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[&lt;a href=&quot;https://ai.facebook.com/research/publications/segment-anything/&quot;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://segment-anything.com/&quot;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://segment-anything.com/demo&quot;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://segment-anything.com/dataset/index.html&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/&quot;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#citing-segment-anything&quot;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png?raw=true&quot; alt=&quot;SAM design&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a &lt;a href=&quot;https://segment-anything.com/dataset/index.html&quot;&gt;dataset&lt;/a&gt; of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.&lt;/p&gt; 
&lt;p float=&quot;left&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks1.png?raw=true&quot; width=&quot;37.25%&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks2.jpg?raw=true&quot; width=&quot;61.5%&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; 
&lt;p&gt;Install Segment Anything:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/facebookresearch/segment-anything.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or clone the repository locally and install with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone git@github.com:facebookresearch/segment-anything.git
cd segment-anything; pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a name=&quot;GettingStarted&quot;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;First download a &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#model-checkpoints&quot;&gt;model checkpoint&lt;/a&gt;. Then the model can be used in just a few lines to get masks from a given prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import SamPredictor, sam_model_registry
sam = sam_model_registry[&quot;&amp;lt;model_type&amp;gt;&quot;](checkpoint=&quot;&amp;lt;path/to/checkpoint&amp;gt;&quot;)
predictor = SamPredictor(sam)
predictor.set_image(&amp;lt;your_image&amp;gt;)
masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or generate masks for an entire image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
sam = sam_model_registry[&quot;&amp;lt;model_type&amp;gt;&quot;](checkpoint=&quot;&amp;lt;path/to/checkpoint&amp;gt;&quot;)
mask_generator = SamAutomaticMaskGenerator(sam)
masks = mask_generator.generate(&amp;lt;your_image&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, masks can be generated for images from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/amg.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --input &amp;lt;image_or_folder&amp;gt; --output &amp;lt;path/to/output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the examples notebooks on &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/predictor_example.ipynb&quot;&gt;using SAM with prompts&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/automatic_mask_generator_example.ipynb&quot;&gt;automatically generating masks&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p float=&quot;left&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook1.png?raw=true&quot; width=&quot;49.1%&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook2.png?raw=true&quot; width=&quot;48.9%&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;ONNX Export&lt;/h2&gt; 
&lt;p&gt;SAM&#39;s lightweight mask decoder can be exported to ONNX format so that it can be run in any environment that supports ONNX runtime, such as in-browser as showcased in the &lt;a href=&quot;https://segment-anything.com/demo&quot;&gt;demo&lt;/a&gt;. Export the model with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --output &amp;lt;path/to/output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/facebookresearch/segment-anything/raw/main/notebooks/onnx_model_example.ipynb&quot;&gt;example notebook&lt;/a&gt; for details on how to combine image preprocessing via SAM&#39;s backbone with mask prediction using the ONNX model. It is recommended to use the latest stable version of PyTorch for ONNX export.&lt;/p&gt; 
&lt;h3&gt;Web demo&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;demo/&lt;/code&gt; folder has a simple one page React app which shows how to run mask prediction with the exported ONNX model in a web browser with multithreading. Please see &lt;a href=&quot;https://github.com/facebookresearch/segment-anything/raw/main/demo/README.md&quot;&gt;&lt;code&gt;demo/README.md&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;&lt;a name=&quot;Models&quot;&gt;&lt;/a&gt;Model Checkpoints&lt;/h2&gt; 
&lt;p&gt;Three model versions of the model are available with different backbone sizes. These models can be instantiated by running&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import sam_model_registry
sam = sam_model_registry[&quot;&amp;lt;model_type&amp;gt;&quot;](checkpoint=&quot;&amp;lt;path/to/checkpoint&amp;gt;&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Click the links below to download the checkpoint for the corresponding model type.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;default&lt;/code&gt; or &lt;code&gt;vit_h&lt;/code&gt;: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&quot;&gt;ViT-H SAM model.&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vit_l&lt;/code&gt;: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth&quot;&gt;ViT-L SAM model.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vit_b&lt;/code&gt;: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&quot;&gt;ViT-B SAM model.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Dataset&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://ai.facebook.com/datasets/segment-anything/&quot;&gt;here&lt;/a&gt; for an overview of the datastet. The dataset can be downloaded &lt;a href=&quot;https://ai.facebook.com/datasets/segment-anything-downloads/&quot;&gt;here&lt;/a&gt;. By downloading the datasets you agree that you have read and accepted the terms of the SA-1B Dataset Research License.&lt;/p&gt; 
&lt;p&gt;We save masks per image as a json file. It can be loaded as a dictionary in python in the below format.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;{
    &quot;image&quot;                 : image_info,
    &quot;annotations&quot;           : [annotation],
}

image_info {
    &quot;image_id&quot;              : int,              # Image id
    &quot;width&quot;                 : int,              # Image width
    &quot;height&quot;                : int,              # Image height
    &quot;file_name&quot;             : str,              # Image filename
}

annotation {
    &quot;id&quot;                    : int,              # Annotation id
    &quot;segmentation&quot;          : dict,             # Mask saved in COCO RLE format.
    &quot;bbox&quot;                  : [x, y, w, h],     # The box around the mask, in XYWH format
    &quot;area&quot;                  : int,              # The area in pixels of the mask
    &quot;predicted_iou&quot;         : float,            # The model&#39;s own prediction of the mask&#39;s quality
    &quot;stability_score&quot;       : float,            # A measure of the mask&#39;s quality
    &quot;crop_box&quot;              : [x, y, w, h],     # The crop of the image used to generate the mask, in XYWH format
    &quot;point_coords&quot;          : [[x, y]],         # The point coordinates input to the model to generate the mask
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Image ids can be found in sa_images_ids.txt which can be downloaded using the above &lt;a href=&quot;https://ai.facebook.com/datasets/segment-anything-downloads/&quot;&gt;link&lt;/a&gt; as well.&lt;/p&gt; 
&lt;p&gt;To decode a mask in COCO RLE format into binary:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from pycocotools import mask as mask_utils
mask = mask_utils.decode(annotation[&quot;segmentation&quot;])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/cocodataset/cocoapi/raw/master/PythonAPI/pycocotools/mask.py&quot;&gt;here&lt;/a&gt; for more instructions to manipulate masks stored in RLE format.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The model is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/LICENSE&quot;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;The Segment Anything project was made possible with the help of many contributors (alphabetical):&lt;/p&gt; 
&lt;p&gt;Aaron Adcock, Vaibhav Aggarwal, Morteza Behrooz, Cheng-Yang Fu, Ashley Gabriel, Ahuva Goldstand, Allen Goodman, Sumanth Gurram, Jiabo Hu, Somya Jain, Devansh Kukreja, Robert Kuo, Joshua Lane, Yanghao Li, Lilian Luong, Jitendra Malik, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, Bram Wasti, Zachary Winstrom&lt;/p&gt; 
&lt;h2&gt;Citing Segment Anything&lt;/h2&gt; 
&lt;p&gt;If you use SAM or SA-1B in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>timeseriesAI/tsai</title>
      <link>https://github.com/timeseriesAI/tsai</link>
      <description>&lt;p&gt;Time series Timeseries Deep Learning Machine Learning Python Pytorch fastai | State-of-the-art Deep Learning library for Time Series and Sequences in Pytorch / fastai&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tsai&lt;/h1&gt; 
&lt;!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! --&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/timeseriesAI/tsai/raw/main/nbs/multimedia/tsai_logo.svg?raw=true&quot; width=&quot;50%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/timeseriesai/tsai/workflows/CI/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt; &lt;a href=&quot;https://pypi.org/project/tsai/#description&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/tsai?color=blue&amp;amp;label=pypi%20version.png&quot; alt=&quot;PyPI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://anaconda.org/timeseriesai/tsai&quot;&gt;&lt;img src=&quot;https://img.shields.io/conda/vn/timeseriesai/tsai?color=brightgreen&amp;amp;label=conda%20version.png&quot; alt=&quot;Conda (channel only)&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://zenodo.org/badge/latestdoi/211822289&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/211822289.svg?sanitize=true&quot; alt=&quot;DOI&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&quot; alt=&quot;PRs&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;State-of-the-art Deep Learning library for Time Series and Sequences.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;code&gt;tsai&lt;/code&gt; is an open-source deep learning package built on top of Pytorch &amp;amp; fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation‚Ä¶&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;tsai&lt;/code&gt; is currently under active development by timeseriesAI.&lt;/p&gt; 
&lt;h2&gt;What‚Äôs new:&lt;/h2&gt; 
&lt;p&gt;During the last few releases, here are some of the most significant additions to &lt;code&gt;tsai&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;New models&lt;/strong&gt;: PatchTST (Accepted by ICLR 2023), RNN with Attention (RNNAttention, LSTMAttention, GRUAttention), TabFusionTransformer, ‚Ä¶&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New datasets&lt;/strong&gt;: we have increased the number of datasets you can download using &lt;code&gt;tsai&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;128 univariate classification datasets&lt;/li&gt; 
   &lt;li&gt;30 multivariate classification datasets&lt;/li&gt; 
   &lt;li&gt;15 regression datasets&lt;/li&gt; 
   &lt;li&gt;62 forecasting datasets&lt;/li&gt; 
   &lt;li&gt;9 long term forecasting datasets&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New tutorials&lt;/strong&gt;: &lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tutorial_nbs/15_PatchTST_a_new_transformer_for_LTSF.ipynb&quot;&gt;PatchTST&lt;/a&gt;. Based on some of your requests, we are planning to release additional tutorials on data preparation and forecasting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New functionality&lt;/strong&gt;: sklearn-type pipeline transforms, walk-foward cross validation, reduced RAM requirements, and a lot of new functionality to perform more accurate time series forecasts.&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.0 support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Pip install&lt;/h3&gt; 
&lt;p&gt;You can install the &lt;strong&gt;latest stable&lt;/strong&gt; version from pip using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install tsai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you plan to develop tsai yourself, or want to be on the cutting edge, you can use an editable install. First install PyTorch, and then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;git clone https://github.com/timeseriesAI/tsai
pip install -e &quot;tsai[dev]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: starting with tsai 0.3.0 tsai will only install hard dependencies. Other soft dependencies (which are only required for selected tasks) will not be installed by default (this is the recommended approach. If you require any of the dependencies that is not installed, tsai will ask you to install it when necessary). If you still want to install tsai with all its dependencies you can do it by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install tsai[extras]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Conda install&lt;/h3&gt; 
&lt;p&gt;You can also install tsai using conda (note that if you replace conda with mamba the install process will be much faster and more reliable):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;conda install -c timeseriesai tsai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Here‚Äôs the link to the &lt;a href=&quot;https://timeseriesai.github.io/tsai/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available models:&lt;/h2&gt; 
&lt;p&gt;Here‚Äôs a list with some of the state-of-the-art models available in &lt;code&gt;tsai&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN.py&quot;&gt;LSTM&lt;/a&gt; (Hochreiter, 1997) (&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963/&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN.py&quot;&gt;GRU&lt;/a&gt; (Cho, 2014) (&lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/MLP.py&quot;&gt;MLP&lt;/a&gt; - Multilayer Perceptron (Wang, 2016) (&lt;a href=&quot;https://arxiv.org/abs/1611.06455&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/FCN.py&quot;&gt;FCN&lt;/a&gt; - Fully Convolutional Network (Wang, 2016) (&lt;a href=&quot;https://arxiv.org/abs/1611.06455&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/ResNet.py&quot;&gt;ResNet&lt;/a&gt; - Residual Network (Wang, 2016) (&lt;a href=&quot;https://arxiv.org/abs/1611.06455&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN_FCN.py&quot;&gt;LSTM-FCN&lt;/a&gt; (Karim, 2017) (&lt;a href=&quot;https://arxiv.org/abs/1709.05206&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN_FCN.py&quot;&gt;GRU-FCN&lt;/a&gt; (Elsayed, 2018) (&lt;a href=&quot;https://arxiv.org/abs/1812.07683&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/mWDN.py&quot;&gt;mWDN&lt;/a&gt; - Multilevel wavelet decomposition network (Wang, 2018) (&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3219819.3220060&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TCN.py&quot;&gt;TCN&lt;/a&gt; - Temporal Convolutional Network (Bai, 2018) (&lt;a href=&quot;https://arxiv.org/abs/1803.01271&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN_FCN.py&quot;&gt;MLSTM-FCN&lt;/a&gt; - Multivariate LSTM-FCN (Karim, 2019) (&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0893608019301200&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/InceptionTime.py&quot;&gt;InceptionTime&lt;/a&gt; (Fawaz, 2019) (&lt;a href=&quot;https://arxiv.org/abs/1909.04939&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/ROCKET.py&quot;&gt;Rocket&lt;/a&gt; (Dempster, 2019) (&lt;a href=&quot;https://arxiv.org/abs/1910.13051&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/XceptionTime.py&quot;&gt;XceptionTime&lt;/a&gt; (Rahimian, 2019) (&lt;a href=&quot;https://arxiv.org/abs/1911.03803&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/ResCNN.py&quot;&gt;ResCNN&lt;/a&gt; - 1D-ResCNN (Zou , 2019) (&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231219311506&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TabModel.py&quot;&gt;TabModel&lt;/a&gt; - modified from fastai‚Äôs &lt;a href=&quot;https://docs.fast.ai/tabular.model.html#TabularModel&quot;&gt;TabularModel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/OmniScaleCNN.py&quot;&gt;OmniScale&lt;/a&gt; - Omni-Scale 1D-CNN (Tang, 2020) (&lt;a href=&quot;https://arxiv.org/abs/2002.10061&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TST.py&quot;&gt;TST&lt;/a&gt; - Time Series Transformer (Zerveas, 2020) (&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3447548.3467401&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TabTransformer.py&quot;&gt;TabTransformer&lt;/a&gt; (Huang, 2020) (&lt;a href=&quot;https://arxiv.org/pdf/2012.06678&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TSiTPlus.py&quot;&gt;TSiT&lt;/a&gt; Adapted from ViT (Dosovitskiy, 2020) (&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/MINIROCKET.py&quot;&gt;MiniRocket&lt;/a&gt; (Dempster, 2021) (&lt;a href=&quot;https://arxiv.org/abs/2102.00457&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/XCM.py&quot;&gt;XCM&lt;/a&gt; - An Explainable Convolutional Neural Network (Fauvel, 2021) (&lt;a href=&quot;https://hal.inria.fr/hal-03469487/document&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/gMLP.py&quot;&gt;gMLP&lt;/a&gt; - Gated Multilayer Perceptron (Liu, 2021) (&lt;a href=&quot;https://arxiv.org/abs/2105.08050&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TSPerceiver.py&quot;&gt;TSPerceiver&lt;/a&gt; - Adapted from Perceiver IO (Jaegle, 2021) (&lt;a href=&quot;https://arxiv.org/abs/2107.14795&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/GatedTabTransformer.py&quot;&gt;GatedTabTransformer&lt;/a&gt; (Cholakov, 2022) (&lt;a href=&quot;https://arxiv.org/abs/2201.00199&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TSSequencerPlus.py&quot;&gt;TSSequencerPlus&lt;/a&gt; - Adapted from Sequencer (Tatsunami, 2022) (&lt;a href=&quot;https://arxiv.org/abs/2205.01972&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/PatchTST.py&quot;&gt;PatchTST&lt;/a&gt; - (Nie, 2022) (&lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;plus other custom models like: TransformerModel, LSTMAttention, GRUAttention, ‚Ä¶&lt;/p&gt; 
&lt;h2&gt;How to start using tsai?&lt;/h2&gt; 
&lt;p&gt;To get to know the tsai package, we‚Äôd suggest you start with this notebook in Google Colab: &lt;strong&gt;&lt;a href=&quot;https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/01_Intro_to_Time_Series_Classification.ipynb&quot;&gt;01_Intro_to_Time_Series_Classification&lt;/a&gt;&lt;/strong&gt; It provides an overview of a time series classification task.&lt;/p&gt; 
&lt;p&gt;We have also develop many other &lt;a href=&quot;https://github.com/timeseriesAI/tsai/tree/main/tutorial_nbs&quot;&gt;tutorial notebooks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use tsai in your own notebooks, the only thing you need to do after you have installed the package is to run this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.all import *
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;These are just a few examples of how you can use &lt;code&gt;tsai&lt;/code&gt;:&lt;/p&gt; 
&lt;h3&gt;Binary, univariate classification&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

X, y, splits = get_classification_data(&#39;ECG200&#39;, split_data=False)
tfms = [None, TSClassification()]
batch_tfms = TSStandardize()
clf = TSClassifier(X, y, splits=splits, path=&#39;models&#39;, arch=&quot;InceptionTimePlus&quot;, tfms=tfms, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())
clf.fit_one_cycle(100, 3e-4)
clf.export(&quot;clf.pkl&quot;) 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

clf = load_learner(&quot;models/clf.pkl&quot;)
probas, target, preds = clf.get_X_preds(X[splits[1]], y[splits[1]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multi-class, multivariate classification&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

X, y, splits = get_classification_data(&#39;LSST&#39;, split_data=False)
tfms = [None, TSClassification()]
batch_tfms = TSStandardize(by_sample=True)
mv_clf = TSClassifier(X, y, splits=splits, path=&#39;models&#39;, arch=&quot;InceptionTimePlus&quot;, tfms=tfms, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())
mv_clf.fit_one_cycle(10, 1e-2)
mv_clf.export(&quot;mv_clf.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

mv_clf = load_learner(&quot;models/mv_clf.pkl&quot;)
probas, target, preds = mv_clf.get_X_preds(X[splits[1]], y[splits[1]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multivariate Regression&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

X, y, splits = get_regression_data(&#39;AppliancesEnergy&#39;, split_data=False)
tfms = [None, TSRegression()]
batch_tfms = TSStandardize(by_sample=True)
reg = TSRegressor(X, y, splits=splits, path=&#39;models&#39;, arch=&quot;TSTPlus&quot;, tfms=tfms, batch_tfms=batch_tfms, metrics=rmse, cbs=ShowGraph(), verbose=True)
reg.fit_one_cycle(100, 3e-4)
reg.export(&quot;reg.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

reg = load_learner(&quot;models/reg.pkl&quot;)
raw_preds, target, preds = reg.get_X_preds(X[splits[1]], y[splits[1]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The ROCKETs (RocketClassifier, RocketRegressor, MiniRocketClassifier, MiniRocketRegressor, MiniRocketVotingClassifier or MiniRocketVotingRegressor) are somewhat different models. They are not actually deep learning models (although they use convolutions) and are used in a different way.&lt;/p&gt; 
&lt;p&gt;‚ö†Ô∏è You‚Äôll also need to install sktime to be able to use them. You can install it separately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install sktime
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install tsai[extras]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from sklearn.metrics import mean_squared_error, make_scorer
from tsai.data.external import get_Monash_regression_data
from tsai.models.MINIROCKET import MiniRocketRegressor

X_train, y_train, *_ = get_Monash_regression_data(&#39;AppliancesEnergy&#39;)
rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
reg = MiniRocketRegressor(scoring=rmse_scorer)
reg.fit(X_train, y_train)
reg.save(&#39;MiniRocketRegressor&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from sklearn.metrics import mean_squared_error
from tsai.data.external import get_Monash_regression_data
from tsai.models.MINIROCKET import load_minirocket

*_, X_test, y_test = get_Monash_regression_data(&#39;AppliancesEnergy&#39;)
reg = load_minirocket(&#39;MiniRocketRegressor&#39;)
y_pred = reg.predict(X_test)
mean_squared_error(y_test, y_pred, squared=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Forecasting&lt;/h3&gt; 
&lt;p&gt;You can use tsai for forecast in the following scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;univariate or multivariate time series input&lt;/li&gt; 
 &lt;li&gt;univariate or multivariate time series output&lt;/li&gt; 
 &lt;li&gt;single or multi-step ahead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You‚Äôll need to: * prepare X (time series input) and the target y (see &lt;a href=&quot;https://timeseriesai.github.io/tsai/data.preparation.html&quot;&gt;documentation&lt;/a&gt;) * select PatchTST or one of tsai‚Äôs models ending in Plus (TSTPlus, InceptionTimePlus, TSiTPlus, etc). The model will auto-configure a head to yield an output with the same shape as the target input y.&lt;/p&gt; 
&lt;h4&gt;Single step&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

ts = get_forecasting_time_series(&quot;Sunspots&quot;).values
X, y = SlidingWindow(60, horizon=1)(ts)
splits = TimeSplitter(235)(y) 
tfms = [None, TSForecasting()]
batch_tfms = TSStandardize()
fcst = TSForecaster(X, y, splits=splits, path=&#39;models&#39;, tfms=tfms, batch_tfms=batch_tfms, bs=512, arch=&quot;TSTPlus&quot;, metrics=mae, cbs=ShowGraph())
fcst.fit_one_cycle(50, 1e-3)
fcst.export(&quot;fcst.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

fcst = load_learner(&quot;models/fcst.pkl&quot;, cpu=False)
raw_preds, target, preds = fcst.get_X_preds(X[splits[1]], y[splits[1]])
raw_preds.shape
# torch.Size([235, 1])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multi-step&lt;/h4&gt; 
&lt;p&gt;This example show how to build a 3-step ahead univariate forecast.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

ts = get_forecasting_time_series(&quot;Sunspots&quot;).values
X, y = SlidingWindow(60, horizon=3)(ts)
splits = TimeSplitter(235, fcst_horizon=3)(y) 
tfms = [None, TSForecasting()]
batch_tfms = TSStandardize()
fcst = TSForecaster(X, y, splits=splits, path=&#39;models&#39;, tfms=tfms, batch_tfms=batch_tfms, bs=512, arch=&quot;TSTPlus&quot;, metrics=mae, cbs=ShowGraph())
fcst.fit_one_cycle(50, 1e-3)
fcst.export(&quot;fcst.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner
fcst = load_learner(&quot;models/fcst.pkl&quot;, cpu=False)
raw_preds, target, preds = fcst.get_X_preds(X[splits[1]], y[splits[1]])
raw_preds.shape
# torch.Size([235, 3])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Input data format&lt;/h2&gt; 
&lt;p&gt;The input format for all time series models and image models in tsai is the same. An np.ndarray (or array-like object like zarr, etc) with 3 dimensions:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[# samples x # variables x sequence length]&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The input format for tabular models in tsai (like TabModel, TabTransformer and TabFusionTransformer) is a pandas dataframe. See &lt;a href=&quot;https://timeseriesai.github.io/tsai/models.TabModel.html&quot;&gt;example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to contribute to tsai?&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of all kinds. Development of enhancements, bug fixes, documentation, tutorial notebooks, ‚Ä¶&lt;/p&gt; 
&lt;p&gt;We have created a guide to help you start contributing to tsai. You can read it &lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/CONTRIBUTING.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Enterprise support and consulting services:&lt;/h2&gt; 
&lt;p&gt;Want to make the most out of timeseriesAI/tsai in a professional setting? Let us help. Send us an email to learn more: &lt;a href=&quot;mailto:info@timeseriesai.co&quot;&gt;info@timeseriesai.co&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citing tsai&lt;/h2&gt; 
&lt;p&gt;If you use tsai in your research please use the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;@Misc{tsai,
    author =       {Ignacio Oguiza},
    title =        {tsai - A state-of-the-art deep learning library for time series and sequential data},
    howpublished = {Github},
    year =         {2023},
    url =          {https://github.com/timeseriesAI/tsai}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>udlbook/udlbook</title>
      <link>https://github.com/udlbook/udlbook</link>
      <description>&lt;p&gt;Understanding Deep Learning - Simon J.D. Prince&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-cookbook</title>
      <link>https://github.com/openai/openai-cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://cookbook.openai.com&quot; target=&quot;_blank&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/openai-cookbook-white.png&quot; style=&quot;max-width: 100%; width: 400px; margin-bottom: 20px&quot; /&gt; 
  &lt;img alt=&quot;OpenAI Cookbook Logo&quot; src=&quot;https://raw.githubusercontent.com/openai/openai-cookbook/main/images/openai-cookbook.png&quot; width=&quot;400px&quot; /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ú® Navigate at &lt;a href=&quot;https://cookbook.openai.com&quot;&gt;cookbook.openai.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Example code and guides for accomplishing common tasks with the &lt;a href=&quot;https://platform.openai.com/docs/introduction&quot;&gt;OpenAI API&lt;/a&gt;. To run these examples, you&#39;ll need an OpenAI account and associated API key (&lt;a href=&quot;https://platform.openai.com/signup&quot;&gt;create a free account here&lt;/a&gt;). Set an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an &lt;code&gt;.env&lt;/code&gt; file at the root of your repo containing &lt;code&gt;OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;, which will be picked up by the notebooks.&lt;/p&gt; 
&lt;p&gt;Most code examples are written in Python, though the concepts can be applied in any language.&lt;/p&gt; 
&lt;p&gt;For other useful tools, guides and courses, check out these &lt;a href=&quot;https://cookbook.openai.com/related_resources&quot;&gt;related resources from around the web&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rasbt/LLMs-from-scratch</title>
      <link>https://github.com/rasbt/LLMs-from-scratch</link>
      <description>&lt;p&gt;Implement a ChatGPT-like LLM in PyTorch from scratch, step by step&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build a Large Language Model (From Scratch)&lt;/h1&gt; 
&lt;p&gt;This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book &lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;Build a Large Language Model (From Scratch)&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123&quot; width=&quot;250px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;In &lt;a href=&quot;http://mng.bz/orYv&quot;&gt;&lt;em&gt;Build a Large Language Model (From Scratch)&lt;/em&gt;&lt;/a&gt;, you&#39;ll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I&#39;ll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.&lt;/p&gt; 
&lt;p&gt;The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Link to the official &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;source code repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://mng.bz/orYv&quot;&gt;Link to the book at Manning (the publisher&#39;s website)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/gp/product/1633437167&quot;&gt;Link to the book page on Amazon.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ISBN 9781633437166&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;http://mng.bz/orYv#reviews&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png&quot; width=&quot;220px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;To download a copy of this repository, click on the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip&quot;&gt;Download ZIP&lt;/a&gt; button or execute the following command in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; for the latest updates.)&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;p&gt;Please note that this &lt;code&gt;README.md&lt;/code&gt; file is a Markdown (&lt;code&gt;.md&lt;/code&gt;) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven&#39;t installed a Markdown editor yet, &lt;a href=&quot;https://ghostwriter.kde.org&quot;&gt;Ghostwriter&lt;/a&gt; is a good free option.&lt;/p&gt; 
&lt;p&gt;You can alternatively view this and other files on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; in your browser, which renders Markdown automatically.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; If you&#39;re seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/README.md&quot;&gt;README.md&lt;/a&gt; file located in the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;setup&lt;/a&gt; directory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Linux&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Windows&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests macOS&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chapter Title&lt;/th&gt; 
   &lt;th&gt;Main Code (for Quick Access)&lt;/th&gt; 
   &lt;th&gt;All Code + Supplementary&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;Setup recommendations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 1: Understanding Large Language Models&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 2: Working with Text Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/ch02.ipynb&quot;&gt;ch02.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/dataloader.ipynb&quot;&gt;dataloader.ipynb&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02&quot;&gt;./ch02&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 3: Coding Attention Mechanisms&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/ch03.ipynb&quot;&gt;ch03.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/multihead-attention.ipynb&quot;&gt;multihead-attention.ipynb&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03&quot;&gt;./ch03&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 4: Implementing a GPT Model from Scratch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/ch04.ipynb&quot;&gt;ch04.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/gpt.py&quot;&gt;gpt.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04&quot;&gt;./ch04&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 5: Pretraining on Unlabeled Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/ch05.ipynb&quot;&gt;ch05.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_train.py&quot;&gt;gpt_train.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_generate.py&quot;&gt;gpt_generate.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05&quot;&gt;./ch05&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 6: Finetuning for Text Classification&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/ch06.ipynb&quot;&gt;ch06.ipynb&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/gpt_class_finetune.py&quot;&gt;gpt_class_finetune.py&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06&quot;&gt;./ch06&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 7: Finetuning to Follow Instructions&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ch07.ipynb&quot;&gt;ch07.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py&quot;&gt;gpt_instruction_finetuning.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ollama_evaluate.py&quot;&gt;ollama_evaluate.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07&quot;&gt;./ch07&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix A: Introduction to PyTorch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part1.ipynb&quot;&gt;code-part1.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part2.ipynb&quot;&gt;code-part2.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/DDP-script.py&quot;&gt;DDP-script.py&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A&quot;&gt;./appendix-A&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix B: References and Further Reading&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix C: Exercise Solutions&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix D: Adding Bells and Whistles to the Training Loop&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D/01_main-chapter-code/appendix-D.ipynb&quot;&gt;appendix-D.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D&quot;&gt;./appendix-D&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix E: Parameter-efficient Finetuning with LoRA&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E/01_main-chapter-code/appendix-E.ipynb&quot;&gt;appendix-E.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E&quot;&gt;./appendix-E&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;p&gt;The mental model below summarizes the contents covered in this book.&lt;/p&gt; 
&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg&quot; width=&quot;650px&quot; /&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;The most important prerequisite is a strong foundation in Python programming. With this knowledge, you will be well prepared to explore the fascinating world of LLMs and understand the concepts and code examples presented in this book.&lt;/p&gt; 
&lt;p&gt;If you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.&lt;/p&gt; 
&lt;p&gt;This book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, &lt;a href=&quot;https://sebastianraschka.com/teaching/pytorch-1h/&quot;&gt;PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs&lt;/a&gt;, helpful for learning about the essentials.&lt;/p&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;p&gt;The code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/raw/main/setup/README.md&quot;&gt;setup&lt;/a&gt; doc for additional recommendations.)&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Video Course&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;A 17-hour and 15-minute companion video course&lt;/a&gt; where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book&#39;s structure so that it can be used as a standalone alternative to the book or complementary code-along resource.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123&quot; width=&quot;350px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Companion Book / Sequel&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;&lt;em&gt;Build A Reasoning Model (From Scratch)&lt;/em&gt;&lt;/a&gt;, while a standalone book, can be considered as a sequel to &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;It starts with a pretrained model and implements different reasoning approaches, including inference-time scaling, reinforcement learning, and distillation, to improve the model&#39;s reasoning capabilities.&lt;/p&gt; 
&lt;p&gt;Similar to &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;, &lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;&lt;em&gt;Build A Reasoning Model (From Scratch)&lt;/em&gt;&lt;/a&gt; takes a hands-on approach implementing these methods from scratch.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/reasoning-from-scratch-images/cover.webp?123&quot; width=&quot;120px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Amazon link (TBD)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;Manning link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch&quot;&gt;GitHub repository&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Exercises&lt;/h2&gt; 
&lt;p&gt;Each chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example, &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;./ch02/01_main-chapter-code/exercise-solutions.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In addition to the code exercises, you can download a free 170-page PDF titled &lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;Test Yourself On Build a Large Language Model (From Scratch)&lt;/a&gt; from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123&quot; width=&quot;150px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Bonus Material&lt;/h2&gt; 
&lt;p&gt;Several folders contain optional materials as a bonus for interested readers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/01_optional-python-setup-preferences&quot;&gt;Python Setup Tips&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/02_installing-python-libraries&quot;&gt;Installing Python Packages and Libraries Used In This Book&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/03_optional-docker-environment&quot;&gt;Docker Environment Setup Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 2: Working with text data&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb&quot;&gt;Byte Pair Encoding (BPE) Tokenizer From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/02_bonus_bytepair-encoder&quot;&gt;Comparing Various Byte Pair Encoding (BPE) Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/03_bonus_embedding-vs-matmul&quot;&gt;Understanding the Difference Between Embedding Layers and Linear Layers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/04_bonus_dataloader-intuition&quot;&gt;Dataloader Intuition with Simple Numbers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 3: Coding attention mechanisms&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb&quot;&gt;Comparing Efficient Multi-Head Attention Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/03_understanding-buffers/understanding-buffers.ipynb&quot;&gt;Understanding PyTorch Buffers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 4: Implementing a GPT model from scratch&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/02_performance-analysis/flops-analysis.ipynb&quot;&gt;FLOPS Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/03_kv-cache&quot;&gt;KV Cache&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 5: Pretraining on unlabeled data:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/02_alternative_weight_loading/&quot;&gt;Alternative Weight Loading Methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/03_bonus_pretraining_on_gutenberg&quot;&gt;Pretraining GPT on the Project Gutenberg Dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/04_learning_rate_schedulers&quot;&gt;Adding Bells and Whistles to the Training Loop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/05_bonus_hparam_tuning&quot;&gt;Optimizing Hyperparameters for Pretraining&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/06_user_interface&quot;&gt;Building a User Interface to Interact With the Pretrained LLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama&quot;&gt;Converting GPT to Llama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb&quot;&gt;Llama 3.2 From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/11_qwen3/&quot;&gt;Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/12_gemma3/&quot;&gt;Gemma 3 From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb&quot;&gt;Memory-efficient Model Weight Loading&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/09_extending-tokenizers/extend-tiktoken.ipynb&quot;&gt;Extending the Tiktoken BPE Tokenizer with New Tokens&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/10_llm-training-speed&quot;&gt;PyTorch Performance Tips for Faster LLM Training&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 6: Finetuning for classification&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/02_bonus_additional-experiments&quot;&gt;Additional experiments finetuning different layers and using larger models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/03_bonus_imdb-classification&quot;&gt;Finetuning different models on 50k IMDb movie review dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/04_user_interface&quot;&gt;Building a User Interface to Interact With the GPT-based Spam Classifier&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 7: Finetuning to follow instructions&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/02_dataset-utilities&quot;&gt;Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/03_model-evaluation&quot;&gt;Evaluating Instruction Responses Using the OpenAI API and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/llama3-ollama.ipynb&quot;&gt;Generating a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/reflection-gpt4.ipynb&quot;&gt;Improving a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb&quot;&gt;Generating a Preference Dataset with Llama 3.1 70B and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&quot;&gt;Direct Preference Optimization (DPO) for LLM Alignment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/06_user_interface&quot;&gt;Building a User Interface to Interact With the Instruction Finetuned GPT Model&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Questions, Feedback, and Contributing to This Repository&lt;/h2&gt; 
&lt;p&gt;I welcome all sorts of feedback, best shared via the &lt;a href=&quot;https://livebook.manning.com/forum?product=raschka&amp;amp;page=1&quot;&gt;Manning Forum&lt;/a&gt; or &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;. Likewise, if you have any questions or just want to bounce ideas off others, please don&#39;t hesitate to post these in the forum as well.&lt;/p&gt; 
&lt;p&gt;Please note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this book or code useful for your research, please consider citing it.&lt;/p&gt; 
&lt;p&gt;Chicago-style citation:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Raschka, Sebastian. &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;. Manning, 2024. ISBN: 978-1633437166.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{build-llms-from-scratch-book,
  author       = {Sebastian Raschka},
  title        = {Build A Large Language Model (From Scratch)},
  publisher    = {Manning},
  year         = {2024},
  isbn         = {978-1633437166},
  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
  github       = {https://github.com/rasbt/LLMs-from-scratch}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/dinov2</title>
      <link>https://github.com/facebookresearch/dinov2</link>
      <description>&lt;p&gt;PyTorch code and models for the DINOv2 self-supervised learning method.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;span&gt;üÜï&lt;/span&gt; [2025-08-14] &lt;em&gt;Please check out the more recent &lt;a href=&quot;https://github.com/facebookresearch/dinov3&quot;&gt;DINOv3&lt;/a&gt; effort continuing this line of work.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[2025-06-11] &lt;em&gt;Added dino.txt inference code, following &lt;a href=&quot;https://arxiv.org/abs/2412.16334&quot;&gt;DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[2023-10-26] &lt;em&gt;Added DINOv2 backbones with registers, following &lt;a href=&quot;https://arxiv.org/abs/2309.16588&quot;&gt;Vision Transformers Need Registers&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ai.facebook.com/research/&quot;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski&lt;/p&gt; 
&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;code&gt;Paper #1&lt;/code&gt;&lt;/a&gt;] &lt;a href=&quot;https://arxiv.org/abs/2309.16588&quot;&gt;&lt;code&gt;Paper #2&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/&quot;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://dinov2.metademolab.com&quot;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/#citing-dinov2&quot;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;PyTorch implementation and pretrained models for DINOv2. For details, see the papers: &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.16588&quot;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&quot;&gt;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&lt;/a&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt;
  Visualization of the three first principal components of the patch features of all frames, mapped to RGB values. 
&lt;/div&gt; 
&lt;h2&gt;Pretrained models&lt;/h2&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;# of&lt;br /&gt;params&lt;/th&gt; 
   &lt;th&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;ImageNet&lt;br /&gt;k-NN&lt;/th&gt; 
   &lt;th&gt;ImageNet&lt;br /&gt;linear&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;21 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;79.0%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;81.1%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;21 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;79.1%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;80.9%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;82.1%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;82.0%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.6%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;300 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.5%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.3%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;300 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.8%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.7%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1,100 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.5%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1,100 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.7%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;87.1%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Pretrained backbones (via PyTorch Hub)&lt;/h3&gt; 
&lt;p&gt;Please follow the instructions &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.&lt;/p&gt; 
&lt;p&gt;A corresponding &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/MODEL_CARD.md&quot;&gt;model card&lt;/a&gt; is included in the repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch

# DINOv2
dinov2_vits14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14&#39;)
dinov2_vitb14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14&#39;)
dinov2_vitl14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14&#39;)
dinov2_vitg14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14&#39;)

# DINOv2 with registers
dinov2_vits14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg&#39;)
dinov2_vitb14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg&#39;)
dinov2_vitl14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg&#39;)
dinov2_vitg14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pretrained heads - Image classification&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ImageNet&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_lreg4_inear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The (full) classifier models can be loaded via PyTorch Hub:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch

# DINOv2
dinov2_vits14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_lc&#39;)
dinov2_vitb14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_lc&#39;)
dinov2_vitl14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_lc&#39;)
dinov2_vitg14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_lc&#39;)

# DINOv2 with registers
dinov2_vits14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg_lc&#39;)
dinov2_vitb14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg_lc&#39;)
dinov2_vitl14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg_lc&#39;)
dinov2_vitg14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg_lc&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pretrained heads - Depth estimation&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th colspan=&quot;2&quot;&gt;download head&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;NYUd&lt;/th&gt; 
   &lt;th&gt;KITTI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Pretrained heads - Semantic segmentation&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th&gt;download model&lt;/th&gt; 
   &lt;th colspan=&quot;2&quot;&gt;download head&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ADE20K&lt;/th&gt; 
   &lt;th&gt;ADE20K&lt;/th&gt; 
   &lt;th&gt;VOC2012&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth&quot;&gt;Mask2Former&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Pretrained heads - Zero-shot tasks with dino.txt&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_dinotxt_tet1280d20h24l_vision_head.pth&quot;&gt;vision head&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_dinotxt_tet1280d20h24l_text_encoder.pth&quot;&gt;text model&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/thirdparty/bpe_simple_vocab_16e6.txt.gz&quot;&gt;vocabulary&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/thirdparty/LICENSE&quot;&gt;vocabulary license&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The (full) dino.txt model can be loaded via PyTorch Hub:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch

# DINOv2
dinov2_vitl14_reg4_dinotxt_tet1280d20h24l = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg4_dinotxt_tet1280d20h24l&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The training and evaluation code requires PyTorch 2.0 and &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&quot;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt; - Clone the repository and then create and activate a &lt;code&gt;dinov2&lt;/code&gt; conda environment using the provided environment definition:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda env create -f conda.yaml
conda activate dinov2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pip.pypa.io/en/stable/getting-started/&quot;&gt;pip&lt;/a&gt;&lt;/em&gt; - Clone the repository and then use the provided &lt;code&gt;requirements.txt&lt;/code&gt; to install the dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For dense tasks (depth estimation and semantic segmentation), there are additional dependencies (specific versions of &lt;code&gt;mmcv&lt;/code&gt; and &lt;code&gt;mmsegmentation&lt;/code&gt;) which are captured in the &lt;code&gt;extras&lt;/code&gt; dependency specifications:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&quot;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda env create -f conda-extras.yaml
conda activate dinov2-extras
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pip.pypa.io/en/stable/getting-started/&quot;&gt;pip&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install -r requirements.txt -r requirements-extras.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Data preparation&lt;/h2&gt; 
&lt;h3&gt;ImageNet-1k&lt;/h3&gt; 
&lt;p&gt;The root directory of the dataset should hold the following contents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00000001.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/[..]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00100000.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n01440764/n01440764_10026.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/[...]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n15075141/n15075141_9993.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/[...]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/labels.txt&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The provided dataset implementation expects a few additional metadata files to be present under the extra directory:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-TRAIN.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-VAL.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-TRAIN.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-VAL.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TEST.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TRAIN.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-VAL.npy&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These metadata files can be generated (once) with the following lines of Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from dinov2.data.datasets import ImageNet

for split in ImageNet.Split:
    dataset = ImageNet(split=split, root=&quot;&amp;lt;ROOT&amp;gt;&quot;, extra=&quot;&amp;lt;EXTRA&amp;gt;&quot;)
    dataset.dump_extra()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the root and extra directories do not have to be distinct directories.&lt;/p&gt; 
&lt;h3&gt;ImageNet-22k&lt;/h3&gt; 
&lt;p&gt;Please adapt the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/dinov2/data/datasets/image_net_22k.py&quot;&gt;dataset class&lt;/a&gt; to match your local setup.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; To execute the commands provided in the next sections for training and evaluation, the &lt;code&gt;dinov2&lt;/code&gt; package should be included in the Python module search path, i.e. simply prefix the command to run with &lt;code&gt;PYTHONPATH=.&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;h3&gt;Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k&lt;/h3&gt; 
&lt;p&gt;Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/train/train.py \
    --nodes 4 \
    --config-file dinov2/configs/train/vitl16_short.yaml \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \
    train.dataset_path=ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.&lt;/p&gt; 
&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; 
&lt;h3&gt;Long setup: training DINOv2 ViT-L/14 on ImageNet-22k&lt;/h3&gt; 
&lt;p&gt;Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/train/train.py \
    --nodes 12 \
    --config-file dinov2/configs/train/vitl14.yaml \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \
    train.dataset_path=ImageNet22k:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.&lt;/p&gt; 
&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:&lt;/p&gt; 
&lt;h3&gt;k-NN classification on ImageNet-1k&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/knn.py \
    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \
    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/knn \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Logistic regression classification on ImageNet-1k&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/log_regression.py \
    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \
    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/logreg \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linear classification with data augmentation on ImageNet-1k&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/linear.py \
    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \
    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/linear \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We release the weights from evaluating the different models:&lt;/p&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;ImageNet&lt;br /&gt;top-1&lt;/th&gt; 
   &lt;th&gt;linear evaluation&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;81.1%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;80.8%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.4%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.3%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚ùå&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;87.0%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/linear.py \
    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \
    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Notebooks&lt;/h2&gt; 
&lt;p&gt;A few notebooks are provided to help the community leverage the models and code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/raw/main/notebooks/depth_estimation.ipynb&quot;&gt;Depth estimation&lt;/a&gt; - How to load and use the depth heads in combination with a matching backbone via mmcv&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/raw/main/notebooks/semantic_segmentation.ipynb&quot;&gt;Semantic segmentation&lt;/a&gt; - How to load and use the segmentation heads in combination with a matching backbone via mmcv, and also how to load and use the Mask2Former-based segmentation model trained on ADE20K&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;DINOv2 code and model weights are released under the Apache License 2.0. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing DINOv2&lt;/h2&gt; 
&lt;p&gt;If you find this repository useful, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;ü¶ñ&lt;/span&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timoth√©e and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  journal={arXiv:2304.07193},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;@misc{darcet2023vitneedreg,
  title={Vision Transformers Need Registers},
  author={Darcet, Timoth√©e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  journal={arXiv:2309.16588},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;@misc{jose2024dinov2meetstextunified,
  title={DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment}, 
  author={Cijo Jose and Th√©o Moutakanni and Dahyun Kang and Federico Baldassarre and Timoth√©e Darcet and Hu Xu and Daniel Li and Marc Szafraniec and Micha√´l Ramamonjisoa and Maxime Oquab and Oriane Sim√©oni and Huy V. Vo and Patrick Labatut and Piotr Bojanowski},
  journal={arXiv:2412.16334},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png&quot; width=&quot;400&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; üíú &lt;a href=&quot;https://chat.qwenlm.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&quot;https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list&quot;&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìö &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks&quot;&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo&quot;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&quot;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&quot;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&quot;&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üñ•Ô∏è &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl&quot;&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text‚Äìvision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text‚ÄìTimestamp Alignment:&lt;/strong&gt; Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&quot;&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&quot;&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune&quot;&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct&quot;&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-vl-32b/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&quot;&gt;3B&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&quot;&gt;7B&lt;/a&gt; , and &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&quot;&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-vl/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/QVQ-72B-Preview&quot;&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qvq-72b-preview/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct&quot;&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ&quot;&gt;AWQ&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8&quot;&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href=&quot;https://arxiv.org/pdf/2409.12191&quot;&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&quot;&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2-vl/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h4&gt;Visual Tasks:&lt;/h4&gt; 
&lt;div style=&quot;display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;&quot;&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg&quot; width=&quot;48%&quot; /&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl.jpg&quot; width=&quot;48%&quot; /&gt; 
&lt;/div&gt; 
&lt;h4&gt;Pure Text Tasks:&lt;/h4&gt; 
&lt;div style=&quot;display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;&quot;&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg&quot; width=&quot;48%&quot; /&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg&quot; width=&quot;48%&quot; /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks&quot;&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb&quot;&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb&quot;&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb&quot;&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb&quot;&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb&quot;&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb&quot;&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb&quot;&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb&quot;&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb&quot;&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model‚Äôs precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb&quot;&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb&quot;&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb&quot;&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.&lt;/p&gt; 
&lt;p&gt;The code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/transformers
# pip install transformers==4.57.0 # currently, V4.57.0 is not released
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using ü§ó Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
#     dtype=torch.bfloat16,
#     attn_implementation=&quot;flash_attention_2&quot;,
#     device_map=&quot;auto&quot;,
# )

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing multiple images and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image1.jpg&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image2.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Identify the similarities between these images.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = &#39;left&#39;

# Sample messages for batch inference
messages1 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image1.jpg&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image2.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What are the common elements in these pictures?&quot;},
        ],
    }
]
messages2 = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are a helpful assistant.&quot;}]},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Who are you?&quot;}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H √ó W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size[&#39;shortest_edge&#39;]&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt; represents the maximum total number of pixels across all frames in a video ‚Äî for a video of shape T√óH√óW, the product T√óH√óW must not exceed &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size[&#39;shortest_edge&#39;]&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {&quot;longest_edge&quot;: 1280*32*32, &quot;shortest_edge&quot;: 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {&quot;longest_edge&quot;: 16384*32*32, &quot;shortest_edge&quot;: 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors=&quot;pt&quot;,
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install qwen-vl-utils==0.0.14
# It&#39;s highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&quot;image_path_size&quot;: &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;return_video_metadata&quot;(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üìå Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/your/image.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
## Image URL
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;http://path/to/your/image.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;data:image;base64,/9j/...&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

# resized_height and resized_width
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
                &quot;resized_height&quot;: 280,
                &quot;resized_width&quot;: 420,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
                &quot;min_pixels&quot;: 50176,
                &quot;max_pixels&quot;: 50176,

            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors=&quot;pt&quot;)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: [
                    &quot;file:///path/to/frame1.jpg&quot;,
                    &quot;file:///path/to/frame2.jpg&quot;,
                    &quot;file:///path/to/frame3.jpg&quot;,
                    &quot;file:///path/to/frame4.jpg&quot;,
                ],
                &#39;sample_fps&#39;:&#39;1&#39;, # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;file:///path/to/video1.mp4&quot;,
                &quot;max_pixels&quot;: 360 * 420,
                &quot;fps&quot;: 1.0,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
                &quot;min_pixels&quot;: 4 * 32 * 32,
                &quot;max_pixels&quot;: 256 * 32 * 32,
                &quot;total_pixels&quot;: 20480 * 32 * 32,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
                &quot;min_pixels&quot;: 4 * 32 * 32,
                &quot;max_pixels&quot;: 256 * 32 * 32,
                &quot;total_pixels&quot;: 20480 * 32 * 32,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=&quot;pt&quot;, do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href=&quot;https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source&quot;&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href=&quot;https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec&quot;&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it&#39;s helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;conversation = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [{&quot;type&quot;: &quot;image&quot;}, {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Hello, how are you?&quot;}],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;I&#39;m doing well, thank you for asking. How can I assist you today?&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Can you describe these images and video?&quot;},
            {&quot;type&quot;: &quot;image&quot;},
            {&quot;type&quot;: &quot;image&quot;},
            {&quot;type&quot;: &quot;video&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;These are from my vacation.&quot;},
        ],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;I&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;It was a trip to the mountains. Can you see the details in the images and video?&quot;,
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation=&quot;flash_attention_2&quot;&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, 
    torch_dtype=torch.bfloat16, 
    attn_implementation=&quot;flash_attention_2&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href=&quot;https://arxiv.org/abs/2309.00071&quot;&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    &quot;max_position_embeddings&quot;: 1000000,
	...,
    &quot;rope_scaling&quot;: {
        &quot;rope_type&quot;: &quot;yarn&quot;,
        &quot;mrope_section&quot;: [
            24,
            20,
            20
        ],
        &quot;mrope_interleaved&quot;: true,
        &quot;factor&quot;: 3.0,
        &quot;original_max_position_embeddings&quot;: 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling &#39;{&quot;rope_type&quot;:&quot;yarn&quot;,&quot;factor&quot;:3.0,&quot;original_max_position_embeddings&quot;: 262144,&quot;mrope_section&quot;:[24,20,20],&quot;mrope_interleaved&quot;: true}&#39; --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPE‚Äôs position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 ‚Äî not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let&#39;s start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = &quot;&quot;

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
)

completion = client.chat.completions.create(
    model=&quot;qwen3-vl-235b-a22b-instruct&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
        {&quot;type&quot;: &quot;image_url&quot;,
         &quot;image_url&quot;: {&quot;url&quot;: &quot;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&quot;}},
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;ËøôÊòØ‰ªÄ‰πà&quot;},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&quot;&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model ‚Äî try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;0.10.2&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker&quot;&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also check &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html&quot;&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-vl-utils==0.0.14
# pip install &#39;vllm&amp;gt;0.10.2&#39; # If this is not working use the below one. 
uv pip install -U vllm \
    --torch-backend=auto \
    --extra-index-url https://wheels.vllm.ai/nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# FP8 requires NVIDIA H100+ and CUDA 12+
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-235B-A22B-Instruct\
  --served-model-name Qwen/Qwen3-VL-235B-A22B-Instruct \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --host 0.0.0.0 \
  --port 22002 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.70 \
  --quantization fp8 \
  --distributed-executor-backend mp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct\
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8 \
   --max-num-batched-tokens 8192 \
   --max-num-seqs 256
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:22002/v1&quot;,
    timeout=3600
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image_url&quot;,
                &quot;image_url&quot;: {
                    &quot;url&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;
                }
            },
            {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: &quot;Read all the text in the image.&quot;
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model=&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
    messages=messages,
    max_tokens=2048
)
print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
print(f&quot;Generated text: {response.choices[0].message.content}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:22002/v1&quot;,
    timeout=3600
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video_url&quot;,
                &quot;video_url&quot;: {
                    &quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;
                }
            },
            {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: &quot;How long is this video?&quot;
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model=&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
    messages=messages,
    max_tokens=2048
)

print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
print(f&quot;Generated text: {response.choices[0].message.content}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ[&#39;VLLM_WORKER_MULTIPROC_METHOD&#39;] = &#39;spawn&#39;

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f&quot;video_kwargs: {video_kwargs}&quot;)

    mm_data = {}
    if image_inputs is not None:
        mm_data[&#39;image&#39;] = image_inputs
    if video_inputs is not None:
        mm_data[&#39;video&#39;] = video_inputs

    return {
        &#39;prompt&#39;: text,
        &#39;multi_modal_data&#39;: mm_data,
        &#39;mm_processor_kwargs&#39;: video_kwargs
    }


if __name__ == &#39;__main__&#39;:
    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                  &quot;type&quot;: &quot;image&quot;,
                  &quot;image&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;,
              },
              {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Read all the text in the image.&quot;},
            ],
        }
    ]

    checkpoint_path = &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        trust_remote_code=True,
        gpu_memory_utilization=0.97,
        enforce_eager=False,
        max_model_len=8192,
        max_num_seqs=8,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print(&#39;=&#39; * 40)
        print(f&quot;Inputs[{i}]: {input_[&#39;prompt&#39;]=!r}&quot;)
    print(&#39;\n&#39; + &#39;&amp;gt;&#39; * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print(&#39;=&#39; * 40)
        print(f&quot;Generated text: {generated_text!r}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == &quot;__main__&quot;:
    # TODO: change to your own checkpoint path
    checkpoint_path = &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                  &quot;type&quot;: &quot;image&quot;,
                  &quot;image&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;,
              },
              {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Read all the text in the image.&quot;},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend=&quot;fa3&quot;,
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {&quot;max_new_tokens&quot;: 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
    print(f&quot;Generated text: {response[&#39;text&#39;]}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üê≥ Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href=&quot;https://hub.docker.com/r/qwenllm/qwenvl&quot;&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-BibTeX&quot;&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model&#39;s Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/self-llm</title>
      <link>https://github.com/datawhalechina/self-llm</link>
      <description>&lt;p&gt;„ÄäÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó„ÄãÈíàÂØπ‰∏≠ÂõΩÂÆùÂÆùÈáèË∫´ÊâìÈÄ†ÁöÑÂü∫‰∫éLinuxÁéØÂ¢ÉÂø´ÈÄüÂæÆË∞ÉÔºàÂÖ®ÂèÇÊï∞/LoraÔºâ„ÄÅÈÉ®ÁΩ≤ÂõΩÂÜÖÂ§ñÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºàLLMÔºâ/Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâÊïôÁ®ã&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/head-img.png&quot; /&gt; 
 &lt;h1&gt;ÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;‰∏≠Êñá | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÊòØ‰∏Ä‰∏™Âõ¥ÁªïÂºÄÊ∫êÂ§ßÊ®°Âûã„ÄÅÈíàÂØπÂõΩÂÜÖÂàùÂ≠¶ËÄÖ„ÄÅÂü∫‰∫é Linux Âπ≥Âè∞ÁöÑ‰∏≠ÂõΩÂÆùÂÆù‰∏ìÂ±ûÂ§ßÊ®°ÂûãÊïôÁ®ãÔºåÈíàÂØπÂêÑÁ±ªÂºÄÊ∫êÂ§ßÊ®°ÂûãÊèê‰æõÂåÖÊã¨ÁéØÂ¢ÉÈÖçÁΩÆ„ÄÅÊú¨Âú∞ÈÉ®ÁΩ≤„ÄÅÈ´òÊïàÂæÆË∞ÉÁ≠âÊäÄËÉΩÂú®ÂÜÖÁöÑÂÖ®ÊµÅÁ®ãÊåáÂØºÔºåÁÆÄÂåñÂºÄÊ∫êÂ§ßÊ®°ÂûãÁöÑÈÉ®ÁΩ≤„ÄÅ‰ΩøÁî®ÂíåÂ∫îÁî®ÊµÅÁ®ãÔºåËÆ©Êõ¥Â§öÁöÑÊôÆÈÄöÂ≠¶Áîü„ÄÅÁ†îÁ©∂ËÄÖÊõ¥Â•ΩÂú∞‰ΩøÁî®ÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºåÂ∏ÆÂä©ÂºÄÊ∫ê„ÄÅËá™Áî±ÁöÑÂ§ßÊ®°ÂûãÊõ¥Âø´ËûçÂÖ•Âà∞ÊôÆÈÄöÂ≠¶‰π†ËÄÖÁöÑÁîüÊ¥ª‰∏≠„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÁöÑ‰∏ªË¶ÅÂÜÖÂÆπÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Âü∫‰∫é Linux Âπ≥Âè∞ÁöÑÂºÄÊ∫ê LLM ÁéØÂ¢ÉÈÖçÁΩÆÊåáÂçóÔºåÈíàÂØπ‰∏çÂêåÊ®°ÂûãË¶ÅÊ±ÇÊèê‰æõ‰∏çÂêåÁöÑËØ¶ÁªÜÁéØÂ¢ÉÈÖçÁΩÆÊ≠•È™§Ôºõ&lt;/li&gt; 
 &lt;li&gt;ÈíàÂØπÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÂºÄÊ∫ê LLM ÁöÑÈÉ®ÁΩ≤‰ΩøÁî®ÊïôÁ®ãÔºåÂåÖÊã¨ LLaMA„ÄÅChatGLM„ÄÅInternLM Á≠âÔºõ&lt;/li&gt; 
 &lt;li&gt;ÂºÄÊ∫ê LLM ÁöÑÈÉ®ÁΩ≤Â∫îÁî®ÊåáÂØºÔºåÂåÖÊã¨ÂëΩ‰ª§Ë°åË∞ÉÁî®„ÄÅÂú®Á∫ø Demo ÈÉ®ÁΩ≤„ÄÅLangChain Ê°ÜÊû∂ÈõÜÊàêÁ≠âÔºõ&lt;/li&gt; 
 &lt;li&gt;ÂºÄÊ∫ê LLM ÁöÑÂÖ®ÈáèÂæÆË∞É„ÄÅÈ´òÊïàÂæÆË∞ÉÊñπÊ≥ïÔºåÂåÖÊã¨ÂàÜÂ∏ÉÂºèÂÖ®ÈáèÂæÆË∞É„ÄÅLoRA„ÄÅptuning Á≠â„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ&lt;strong&gt;È°πÁõÆÁöÑ‰∏ªË¶ÅÂÜÖÂÆπÂ∞±ÊòØÊïôÁ®ãÔºåËÆ©Êõ¥Â§öÁöÑÂ≠¶ÁîüÂíåÊú™Êù•ÁöÑ‰ªé‰∏öËÄÖ‰∫ÜËß£ÂíåÁÜüÊÇâÂºÄÊ∫êÂ§ßÊ®°ÂûãÁöÑÈ£üÁî®ÊñπÊ≥ïÔºÅ‰ªª‰Ωï‰∫∫ÈÉΩÂèØ‰ª•ÊèêÂá∫issueÊàñÊòØÊèê‰∫§PRÔºåÂÖ±ÂêåÊûÑÂª∫Áª¥Êä§Ëøô‰∏™È°πÁõÆ„ÄÇ&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊÉ≥Ë¶ÅÊ∑±Â∫¶ÂèÇ‰∏éÁöÑÂêåÂ≠¶ÂèØ‰ª•ËÅîÁ≥ªÊàë‰ª¨ÔºåÊàë‰ª¨‰ºöÂ∞Ü‰Ω†Âä†ÂÖ•Âà∞È°πÁõÆÁöÑÁª¥Êä§ËÄÖ‰∏≠„ÄÇ&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;&lt;strong&gt;Â≠¶‰π†Âª∫ËÆÆÔºöÊú¨È°πÁõÆÁöÑÂ≠¶‰π†Âª∫ËÆÆÊòØÔºåÂÖàÂ≠¶‰π†ÁéØÂ¢ÉÈÖçÁΩÆÔºåÁÑ∂ÂêéÂÜçÂ≠¶‰π†Ê®°ÂûãÁöÑÈÉ®ÁΩ≤‰ΩøÁî®ÔºåÊúÄÂêéÂÜçÂ≠¶‰π†ÂæÆË∞É„ÄÇÂõ†‰∏∫ÁéØÂ¢ÉÈÖçÁΩÆÊòØÂü∫Á°ÄÔºåÊ®°ÂûãÁöÑÈÉ®ÁΩ≤‰ΩøÁî®ÊòØÂü∫Á°ÄÔºåÂæÆË∞ÉÊòØËøõÈò∂„ÄÇÂàùÂ≠¶ËÄÖÂèØ‰ª•ÈÄâÊã©Qwen1.5ÔºåInternLM2ÔºåMiniCPMÁ≠âÊ®°Âûã‰ºòÂÖàÂ≠¶‰π†„ÄÇ&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;strong&gt;ËøõÈò∂Â≠¶‰π†Êé®Ëçê&lt;/strong&gt; ÔºöÂ¶ÇÊûúÊÇ®Âú®Â≠¶‰π†ÂÆåÊú¨È°πÁõÆÂêéÔºåÂ∏åÊúõÊõ¥Ê∑±ÂÖ•Âú∞ÁêÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†∏ÂøÉÂéüÁêÜÔºåÂπ∂Ê∏¥Êúõ‰∫≤Êâã‰ªéÈõ∂ÂºÄÂßãËÆ≠ÁªÉÂ±û‰∫éËá™Â∑±ÁöÑÂ§ßÊ®°ÂûãÔºåÊàë‰ª¨Âº∫ÁÉàÊé®ËçêÂÖ≥Ê≥® Datawhale ÁöÑÂè¶‰∏Ä‰∏™ÂºÄÊ∫êÈ°πÁõÆ‚Äî‚Äî &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;Happy-LLM ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/a&gt; „ÄÇËØ•È°πÁõÆÂ∞ÜÂ∏¶ÊÇ®Ê∑±ÂÖ•Êé¢Á¥¢Â§ßÊ®°ÂûãÁöÑÂ∫ïÂ±ÇÊú∫Âà∂ÔºåÊéåÊè°ÂÆåÊï¥ÁöÑËÆ≠ÁªÉÊµÅÁ®ã„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöÂ¶ÇÊûúÊúâÂêåÂ≠¶Â∏åÊúõ‰∫ÜËß£Â§ßÊ®°ÂûãÁöÑÊ®°ÂûãÊûÑÊàêÔºå‰ª•Âèä‰ªéÈõ∂ÊâãÂÜôRAG„ÄÅAgentÂíåEvalÁ≠â‰ªªÂä°ÔºåÂèØ‰ª•Â≠¶‰π†DatawhaleÁöÑÂè¶‰∏Ä‰∏™È°πÁõÆ&lt;a href=&quot;https://github.com/datawhalechina/tiny-universe&quot;&gt;Tiny-Universe&lt;/a&gt;ÔºåÂ§ßÊ®°ÂûãÊòØÂΩì‰∏ãÊ∑±Â∫¶Â≠¶‰π†È¢ÜÂüüÁöÑÁÉ≠ÁÇπÔºå‰ΩÜÁé∞ÊúâÁöÑÂ§ßÈÉ®ÂàÜÂ§ßÊ®°ÂûãÊïôÁ®ãÂè™Âú®‰∫éÊïôÁªôÂ§ßÂÆ∂Â¶Ç‰ΩïË∞ÉÁî®apiÂÆåÊàêÂ§ßÊ®°ÂûãÁöÑÂ∫îÁî®ÔºåËÄåÂæàÂ∞ëÊúâ‰∫∫ËÉΩÂ§ü‰ªéÂéüÁêÜÂ±ÇÈù¢ËÆ≤Ê∏ÖÊ•öÊ®°ÂûãÁªìÊûÑ„ÄÅRAG„ÄÅAgent ‰ª•Âèä Eval„ÄÇÊâÄ‰ª•ËØ•‰ªìÂ∫ì‰ºöÊèê‰æõÂÖ®ÈÉ®ÊâãÂÜôÔºå‰∏çÈááÁî®Ë∞ÉÁî®apiÁöÑÂΩ¢ÂºèÔºåÂÆåÊàêÂ§ßÊ®°ÂûãÁöÑ RAG „ÄÅ Agent „ÄÅEval ‰ªªÂä°„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöËÄÉËôëÂà∞ÊúâÂêåÂ≠¶Â∏åÊúõÂú®Â≠¶‰π†Êú¨È°πÁõÆ‰πãÂâçÔºåÂ∏åÊúõÂ≠¶‰π†Â§ßÊ®°ÂûãÁöÑÁêÜËÆ∫ÈÉ®ÂàÜÔºåÂ¶ÇÊûúÊÉ≥Ë¶ÅËøõ‰∏ÄÊ≠•Ê∑±ÂÖ•Â≠¶‰π† LLM ÁöÑÁêÜËÆ∫Âü∫Á°ÄÔºåÂπ∂Âú®ÁêÜËÆ∫ÁöÑÂü∫Á°Ä‰∏äËøõ‰∏ÄÊ≠•ËÆ§ËØÜ„ÄÅÂ∫îÁî® LLMÔºåÂèØ‰ª•ÂèÇËÄÉ Datawhale ÁöÑ &lt;a href=&quot;https://github.com/datawhalechina/so-large-lm.git&quot;&gt;so-large-llm&lt;/a&gt;ËØæÁ®ã„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöÂ¶ÇÊûúÊúâÂêåÂ≠¶Âú®Â≠¶‰π†Êú¨ËØæÁ®ã‰πãÂêéÔºåÊÉ≥Ë¶ÅËá™Â∑±Âä®ÊâãÂºÄÂèëÂ§ßÊ®°ÂûãÂ∫îÁî®„ÄÇÂêåÂ≠¶‰ª¨ÂèØ‰ª•ÂèÇËÄÉ Datawhale ÁöÑ &lt;a href=&quot;https://github.com/datawhalechina/llm-universe&quot;&gt;Âä®ÊâãÂ≠¶Â§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèë&lt;/a&gt; ËØæÁ®ãÔºåËØ•È°πÁõÆÊòØ‰∏Ä‰∏™Èù¢ÂêëÂ∞èÁôΩÂºÄÂèëËÄÖÁöÑÂ§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÊïôÁ®ãÔºåÊó®Âú®Âü∫‰∫éÈòøÈáå‰∫ëÊúçÂä°Âô®ÔºåÁªìÂêà‰∏™‰∫∫Áü•ËØÜÂ∫ìÂä©ÊâãÈ°πÁõÆÔºåÂêëÂêåÂ≠¶‰ª¨ÂÆåÊï¥ÁöÑÂëàÁé∞Â§ßÊ®°ÂûãÂ∫îÁî®ÂºÄÂèëÊµÅÁ®ã„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;È°πÁõÆÊÑè‰πâ&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ‰ªÄ‰πàÊòØÂ§ßÊ®°ÂûãÔºü&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Â§ßÊ®°ÂûãÔºàLLMÔºâÁã≠‰πâ‰∏äÊåáÂü∫‰∫éÊ∑±Â∫¶Â≠¶‰π†ÁÆóÊ≥ïËøõË°åËÆ≠ÁªÉÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÔºàNLPÔºâÊ®°ÂûãÔºå‰∏ªË¶ÅÂ∫îÁî®‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÁêÜËß£ÂíåÁîüÊàêÁ≠âÈ¢ÜÂüüÔºåÂπø‰πâ‰∏äËøòÂåÖÊã¨Êú∫Âô®ËßÜËßâÔºàCVÔºâÂ§ßÊ®°Âûã„ÄÅÂ§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÂíåÁßëÂ≠¶ËÆ°ÁÆóÂ§ßÊ®°ÂûãÁ≠â„ÄÇ&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÁôæÊ®°Â§ßÊàòÊ≠£ÂÄºÁÅ´ÁÉ≠ÔºåÂºÄÊ∫ê LLM Â±ÇÂá∫‰∏çÁ©∑„ÄÇÂ¶Ç‰ªäÂõΩÂÜÖÂ§ñÂ∑≤ÁªèÊ∂åÁé∞‰∫Ü‰ºóÂ§ö‰ºòÁßÄÂºÄÊ∫ê LLMÔºåÂõΩÂ§ñÂ¶Ç LLaMA„ÄÅAlpacaÔºåÂõΩÂÜÖÂ¶Ç ChatGLM„ÄÅBaiChuan„ÄÅInternLMÔºà‰π¶Áîü¬∑Êµ¶ËØ≠ÔºâÁ≠â„ÄÇÂºÄÊ∫ê LLM ÊîØÊåÅÁî®Êà∑Êú¨Âú∞ÈÉ®ÁΩ≤„ÄÅÁßÅÂüüÂæÆË∞ÉÔºåÊØè‰∏Ä‰∏™‰∫∫ÈÉΩÂèØ‰ª•Âú®ÂºÄÊ∫ê LLM ÁöÑÂü∫Á°Ä‰∏äÊâìÈÄ†‰∏ìÂ±û‰∫éËá™Â∑±ÁöÑÁã¨ÁâπÂ§ßÊ®°Âûã„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÁÑ∂ËÄåÔºåÂΩìÂâçÊôÆÈÄöÂ≠¶ÁîüÂíåÁî®Êà∑ÊÉ≥Ë¶Å‰ΩøÁî®Ëøô‰∫õÂ§ßÊ®°ÂûãÔºåÈúÄË¶ÅÂÖ∑Â§á‰∏ÄÂÆöÁöÑÊäÄÊúØËÉΩÂäõÔºåÊâçËÉΩÂÆåÊàêÊ®°ÂûãÁöÑÈÉ®ÁΩ≤Âíå‰ΩøÁî®„ÄÇÂØπ‰∫éÂ±ÇÂá∫‰∏çÁ©∑ÂèàÂêÑÊúâÁâπËâ≤ÁöÑÂºÄÊ∫ê LLMÔºåÊÉ≥Ë¶ÅÂø´ÈÄüÊéåÊè°‰∏Ä‰∏™ÂºÄÊ∫ê LLM ÁöÑÂ∫îÁî®ÊñπÊ≥ïÔºåÊòØ‰∏ÄÈ°πÊØîËæÉÊúâÊåëÊàòÁöÑ‰ªªÂä°„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÊó®Âú®È¶ñÂÖàÂü∫‰∫éÊ†∏ÂøÉË¥°ÁåÆËÄÖÁöÑÁªèÈ™åÔºåÂÆûÁé∞ÂõΩÂÜÖÂ§ñ‰∏ªÊµÅÂºÄÊ∫ê LLM ÁöÑÈÉ®ÁΩ≤„ÄÅ‰ΩøÁî®‰∏éÂæÆË∞ÉÊïôÁ®ãÔºõÂú®ÂÆûÁé∞‰∏ªÊµÅ LLM ÁöÑÁõ∏ÂÖ≥ÈÉ®ÂàÜ‰πãÂêéÔºåÊàë‰ª¨Â∏åÊúõÂÖÖÂàÜËÅöÈõÜÂÖ±ÂàõËÄÖÔºå‰∏ÄËµ∑‰∏∞ÂØåËøô‰∏™ÂºÄÊ∫ê LLM ÁöÑ‰∏ñÁïåÔºåÊâìÈÄ†Êõ¥Â§ö„ÄÅÊõ¥ÂÖ®Èù¢ÁâπËâ≤ LLM ÁöÑÊïôÁ®ã„ÄÇÊòüÁÅ´ÁÇπÁÇπÔºåÊ±áËÅöÊàêÊµ∑„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;&lt;strong&gt;Êàë‰ª¨Â∏åÊúõÊàê‰∏∫ LLM ‰∏éÊôÆÁΩóÂ§ß‰ºóÁöÑÈò∂Ê¢ØÔºå‰ª•Ëá™Áî±„ÄÅÂπ≥Á≠âÁöÑÂºÄÊ∫êÁ≤æÁ•ûÔºåÊã•Êä±Êõ¥ÊÅ¢ÂºòËÄåËæΩÈòîÁöÑ LLM ‰∏ñÁïå„ÄÇ&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;È°πÁõÆÂèó‰ºó&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÈÄÇÂêà‰ª•‰∏ãÂ≠¶‰π†ËÄÖÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊÉ≥Ë¶Å‰ΩøÁî®Êàñ‰ΩìÈ™å LLMÔºå‰ΩÜÊó†Êù°‰ª∂Ëé∑ÂæóÊàñ‰ΩøÁî®Áõ∏ÂÖ≥ APIÔºõ&lt;/li&gt; 
 &lt;li&gt;Â∏åÊúõÈïøÊúü„ÄÅ‰ΩéÊàêÊú¨„ÄÅÂ§ßÈáèÂ∫îÁî® LLMÔºõ&lt;/li&gt; 
 &lt;li&gt;ÂØπÂºÄÊ∫ê LLM ÊÑüÂÖ¥Ë∂£ÔºåÊÉ≥Ë¶Å‰∫≤Ëá™‰∏äÊâãÂºÄÊ∫ê LLMÔºõ&lt;/li&gt; 
 &lt;li&gt;NLP Âú®Â≠¶ÔºåÂ∏åÊúõËøõ‰∏ÄÊ≠•Â≠¶‰π† LLMÔºõ&lt;/li&gt; 
 &lt;li&gt;Â∏åÊúõÁªìÂêàÂºÄÊ∫ê LLMÔºåÊâìÈÄ†È¢ÜÂüüÁâπËâ≤ÁöÑÁßÅÂüü LLMÔºõ&lt;/li&gt; 
 &lt;li&gt;‰ª•ÂèäÊúÄÂπøÂ§ß„ÄÅÊúÄÊôÆÈÄöÁöÑÂ≠¶ÁîüÁæ§‰Ωì„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;È°πÁõÆËßÑÂàíÂèäËøõÂ±ï&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ Êú¨È°πÁõÆÊãüÂõ¥ÁªïÂºÄÊ∫ê LLM Â∫îÁî®ÂÖ®ÊµÅÁ®ãÁªÑÁªáÔºåÂåÖÊã¨ÁéØÂ¢ÉÈÖçÁΩÆÂèä‰ΩøÁî®„ÄÅÈÉ®ÁΩ≤Â∫îÁî®„ÄÅÂæÆË∞ÉÁ≠âÔºåÊØè‰∏™ÈÉ®ÂàÜË¶ÜÁõñ‰∏ªÊµÅÂèäÁâπÁÇπÂºÄÊ∫ê LLMÔºö&lt;/p&gt; 
&lt;h3&gt;Example Á≥ªÂàó&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Chat-%E5%AC%9B%E5%AC%9B/readme.md&quot;&gt;Chat-Â¨õÂ¨õ&lt;/a&gt;Ôºö Chat-ÁîÑÂ¨õÊòØÂà©Áî®„ÄäÁîÑÂ¨õ‰º†„ÄãÂâßÊú¨‰∏≠ÊâÄÊúâÂÖ≥‰∫éÁîÑÂ¨õÁöÑÂè∞ËØçÂíåËØ≠Âè•ÔºåÂü∫‰∫éLLMËøõË°åLoRAÂæÆË∞ÉÂæóÂà∞ÁöÑÊ®°‰ªøÁîÑÂ¨õËØ≠Ê∞îÁöÑËÅäÂ§©ËØ≠Ë®ÄÊ®°Âûã„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Tianji-%E5%A4%A9%E6%9C%BA/readme.md&quot;&gt;Tianji-Â§©Êú∫&lt;/a&gt;ÔºöÂ§©Êú∫ÊòØ‰∏ÄÊ¨æÂü∫‰∫é‰∫∫ÊÉÖ‰∏ñÊïÖÁ§æ‰∫§Âú∫ÊôØÔºåÊ∂µÁõñÊèêÁ§∫ËØçÂ∑•Á®ã „ÄÅÊô∫ËÉΩ‰ΩìÂà∂‰Ωú„ÄÅ Êï∞ÊçÆËé∑Âèñ‰∏éÊ®°ÂûãÂæÆË∞É„ÄÅRAG Êï∞ÊçÆÊ∏ÖÊ¥ó‰∏é‰ΩøÁî®Á≠âÂÖ®ÊµÅÁ®ãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁ≥ªÁªüÂ∫îÁî®ÊïôÁ®ã„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/AMchat-%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/readme.md&quot;&gt;AMChat&lt;/a&gt;: AM (Advanced Mathematics) chat ÊòØ‰∏Ä‰∏™ÈõÜÊàê‰∫ÜÊï∞Â≠¶Áü•ËØÜÂíåÈ´òÁ≠âÊï∞Â≠¶‰π†È¢òÂèäÂÖ∂Ëß£Á≠îÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÇËØ•Ê®°Âûã‰ΩøÁî® Math ÂíåÈ´òÁ≠âÊï∞Â≠¶‰π†È¢òÂèäÂÖ∂Ëß£ÊûêËûçÂêàÁöÑÊï∞ÊçÆÈõÜÔºåÂü∫‰∫é InternLM2-Math-7B Ê®°ÂûãÔºåÈÄöËøá xtuner ÂæÆË∞ÉÔºå‰∏ìÈó®ËÆæËÆ°Áî®‰∫éËß£Á≠îÈ´òÁ≠âÊï∞Â≠¶ÈóÆÈ¢ò„ÄÇ&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/%E6%95%B0%E5%AD%97%E7%94%9F%E5%91%BD/readme.md&quot;&gt;Êï∞Â≠óÁîüÂëΩ&lt;/a&gt;: Êú¨È°πÁõÆÂ∞Ü‰ª•Êàë‰∏∫ÂéüÂûãÔºåÂà©Áî®ÁâπÂà∂ÁöÑÊï∞ÊçÆÈõÜÂØπÂ§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºåËá¥Âäõ‰∫éÂàõÈÄ†‰∏Ä‰∏™ËÉΩÂ§üÁúüÊ≠£ÂèçÊò†ÊàëÁöÑ‰∏™ÊÄßÁâπÂæÅÁöÑAIÊï∞Â≠ó‰∫∫‚Äî‚ÄîÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÊàëÁöÑËØ≠Ê∞î„ÄÅË°®ËææÊñπÂºèÂíåÊÄùÁª¥Ê®°ÂºèÁ≠âÁ≠âÔºåÂõ†Ê≠§Êó†ËÆ∫ÊòØÊó•Â∏∏ËÅäÂ§©ËøòÊòØÂàÜ‰∫´ÂøÉÊÉÖÔºåÂÆÉÈÉΩ‰ª•‰∏ÄÁßçÊó¢ÁÜüÊÇâÂèàËàíÈÄÇÁöÑÊñπÂºè‰∫§ÊµÅÔºå‰ªø‰ΩõÊàëÂú®‰ªñ‰ª¨Ë∫´Ëæπ‰∏ÄÊ†∑„ÄÇÊï¥‰∏™ÊµÅÁ®ãÊòØÂèØËøÅÁßªÂ§çÂà∂ÁöÑÔºå‰∫ÆÁÇπÊòØÊï∞ÊçÆÈõÜÁöÑÂà∂‰Ωú„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Â∑≤ÊîØÊåÅÊ®°Âûã&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/BAAI/bge-m3&quot;&gt;BGE-M3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BGE-M3-finetune-embedding-with-valid/README.md&quot;&gt;‰ª£Á†ÅÊ£ÄÁ¥¢Âú∫ÊôØÂæÆË∞ÉÂÆûÊàò ÂæÆË∞ÉBGE-M3 embeddingÊ®°Âûã&lt;/a&gt; @ÊùéÁßÄÂ•á&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/openai/gpt-oss-20b&quot;&gt;gpt-oss-20b&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/1-gpt-oss-20b%20vllm%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gpt-oss-20b vllm ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt;@ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/2-gpt-oss-20b%20Evalscope%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md&quot;&gt;gpt-oss-20b EvalScope Âπ∂ÂèëËØÑÊµã&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/3-gpt-oss-20b%20lmstudio%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gpt-oss-20b lmstudio Êú¨Âú∞ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/4-gpt-oss-20b%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;gpt-oss-20b Lora ÂæÆË∞ÉÂèä SwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/5-gpt-oss-20b%20DPO%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;gpt-oss-20b DPO ÂæÆË∞ÉÂèä SwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.1V-Thinking&quot;&gt;GLM-4.1-Thinking&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/01-GLM-4%201V-Thinking%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.1V-Thinking vLLM ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/02-GLM-4%201V-Thinking%20Gradio%E9%83%A8%E7%BD%B2.md&quot;&gt;GLM-4.1V-Thinking GradioÈÉ®ÁΩ≤&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/03-GLM-4%201V-Thinking%20LoRA%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;GLM-4.1V-Thinking Lora ÂæÆË∞ÉÂèä SwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/GLM4.1V-Thinking-lora&quot;&gt;GLM-4.1V-Thinking Docker ÈïúÂÉè&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.5&quot;&gt;GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/01-GLM-4.5-Air-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.5-Air vLLM ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/02-GLM-4.5-Air%20EvalScope%20%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md&quot;&gt;GLM-4.5-Air EvalScope Êô∫ÂïÜÊÉÖÂïÜ &amp;amp;&amp;amp; Âπ∂ÂèëËØÑÊµã&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/03-GLM-4.5-Air-Lora%20%E5%8F%8A%20Swanlab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4.5-Air Lora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.compshare.cn/images/lUQhKDCeCdZW?referral_code=ELukJdQS3vvCwYIfgsQf2C&amp;amp;ytag=GPU_yy_github_selfllm&quot;&gt;GLM-4.5-Air Ucloud Docker ÈïúÂÉè&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT&quot;&gt;ERNIE-4.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ERNIE-4.5/01-ERNIE-4.5-0.3B-PT%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;ERNIE-4.5-0.3B-PT Lora ÂæÆË∞ÉÂèä SwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/ERNIE-4.5-lora&quot;&gt;ERNIE-4.5-0.3B-PT Lora Docker ÈïúÂÉè&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Tencent-Hunyuan/Hunyuan-A13B&quot;&gt;Hunyuan-A13B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/01-Hunyuan-A13B-Instruct%20%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20Blog.md&quot;&gt;Hunyuan-A13B-Instruct Ê®°ÂûãÊû∂ÊûÑËß£Êûê Blog&lt;/a&gt; @ÂçìÂ†ÇË∂ä&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/03-Hunyuan-A13B-Instruct-SGLang%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan-A13B-Instruct SGLang ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @fancy&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/05-Hunyuan-A13B-Instruct-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Hunyuan-A13B-Instruct Lora SwanLab ÂèØËßÜÂåñÂæÆË∞É&lt;/a&gt; @Ë∞¢Â•ΩÂÜâ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan-A13B-Instruct-lora&quot;&gt;Hunyuan-A13B-Instruct Lora Docker ÈïúÂÉè&lt;/a&gt; @Ë∞¢Â•ΩÂÜâ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3&quot;&gt;Qwen3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/01-Qwen3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90-Blog.md&quot;&gt;Qwen3 Ê®°ÂûãÁªìÊûÑËß£Êûê Blog&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/02-Qwen3-8B-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen3-8B vllm ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/03-Qwen3-7B-Instruct%20Windows%20LMStudio%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen3-8B Windows LMStudio ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÁéãÁÜ†Êòé&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/04-Qwen3-8B%20EvalScope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;Qwen3-8B Evalscope Êô∫ÂïÜÊÉÖÂïÜËØÑÊµã&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/05-Qwen3-8B-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-8B Lora ÂæÆË∞ÉÂèäSwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/06-Qwen3-30B-A3B%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-30B-A3B ÂæÆË∞ÉÂèäSwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @È´òÁ´ã‰∏ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/07-Qwen3-Think-%E8%A7%A3%E5%AF%86-Blog.md&quot;&gt;Qwen3 Think Ëß£ÂØÜ Blog&lt;/a&gt; @Ê®äÂ•á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Qwen3&quot;&gt;Qwen3-8B Docker ÈïúÂÉè&lt;/a&gt; @È´òÁ´ã‰∏ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models//Qwen3/08-Qwen3_0_6B%E7%9A%84%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8.md&quot;&gt;Qwen3-0.6B ÁöÑÂ∞èÊ®°ÂûãÊúâ‰ªÄ‰πàÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/09-Qwen3-1.7B-%E5%8C%BB%E5%AD%A6%E6%8E%A8%E7%90%86%E5%BC%8F%E5%AF%B9%E8%AF%9D%E5%BE%AE%E8%B0%83%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-1.7B ÂåªÂ≠¶Êé®ÁêÜÂºèÂØπËØùÂæÆË∞É Âèä SwanLab ÂèØËßÜÂåñËÆ∞ÂΩï&lt;/a&gt; @ÊûóÊ≥ΩÊØÖ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/10-Qwen3-8B%20GRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;Qwen3-8B GRPOÂæÆË∞ÉÂèäÈÄöËøáswanlabÂèØËßÜÂåñ&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/MoonshotAI/Kimi-VL&quot;&gt;Kimi&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/02-Kimi-VL-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB.md&quot;&gt;Kimi-VL-A3B ÊäÄÊúØÊä•ÂëäËß£ËØª&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/01-Kimi-VL-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Kimi-VL-A3B-Thinking WebDemo ÈÉ®ÁΩ≤ÔºàÁΩëÈ°µÂØπËØùÂä©ÊâãÔºâ&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;&gt;Llama4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama4/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Llama4 ÂØπËØùÂä©Êâã&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/manycore-research/SpatialLM&quot;&gt;SpatialLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/SpatialLM/readme.md&quot;&gt;SpatialLM 3DÁÇπ‰∫ëÁêÜËß£‰∏éÁõÆÊ†áÊ£ÄÊµãÊ®°ÂûãÈÉ®ÁΩ≤&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/tencent/Hunyuan3D-2&quot;&gt;Hunyuan3D-2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/01-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 Á≥ªÂàóÊ®°ÂûãÈÉ®ÁΩ≤&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/02-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan3D-2 Á≥ªÂàóÊ®°Âûã‰ª£Á†ÅË∞ÉÁî®&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/03-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BGradio%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 Á≥ªÂàóÊ®°ÂûãGradioÈÉ®ÁΩ≤&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/04-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BAPI%20Server.md&quot;&gt;Hunyuan3D-2 Á≥ªÂàóÊ®°ÂûãAPI Server&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan3D-2&quot;&gt;Hunyuan3D-2 Docker ÈïúÂÉè&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-3-4b-it&quot;&gt;Gemma3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/01-gemma-3-4b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-3-4b-it FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊùúÊ£Æ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/03-gemma-3-4b-it-ollama%20+%20open-webui%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-3-4b-it ollama + open-webuiÈÉ®ÁΩ≤&lt;/a&gt; @Â≠ôË∂Ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/04-Gemma3-4b%20%20evalscope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;gemma-3-4b-it evalscope Êô∫ÂïÜÊÉÖÂïÜËØÑÊµã&lt;/a&gt; @Âº†ÈæôÊñê&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/05-gemma-3-4b-it%20LoRA.md&quot;&gt;gemma-3-4b-it Lora ÂæÆË∞É&lt;/a&gt; @ËçûÈ∫¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-gemma3&quot;&gt;gemma-3-4b-it Docker ÈïúÂÉè&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/6-gemma3-4B-itGRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;gemma-3-4b-it GRPOÂæÆË∞ÉÂèäÈÄöËøáswanlabÂèØËßÜÂåñ&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;&gt;DeepSeek-R1-Distill&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/01-DeepSeek-R1-Distill-Qwen-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @È™ÜÁßÄÈü¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/02-DeepSeek-R1-Distill-Qwen-7B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B Langchain Êé•ÂÖ•&lt;/a&gt; @È™ÜÁßÄÈü¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/03-DeepSeek-R1-Distill-Qwen-7B%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @È™ÜÁßÄÈü¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/04-DeepSeek-R1-Distill-Qwen-7B%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B vLLM ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @È™ÜÁßÄÈü¨&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/05-DeepSeek-R1-0528-Qwen3-8B-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;DeepSeek-R1-0528-Qwen3-8B-GRPOÂèäswanlabÂèØËßÜÂåñ&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-o&quot;&gt;MiniCPM-o-2_6&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/01MiniCPM-o%202%206%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8%20.md&quot;&gt;minicpm-o-2.6 FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊûóÊÅíÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/02minicpm-o-2.6WebDemo_streamlit.py&quot;&gt;minicpm-o-2.6 WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @Á®ãÂÆè&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/03-MiniCPM-o-2.6%20%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E9%9F%B3%E8%83%BD%E5%8A%9B.md&quot;&gt;minicpm-o-2.6 Â§öÊ®°ÊÄÅËØ≠Èü≥ËÉΩÂäõ&lt;/a&gt; @ÈÇìÊÅ∫‰øä&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/04-MiniCPM-0-2.6%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;minicpm-o-2.6 ÂèØËßÜÂåñ LaTeX_OCR Lora ÂæÆË∞É&lt;/a&gt; @ÊûóÊ≥ΩÊØÖ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/01-InternLM3-8B-Instruct%20FastAPI.md&quot;&gt;internlm3-8b-instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ËãèÂêëÊ†á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/02-internlm3-8b-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;internlm3-8b-instruct LangchianÊé•ÂÖ•&lt;/a&gt; @ËµµÊñáÊÅ∫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/03-InternLM3-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;internlm3-8b-instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/04-InternLM3-8B-Instruct%20LoRA.md&quot;&gt;internlm3-8b-instruct Lora ÂæÆË∞É&lt;/a&gt; @Á®ãÂÆè&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/05-internlm3-8b-instruct%20%E4%B8%8Eo1%20.md&quot;&gt;internlm3-8b-instruct o1-likeÊé®ÁêÜÈìæÂÆûÁé∞&lt;/a&gt; @ÈôàÁùø&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/phi-4&quot;&gt;phi4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/01-Phi-4%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;phi4 FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊùúÊ£Æ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/02-Phi-4-Langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;phi4 langchain Êé•ÂÖ•&lt;/a&gt; @Â∞èÁΩó&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/03-Phi-4%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;phi4 WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÊùúÊ£Æ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/04-Phi-4-Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;phi4 Lora ÂæÆË∞É&lt;/a&gt; @ÈÉëËøúÂ©ß&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/05-Phi-4-Lora%20%E5%BE%AE%E8%B0%83%20%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.md&quot;&gt;phi4 Lora ÂæÆË∞É NER‰ªªÂä° SwanLab ÂèØËßÜÂåñËÆ∞ÂΩïÁâà&lt;/a&gt; @ÊûóÊ≥ΩÊØÖ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/06-Phi-4-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;phi4 GRPOÂæÆË∞ÉÂèäÈÄöËøáswanlabÂèØËßÜÂåñ&lt;/a&gt; @ÈÉ≠ÂÆ£‰ºØ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Coder&quot;&gt;Qwen2.5-Coder&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/01-Qwen2.5-Coder-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct FastApiÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ËµµÊñáÊÅ∫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-Coder-7B-Instruct LangchianÊé•ÂÖ•&lt;/a&gt; @Êù®Êô®Êó≠&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/03-Qwen2.5-Coder-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-Coder-7B-Instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/04-Qwen2.5-Coder-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct vLLM ÈÉ®ÁΩ≤&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora ÂæÆË∞É&lt;/a&gt; @ËçûÈ∫¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/05-Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora ÂæÆË∞É SwanLab ÂèØËßÜÂåñËÆ∞ÂΩïÁâà&lt;/a&gt; @Êù®Âçì&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2-VL&quot;&gt;Qwen2-vl&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/01-Qwen2-VL-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/02-Qwen2-VL-2B-Instruct%20Web%20Demo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-vl-2B WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @Ëµµ‰ºü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B vLLM ÈÉ®ÁΩ≤&lt;/a&gt; @ËçûÈ∫¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/04-Qwen2-VL-2B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-vl-2B Lora ÂæÆË∞É&lt;/a&gt; @ÊùéÊüØËæ∞&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/05-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2-vl-2B Lora ÂæÆË∞É SwanLab ÂèØËßÜÂåñËÆ∞ÂΩïÁâà&lt;/a&gt; @ÊûóÊ≥ΩÊØÖ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/06-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%E6%A1%88%E4%BE%8B%20-%20LaTexOCR.md&quot;&gt;Qwen2-vl-2B Lora ÂæÆË∞ÉÊ°à‰æã - LaTexOCR&lt;/a&gt; @ÊûóÊ≥ΩÊØÖ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5&quot;&gt;Qwen2.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/01-Qwen2.5-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Â®ÑÂ§©Â••&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-7B-Instruct langchain Êé•ÂÖ•&lt;/a&gt; @Â®ÑÂ§©Â••&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/03-Qwen2.5-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct vLLM ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/04-Qwen2_5-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-7B-Instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @È´òÁ´ã‰∏ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-7B-Instruct Lora ÂæÆË∞É&lt;/a&gt; @Â∑¶Êò•Áîü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/06-Qwen2.5-7B-Instruct%20o1-like%20%E6%8E%A8%E7%90%86%E9%93%BE%E5%AE%9E%E7%8E%B0.md&quot;&gt;Qwen2.5-7B-Instruct o1-like Êé®ÁêÜÈìæÂÆûÁé∞&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/07-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-7B-Instruct Lora ÂæÆË∞É SwanLab ÂèØËßÜÂåñËÆ∞ÂΩïÁâà&lt;/a&gt; @ÊûóÊ≥ΩÊØÖ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://machinelearning.apple.com/research/openelm&quot;&gt;Apple OpenELM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/01-OpenELM-3B-Instruct%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;OpenELM-3B-Instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/02-OpenELM-3B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;OpenELM-3B-Instruct Lora ÂæÆË∞É&lt;/a&gt; @ÁéãÊ≥ΩÂÆá&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&quot;&gt;Llama3_1-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/01-Llama3_1-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Llama3_1-8B-Instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/02-Llama3_1-8B-Instruct%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;Llama3_1-8B-Instruct langchain Êé•ÂÖ•&lt;/a&gt; @Âº†Êôã&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/03-Llama3_1-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Llama3_1-8B-Instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @Âº†Êôã&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/04-Llama3_1-8B--Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Llama3_1-8B-Instruct Lora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/%E5%8A%A8%E6%89%8B%E8%BD%AC%E6%8D%A2GGUF%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%BD%BF%E7%94%A8Ollama%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2.md&quot;&gt;Âä®ÊâãËΩ¨Êç¢GGUFÊ®°ÂûãÂπ∂‰ΩøÁî®OllamaÊú¨Âú∞ÈÉ®ÁΩ≤&lt;/a&gt; @Gaoboy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-2-9b-it&quot;&gt;Gemma-2-9b-it&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/01-Gemma-2-9b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Gemma-2-9b-it FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/02-Gemma-2-9b-it%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Gemma-2-9b-it langchain Êé•ÂÖ•&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/03-Gemma-2-9b-it%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Gemma-2-9b-it WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/04-Gemma-2-9b-it%20peft%20lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Gemma-2-9b-it Peft Lora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan-2.0&quot;&gt;Yuan2.0&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/01-Yuan2.0-2B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/02-Yuan2.0-2B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-2B Langchain Êé•ÂÖ•&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/03-Yuan2.0-2B%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-2B WebDemoÈÉ®ÁΩ≤&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/04-Yuan2.0-2B%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B vLLMÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/05-Yuan2.0-2B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Yuan2.0-2B LoraÂæÆË∞É&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan2.0-M32&quot;&gt;Yuan2.0-M32&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/01-Yuan2.0-M32%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-M32 FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/02-Yuan2.0-M32%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-M32 Langchain Êé•ÂÖ•&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/03-Yuan2.0-M32%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-M32 WebDemoÈÉ®ÁΩ≤&lt;/a&gt; @Âº†Â∏Ü&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-Coder-V2&quot;&gt;DeepSeek-Coder-V2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/01-DeepSeek-Coder-V2-Lite-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/02-DeepSeek-Coder-V2-Lite-Instruct%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct langchain Êé•ÂÖ•&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/03-DeepSeek-Coder-V2-Lite-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/04-DeepSeek-Coder-V2-Lite-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct Lora ÂæÆË∞É&lt;/a&gt; @‰ΩôÊ¥ã&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bilibili/Index-1.9B&quot;&gt;ÂìîÂì©ÂìîÂì© Index-1.9B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/01-Index-1.9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Index-1.9B-Chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈÇìÊÅ∫‰øä&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/02-Index-1.9B-Chat%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;Index-1.9B-Chat langchain Êé•ÂÖ•&lt;/a&gt; @Âº†Âèã‰∏ú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/03-Index-1.9B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Index-1.9B-Chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @Á®ãÂÆè&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/04-Index-1.9B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Index-1.9B-Chat Lora ÂæÆË∞É&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2&quot;&gt;Qwen2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/01-Qwen2-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Â∫∑Â©ßÊ∑á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/02-Qwen2-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2-7B-Instruct langchain Êé•ÂÖ•&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/03-Qwen2-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-7B-Instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @‰∏âÊ∞¥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/04-Qwen2-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct vLLM ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÂßúËàíÂá°&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/05-Qwen2-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-7B-Instruct Lora ÂæÆË∞É&lt;/a&gt; @Êï£Ê≠•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/GLM-4.git&quot;&gt;GLM-4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/01-GLM-4-9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Âº†Âèã‰∏ú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/02-GLM-4-9B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;GLM-4-9B-chat langchain Êé•ÂÖ•&lt;/a&gt; @Ë∞≠ÈÄ∏ÁèÇ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/03-GLM-4-9B-Chat%20WebDemo.md&quot;&gt;GLM-4-9B-chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @‰ΩïËá≥ËΩ©&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/04-GLM-4-9B-Chat%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat vLLM ÈÉ®ÁΩ≤&lt;/a&gt; @ÁéãÁÜ†Êòé&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat Lora ÂæÆË∞É&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat-hf%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat-hf Lora ÂæÆË∞É&lt;/a&gt; @‰ªòÂøóËøú&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen1.5.git&quot;&gt;Qwen 1.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/01-Qwen1.5-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @È¢úÈë´&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/02-Qwen1.5-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen1.5-7B-chat langchain Êé•ÂÖ•&lt;/a&gt; @È¢úÈë´&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/03-Qwen1.5-7B-Chat%20WebDemo.md&quot;&gt;Qwen1.5-7B-chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @È¢úÈë´&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/04-Qwen1.5-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen1.5-7B-chat Lora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/05-Qwen1.5-7B-Chat-GPTQ-Int4%20%20WebDemo.md&quot;&gt;Qwen1.5-72B-chat-GPTQ-Int4 ÈÉ®ÁΩ≤ÁéØÂ¢É&lt;/a&gt; @byx020119&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/06-Qwen1.5-MoE-A2.7B.md&quot;&gt;Qwen1.5-MoE-chat Transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏ÅÊÇ¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/07-Qwen1.5-7B-Chat%20vLLM%20%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat vLLMÊé®ÁêÜÈÉ®ÁΩ≤&lt;/a&gt; @È´òÁ´ã‰∏ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/08-Qwen1.5-7B-chat%20LoRA%E5%BE%AE%E8%B0%83%E6%8E%A5%E5%85%A5%E5%AE%9E%E9%AA%8C%E7%AE%A1%E7%90%86.md&quot;&gt;Qwen1.5-7B-chat Lora ÂæÆË∞É Êé•ÂÖ•SwanLabÂÆûÈ™åÁÆ°ÁêÜÂπ≥Âè∞&lt;/a&gt; @ÈªÑÊüèÁâπ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-7b-it&quot;&gt;Ë∞∑Ê≠å-Gemma&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/01-Gemma-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-2b-it FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî® &lt;/a&gt; @‰∏ú‰∏ú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/02-Gemma-2B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;gemma-2b-it langchain Êé•ÂÖ• &lt;/a&gt; @‰∏ú‰∏ú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/03-Gemma-2B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-2b-it WebDemo ÈÉ®ÁΩ≤ &lt;/a&gt; @‰∏ú‰∏ú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/04-Gemma-2B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;gemma-2b-it Peft Lora ÂæÆË∞É &lt;/a&gt; @‰∏ú‰∏ú&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct&quot;&gt;phi-3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/01-Phi-3-mini-4k-instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Phi-3-mini-4k-instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈÉëÁöìÊ°¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/02-Phi-3-mini-4k-instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Phi-3-mini-4k-instruct langchain Êé•ÂÖ•&lt;/a&gt; @ÈÉëÁöìÊ°¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/03-Phi-3-mini-4k-instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Phi-3-mini-4k-instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @‰∏ÅÊÇ¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/04-Phi-3-mini-4k-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Phi-3-mini-4k-instruct Lora ÂæÆË∞É&lt;/a&gt; @‰∏ÅÊÇ¶&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/thu-coai/CharacterGLM-6B&quot;&gt;CharacterGLM-6B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/01-CharacterGLM-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B Transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Â≠ôÂÅ•Â£Æ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/02-CharacterGLM-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Â≠ôÂÅ•Â£Æ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/03-CharacterGLM-6B-chat.md&quot;&gt;CharacterGLM-6B webdemo ÈÉ®ÁΩ≤&lt;/a&gt; @Â≠ôÂÅ•Â£Æ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/04-CharacterGLM-6B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;CharacterGLM-6B Lora ÂæÆË∞É&lt;/a&gt; @Â≠ôÂÅ•Â£Æ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/meta-llama/llama3.git&quot;&gt;LLaMA3-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;LLaMA3-8B-Instruct FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @È´òÁ´ã‰∏ö&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/02-LLaMA3-8B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;LLaMA3-8B-Instruct langchain Êé•ÂÖ•&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/03-LLaMA3-8B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;LLaMA3-8B-Instruct WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/04-LLaMA3-8B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;LLaMA3-8B-Instruct Lora ÂæÆË∞É&lt;/a&gt; @È´òÁ´ã‰∏ö&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/xverse/XVERSE-7B-Chat/summary&quot;&gt;XVERSE-7B-Chat&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/01-XVERSE-7B-chat%20Transformers%E6%8E%A8%E7%90%86.md&quot;&gt;XVERSE-7B-Chat transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/02-XVERSE-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/03-XVERSE-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;XVERSE-7B-Chat langchain Êé•ÂÖ•&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/04-XVERSE-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/05-XVERSE-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;XVERSE-7B-Chat Lora ÂæÆË∞É&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenNLPLab/TransnormerLLM.git&quot;&gt;TransNormerLLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/01-TransNormer-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;TransNormerLLM-7B-Chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÁéãËåÇÈúñ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/02-TransNormer-7B%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;TransNormerLLM-7B-Chat langchain Êé•ÂÖ•&lt;/a&gt; @ÁéãËåÇÈúñ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/03-TransNormer-7B%20WebDemo.md&quot;&gt;TransNormerLLM-7B-Chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÁéãËåÇÈúñ&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/04-TrasnNormer-7B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;TransNormerLLM-7B-Chat Lora ÂæÆË∞É&lt;/a&gt; @ÁéãËåÇÈúñ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/vivo-ai-lab/BlueLM.git&quot;&gt;BlueLM Vivo ËìùÂøÉÂ§ßÊ®°Âûã&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/01-BlueLM-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat FatApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/02-BlueLM-7B-Chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;BlueLM-7B-Chat langchain Êé•ÂÖ•&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/03-BlueLM-7B-Chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/04-BlueLM-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;BlueLM-7B-Chat Lora ÂæÆË∞É&lt;/a&gt; @ÈÉ≠ÂøóËà™&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/01-InternLM2-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/02-InternLM2-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;InternLM2-7B-chat langchain Êé•ÂÖ•&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/03-InternLM2-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat WebDemo ÈÉ®ÁΩ≤&lt;/a&gt; @ÈÉëÁöìÊ°¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/04-InternLM2-7B-chat%20Xtuner%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;InternLM2-7B-chat Xtuner Qlora ÂæÆË∞É&lt;/a&gt; @ÈÉëÁöìÊ°¶&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-LLM&quot;&gt;DeepSeek Ê∑±Â∫¶Ê±ÇÁ¥¢&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/01-DeepSeek-7B-chat%20FastApi.md&quot;&gt;DeepSeek-7B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/02-DeepSeek-7B-chat%20langchain.md&quot;&gt;DeepSeek-7B-chat langchain Êé•ÂÖ•&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/03-DeepSeek-7B-chat%20WebDemo.md&quot;&gt;DeepSeek-7B-chat WebDemo&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/04-DeepSeek-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat Lora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/05-DeepSeek-7B-chat%204bits%E9%87%8F%E5%8C%96%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat 4bitsÈáèÂåñ Qlora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-MoE-16b-chat Transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20FastApi.md&quot;&gt;DeepSeek-MoE-16b-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/07-deepseek_fine_tune.ipynb&quot;&gt;DeepSeek-coder-6.7b finetune colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/08-deepseek_web_demo.ipynb&quot;&gt;Deepseek-coder-6.7b webdemo colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM.git&quot;&gt;MiniCPM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;MiniCPM-2B-chat langchain Êé•ÂÖ•&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;MiniCPM-2B-chat webdemo ÈÉ®ÁΩ≤&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20Lora%20&amp;amp;&amp;amp;%20Full%20%E5%BE%AE%E8%B0%83.md&quot;&gt;MiniCPM-2B-chat Lora &amp;amp;&amp;amp; Full ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; ÂÆòÊñπÂèãÊÉÖÈìæÊé•Ôºö&lt;a href=&quot;https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg&quot;&gt;Èù¢Â£ÅÂ∞èÈí¢ÁÇÆMiniCPMÊïôÁ®ã&lt;/a&gt; @OpenBMB&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; ÂÆòÊñπÂèãÊÉÖÈìæÊé•Ôºö&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-CookBook&quot;&gt;MiniCPM-Cookbook&lt;/a&gt; @OpenBMB&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen-Audio.git&quot;&gt;Qwen-Audio&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/01-Qwen-Audio-chat%20FastApi.md&quot;&gt;Qwen-Audio FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÈôàÊÄùÂ∑û&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/02-Qwen-Audio-chat%20WebDemo.md&quot;&gt;Qwen-Audio WebDemo&lt;/a&gt; @ÈôàÊÄùÂ∑û&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen.git&quot;&gt;Qwen&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/01-Qwen-7B-Chat%20Transformers%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat Transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/02-Qwen-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/03-Qwen-7B-Chat%20WebDemo.md&quot;&gt;Qwen-7B-chat WebDemo&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/04-Qwen-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat Lora ÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/05-Qwen-7B-Chat%20Ptuning%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ptuning ÂæÆË∞É&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/06-Qwen-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ÂÖ®ÈáèÂæÆË∞É&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/07-Qwen-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen-7B-Chat Êé•ÂÖ•langchainÊê≠Âª∫Áü•ËØÜÂ∫ìÂä©Êâã&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/08-Qwen-7B-Chat%20Lora%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ‰ΩéÁ≤æÂ∫¶ËÆ≠ÁªÉ&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/09-Qwen-1_8B-chat%20CPU%20%E9%83%A8%E7%BD%B2%20.md&quot;&gt;Qwen-1_8B-chat CPU ÈÉ®ÁΩ≤&lt;/a&gt; @Êï£Ê≠•&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/01-ai/Yi.git&quot;&gt;Yi Èõ∂‰∏Ä‰∏áÁâ©&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/01-Yi-6B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yi-6B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊùéÊüØËæ∞&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/02-Yi-6B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Yi-6B-chat langchainÊé•ÂÖ•&lt;/a&gt; @ÊùéÊüØËæ∞&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/03-Yi-6B-chat%20WebDemo.md&quot;&gt;Yi-6B-chat WebDemo&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/04-Yi-6B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Yi-6B-chat Lora ÂæÆË∞É&lt;/a&gt; @ÊùéÂ®áÂ®á&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.baichuan-ai.com/home&quot;&gt;Baichuan ÁôæÂ∑ùÊô∫ËÉΩ&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/BaiChuan/01-Baichuan2-7B-chat%2BFastApi%2B%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Baichuan2-7B-chat FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @ÊÉ†‰Ω≥Ë±™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/02-Baichuan-7B-chat%2BWebDemo.md&quot;&gt;Baichuan2-7B-chat WebDemo&lt;/a&gt; @ÊÉ†‰Ω≥Ë±™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/03-Baichuan2-7B-chat%E6%8E%A5%E5%85%A5LangChain%E6%A1%86%E6%9E%B6.md&quot;&gt;Baichuan2-7B-chat Êé•ÂÖ• LangChain Ê°ÜÊû∂&lt;/a&gt; @ÊÉ†‰Ω≥Ë±™&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/04-Baichuan2-7B-chat%2Blora%2B%E5%BE%AE%E8%B0%83.md&quot;&gt;Baichuan2-7B-chat Lora ÂæÆË∞É&lt;/a&gt; @ÊÉ†‰Ω≥Ë±™&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM.git&quot;&gt;InternLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/01-InternLM-Chat-7B%20Transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;InternLM-Chat-7B Transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @Â∞èÁΩó&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/02-internLM-Chat-7B%20FastApi.md&quot;&gt;InternLM-Chat-7B FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/03-InternLM-Chat-7B.md&quot;&gt;InternLM-Chat-7B WebDemo&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/04-Lagent+InternLM-Chat-7B-V1.1.md&quot;&gt;Lagent+InternLM-Chat-7B-V1.1 WebDemo&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/05-%E6%B5%A6%E8%AF%AD%E7%81%B5%E7%AC%94%E5%9B%BE%E6%96%87%E7%90%86%E8%A7%A3&amp;amp;%E5%88%9B%E4%BD%9C.md&quot;&gt;Êµ¶ËØ≠ÁÅµÁ¨îÂõæÊñáÁêÜËß£&amp;amp;Âàõ‰Ωú WebDemo&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/06-InternLM%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;InternLM-Chat-7B Êé•ÂÖ• LangChain Ê°ÜÊû∂&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://hf-mirror.com/FlagAlpha/Atom-7B-Chat&quot;&gt;Atom (llama2)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/01-Atom-7B-chat-WebDemo.md&quot;&gt;Atom-7B-chat WebDemo&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/02-Atom-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat Lora ÂæÆË∞É&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/03-Atom-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Atom-7B-Chat Êé•ÂÖ•langchainÊê≠Âª∫Áü•ËØÜÂ∫ìÂä©Êâã&lt;/a&gt; @ÈôàÊÄùÂ∑û&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/04-Atom-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat ÂÖ®ÈáèÂæÆË∞É&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/ChatGLM3.git&quot;&gt;ChatGLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/01-ChatGLM3-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B Transformers ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏ÅÊÇ¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/02-ChatGLM3-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B FastApi ÈÉ®ÁΩ≤Ë∞ÉÁî®&lt;/a&gt; @‰∏ÅÊÇ¶&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/03-ChatGLM3-6B-chat.md&quot;&gt;ChatGLM3-6B chat WebDemo&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/04-ChatGLM3-6B-Code-Interpreter.md&quot;&gt;ChatGLM3-6B Code Interpreter WebDemo&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/05-ChatGLM3-6B%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;ChatGLM3-6B Êé•ÂÖ• LangChain Ê°ÜÊû∂&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/06-ChatGLM3-6B-Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;ChatGLM3-6B Lora ÂæÆË∞É&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÈÄöÁî®ÁéØÂ¢ÉÈÖçÁΩÆ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/01-pip%E3%80%81conda%E6%8D%A2%E6%BA%90.md&quot;&gt;pip„ÄÅconda Êç¢Ê∫ê&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/02-AutoDL%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3.md&quot;&gt;AutoDL ÂºÄÊîæÁ´ØÂè£&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; ÈïúÂÉè‰∏ãËΩΩ @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;modelscope&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;git-lfs&lt;/a&gt; @‰∏çË¶ÅËë±ÂßúËíú&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;Openxlab&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Issue &amp;amp;&amp;amp; PR&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;Issue Êèê‰∫§&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;PR Êèê‰∫§&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;forkÊõ¥Êñ∞&lt;/a&gt; @ËÇñÈ∏øÂÑí&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;h3&gt;Ê†∏ÂøÉË¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;ÂÆãÂøóÂ≠¶(‰∏çË¶ÅËë±ÂßúËíú)-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; ÔºàDatawhaleÊàêÂëò-‰∏≠ÂõΩÁüø‰∏öÂ§ßÂ≠¶(Âåó‰∫¨)Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;ÈÇπÈõ®Ë°°-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; ÔºàDatawhaleÊàêÂëò-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Tsumugii24&quot;&gt;ÂßúËàíÂá°&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;ËÇñÈ∏øÂÑí&lt;/a&gt; ÔºàDatawhaleÊàêÂëò-ÂêåÊµéÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/acwwt&quot;&gt;ÈÉ≠ÂøóËà™&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Zeyi-Lin&quot;&gt;ÊûóÊ≥ΩÊØÖ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-SwanLab‰∫ßÂìÅË¥üË¥£‰∫∫Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LINHYYY&quot;&gt;ÊûóÊÅíÂÆá&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Âπø‰∏ú‰∏úËΩØÂ≠¶Èô¢-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Twosugar666&quot;&gt;ÈÉ≠ÂÆ£‰ºØ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Âåó‰∫¨Ëà™Á©∫Ëà™Â§©Â§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhangfanTJU&quot;&gt;Âº†Â∏Ü&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/moyitech&quot;&gt;ÁéãÊ≥ΩÂÆá&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Â§™ÂéüÁêÜÂ∑•Â§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aphasia0515&quot;&gt;ÊùéÂ®áÂ®á&lt;/a&gt; ÔºàDatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/0-yy-0&quot;&gt;È´òÁ´ã‰∏ö&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DataWhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dingyue772&quot;&gt;‰∏ÅÊÇ¶&lt;/a&gt; ÔºàDatawhale-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/L4HeyXiao&quot;&gt;ÊÉ†‰Ω≥Ë±™&lt;/a&gt; ÔºàDatawhale-ÂÆ£‰º†Â§ß‰ΩøÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mlw67&quot;&gt;ÁéãËåÇÈúñ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Caleb-Sun-jz&quot;&gt;Â≠ôÂÅ•Â£Æ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LucaChen&quot;&gt;‰∏ú‰∏ú&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Ë∞∑Ê≠åÂºÄÂèëËÄÖÊú∫Âô®Â≠¶‰π†ÊäÄÊúØ‰∏ìÂÆ∂Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yeyeyeyeeeee&quot;&gt;ËçûÈ∫¶&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kailigithub&quot;&gt;Kailigithub&lt;/a&gt; ÔºàDatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/BaiYu96&quot;&gt;ÈÉëÁöìÊ°¶&lt;/a&gt; ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Joe-2002&quot;&gt;ÊùéÊüØËæ∞&lt;/a&gt; ÔºàDatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chg0901&quot;&gt;Á®ãÂÆè&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊÑèÂêëÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anine09&quot;&gt;È™ÜÁßÄÈü¨&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëò-‰ººÁÑ∂ÂÆûÈ™åÂÆ§Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ilovexsir&quot;&gt;Ë∞¢Â•ΩÂÜâ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jjyaoao&quot;&gt;ÈôàÊÄùÂ∑û&lt;/a&gt; ÔºàDatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sanbuphy&quot;&gt;Êï£Ê≠•&lt;/a&gt; ÔºàDatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/thomas-yanxin&quot;&gt;È¢úÈë´&lt;/a&gt; ÔºàDatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/study520ai520&quot;&gt;ÊùúÊ£Æ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëò-ÂçóÈò≥ÁêÜÂ∑•Â≠¶Èô¢Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cswangxiaowei&quot;&gt;Swiftie&lt;/a&gt; ÔºàÂ∞èÁ±≥NLPÁÆóÊ≥ïÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KashiwaByte&quot;&gt;ÈªÑÊüèÁâπ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Ë•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AXYZdong&quot;&gt;Âº†Âèã‰∏ú&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YangYu-NUAA&quot;&gt;‰ΩôÊ¥ã&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Jin-Zhang-Yaoguang&quot;&gt;Âº†Êôã&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lta155&quot;&gt;Â®ÑÂ§©Â••&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-‰∏≠ÂõΩÁßëÂ≠¶Èô¢Â§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LinChentang&quot;&gt;Â∑¶Êò•Áîü&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/little1d&quot;&gt;Êù®Âçì&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Ë•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lyj11111111&quot;&gt;Â∞èÁΩó&lt;/a&gt; ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kedreamix&quot;&gt;ÈÇìÊÅ∫‰øä&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/XiLinky&quot;&gt;ËµµÊñáÊÅ∫&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Â§™ÂéüÁêÜÂ∑•Â§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/comfzy&quot;&gt;‰ªòÂøóËøú&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Êµ∑ÂçóÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/isaacahahah&quot;&gt;ÈÉëËøúÂ©ß&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©Êïô-Á¶èÂ∑ûÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bald0Wang&quot;&gt;ÁéãÁÜ†Êòé&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LikeGiver&quot;&gt;Ë∞≠ÈÄ∏ÁèÇ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pod2c&quot;&gt;‰ΩïËá≥ËΩ©&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jodie-kang&quot;&gt;Â∫∑Â©ßÊ∑á&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sssanssss&quot;&gt;‰∏âÊ∞¥&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langlibai66&quot;&gt;Êù®Êô®Êó≠&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Â§™ÂéüÁêÜÂ∑•Â§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/2710932616&quot;&gt;Ëµµ‰ºü&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gzhuuser&quot;&gt;ËãèÂêëÊ†á&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ÂπøÂ∑ûÂ§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/riannyway&quot;&gt;ÈôàÁùø&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Ë•ø‰∫§Âà©Áâ©Êµ¶Â§ßÂ≠¶-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Feimike09&quot;&gt;Âº†ÈæôÊñê&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anarchysaiko&quot;&gt;Â≠ôË∂Ö&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fanqiNO1&quot;&gt;Ê®äÂ•á&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-‰∏äÊµ∑‰∫§ÈÄöÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nusakom&quot;&gt;ÂçìÂ†ÇË∂ä&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fancyboi999&quot;&gt;fancy&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-È≤∏Ëã±Âä©ÊïôÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/li-xiu-qi&quot;&gt;ÊùéÁßÄÂ•á&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DataWhaleÊÑèÂêëÊàêÂëòÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Ê≥®ÔºöÊéíÂêçÊ†πÊçÆË¥°ÁåÆÁ®ãÂ∫¶ÊéíÂ∫è&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;ÂÖ∂‰ªñ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÁâπÂà´ÊÑüË∞¢&lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt;ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;ÈÉ®ÂàÜlora‰ª£Á†ÅÂíåËÆ≤Ëß£ÂèÇËÄÉ‰ªìÂ∫ìÔºö&lt;a href=&quot;https://github.com/zyds/transformers-code.git&quot;&gt;https://github.com/zyds/transformers-code.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Â¶ÇÊûúÊúâ‰ªª‰ΩïÊÉ≥Ê≥ïÂèØ‰ª•ËÅîÁ≥ªÊàë‰ª¨ DataWhale ‰πüÊ¨¢ËøéÂ§ßÂÆ∂Â§öÂ§öÊèêÂá∫ issue&lt;/li&gt; 
 &lt;li&gt;ÁâπÂà´ÊÑüË∞¢‰ª•‰∏ã‰∏∫ÊïôÁ®ãÂÅöÂá∫Ë¥°ÁåÆÁöÑÂêåÂ≠¶ÔºÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/self-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/self-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/star-history-202572.png&quot; /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>microsoft/ai-agents-for-beginners</title>
      <link>https://github.com/microsoft/ai-agents-for-beginners</link>
      <description>&lt;p&gt;12 Lessons to Get Started Building AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents for Beginners - A Course&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnailv2.png&quot; alt=&quot;Generative AI For Beginners&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;A course teaching everything you need to know to start building AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;üåê Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href=&quot;https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/kzRShWzttr&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/kzRShWzttr&quot; alt=&quot;Azure AI Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üå± Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;There is multi-language support for this course. Go to our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/#-multi-language-support&quot;&gt;available languages here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If this is your first time building with Generative AI models, check out our &lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI For Beginners&lt;/a&gt; course, which includes 21 lessons on building with GenAI.&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to &lt;a href=&quot;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&quot;&gt;star (üåü) this repo&lt;/a&gt; and &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/fork&quot;&gt;fork this repo&lt;/a&gt; to run the code.&lt;/p&gt; 
&lt;h3&gt;Meet Other Learners, Get Your Questions Answered&lt;/h3&gt; 
&lt;p&gt;If you get stuck or have any questions about building AI Agents, join our dedicated Discord Channel in the &lt;a href=&quot;https://aka.ms/ai-agents/discord&quot;&gt;Azure AI Foundry Community Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;What You Need&lt;/h3&gt; 
&lt;p&gt;Each lesson in this course includes code examples, which can be found in the code_samples folder. You can &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/fork&quot;&gt;fork this repo&lt;/a&gt; to create your own copy.&lt;/p&gt; 
&lt;p&gt;The code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/github-models&quot;&gt;Github Models&lt;/a&gt; - Free / Limited&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt; - Azure Account Required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This course also uses the following AI Agent frameworks and services from Microsoft:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/ai-agent-service&quot;&gt;Azure AI Agent Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/semantic-kernel&quot;&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents/autogen&quot;&gt;AutoGen&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information on running the code for this course, go to the &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/00-course-setup/README.md&quot;&gt;Course Setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üôè Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst&quot;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&quot;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;üìÇ Each lesson includes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A written lesson located in the README and a short video&lt;/li&gt; 
 &lt;li&gt;Python code samples supporting Azure AI Foundry and Github Models (Free)&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üóÉÔ∏è Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Text &amp;amp; Code&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Intro to AI Agents and Agent Use Cases&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/01-intro-to-ai-agents/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Exploring AI Agentic Frameworks&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/02-explore-agentic-frameworks/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Understanding AI Agentic Design Patterns&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/03-agentic-design-patterns/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tool Use Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/04-tool-use/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agentic RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/05-agentic-rag/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Trustworthy AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/06-building-trustworthy-agents/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/07-planning-design/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-Agent Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/08-multi-agent/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metacognition Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/09-metacognition/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents in Production&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/10-ai-agents-production/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Using Agentic Protocols (MCP, A2A and NLWeb)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/11-agentic-protocols/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/X-Dh9R3Opn8&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Context Engineering for AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/12-context-engineering/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/F5zqRV7gEag&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Managing Agentic Memory&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/13-agent-memory/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Evaluating AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 18th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Computer Use Agents (CUA)&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deploying Scalable Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Creating Local AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 3rd&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Securing AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 10th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üéí Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol (MCP) For Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üåü Community Thanks&lt;/h2&gt; 
&lt;p&gt;Thanks to &lt;a href=&quot;https://www.linkedin.com/in/shivam2003/&quot;&gt;Shivam Goyal&lt;/a&gt; for contributing important code samples demonstrating Agentic RAG.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&quot;https://cla.opensource.microsoft.com&quot;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href=&quot;https://opensource.microsoft.com/codeofconduct/&quot;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&quot;https://opensource.microsoft.com/codeofconduct/faq/&quot;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&quot;mailto:opencode@microsoft.com&quot;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&quot;https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general&quot;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-parties&#39; policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ageron/handson-ml3</title>
      <link>https://github.com/ageron/handson-ml3</link>
      <description>&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks, 3rd edition&lt;/h1&gt; 
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the third edition of my O&#39;Reilly book &lt;a href=&quot;https://homl.info/er3&quot;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow (3rd edition)&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://homl.info/er3&quot;&gt;&lt;img src=&quot;https://learning.oreilly.com/library/cover/9781098125967/300w/&quot; title=&quot;book&quot; width=&quot;150&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the second edition notebooks, check out &lt;a href=&quot;https://github.com/ageron/handson-ml2&quot;&gt;ageron/handson-ml2&lt;/a&gt;. For the first edition, see &lt;a href=&quot;https://github.com/ageron/handson-ml&quot;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/ageron/handson-ml3/blob/main/&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt; (recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;‚ö† &lt;em&gt;Colab provides a temporary environment: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;p&gt;Other services may work as well, but I have not fully tested them:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://homl.info/kaggle3/&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://mybinder.org/v2/gh/ageron/handson-ml3/HEAD?filepath=%2Findex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg?sanitize=true&quot; alt=&quot;Launch binder&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://homl.info/deepnote3/&quot;&gt;&lt;img src=&quot;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&quot; alt=&quot;Launch in Deepnote&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/ageron/handson-ml3/blob/main/index.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&quot; alt=&quot;Render nbviewer&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/ageron/handson-ml3/raw/main/index.ipynb&quot;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; 
&lt;p&gt;Read the &lt;a href=&quot;https://github.com/ageron/handson-ml3/tree/main/docker&quot;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; 
&lt;p&gt;Start by installing &lt;a href=&quot;https://www.anaconda.com/products/distribution&quot;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;Miniconda&lt;/a&gt;), &lt;a href=&quot;https://git-scm.com/downloads&quot;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&quot;https://www.nvidia.com/Download/index.aspx&quot;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; 
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml3.git
$ cd handson-ml3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
$ conda activate homl3
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you need further instructions, read the &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&quot;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;I recommend Python 3.10. If you follow the installation instructions above, that&#39;s the version you will get. Any version ‚â•3.7 should work as well.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration. If it&#39;s an SSL error, see the next question.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&quot;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&quot;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.10/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.10&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&quot;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&quot;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;I would like to thank everyone &lt;a href=&quot;https://github.com/ageron/handson-ml3/graphs/contributors&quot;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions. Thanks a lot to Victor Khaustov who submitted plenty of excellent PRs, fixing many errors. And lastly, thanks to Google ML Developer Programs team who supported this work by providing Google Cloud Credit.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
