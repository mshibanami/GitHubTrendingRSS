<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Daily Trending</title>
    <description>Daily Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:33:19 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>maxim5/cs229-2018-autumn</title>
      <link>https://github.com/maxim5/cs229-2018-autumn</link>
      <description>&lt;p&gt;All notes and materials for the CS229: Machine Learning course by Stanford University&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CS229 Autumn 2018&lt;/h1&gt; 
&lt;p&gt;All lecture notes, slides and assignments for &lt;a href=&quot;http://cs229.stanford.edu/&quot;&gt;CS229: Machine Learning&lt;/a&gt; course by Stanford University.&lt;/p&gt; 
&lt;p&gt;The videos of all lectures are available &lt;a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&quot;&gt;on YouTube&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Useful links:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maxim5/cs229-2019-summer&quot;&gt;CS229 Summer 2019 edition&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>google-gemini/cookbook</title>
      <link>https://github.com/google-gemini/cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the Gemini API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to the Gemini API Cookbook&lt;/h1&gt; 
&lt;p&gt;This cookbook provides a structured learning path for using the Gemini API, focusing on hands-on tutorials and practical examples.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;For comprehensive API documentation, visit &lt;a href=&quot;https://ai.google.dev/gemini-api/docs&quot;&gt;ai.google.dev&lt;/a&gt;.&lt;/strong&gt; &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Navigating the Cookbook&lt;/h2&gt; 
&lt;p&gt;This cookbook is organized into two main categories:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;Quick Starts&lt;/a&gt;:&lt;/strong&gt; Step-by-step guides covering both introductory topics (&quot;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;Get Started&lt;/a&gt;&quot;) and specific API features.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/examples/&quot;&gt;Examples&lt;/a&gt;:&lt;/strong&gt; Practical use cases demonstrating how to combine multiple features.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We also showcase &lt;strong&gt;Demos&lt;/strong&gt; in separate repositories, illustrating end-to-end applications of the Gemini API. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;What&#39;s New?&lt;/h2&gt; 
&lt;p&gt;Here are the recent additions and updates to the Gemini API and the Cookbook:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini 2.5 models:&lt;/strong&gt; Explore the capabilities of the latest Gemini 2.5 models (Flash and Pro)! See the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;Get Started Guide&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_thinking.ipynb&quot;&gt;thinking guide&lt;/a&gt; as they&#39;ll all be thinking ones.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Imagen and Veo&lt;/strong&gt;: Get started with our media generation model with this brand new &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_Veo.ipynb&quot;&gt;Veo guide&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb&quot;&gt;Imagen guide&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Gemini Robotics-ER 1.5&lt;/strong&gt;: Learn about this new Gemini model specifically for spatial understanding and reasoning for &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/gemini-robotics-er.ipynb&quot;&gt;robotics applications&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Lyria and TTS&lt;/strong&gt;: Get started with podcast and music generation with the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_TTS.ipynb&quot;&gt;TTS&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LyriaRealTime.ipynb&quot;&gt;Lyria RealTime&lt;/a&gt; models.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;LiveAPI&lt;/strong&gt;: Get started with the &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb&quot;&gt;multimodal Live API&lt;/a&gt; and unlock new interactivity with Gemini.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Recently Added Guides:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb&quot;&gt;Image-out&lt;/a&gt;: Use Gemini&#39;s native image generation capabilities to edit images with high consistency or generate visual stories.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Grounding.ipynb&quot;&gt;Grounding&lt;/a&gt;: Discover different ways to ground Gemini&#39;s answer using different tools, from Google Search to Youtube and the new &lt;strong&gt;url context&lt;/strong&gt; tool.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Batch_mode.ipynb&quot;&gt;Batch-mode&lt;/a&gt;: Use Batch-mode to send large volume of non-time-sensitive requests to the model and get a 50% discount.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;1. Quick Starts&lt;/h2&gt; 
&lt;p&gt;The &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;quickstarts section&lt;/a&gt; contains step-by-step tutorials to get you started with Gemini and learn about its specific features.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;To begin, you&#39;ll need:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A Google account.&lt;/li&gt; 
 &lt;li&gt;An API key (create one in &lt;a href=&quot;https://aistudio.google.com/app/apikey&quot;&gt;Google AI Studio&lt;/a&gt;). &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;We recommend starting with the following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Authentication.ipynb&quot;&gt;Authentication&lt;/a&gt;: Set up your API key for access.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started.ipynb&quot;&gt;&lt;strong&gt;Get started&lt;/strong&gt;&lt;/a&gt;: Get started with Gemini models and the Gemini API, covering basic prompting and multimodal input. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Then, explore the other quickstarts tutorials to learn about individual features:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_LiveAPI.ipynb&quot;&gt;Get started with Live API&lt;/a&gt;: Get started with the live API with this comprehensive overview of its capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_Veo.ipynb&quot;&gt;Get started with Veo&lt;/a&gt;: Get started with our video generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Get_started_imagen.ipynb&quot;&gt;Get started with Imagen&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Image_out.ipynb&quot;&gt;Image-out&lt;/a&gt;: Get started with our image generation capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Search_Grounding.ipynb&quot;&gt;Grounding&lt;/a&gt;: use Google Search for grounded responses&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/quickstarts/Code_Execution.ipynb&quot;&gt;Code execution&lt;/a&gt;: Generating and running Python code to solve complex tasks and even ouput graphs&lt;/li&gt; 
 &lt;li&gt;And &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/&quot;&gt;many more&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;2. Examples (Practical Use Cases)&lt;/h2&gt; 
&lt;p&gt;These examples demonstrate how to combine multiple Gemini API features or 3rd-party tools to build more complex applications.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Browser_as_a_tool.ipynb&quot;&gt;Browser as a tool&lt;/a&gt;: Use a web browser for live and internal (intranet) web interactions&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Book_illustration.ipynb&quot;&gt;Illustrate a book&lt;/a&gt;: Use Gemini and Imagen to create illustration for an open-source book&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Animated_Story_Video_Generation_gemini.ipynb&quot;&gt;Animated Story Generation&lt;/a&gt;: Create animated videos by combining Gemini&#39;s story generation, Imagen, and audio synthesis&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/LiveAPI_plotting_and_mapping.ipynb&quot;&gt;Plotting and mapping Live&lt;/a&gt;: Mix &lt;em&gt;Live API&lt;/em&gt; and &lt;em&gt;Code execution&lt;/em&gt; to solve complex tasks live&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/Spatial_understanding_3d.ipynb&quot;&gt;3D Spatial understanding&lt;/a&gt;: Use Gemini &lt;em&gt;3D spatial&lt;/em&gt; abilities to understand 3D scenes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/examples/gradio_audio.py&quot;&gt;Gradio and live API&lt;/a&gt;: Use gradio to deploy your own instance of the &lt;em&gt;Live API&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;And &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/examples/&quot;&gt;many many more&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;3. Demos (End-to-End Applications)&lt;/h2&gt; 
&lt;p&gt;These fully functional, end-to-end applications showcase the power of Gemini in real-world scenarios.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/gemini-api-quickstart&quot;&gt;Gemini API quickstart&lt;/a&gt;: Python Flask App running with the Google AI Gemini API, designed to get you started building with Gemini&#39;s multi-modal capabilities&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/multimodal-live-api-web-console&quot;&gt;Multimodal Live API Web Console&lt;/a&gt;: React-based starter app for using the Multimodal Live API over a websocket&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google-gemini/starter-applets&quot;&gt;Google AI Studio Starter Applets&lt;/a&gt;: A collection of small apps that demonstrate how Gemini can be used to create interactive experiences &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Official SDKs&lt;/h2&gt; 
&lt;p&gt;The Gemini API is a REST API. You can call it directly using tools like &lt;code&gt;curl&lt;/code&gt; (see &lt;a href=&quot;https://github.com/google-gemini/cookbook/tree/main/quickstarts/rest/&quot;&gt;REST examples&lt;/a&gt; or the great &lt;a href=&quot;https://www.postman.com/ai-on-postman/google-gemini-apis/overview&quot;&gt;Postman workspace&lt;/a&gt;), or use one of our official SDKs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/googleapis/python-genai&quot;&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-go&quot;&gt;Go&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-js&quot;&gt;Node.js&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-dart&quot;&gt;Dart (Flutter)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-android&quot;&gt;Android&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/generative-ai-swift&quot;&gt;Swift&lt;/a&gt; &lt;br /&gt;&lt;br /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Important: Migration&lt;/h2&gt; 
&lt;p&gt;With Gemini 2 we are offering a &lt;a href=&quot;https://github.com/googleapis/python-genai&quot;&gt;new SDK&lt;/a&gt; (&lt;code&gt;&lt;a href=&quot;https://pypi.org/project/google-genai/&quot;&gt;google-genai&lt;/a&gt;&lt;/code&gt;, &lt;code&gt;v1.0&lt;/code&gt;). The updated SDK is fully compatible with all Gemini API models and features, including recent additions like the &lt;a href=&quot;https://aistudio.google.com/live&quot;&gt;live API&lt;/a&gt; (audio + video streaming), improved tool usage ( &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/code-execution?lang=python&quot;&gt;code execution&lt;/a&gt;, &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/function-calling/tutorial?lang=python&quot;&gt;function calling&lt;/a&gt; and integrated &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/grounding?lang=python&quot;&gt;Google search grounding&lt;/a&gt;), and media generation (&lt;a href=&quot;https://ai.google.dev/gemini-api/docs/imagen&quot;&gt;Imagen&lt;/a&gt; and &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/video&quot;&gt;Veo&lt;/a&gt;). This SDK allows you to connect to the Gemini API through either &lt;a href=&quot;https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-flash&quot;&gt;Google AI Studio&lt;/a&gt; or &lt;a href=&quot;https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2&quot;&gt;Vertex AI&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;&lt;a href=&quot;https://pypi.org/project/google-generativeai&quot;&gt;google-generativeai&lt;/a&gt;&lt;/code&gt; package will continue to support the original Gemini models. It &lt;em&gt;can&lt;/em&gt; also be used with Gemini 2 models, just with a limited feature set. All new features will be developed in the new Google GenAI SDK.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://ai.google.dev/gemini-api/docs/migrate&quot;&gt;migration guide&lt;/a&gt; for details. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt; 
&lt;h2&gt;Get Help&lt;/h2&gt; 
&lt;p&gt;Ask a question on the &lt;a href=&quot;https://discuss.ai.google.dev/&quot;&gt;Google AI Developer Forum&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;The Gemini API on Google Cloud Vertex AI&lt;/h2&gt; 
&lt;p&gt;For enterprise developers, the Gemini API is also available on Google Cloud Vertex AI. See &lt;a href=&quot;https://github.com/GoogleCloudPlatform/generative-ai&quot;&gt;this repo&lt;/a&gt; for examples.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! See &lt;a href=&quot;https://raw.githubusercontent.com/google-gemini/cookbook/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;p&gt;Thank you for developing with the Gemini API! We&#39;re excited to see what you create.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/generative-ai-for-beginners</title>
      <link>https://github.com/microsoft/generative-ai-for-beginners</link>
      <description>&lt;p&gt;21 Lessons, Get Started Building with Generative AI&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/images/repo-thumbnailv4-fixed.png?WT.mc_id=academic-105485-koreyst&quot; alt=&quot;Generative AI For Beginners&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;21 Lessons teaching everything you need to know to start building Generative AI applications&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-For-Beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🌐 Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/lt/README.md&quot;&gt;Lithuanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Generative AI for Beginners (Version 3) - A Course&lt;/h1&gt; 
&lt;p&gt;Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.&lt;/p&gt; 
&lt;h2&gt;🌱 Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has 21 lessons. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;Lessons are labeled either &quot;Learn&quot; lessons explaining a Generative AI concept or &quot;Build&quot; lessons that explain a concept and code examples in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt; when possible.&lt;/p&gt; 
&lt;p&gt;For .NET Developers checkout &lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners (.NET Edition)&lt;/a&gt;!&lt;/p&gt; 
&lt;p&gt;Each lesson also includes a &quot;Keep Learning&quot; section with additional learning tools.&lt;/p&gt; 
&lt;h2&gt;What You Need&lt;/h2&gt; 
&lt;h3&gt;To run the code of this course, you can use either:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/azure-open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;Azure OpenAI Service&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;aoai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/gh-models?WT.mc_id=academic-105485-koreyst&quot;&gt;GitHub Marketplace Model Catalog&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;githubmodels&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://aka.ms/genai-beginners/open-ai?WT.mc_id=academic-105485-koreyst&quot;&gt;OpenAI API&lt;/a&gt; - &lt;strong&gt;Lessons:&lt;/strong&gt; &quot;oai-assignment&quot;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Basic knowledge of Python or TypeScript is helpful - *For absolute beginners check out these &lt;a href=&quot;https://aka.ms/genai-beginners/python?WT.mc_id=academic-105485-koreyst&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://aka.ms/genai-beginners/typescript?WT.mc_id=academic-105485-koreyst&quot;&gt;TypeScript&lt;/a&gt; courses&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;A GitHub account to &lt;a href=&quot;https://aka.ms/genai-beginners/github?WT.mc_id=academic-105485-koreyst&quot;&gt;fork this entire repo&lt;/a&gt; to your own GitHub account&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We have created a &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/strong&gt; lesson to help you with setting up your development environment.&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to &lt;a href=&quot;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&quot;&gt;star (🌟) this repo&lt;/a&gt; to find it easier later.&lt;/p&gt; 
&lt;h2&gt;🧠 Ready to Deploy?&lt;/h2&gt; 
&lt;p&gt;If you are looking for more advanced code samples, check out our &lt;a href=&quot;https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst&quot;&gt;collection of Generative AI Code Samples&lt;/a&gt; in both &lt;strong&gt;Python&lt;/strong&gt; and &lt;strong&gt;TypeScript&lt;/strong&gt;.&lt;/p&gt; 
&lt;h2&gt;🗣️ Meet Other Learners, Get Support&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst&quot;&gt;official Azure AI Foundry Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;p&gt;Ask questions or share product feedback in our &lt;a href=&quot;https://aka.ms/azureaifoundry/forum&quot;&gt;Azure AI Foundry Developer Forum&lt;/a&gt; on Github.&lt;/p&gt; 
&lt;h2&gt;🚀 Building a Startup?&lt;/h2&gt; 
&lt;p&gt;Visit &lt;a href=&quot;https://www.microsoft.com/startups&quot;&gt;Microsoft for Startups&lt;/a&gt; to find out how to get started building with Azure credits today.&lt;/p&gt; 
&lt;h2&gt;🙏 Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst&quot;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&quot;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📂 Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A short video introduction to the topic&lt;/li&gt; 
 &lt;li&gt;A written lesson located in the README&lt;/li&gt; 
 &lt;li&gt;Python and TypeScript code samples supporting Azure OpenAI and OpenAI API&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🗃️ Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;#&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson Link&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;00&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/00-course-setup/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to Setup Your Development Environment&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;01&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Introduction to Generative AI and LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Understanding what Generative AI is and how Large Language Models (LLMs) work.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;02&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Exploring and comparing different LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to select the right model for your use case&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;03&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Using Generative AI Responsibly&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to build Generative AI Applications responsibly&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;04&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Understanding Prompt Engineering Fundamentals&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; Hands-on Prompt Engineering Best Practices&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;05&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Creating Advanced Prompts&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply prompt engineering techniques that improve the outcome of your prompts.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;06&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Text Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A text generation app using Azure OpenAI / OpenAI API&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;07&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Chat Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; Techniques for efficiently building and integrating chat applications.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;08&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Search Apps Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A search application that uses Embeddings to search for data.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;09&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Image Generation Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An image generation application&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;10&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building Low Code AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; A Generative AI application using Low Code tools&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;11&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Integrating External Applications with Function Calling&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; What is function calling and its use cases for applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;12&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Designing UX for AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; How to apply UX design principles when developing Generative AI Applications&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;13&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Securing Your Generative AI Applications&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The threats and risks to AI systems and methods to secure these systems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;14&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;The Generative AI Application Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The tools and metrics to manage the LLM Lifecycle and LLMOps&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;15&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Retrieval Augmented Generation (RAG) and Vector Databases&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using a RAG Framework to retrieve embeddings from a Vector Databases&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;16&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Open Source Models and Hugging Face&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using open source models available on Hugging Face&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;17&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;AI Agents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Build:&lt;/strong&gt; An application using an AI Agent Framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;18&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Fine-Tuning LLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The what, why and how of fine-tuning LLMs&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;19&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/19-slm/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with SLMs&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The benefits of building with Small Language Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;20&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/20-mistral/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Mistral Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Mistral Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;21&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/generative-ai-for-beginners/main/21-meta/README.md?WT.mc_id=academic-105485-koreyst&quot;&gt;Building with Meta Models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Learn:&lt;/strong&gt; The features and differences of the Meta Family Models&lt;/td&gt; 
   &lt;td&gt;Video Coming Soon&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Learn More&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🌟 Special thanks&lt;/h3&gt; 
&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/john0isaac/&quot;&gt;&lt;strong&gt;John Aziz&lt;/strong&gt;&lt;/a&gt; for creating all of the GitHub Actions and workflows&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/bernhard-merkle-738b73/&quot;&gt;&lt;strong&gt;Bernhard Merkle&lt;/strong&gt;&lt;/a&gt; for making key contributions to each lesson to improve the learner and code experience.&lt;/p&gt; 
&lt;h2&gt;🎒 Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mcp-for-beginners&quot;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners&quot;&gt;AI Agents for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-js-course&quot;&gt;Generative AI for Beginners using JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genaijava&quot;&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>roboflow/notebooks</title>
      <link>https://github.com/roboflow/notebooks</link>
      <description>&lt;p&gt;A collection of tutorials on state-of-the-art computer vision models and techniques. Explore everything from foundational architectures like ResNet to cutting-edge models like YOLO11, RT-DETR, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5VL.&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt; &lt;a align=&quot;center&quot; href=&quot;&quot; target=&quot;_blank&quot;&gt; &lt;img width=&quot;850&quot; src=&quot;https://raw.githubusercontent.com/roboflow/notebooks/main/assets/roboflow-notebooks-banner.png&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt;&lt;a href=&quot;https://github.com/roboflow/notebooks&quot;&gt;notebooks&lt;/a&gt; | &lt;a href=&quot;https://github.com/roboflow/inference&quot;&gt;inference&lt;/a&gt; | &lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;autodistill&lt;/a&gt; | &lt;a href=&quot;https://github.com/roboflow/rf-detr&quot;&gt;RF-DETR&lt;/a&gt;&lt;/p&gt; 
 &lt;br /&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://youtube.com/roboflow&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://www.linkedin.com/company/roboflow-ai/&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://docs.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt; 
  &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; 
  &lt;a href=&quot;https://discuss.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&quot; width=&quot;3%&quot; /&gt; &lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/raw/master/icons/transparent.png&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt;
  &lt;a href=&quot;https://blog.roboflow.com&quot;&gt; &lt;img src=&quot;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&quot; width=&quot;3%&quot; /&gt; &lt;/a&gt;  
 &lt;/div&gt; 
&lt;/div&gt; 
&lt;h2&gt;👋 hello&lt;/h2&gt; 
&lt;p&gt;This repository offers a growing collection of computer vision tutorials. Learn to use SOTA models like YOLOv11, SAM 2, Florence-2, PaliGemma 2, and Qwen2.5-VL for tasks ranging from object detection, segmentation, and pose estimation to data extraction and OCR. Dive in and explore the exciting world of computer vision!&lt;/p&gt; 
&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; 
&lt;!--
   WARNING: DO NOT EDIT THIS TABLE MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
--&gt; 
&lt;h2&gt;🚀 model tutorials (51 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-google-gamini-2-5.ipynb&quot;&gt;Zero-Shot Object Detection and Segmentation with Google Gemini 2.5&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-and-segmentation-with-google-gamini-2-5.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-google-gamini-2-5.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/gemini-2-5-object-detection-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2507.06261&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2507.06261-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb&quot;&gt;Fine-Tune RF-DETR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-rf-detr-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/rf-detr&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/yHW0ip-2i54&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/rf-detr&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;Zero-Shot Object Detection and Segmentation with YOLOE&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-and-segmentation-with-yoloe.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yoloe-zero-shot-object-detection-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=eHAnIehnCt4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/THU-MIG/yoloe&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2503.07465&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2503.07465-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;Fine-Tune YOLOv12 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov12-object-detection-model.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov12-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/sunsmarterjie/yolov12&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.12524&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;Zero-Shot Object Detection with Qwen2.5-VL&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-qwen2-5-vl.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/qwen2-5-vl-zero-shot-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=xEfh0IR8Fvo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;Fine-Tune Qwen2.5-VL for JSON Data Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-qwen2-5-vl-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xEfh0IR8Fvo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2502.13923-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma2 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;Fine-Tune PaliGemma2 for JSON Data Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-for-json-data-extraction.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma2 for LaTeX OCR&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma2-on-latex-ocr-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-paligemma-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2412.03555-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;Fine-Tune SAM-2.1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/fine-tune-sam-2.1.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-sam-2-1/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=QnCGcFHZy9s&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/sam2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;Fine-Tune GPT-4o on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/openai-gpt-4o-fine-tuning.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/gpt-4o-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=6Q6TieCBA4E&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO11 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov11-how-to-train-custom-data/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=jE_s4tVgPHA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO11 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo11-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolo11-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=jE_s4tVgPHA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;Segment Images with SAM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-images-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/Dv003fTyO-Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;Segment Videos with SAM2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-videos-with-sam-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/live/Dv003fTyO-Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2408.00714-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;Fine-Tune RT-DETR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rt-detr-on-custom-dataset-with-transformers.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-rt-detr-custom-dataset-transformers/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/lyuwenyu/RT-DETR&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.08069&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.08069-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;Fine-Tune Florence-2 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-florence-2-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/fine-tune-florence-2-object-detection/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=i3KjYgxNH6w&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06242&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;Run Different Vision Tasks with Florence-2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-run-different-vision-tasks-with-florence-2.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/florence-2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=hj_ybcRdk5Y&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06242&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2311.06242-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;Fine-Tune PaliGemma on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-finetune-paligemma-on-detection-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-fine-tune-paligemma/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=OMBmVInx68M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/google-research/big_vision/raw/main/big_vision/configs/proj/paligemma/README.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2407.07726&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2407.07726-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv10 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov10-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov10-how-to-train/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/THU-MIG/yolov10&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2405.14458&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2405.14458-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;Zero-Shot Object Detection with YOLO-World&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-yolo-world.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-yolo-world/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=X7gKBGVz4vs&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/AILab-CVC/YOLO-World&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2401.17270&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2401.17270-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv9 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov9-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/XHT2c8jT3Bc&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov9&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2402.13616&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2402.13616-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune RTMDet on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-rtmdet-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-rtmdet-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/5kgWyo6Sg4E&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2212.07784&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2212.07784-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;Segment Images with FastSAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-fast-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-fastsam&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/yHNPyqazYYU&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/CASIA-IVA-Lab/FastSAM&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2306.12156&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2306.12156-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLO-NAS on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolo-nas-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolo-nas-how-to-train-on-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/V-H3eoPUnA8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Deci-AI/super-gradients/raw/master/YOLONAS.md&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;Segment Images with Segment Anything Model (SAM)&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-segment-anything-with-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-segment-anything-model-sam&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.02643&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;Zero-Shot Object Detection with Grounding DINO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/grounding-dino-zero-shot-object-detection&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/cMa77r3YrDk&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;Fine-Tune DETR Transformer on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-huggingface-detr-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/AM8D4j9KoaU&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/AM8D4j9KoaU&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/detr&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2005.12872&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2005.12872-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&quot;&gt;Classify Images with DINOv2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-classify-images-with-dinov2/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/wuZtUMEiKWY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;Fine-Tune YOLOv8 on Pose Estimation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-keypoint.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-a-custom-yolov8-pose-estimation-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;Fine-Tune YOLOv8 on Oriented Bounding Boxes (OBB) Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-obb.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https:/blog.roboflow.com/train-yolov8-obb-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov8-instance-segmentation/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/pFiGSrRtaU4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;Fine-Tune YOLOv8 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov8-classification-on-custom-dataset.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-a-yolov8-classification-model/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv7 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov7-custom-dataset-training-tutorial&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=5nsmXLyDaU4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv7 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov7-instance-segmentation-on-custom-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vFGxM2KLs10&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune MT-YOLOv6 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=fFCWrMFH2UY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/meituan/YOLOv6&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2209.02976&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2209.02976-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolov5-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/watch?v=x0ThXHbtqCQ&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov5-classification-custom-data&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=DPjp9Kq4qn8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv5 on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov5-instance-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolov5-instance-segmentation-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=vKzfvtEtiYo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/yolov5&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune Faster RCNN on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-detectron2-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-detectron2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/e8LPflX0nwQ&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1703.06870v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1703.06870v3-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;Fine-Tune SegFormer on Instance Segmentation Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segformer-segmentation-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-segformer-on-a-custom-dataset-with-pytorch-lightning&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=4HNkBMfw-2o&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/NVlabs/SegFormer&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2105.15203v3&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2105.15203v3-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;Fine-Tune ViT on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-vision-transformer-classification-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-vision-transformer&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=8yRE2Pa-8_I&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/lucidrains/vit-pytorch&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2010.11929-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune Scaled-YOLOv4 on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-scaled-yolov4-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-scaled-yolov4&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=rEbpKxZbvIo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/WongKinYiu/ScaledYOLOv4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2004.10934&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2004.10934-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOS on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolos-huggingface-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolos-transformer-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=N0V0xxSi6Xc&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2106.00666&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2106.00666-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOR on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolor-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-yolor-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=sZ5DiXDOHEM&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/yolor&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1506.02640-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOX on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolox-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-yolox-on-a-custom-dataset&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=q3RbFbaQQGw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/Megvii-BaseDetection/YOLOX&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2107.08430&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2107.08430-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;Fine-Tune ResNet34 on Classification Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-resnet34-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-train-a-custom-resnet34-model&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=93kXzUOiYY4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;Image Classification with OpenAI Clip&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-openai-clip-classification.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-use-openai-clip&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=8o701AEoZ8I&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;Fine-Tune YOLOv4-tiny Darknet on Object Detection Dataset&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov4-tiny-object-detection-on-custom-data.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.ai/train-yolov4-tiny-on-custom-data-lighting-fast-detection&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=NTnZgLsk_DA&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/darknet&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2011.04244&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2011.04244-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;Train a YOLOv8 Classification Model with No Labeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-train-yolov8-classification-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-classification-model-no-labeling/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ultralytics/ultralytics&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;📍 tracker tutorials (2 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-sort-tracker.ipynb&quot;&gt;How to Track Objects with SORT Tracker&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-objects-with-sort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-sort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/trackers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1602.00763&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1602.00763-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-deepsort-tracker.ipynb&quot;&gt;How to Track Objects with DeepSORT Tracker&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-objects-with-deepsort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-objects-with-deepsort-tracker.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/trackers&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/1703.07402&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-1703.07402-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🛠️ computer vision skills (24 notebooks)&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;notebook&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;open in colab / kaggle / sagemaker studio lab&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;repository / paper&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb&quot;&gt;Basketball AI: Make or Miss - Jumpshot Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-make-or-miss-jumpshot-detection.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb&quot;&gt;Basketball AI: Detect NBA 3 Second Violation&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-automatic-detection-of-3-second-violations.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/detect-3-second-violation-ai-basketball&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&quot;&gt;Basketball AI: How to Detect Track and Identify Basketball Players&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/football-ai.ipynb&quot;&gt;Football AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/football-ai.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/football-ai.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/camera-calibration-sports-computer-vision/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/aBVGKoNZQUw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/sports&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;Auto-Annotate Dataset with GroundedSAM 2&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/grounded-sam-2-auto-label.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/what-is-segment-anything-2&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;Run YOLOv7 Object Detection with OpenVINO + TorchORT&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-yolov7-object-detection-on-custom-data-openvino-torch-ort.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/accelerate-pytorch-openvino-torch-ort&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/yolov7&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2207.02696&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2207.02696-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;Estimate Vehicle Speed with YOLOv8&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-estimate-vehicle-speed-with-computer-vision.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/estimate-speed-computer-vision/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/uWP6UjDeZvY&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision/tree/develop/examples/speed_estimation&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;Detect and Count Objects in Polygon Zone with YOLOv5 / YOLOv8 / Detectron2 + Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-detect-and-count-objects-in-polygon-zone.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-count-objects-in-a-zone&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/l_kf9CfZ_8M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;Track and Count Vehicles with YOLOv8 + ByteTRACK + Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-and-count-vehicles-with-yolov8.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/yolov8-tracking-and-counting/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/OS5qI9YBkfk&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/supervision&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.06864&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;Football Players Tracking with YOLOv5 + ByteTRACK&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-track-football-players.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/track-football-players&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/QCG8QMhga9k&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/ifzhang/ByteTrack&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2110.06864&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2110.06864-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;Auto Train YOLOv8 Model with Autodistill&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-auto-train-yolov8-model-with-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/autodistill&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/gKTYMfwPo4M&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;Image Embeddings Analysis - Part 1&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image_embeddings_analysis_part_1.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/YxJkE6FvGF4&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/openai/CLIP&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2103.00020&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2103.00020-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO and SAM&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/enhance-image-annotation-with-grounding-dino-and-sam/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;Automated Dataset Annotation and Evaluation with Grounding DINO&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://youtu.be/C4NqaRBz_Kw&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/IDEA-Research/GroundingDINO&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2303.05499&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;Roboflow Video Inference with Custom Annotators&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/roboflow_video_inference_with_custom_annotators.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/custom-annotator-video-inference&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow/inference&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;DINO-GPT-4V Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dino-gpt4v-autodistill.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/dino-gpt-4v/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;Train a Segmentation Model with No Labeling&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-segmentation-model-with-no-labeling.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/train-a-segmentation-model-no-labeling/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;DINOv2 Image Retrieval&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/dinov2-image-retrieval.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2304.07193-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;Vector Analysis with Scikit-learn and Bokeh&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/vector-analysis-with-sklearn-and-bokeh.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/vector-analysis&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;RF100 Object Detection Model Benchmarking&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-rf100.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/roboflow-100&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/jIgZMr-PBMo&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/youtube.svg?sanitize=true&quot; alt=&quot;YouTube&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/roboflow-100-benchmark&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2211.13523&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/arXiv-2211.13523-b31b1b.svg?sanitize=true&quot; alt=&quot;arXiv&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;Create Segmentation Masks with Roboflow&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-generate-segmentation-mask-with-roboflow.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/how-to-create-segmentation-masks-with-roboflow&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;How to Use PolygonZone and Roboflow Supervision&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/how-to-use-polygonzone-annotate-and-supervision.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/polygonzone/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;Train a Package Detector With Two Labeled Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/train-package-detector-two-labeled-images.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/package-detector/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/autodistill/autodistill-seggpt&quot;&gt;&lt;img src=&quot;https://badges.aleen42.com/src/github.svg?sanitize=true&quot; alt=&quot;GitHub&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;Image-to-Image Search with CLIP and faiss&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://kaggle.com/kernels/welcome?src=https://github.com/roboflow-ai/notebooks/raw/main/notebooks/image-to-image-search-clip-faiss.ipynb&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Kaggle&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://blog.roboflow.com/clip-image-search-faiss/&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg?sanitize=true&quot; alt=&quot;Roboflow&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;!-- AUTOGENERATED-NOTEBOOKS-TABLE --&gt; 
&lt;h2&gt;🎬 videos&lt;/h2&gt; 
&lt;p&gt;Almost every week we create tutorials showing you the hottest models in Computer Vision. 🔥 &lt;a href=&quot;https://www.youtube.com/@Roboflow&quot;&gt;Subscribe&lt;/a&gt;, and stay up to date with our latest YouTube videos!&lt;/p&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/CilXrt3S-ws&quot; title=&quot;How to Choose the Best Computer Vision Model for Your Project&quot;&gt;&lt;img src=&quot;https://github.com/roboflow/notebooks/assets/26109316/73a01d3b-cf70-40c3-a5e4-e4bc5be38d42&quot; alt=&quot;How to Choose the Best Computer Vision Model for Your Project&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/CilXrt3S-ws&quot; title=&quot;How to Choose the Best Computer Vision Model for Your Project&quot;&gt;&lt;strong&gt;How to Choose the Best Computer Vision Model for Your Project&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 26 May 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 26 May 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;br /&gt; In this video, we will dive into the complexity of choosing the right computer vision model for your unique project. From the importance of high-quality datasets to hardware considerations, interoperability, benchmarking, and licensing issues, this video covers it all... 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot; title=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/ae1ca38e-40b7-4b35-8582-e8ea5de3806e&quot; alt=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/oEQYStnF2l8&quot; title=&quot;Accelerate Image Annotation with SAM and Grounding DINO&quot;&gt;&lt;strong&gt;Accelerate Image Annotation with SAM and Grounding DINO&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 20 Apr 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 20 Apr 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;br /&gt; Discover how to speed up your image annotation process using Grounding DINO and Segment Anything Model (SAM). Learn how to convert object detection datasets into instance segmentation datasets, and see the potential of using these models to automatically annotate your datasets for real-time detectors like YOLOv8... 
&lt;p&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot; title=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot;&gt;&lt;img src=&quot;https://github.com/SkalskiP/SkalskiP/assets/26109316/6913ff11-53c6-4341-8d90-eaff3023c3fd&quot; alt=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot; width=&quot;300px&quot; align=&quot;left&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://youtu.be/D-D6ZmadzPE&quot; title=&quot;SAM - Segment Anything Model by Meta AI: Complete Guide&quot;&gt;&lt;strong&gt;SAM - Segment Anything Model by Meta AI: Complete Guide&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;
&lt;div&gt;
 &lt;strong&gt;Created: 11 Apr 2023&lt;/strong&gt; | 
 &lt;strong&gt;Updated: 11 Apr 2023&lt;/strong&gt;
&lt;/div&gt; 
&lt;p&gt;&lt;br /&gt; Discover the incredible potential of Meta AI&#39;s Segment Anything Model (SAM)! We dive into SAM, an efficient and promptable model for image segmentation, which has revolutionized computer vision tasks. With over 1 billion masks on 11M licensed and privacy-respecting images, SAM&#39;s zero-shot performance is often superior to prior fully supervised results... &lt;/p&gt;
&lt;p&gt;&lt;/p&gt; 
&lt;h2&gt;💻 run locally&lt;/h2&gt; 
&lt;p&gt;We try to make it as easy as possible to run Roboflow Notebooks in Colab and Kaggle, but if you still want to run them locally, below you will find instructions on how to do it. Remember don&#39;t install your dependencies globally, use &lt;a href=&quot;https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/&quot;&gt;venv&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-console&quot;&gt;# clone repository and navigate to root directory
git clone git@github.com:roboflow-ai/notebooks.git
cd notebooks

# setup python environment and activate it
python3 -m venv venv
source venv/bin/activate

# install and run jupyter notebook
pip install notebook
jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;☁️ run in sagemaker studio lab&lt;/h2&gt; 
&lt;p&gt;You can now open our tutorial notebooks in &lt;a href=&quot;https://aws.amazon.com/sagemaker/studio-lab/&quot;&gt;Amazon SageMaker Studio Lab&lt;/a&gt; - a free machine learning development environment that provides the compute, storage, and security—all at no cost—for anyone to learn and experiment with ML.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Stable Diffusion Image Generation&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;YOLOv5 Custom Dataset Training&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;YOLOv7 Custom Dataset Training&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/stable-diffusion-image-generation.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov5-custom-training.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://studiolab.sagemaker.aws/import/github/roboflow-ai/notebooks/blob/main/notebooks/sagemaker-studiolab/yolov7-custom-training.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/sage-maker.svg?sanitize=true&quot; alt=&quot;SageMaker&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🐞 bugs &amp;amp; 🦸 contribution&lt;/h2&gt; 
&lt;p&gt;Computer Vision moves fast! Sometimes our notebooks lag a tad behind the ever-pushing forward libraries. If you notice that any of the notebooks is not working properly, create a &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=bug%2Ctriage&amp;amp;template=bug-report.yml&quot;&gt;bug report&lt;/a&gt; and let us know.&lt;/p&gt; 
&lt;p&gt;If you have an idea for a new tutorial we should do, create a &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;template=feature-request.yml&quot;&gt;feature request&lt;/a&gt;. We are constantly looking for new ideas. If you feel up to the task and want to create a tutorial yourself, please take a peek at our &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/raw/main/CONTRIBUTING.md&quot;&gt;contribution guide&lt;/a&gt;. There you can find all the information you need.&lt;/p&gt; 
&lt;p&gt;We are here for you, so don&#39;t hesitate to &lt;a href=&quot;https://github.com/roboflow-ai/notebooks/discussions&quot;&gt;reach out&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pyannote/pyannote-audio</title>
      <link>https://github.com/pyannote/pyannote-audio</link>
      <description>&lt;p&gt;Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Using &lt;code&gt;pyannote.audio&lt;/code&gt; open-source toolkit in production?&lt;br /&gt; Consider switching to &lt;a href=&quot;https://www.pyannote.ai&quot;&gt;pyannoteAI&lt;/a&gt; for better and faster options.&lt;/p&gt; 
&lt;h1&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; speaker diarization toolkit&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; is an open-source toolkit written in Python for speaker diarization. Based on &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/pytorch.org&quot;&gt;PyTorch&lt;/a&gt; machine learning framework, it comes with state-of-the-art &lt;a href=&quot;https://hf.co/pyannote&quot;&gt;pretrained models and pipelines&lt;/a&gt;, that can be further finetuned to your own data for even better performance.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=37R_R82lfwA&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/37R_R82lfwA/0.jpg&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;TL;DR&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://github.com/pyannote/pyannote-audio&quot;&gt;&lt;code&gt;pyannote.audio&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;pip install pyannote.audio&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Accept &lt;a href=&quot;https://hf.co/pyannote/segmentation-3.0&quot;&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/a&gt; user conditions&lt;/li&gt; 
 &lt;li&gt;Accept &lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;&lt;code&gt;pyannote/speaker-diarization-3.1&lt;/code&gt;&lt;/a&gt; user conditions&lt;/li&gt; 
 &lt;li&gt;Create access token at &lt;a href=&quot;https://hf.co/settings/tokens&quot;&gt;&lt;code&gt;hf.co/settings/tokens&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained(
    &quot;pyannote/speaker-diarization-3.1&quot;,
    use_auth_token=&quot;HUGGINGFACE_ACCESS_TOKEN_GOES_HERE&quot;)

# send pipeline to GPU (when available)
import torch
pipeline.to(torch.device(&quot;cuda&quot;))

# apply pretrained pipeline
diarization = pipeline(&quot;audio.wav&quot;)

# print the result
for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f&quot;start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}&quot;)
# start=0.2s stop=1.5s speaker_0
# start=1.8s stop=3.9s speaker_1
# start=4.2s stop=5.7s speaker_0
# ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;🤗&lt;/span&gt; pretrained &lt;a href=&quot;https://hf.co/models?other=pyannote-audio-pipeline&quot;&gt;pipelines&lt;/a&gt; (and &lt;a href=&quot;https://hf.co/models?other=pyannote-audio-model&quot;&gt;models&lt;/a&gt;) on &lt;a href=&quot;https://huggingface.co/pyannote&quot;&gt;&lt;span&gt;🤗&lt;/span&gt; model hub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;🤯&lt;/span&gt; state-of-the-art performance (see &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/#benchmark&quot;&gt;Benchmark&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;🐍&lt;/span&gt; Python-first API&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;⚡&lt;/span&gt; multi-GPU training with &lt;a href=&quot;https://pytorchlightning.ai/&quot;&gt;pytorch-lightning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/CHANGELOG.md&quot;&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/FAQ.md&quot;&gt;Frequently asked questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Models 
  &lt;ul&gt; 
   &lt;li&gt;Available tasks explained&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/applying_a_model.ipynb&quot;&gt;Applying a pretrained model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/training_a_model.ipynb&quot;&gt;Training, fine-tuning, and transfer learning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Pipelines 
  &lt;ul&gt; 
   &lt;li&gt;Available pipelines explained&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/applying_a_pipeline.ipynb&quot;&gt;Applying a pretrained pipeline&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/adapting_pretrained_pipeline.ipynb&quot;&gt;Adapting a pretrained pipeline to your own data&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/voice_activity_detection.ipynb&quot;&gt;Training a pipeline&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Contributing 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/add_your_own_model.ipynb&quot;&gt;Adding a new model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/add_your_own_task.ipynb&quot;&gt;Adding a new task&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Adding a new pipeline&lt;/li&gt; 
   &lt;li&gt;Sharing pretrained models and pipelines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Blog 
  &lt;ul&gt; 
   &lt;li&gt;2022-12-02 &amp;gt; &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/adapting_pretrained_pipeline.ipynb&quot;&gt;&quot;How I reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022 speaker diarization challenges&quot;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2022-10-23 &amp;gt; &lt;a href=&quot;https://herve.niderb.fr/fastpages/2022/10/23/One-speaker-segmentation-model-to-rule-them-all&quot;&gt;&quot;One speaker segmentation model to rule them all&quot;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2021-08-05 &amp;gt; &lt;a href=&quot;https://herve.niderb.fr/fastpages/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html&quot;&gt;&quot;Streaming voice activity detection with pyannote.audio&quot;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Videos 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://umotion.univ-lemans.fr/video/9513-speech-segmentation-and-speaker-diarization/&quot;&gt;Introduction to speaker diarization&lt;/a&gt; / JSALT 2023 summer school / 90 min&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wDH2rvkjymY&quot;&gt;Speaker segmentation model&lt;/a&gt; / Interspeech 2021 / 3 min&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=37R_R82lfwA&quot;&gt;First release of pyannote.audio&lt;/a&gt; / ICASSP 2020 / 8 min&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Community contributions (not maintained by the core team) 
  &lt;ul&gt; 
   &lt;li&gt;2024-04-05 &amp;gt; &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/community/offline_usage_speaker_diarization.ipynb&quot;&gt;Offline speaker diarization (speaker-diarization-3.1)&lt;/a&gt; by &lt;a href=&quot;https://github.com/simonottenhauskenbun&quot;&gt;Simon Ottenhaus&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;Out of the box, &lt;code&gt;pyannote.audio&lt;/code&gt; speaker diarization &lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;pipeline&lt;/a&gt; v3.1 is expected to be much better (and faster) than v2.x. Those numbers are diarization error rates (in %):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-2.1&quot;&gt;v2.1&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;v3.1&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://www.pyannote.ai&quot;&gt;pyannoteAI&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.03603&quot;&gt;AISHELL-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14.1&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
   &lt;td&gt;11.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.openslr.org/119/&quot;&gt;AliMeeting&lt;/a&gt; (channel 1)&lt;/td&gt; 
   &lt;td&gt;27.4&lt;/td&gt; 
   &lt;td&gt;24.4&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;AMI&lt;/a&gt; (IHM)&lt;/td&gt; 
   &lt;td&gt;18.9&lt;/td&gt; 
   &lt;td&gt;18.8&lt;/td&gt; 
   &lt;td&gt;16.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;AMI&lt;/a&gt; (SDM)&lt;/td&gt; 
   &lt;td&gt;27.1&lt;/td&gt; 
   &lt;td&gt;22.4&lt;/td&gt; 
   &lt;td&gt;20.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.14448&quot;&gt;AVA-AVD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;66.3&lt;/td&gt; 
   &lt;td&gt;50.0&lt;/td&gt; 
   &lt;td&gt;39.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2001S97&quot;&gt;CALLHOME&lt;/a&gt; (&lt;a href=&quot;https://github.com/BUTSpeechFIT/CALLHOME_sublists/issues/1&quot;&gt;part 2&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;31.6&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
   &lt;td&gt;22.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2022S14&quot;&gt;DIHARD 3&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2012.01477&quot;&gt;full&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;26.9&lt;/td&gt; 
   &lt;td&gt;21.7&lt;/td&gt; 
   &lt;td&gt;17.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/revdotcom/speech-datasets&quot;&gt;Earnings21&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;17.0&lt;/td&gt; 
   &lt;td&gt;9.4&lt;/td&gt; 
   &lt;td&gt;9.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.07058&quot;&gt;Ego4D&lt;/a&gt; (dev.)&lt;/td&gt; 
   &lt;td&gt;61.5&lt;/td&gt; 
   &lt;td&gt;51.2&lt;/td&gt; 
   &lt;td&gt;43.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/X-LANCE/MSDWILD&quot;&gt;MSDWild&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;32.8&lt;/td&gt; 
   &lt;td&gt;25.3&lt;/td&gt; 
   &lt;td&gt;19.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.openslr.org/123/&quot;&gt;RAMC&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
   &lt;td&gt;22.2&lt;/td&gt; 
   &lt;td&gt;18.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.islrn.org/resources/360-758-359-485-0/&quot;&gt;REPERE&lt;/a&gt; (phase2)&lt;/td&gt; 
   &lt;td&gt;8.2&lt;/td&gt; 
   &lt;td&gt;7.8&lt;/td&gt; 
   &lt;td&gt;7.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/joonson/voxconverse&quot;&gt;VoxConverse&lt;/a&gt; (v0.3)&lt;/td&gt; 
   &lt;td&gt;11.2&lt;/td&gt; 
   &lt;td&gt;11.3&lt;/td&gt; 
   &lt;td&gt;9.4&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;http://pyannote.github.io/pyannote-metrics/reference.html#diarization&quot;&gt;Diarization error rate&lt;/a&gt; (in %)&lt;/p&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;p&gt;If you use &lt;code&gt;pyannote.audio&lt;/code&gt; please use the following citations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Plaquet23,
  author={Alexis Plaquet and Hervé Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Bredin23,
  author={Hervé Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;The commands below will setup pre-commit hooks and packages needed for developing the &lt;code&gt;pyannote.audio&lt;/code&gt; library.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e .[dev,testing]
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Test&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pytest
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;🦜🔗 Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot; /&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot; /&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot; /&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pepy/dt/langchain&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in Github Codespace&quot; title=&quot;Open in Github Codespace&quot; width=&quot;150&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codspeed.io/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If you’re looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChain’s ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.langchain.com/langgraph-platform&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://forum.langchain.com/&quot;&gt;LangChain Forum&lt;/a&gt;: Connect with the community and share all of your technical questions, ideas, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://chat.langchain.com/&quot;&gt;Chat LangChain&lt;/a&gt;: Ask questions &amp;amp; chat with our documentation.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/happy-llm</title>
      <link>https://github.com/datawhalechina/happy-llm</link>
      <description>&lt;p&gt;📚 从零开始的大语言模型原理与实践教程&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/head.jpg&quot; alt=&quot;alt text&quot; width=&quot;100%&quot; /&gt; 
 &lt;h1&gt;Happy-LLM&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub stars&quot; /&gt; 
 &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub forks&quot; /&gt; 
 &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot; /&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub Project&quot; /&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://swanlab.cn/@kmno4/Happy-LLM/overview&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg?sanitize=true&quot; alt=&quot;SwanLab&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://trendshift.io/repositories/14175&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14175&quot; alt=&quot;datawhalechina%2Fhappy-llm | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README.md&quot;&gt;中文&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://datawhalechina.github.io/happy-llm/&quot;&gt;📚 在线阅读地址&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;📚 从零开始的大语言模型原理与实践教程&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;深入理解 LLM 核心原理，动手实现你的第一个大模型&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🎯 项目介绍&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;  &lt;em&gt;很多小伙伴在看完 Datawhale开源项目： &lt;a href=&quot;https://github.com/datawhalechina/self-llm&quot;&gt;self-llm 开源大模型食用指南&lt;/a&gt; 后，感觉意犹未尽，想要深入了解大语言模型的原理和训练过程。于是我们（Datawhale）决定推出《Happy-LLM》项目，旨在帮助大家深入理解大语言模型的原理和训练过程。&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;  本项目是一个&lt;strong&gt;系统性的 LLM 学习教程&lt;/strong&gt;，将从 NLP 的基本研究方法出发，根据 LLM 的思路及原理逐层深入，依次为读者剖析 LLM 的架构基础和训练过程。同时，我们会结合目前 LLM 领域最主流的代码框架，演练如何亲手搭建、训练一个 LLM，期以实现授之以鱼，更授之以渔。希望大家能从这本书开始走入 LLM 的浩瀚世界，探索 LLM 的无尽可能。&lt;/p&gt; 
&lt;h3&gt;✨ 你将收获什么？&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;📚 &lt;strong&gt;Datawhale 开源免费&lt;/strong&gt; 完全免费的学习本项目所有内容&lt;/li&gt; 
 &lt;li&gt;🔍 &lt;strong&gt;深入理解&lt;/strong&gt; Transformer 架构和注意力机制&lt;/li&gt; 
 &lt;li&gt;📚 &lt;strong&gt;掌握&lt;/strong&gt; 预训练语言模型的基本原理&lt;/li&gt; 
 &lt;li&gt;🧠 &lt;strong&gt;了解&lt;/strong&gt; 现有大模型的基本结构&lt;/li&gt; 
 &lt;li&gt;🏗️ &lt;strong&gt;动手实现&lt;/strong&gt; 一个完整的 LLaMA2 模型&lt;/li&gt; 
 &lt;li&gt;⚙️ &lt;strong&gt;掌握训练&lt;/strong&gt; 从预训练到微调的全流程&lt;/li&gt; 
 &lt;li&gt;🚀 &lt;strong&gt;实战应用&lt;/strong&gt; RAG、Agent 等前沿技术&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📖 内容导航&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;章节&lt;/th&gt; 
   &lt;th&gt;关键内容&lt;/th&gt; 
   &lt;th&gt;状态&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/%E5%89%8D%E8%A8%80.md&quot;&gt;前言&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;本项目的缘起、背景及读者建议&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20NLP%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md&quot;&gt;第一章 NLP 基础概念&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;什么是 NLP、发展历程、任务分类、文本表示演进&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Transformer%E6%9E%B6%E6%9E%84.md&quot;&gt;第二章 Transformer 架构&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;注意力机制、Encoder-Decoder、手把手搭建 Transformer&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&quot;&gt;第三章 预训练语言模型&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Encoder-only、Encoder-Decoder、Decoder-Only 模型对比&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&quot;&gt;第四章 大语言模型&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;LLM 定义、训练策略、涌现能力分析&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B.md&quot;&gt;第五章 动手搭建大模型&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;实现 LLaMA2、训练 Tokenizer、预训练小型 LLM&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5.md&quot;&gt;第六章 大模型训练实践&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;预训练、有监督微调、LoRA/QLoRA 高效微调&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8.md&quot;&gt;第七章 大模型应用&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;模型评测、RAG 检索增强、Agent 智能体&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&quot;&gt;Extra Chapter LLM Blog&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;优秀的大模型 学习笔记/Blog ，欢迎大家来 PR ！&lt;/td&gt; 
   &lt;td&gt;🚧&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Extra Chapter LLM Blog&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/why-fine-tune-small-large-language-models/readme.md&quot;&gt;大模型都这么厉害了，微调0.6B的小模型有什么意义？&lt;/a&gt; @&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;不要葱姜蒜&lt;/a&gt; 2025-7-11&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/transformer-architecture/&quot;&gt;Transformer 整体模块设计解读&lt;/a&gt; @&lt;a href=&quot;https://github.com/ditingdapeng&quot;&gt;ditingdapeng&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/text-data-processing/readme.md&quot;&gt;文本数据处理详解&lt;/a&gt; @&lt;a href=&quot;https://github.com/xinala-781&quot;&gt;蔡鋆捷&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/vlm-concatenation-finetune/README.md&quot;&gt;Qwen3-&quot;VL&quot;——超小中文多模态模型的“拼接微调”之路&lt;/a&gt; @&lt;a href=&quot;https://github.com/ShaohonChen&quot;&gt;ShaohonChen&lt;/a&gt; 2025-7-30&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/s1-vllm-thinking-budget/readme.md&quot;&gt;S1: Thinking Budget with vLLM&lt;/a&gt; @&lt;a href=&quot;https://github.com/kmno4-zx&quot;&gt;kmno4-zx&lt;/a&gt; 2025-8-03&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/CDDRS/readme.md&quot;&gt;CDDRS: 使用细粒度语义信息指导增强的RAG检索方法&lt;/a&gt; @&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;Hongru0306&lt;/a&gt; 2025-8-21&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;  &lt;em&gt;如果大家在学习 Happy-LLM 项目或 LLM 相关知识中有自己独到的见解、认知、实践，欢迎大家 PR 在 &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&quot;&gt;Extra Chapter LLM Blog&lt;/a&gt; 中。请遵守 Extra Chapter LLM Blog 的 &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/Readme.md&quot;&gt;PR 规范&lt;/a&gt;，我们会视 PR 内容的质量和价值来决定是否合并或补充到 Happy-LLM 正文中来。&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;模型下载&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;模型名称&lt;/th&gt; 
   &lt;th&gt;下载地址&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-Base-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-base&quot;&gt;🤖 ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-SFT-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-sft&quot;&gt;🤖 ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;ModelScope 创空间体验地址：&lt;a href=&quot;https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft&quot;&gt;🤖 创空间&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;PDF 版本下载&lt;/h3&gt; 
&lt;p&gt;  &lt;em&gt;&lt;strong&gt;本 Happy-LLM PDF 教程完全开源免费。为防止各类营销号加水印后贩卖给大模型初学者，我们特地在 PDF 文件中预先添加了不影响阅读的 Datawhale 开源标志水印，敬请谅解～&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Happy-LLM PDF : &lt;a href=&quot;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&quot;&gt;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;💡 如何学习&lt;/h2&gt; 
&lt;p&gt;  本项目适合大学生、研究人员、LLM 爱好者。在学习本项目之前，建议具备一定的编程经验，尤其是要对 Python 编程语言有一定的了解。最好具备深度学习的相关知识，并了解 NLP 领域的相关概念和术语，以便更轻松地学习本项目。&lt;/p&gt; 
&lt;p&gt;  本项目分为两部分——基础知识与实战应用。第1章～第4章是基础知识部分，从浅入深介绍 LLM 的基本原理。其中，第1章简单介绍 NLP 的基本任务和发展，为非 NLP 领域研究者提供参考；第2章介绍 LLM 的基本架构——Transformer，包括原理介绍及代码实现，作为 LLM 最重要的理论基础；第3章整体介绍经典的 PLM，包括 Encoder-Only、Encoder-Decoder 和 Decoder-Only 三种架构，也同时介绍了当前一些主流 LLM 的架构和思想；第4章则正式进入 LLM 部分，详细介绍 LLM 的特点、能力和整体训练过程。第5章～第7章是实战应用部分，将逐步带领大家深入 LLM 的底层细节。其中，第5章将带领大家者基于 PyTorch 层亲手搭建一个 LLM，并实现预训练、有监督微调的全流程；第6章将引入目前业界主流的 LLM 训练框架 Transformers，带领学习者基于该框架快速、高效地实现 LLM 训练过程；第7章则将介绍 基于 LLM 的各种应用，补全学习者对 LLM 体系的认知，包括 LLM 的评测、检索增强生成（Retrieval-Augmented Generation，RAG）、智能体（Agent）的思想和简单实现。你可以根据个人兴趣和需求，选择性地阅读相关章节。&lt;/p&gt; 
&lt;p&gt;  在阅读本书的过程中，建议你将理论和实际相结合。LLM 是一个快速发展、注重实践的领域，我们建议你多投入实战，复现本书提供的各种代码，同时积极参加 LLM 相关的项目与比赛，真正投入到 LLM 开发的浪潮中。我们鼓励你关注 Datawhale 及其他 LLM 相关开源社区，当遇到问题时，你可以随时在本项目的 issue 区提问。&lt;/p&gt; 
&lt;p&gt;  最后，欢迎每一位读者在学习完本项目后加入到 LLM 开发者的行列。作为国内 AI 开源社区，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更全面特色 LLM 的教程。星火点点，汇聚成海。我们希望成为 LLM 与普罗大众的阶梯，以自由、平等的开源精神，拥抱更恢弘而辽阔的 LLM 世界。&lt;/p&gt; 
&lt;h2&gt;🤝 如何贡献&lt;/h2&gt; 
&lt;p&gt;我们欢迎任何形式的贡献！&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;报告 Bug&lt;/strong&gt; - 发现问题请提交 Issue&lt;/li&gt; 
 &lt;li&gt;💡 &lt;strong&gt;功能建议&lt;/strong&gt; - 有好想法就告诉我们&lt;/li&gt; 
 &lt;li&gt;📝 &lt;strong&gt;内容完善&lt;/strong&gt; - 帮助改进教程内容&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;代码优化&lt;/strong&gt; - 提交 Pull Request&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🙏 致谢&lt;/h2&gt; 
&lt;h3&gt;核心贡献者&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;宋志学-项目负责人&lt;/a&gt; (Datawhale成员-中国矿业大学(北京))&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;邹雨衡-项目负责人&lt;/a&gt; (Datawhale成员-对外经济贸易大学)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://xinzhongzhu.github.io/&quot;&gt;朱信忠-指导专家&lt;/a&gt;（Datawhale首席科学家-浙江师范大学杭州人工智能研究院教授）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter 贡献者&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ditingdapeng&quot;&gt;ditingdapeng&lt;/a&gt;（内容贡献者-云原生基础架构工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinala-781&quot;&gt;蔡鋆捷&lt;/a&gt;（内容贡献者-福州大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ShaohonChen&quot;&gt;ShaohonChen&lt;/a&gt; （情感机器实验室研究员-西安电子科技大学在读硕士）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;肖鸿儒, 庄健琨&lt;/a&gt; (内容贡献者-同济大学)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;特别感谢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;感谢 &lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt; 对本项目的帮助与支持&lt;/li&gt; 
 &lt;li&gt;感谢所有为本项目做出贡献的开发者们 ❤️&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/happy-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/happy-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/star-history-2025710.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot; /&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;⭐ 如果这个项目对你有帮助，请给我们一个 Star！&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;关于 Datawhale&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot; /&gt; 
 &lt;p&gt;扫描二维码关注 Datawhale 公众号，获取更多优质开源内容&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📜 开源协议&lt;/h2&gt; 
&lt;p&gt;本作品采用&lt;a href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议&lt;/a&gt;进行许可。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>jakevdp/PythonDataScienceHandbook</title>
      <link>https://github.com/jakevdp/PythonDataScienceHandbook</link>
      <description>&lt;p&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Data Science Handbook&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge.svg?sanitize=true&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository contains the entire &lt;a href=&quot;http://shop.oreilly.com/product/0636920034919.do&quot;&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/PDSH-cover.png&quot; alt=&quot;cover image&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;How to Use this Book&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Read the book in its entirety online at &lt;a href=&quot;https://jakevdp.github.io/PythonDataScienceHandbook/&quot;&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the code using the Jupyter notebooks available in this repository&#39;s &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks&quot;&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch executable versions of these notebooks using &lt;a href=&quot;http://colab.research.google.com&quot;&gt;Google Colab&lt;/a&gt;: &lt;a href=&quot;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href=&quot;https://beta.mybinder.org/&quot;&gt;binder&lt;/a&gt;: &lt;a href=&quot;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge.svg?sanitize=true&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Buy the printed book through &lt;a href=&quot;http://shop.oreilly.com/product/0636920034919.do&quot;&gt;O&#39;Reilly Media&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt; 
&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href=&quot;http://ipython.org&quot;&gt;IPython&lt;/a&gt;, &lt;a href=&quot;http://numpy.org&quot;&gt;NumPy&lt;/a&gt;, &lt;a href=&quot;http://pandas.pydata.org&quot;&gt;Pandas&lt;/a&gt;, &lt;a href=&quot;http://matplotlib.org&quot;&gt;Matplotlib&lt;/a&gt;, &lt;a href=&quot;http://scikit-learn.org&quot;&gt;Scikit-Learn&lt;/a&gt;, and related packages. Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project, &lt;a href=&quot;https://github.com/jakevdp/WhirlwindTourOfPython&quot;&gt;A Whirlwind Tour of Python&lt;/a&gt;: it&#39;s a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&quot;&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt; 
&lt;h2&gt;Software&lt;/h2&gt; 
&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt; 
&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/requirements.txt&quot;&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use). To install the requirements using &lt;a href=&quot;http://conda.pydata.org&quot;&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can read more about using conda environments in the &lt;a href=&quot;http://conda.pydata.org/docs/using/envs.html&quot;&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;h3&gt;Code&lt;/h3&gt; 
&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-CODE&quot;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Text&lt;/h3&gt; 
&lt;p&gt;The text content of the book is released under the &lt;a href=&quot;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-TEXT&quot;&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href=&quot;https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode&quot;&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>onnx/models</title>
      <link>https://github.com/onnx/models</link>
      <description>&lt;p&gt;A collection of pre-trained, state-of-the-art models in the ONNX format&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Deprecation Notice&lt;/strong&gt;: We sincerely thank the community for participating in the ONNX Model Zoo effort. As the machine learning ecosystem has evolved, much of the novel model sharing has successfully transitioned to Hugging Face, which maintains a vibrant and healthy state. We are preserving the ONNX Model Zoo repository for historical purposes only. Please note that models will no longer be available for LFS download starting July 1st, 2025. You can still get access to the models that were originally available on this repository by going to &lt;a href=&quot;https://huggingface.co/onnxmodelzoo&quot;&gt;https://huggingface.co/onnxmodelzoo&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;ONNX Model Zoo&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to the ONNX Model Zoo! The Open Neural Network Exchange (ONNX) is an open standard format created to represent machine learning models. Supported by a robust community of partners, ONNX defines a common set of operators and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.&lt;/p&gt; 
&lt;p&gt;This repository is a curated collection of pre-trained, state-of-the-art models in the ONNX format. These models are sourced from prominent open-source repositories and have been contributed by a diverse group of community members. Our aim is to facilitate the spread and usage of machine learning models among a wider audience of developers, researchers, and enthusiasts.&lt;/p&gt; 
&lt;p&gt;To handle ONNX model files, which can be large, we use Git LFS (Large File Storage).&lt;/p&gt; 
&lt;h2&gt;Models&lt;/h2&gt; 
&lt;p&gt;Currently, we are expanding the ONNX Model Zoo by incorporating additional models from the following categories. As we are rigorously validating the new models for accuracy, refer to the &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#validated-models&quot;&gt;validated models&lt;/a&gt; below that have been successfully validated for accuracy:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Computer Vision&lt;/li&gt; 
 &lt;li&gt;Natural Language Processing (NLP)&lt;/li&gt; 
 &lt;li&gt;Generative AI&lt;/li&gt; 
 &lt;li&gt;Graph Machine Learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These models are sourced from prominent open-source repositories such as &lt;a href=&quot;https://github.com/huggingface/pytorch-image-models&quot;&gt;timm&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/vision&quot;&gt;torchvision&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/hub/&quot;&gt;torch_hub&lt;/a&gt;, and &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformers&lt;/a&gt;, and exported into the ONNX format using the open-source &lt;a href=&quot;https://github.com/onnx/turnkeyml&quot;&gt;TurnkeyML toolchain&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Validated Models&lt;/h2&gt; 
&lt;h4&gt;Vision&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#image_classification&quot;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#object_detection&quot;&gt;Object Detection &amp;amp; Image Segmentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#body_analysis&quot;&gt;Body, Face &amp;amp; Gesture Analysis&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#image_manipulation&quot;&gt;Image Manipulation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Language&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#machine_comprehension&quot;&gt;Machine Comprehension&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#machine_translation&quot;&gt;Machine Translation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#language_modelling&quot;&gt;Language Modelling&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Other&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#visual_qna&quot;&gt;Visual Question Answering &amp;amp; Dialog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#speech&quot;&gt;Speech &amp;amp; Audio Processing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#others&quot;&gt;Other interesting models&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Read the &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#usage-&quot;&gt;Usage&lt;/a&gt; section below for more details on the file formats in the ONNX Model Zoo (.onnx, .pb, .npz), downloading multiple ONNX models through &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/#gitlfs-&quot;&gt;Git LFS command line&lt;/a&gt;, and starter Python code for validating your ONNX model using test data.&lt;/p&gt; 
&lt;p&gt;INT8 models are generated by &lt;a href=&quot;https://github.com/intel/neural-compressor&quot;&gt;Intel® Neural Compressor&lt;/a&gt;. &lt;a href=&quot;https://github.com/intel/neural-compressor&quot;&gt;Intel® Neural Compressor&lt;/a&gt; is an open-source Python library which supports automatic accuracy-driven tuning strategies to help user quickly find out the best quantized model. It implements dynamic and static quantization for ONNX models and can represent quantized ONNX models with operator oriented as well as tensor oriented (QDQ) ways. Users can use web-based UI service or python code to do quantization. Read the &lt;a href=&quot;https://github.com/intel/neural-compressor/raw/master/README.md&quot;&gt;Introduction&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h3&gt;Image Classification &lt;a name=&quot;image_classification&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This collection of models take images as input, then classifies the major objects in the images into 1000 object categories such as keyboard, mouse, pencil, and many animals.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Huggingface Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/mobilenet&quot;&gt;MobileNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;Sandler et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light-weight deep neural network best suited for mobile and embedded vision applications. &lt;br /&gt;Top-5 error from paper - ~10%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/resnet&quot;&gt;ResNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;He et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN model (up to 152 layers). Uses shortcut connections to achieve higher accuracy when classifying images. &lt;br /&gt; Top-5 error from paper - ~3.6%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ResNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/squeezenet&quot;&gt;SqueezeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;Iandola et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A light-weight CNN model providing AlexNet level accuracy with 50x fewer parameters. &lt;br /&gt;Top-5 error from paper - ~20%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/SqueezeNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/vgg&quot;&gt;VGG&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.1556&quot;&gt;Simonyan et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model(up to 19 layers). Similar to AlexNet but uses multiple smaller kernel-sized filters that provides more accuracy when classifying images. &lt;br /&gt;Top-5 error from paper - ~8%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/VGG&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/alexnet&quot;&gt;AlexNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A Deep CNN model (up to 8 layers) where the input is an image and the output is a vector of 1000 numbers. &lt;br /&gt; Top-5 error from paper - ~15%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/AlexNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/inception_and_googlenet/googlenet&quot;&gt;GoogleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.4842.pdf&quot;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model(up to 22 layers). Comparatively smaller and faster than VGG and more accurate in detailing than AlexNet. &lt;br /&gt; Top-5 error from paper - ~6.7%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/GoogleNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/caffenet&quot;&gt;CaffeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf&quot;&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN variation of AlexNet for Image Classification in Caffe where the max pooling precedes the local response normalization (LRN) so that the LRN takes less compute and memory.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/CaffeNet&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/rcnn_ilsvrc13&quot;&gt;RCNN_ILSVRC13&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2524&quot;&gt;Girshick et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Pure Caffe implementation of R-CNN for image classification. This model uses localization of regions to classify and extract features from images.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/densenet-121&quot;&gt;DenseNet-121&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.06993&quot;&gt;Huang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model that has every layer connected to every other layer and passes on its own feature providing strong gradient flow and more diversified features.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/DenseNet-121&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/inception_and_googlenet/inception_v1&quot;&gt;Inception_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.4842&quot;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model is same as GoogLeNet, implemented through Caffe2 that has improved utilization of the computing resources inside the network and helps with the vanishing gradient problem. &lt;br /&gt; Top-5 error from paper - ~6.7%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/Inception_v1&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/inception_and_googlenet/inception_v2&quot;&gt;Inception_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.00567&quot;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model for Image Classification as an adaptation to Inception v1 with batch normalization. This model has reduced computational cost and improved image resolution compared to Inception v1. &lt;br /&gt; Top-5 error from paper ~4.82%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/shufflenet&quot;&gt;ShuffleNet_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.01083&quot;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This model greatly reduces the computational cost and provides a ~13x speedup over AlexNet on ARM-based mobile devices. Compared to MobileNet, ShuffleNet achieves superior performance by a significant margin due to it&#39;s efficient structure. &lt;br /&gt; Top-1 error from paper - ~32.6%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/shufflenet&quot;&gt;ShuffleNet_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1807.11164&quot;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This network architecture design considers direct metric such as speed, instead of indirect metric like FLOP. &lt;br /&gt; Top-1 error from paper - ~30.6%&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/zfnet-512&quot;&gt;ZFNet-512&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1311.2901&quot;&gt;Zeiler et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model (up to 8 layers) that increased the number of features that the network is capable of detecting that helps to pick image features at a finer level of resolution. &lt;br /&gt; Top-5 error from paper - ~14.3%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ZFNet-512&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/efficientnet-lite4&quot;&gt;EfficientNet-Lite4&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;Tan et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;CNN model with an order of magnitude of few computations and parameters, while still acheiving state-of-the-art accuracy and better efficiency than previous ConvNets. &lt;br /&gt; Top-5 error from paper - ~2.9%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/EfficientNet-Lite4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Domain-based Image Classification &lt;a name=&quot;domain_based_image&quot;&gt;&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;This subset of models classify images for specific domains and datasets.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/classification/mnist&quot;&gt;MNIST-Handwritten Digit Recognition&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Microsoft/CNTK/raw/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb&quot;&gt;Convolutional Neural Network with MNIST&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN model for handwritten digit identification&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Object Detection &amp;amp; Image Segmentation &lt;a name=&quot;object_detection&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Object detection models detect the presence of multiple objects in an image and segment out areas of the image where the objects are detected. Semantic segmentation models partition an input image by labeling each pixel into a set of pre-defined categories.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/tiny-yolov2&quot;&gt;Tiny YOLOv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.08242.pdf&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time CNN for object detection that detects 20 different classes. A smaller version of the more complex full YOLOv2 network.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/ssd&quot;&gt;SSD&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot;&gt;Liu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Single Stage Detector: real-time CNN for object detection that detects 80 different classes.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/ssd-mobilenetv1&quot;&gt;SSD-MobileNetV1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04861&quot;&gt;Howard et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A variant of MobileNet that uses the Single Shot Detector (SSD) model framework. The model detects 80 different object classes and locates up to 10 objects in an image.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/faster-rcnn&quot;&gt;Faster-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01497&quot;&gt;Ren et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Increases efficiency from R-CNN by connecting a RPN with a CNN to create a single, unified network for object detection that detects 80 different classes.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/faster-rcnn&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/mask-rcnn&quot;&gt;Mask-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.06870&quot;&gt;He et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time neural network for object instance segmentation that detects 80 different classes. Extends Faster R-CNN as each of the 300 elected ROIs go through 3 parallel branches of the network: label prediction, bounding box prediction and mask prediction.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/mask-rcnn&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/retinanet&quot;&gt;RetinaNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1708.02002&quot;&gt;Lin et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A real-time dense detector network for object detection that addresses class imbalance through Focal Loss. RetinaNet is able to match the speed of previous one-stage detectors and defines the state-of-the-art in two-stage detectors (surpassing R-CNN).&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/yolov2-coco&quot;&gt;YOLO v2-coco&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.08242&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN model for real-time object detection system that can detect over 9000 object categories. It uses a single network evaluation, enabling it to be more than 1000x faster than R-CNN and 100x faster than Faster R-CNN. This model is trained with COCO dataset and contains 80 classes.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/yolov3&quot;&gt;YOLO v3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.02767.pdf&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A deep CNN model for real-time object detection that detects 80 different classes. A little bigger than YOLOv2 but still very fast. As accurate as SSD but 3 times faster.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/tiny-yolov3&quot;&gt;Tiny YOLOv3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1804.02767.pdf&quot;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A smaller version of YOLOv3 model.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/yolov4&quot;&gt;YOLOv4&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.10934&quot;&gt;Bochkovskiy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Optimizes the speed and accuracy of object detection. Two times faster than EfficientDet. It improves YOLOv3&#39;s AP and FPS by 10% and 12%, respectively, with mAP50 of 52.32 on the COCO 2017 dataset and FPS of 41.7 on a Tesla V100.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/yolov4&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/duc&quot;&gt;DUC&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.08502&quot;&gt;Wang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN based pixel-wise semantic segmentation model with &amp;gt;80% &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/models/semantic_segmentation/DUC/README.md/#metric&quot;&gt;mIOU&lt;/a&gt; (mean Intersection Over Union). Trained on cityscapes dataset, which can be effectively implemented in self driving vehicle systems.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/DUC&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/object_detection_segmentation/fcn&quot;&gt;FCN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&quot;&gt;Long et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN based segmentation model trained end-to-end, pixel-to-pixel that produces efficient inference and learning. Built off of AlexNet, VGG net, GoogLeNet classification methods. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/FCN&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Body, Face &amp;amp; Gesture Analysis &lt;a name=&quot;body_analysis&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Face detection models identify and/or recognize human faces and emotions in given images. Body and Gesture Analysis models identify gender and age in given image.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/arcface&quot;&gt;ArcFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.07698&quot;&gt;Deng et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ArcFace&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/ultraface&quot;&gt;UltraFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&quot;&gt;Ultra-lightweight face detection model&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model is a lightweight facedetection model designed for edge computing devices.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/ultraface&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/emotion_ferplus&quot;&gt;Emotion FerPlus&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.01041&quot;&gt;Barsoum et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Deep CNN for emotion recognition trained on images of faces.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/body_analysis/age_gender&quot;&gt;Age and Gender Classification using Convolutional Neural Networks&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://data.vision.ee.ethz.ch/cvl/publications/papers/proceedings/eth_biwi_01229.pdf&quot;&gt;Rothe et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model accurately classifies gender and age even the amount of learning data is limited.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Image Manipulation &lt;a name=&quot;image_manipulation&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;Image manipulation models use neural networks to transform input images to modified output images. Some popular models in this category involve style transfer or enhancing images by increasing resolution.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Unpaired Image to Image Translation using Cycle consistent Adversarial Network&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.10593&quot;&gt;Zhu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The model uses learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/super_resolution/sub_pixel_cnn_2016&quot;&gt;Super Resolution with sub-pixel CNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.05158&quot;&gt;Shi et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A deep CNN that uses sub-pixel convolution layers to upscale the input image.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/sub_pixel_cnn_2016&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/vision/style_transfer/fast_neural_style&quot;&gt;Fast Neural Style Transfer&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.08155&quot;&gt;Johnson et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This method uses a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process.&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Speech &amp;amp; Audio Processing &lt;a name=&quot;speech&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This class of models uses audio data to train models that can identify voice, generate music, or even read text out loud.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Speech recognition with deep recurrent neural networks&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf&quot;&gt;Graves et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A RNN model for sequential data for speech recognition. Labels problems where the input-output alignment is unknown&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deep voice: Real time neural text to speech&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.07825&quot;&gt;Arik et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A DNN model that performs end-to-end neural speech synthesis. Requires fewer parameters and it is faster than other systems. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Sound Generative models&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.03499&quot;&gt;WaveNet: A Generative Model for Raw Audio &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A CNN model that generates raw audio waveforms. Has predictive distribution for each audio sample. Generates realistic music fragments. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Machine Comprehension &lt;a name=&quot;machine_comprehension&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This subset of natural language processing models that answer questions about a given context paragraph.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/bidirectional_attention_flow&quot;&gt;Bidirectional Attention Flow&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.01603&quot;&gt;Seo et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A model that answers a query about a given context paragraph.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/BiDAF&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/bert-squad&quot;&gt;BERT-Squad&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;Devlin et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model answers questions based on the context of the given input paragraph.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/BERT-Squad&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/roberta&quot;&gt;RoBERTa&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1907.11692.pdf&quot;&gt;Liu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A large transformer-based model that predicts sentiment based on given input text.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/RoBERTa&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/gpt-2&quot;&gt;GPT-2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;Radford et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A large transformer-based language model that given a sequence of words within some text, predicts the next word.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/GPT-2&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;b&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/validated/text/machine_comprehension/t5&quot;&gt;T5&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;Raffel et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A large transformer-based language model trained on multiple tasks at once to achieve better semantic understanding of the prompt, capable of sentiment-analysis, question-answering, similarity-detection, translation, summarization, etc.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://huggingface.co/spaces/onnx/T5&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&quot; alt=&quot;Hugging Face Spaces&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Machine Translation &lt;a name=&quot;machine_translation&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This class of natural language processing models learns how to translate input text to another language.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Neural Machine Translation by jointly learning to align and translate&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Bahdanau et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Aims to build a single neural network that can be jointly tuned to maximize the translation performance. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Google&#39;s Neural Machine Translation System&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1609.08144&quot;&gt;Wu et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;This model helps to improve issues faced by the Neural Machine Translation (NMT) systems like parallelism that helps accelerate the final translation speed.&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Language Modelling &lt;a name=&quot;language_modelling&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This subset of natural language processing models learns representations of language from large corpuses of text.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deep Neural Network Language Models&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://pdfs.semanticscholar.org/a177/45f1d7045636577bcd5d513620df5860e9e5.pdf&quot;&gt;Arisoy et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A DNN acoustic model. Used in many natural language technologies. Represents a probability distribution over all possible word strings in a language. &lt;br /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Visual Question Answering &amp;amp; Dialog &lt;a name=&quot;visual_qna&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;This subset of natural language processing models uses input images to answer questions about those images.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VQA: Visual Question Answering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1505.00468v6.pdf&quot;&gt;Agrawal et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A model that takes an image and a free-form, open-ended natural language question about the image and outputs a natural-language answer. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Yin and Yang: Balancing and Answering Binary Visual Questions&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1511.05099.pdf&quot;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Addresses VQA by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image. Next, if the concept can be found in the image, it provides a “yes” or “no” answer. Its performance matches the traditional VQA approach on unbalanced dataset, and outperforms it on the balanced dataset. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Making the V in VQA Matter&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1612.00837.pdf&quot;&gt;Goyal et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Balances the VQA dataset by collecting complementary images such that every question is associated with a pair of similar images that result in two different answers to the question, providing a unique interpretable model that provides a counter-example based explanation. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Visual Dialog&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.08669&quot;&gt;Das et al.&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An AI agent that holds a meaningful dialog with humans in natural, conversational language about visual content. Curates a large-scale Visual Dialog dataset (VisDial). &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Other interesting models &lt;a name=&quot;others&quot;&gt;&lt;/a&gt;&lt;/h3&gt; 
&lt;p&gt;There are many interesting deep learning models that do not fit into the categories described above. The ONNX team would like to highly encourage users and researchers to &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt; their models to the growing model zoo.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model Class&lt;/th&gt; 
   &lt;th&gt;Reference&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Text to Image&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1605.05396&quot;&gt;Generative Adversarial Text to image Synthesis &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Effectively bridges the advances in text and image modeling, translating visual concepts from characters to pixels. Generates plausible images of birds and flowers from detailed text descriptions. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Time Series Forecasting&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.07015.pdf&quot;&gt;Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks &lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The model extracts short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It helps to predict solar plant energy output, electricity consumption, and traffic jam situations. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Recommender systems&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf&quot;&gt;DropoutNet: Addressing Cold Start in Recommender Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A collaborative filtering method that makes predictions about an individual’s preference based on preference information from other users.&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Collaborative filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1708.05031.pdf&quot;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;A DNN model based on the interaction between user and item features using matrix factorization. &lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Autoencoders&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.01057&quot;&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;An LSTM (long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs.&lt;br /&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribute&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Usage &lt;a name=&quot;usage-&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Every ONNX backend should support running the models out of the box. After downloading and extracting the tarball of each model, you will find:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A protobuf file &lt;code&gt;model.onnx&lt;/code&gt; that represents the serialized ONNX model.&lt;/li&gt; 
 &lt;li&gt;Test data (in the form of serialized protobuf TensorProto files or serialized NumPy archives).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Usage - Test data starter code&lt;/h3&gt; 
&lt;p&gt;The test data files can be used to validate ONNX models from the Model Zoo. We have provided the following interface examples for you to get started. Please replace &lt;code&gt;onnx_backend&lt;/code&gt; in your code with the appropriate framework of your choice that provides ONNX inferencing support, and likewise replace &lt;code&gt;backend.run_model&lt;/code&gt; with the framework&#39;s model evaluation logic.&lt;/p&gt; 
&lt;p&gt;There are two different formats for the test data files:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serialized protobuf TensorProtos (.pb), stored in folders with the naming convention &lt;code&gt;test_data_set_*&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
import onnx
import os
import glob
import onnx_backend as backend

from onnx import numpy_helper

model = onnx.load(&#39;model.onnx&#39;)
test_data_dir = &#39;test_data_set_0&#39;

# Load inputs
inputs = []
inputs_num = len(glob.glob(os.path.join(test_data_dir, &#39;input_*.pb&#39;)))
for i in range(inputs_num):
    input_file = os.path.join(test_data_dir, &#39;input_{}.pb&#39;.format(i))
    tensor = onnx.TensorProto()
    with open(input_file, &#39;rb&#39;) as f:
        tensor.ParseFromString(f.read())
    inputs.append(numpy_helper.to_array(tensor))

# Load reference outputs
ref_outputs = []
ref_outputs_num = len(glob.glob(os.path.join(test_data_dir, &#39;output_*.pb&#39;)))
for i in range(ref_outputs_num):
    output_file = os.path.join(test_data_dir, &#39;output_{}.pb&#39;.format(i))
    tensor = onnx.TensorProto()
    with open(output_file, &#39;rb&#39;) as f:
        tensor.ParseFromString(f.read())
    ref_outputs.append(numpy_helper.to_array(tensor))

# Run the model on the backend
outputs = list(backend.run_model(model, inputs))

# Compare the results with reference outputs.
for ref_o, o in zip(ref_outputs, outputs):
    np.testing.assert_almost_equal(ref_o, o)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Serialized Numpy archives, stored in files with the naming convention &lt;code&gt;test_data_*.npz&lt;/code&gt;. Each file contains one set of test inputs and outputs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import numpy as np
import onnx
import onnx_backend as backend

# Load the model and sample inputs and outputs
model = onnx.load(model_pb_path)
sample = np.load(npz_path, encoding=&#39;bytes&#39;)
inputs = list(sample[&#39;inputs&#39;])
outputs = list(sample[&#39;outputs&#39;])

# Run the model with an onnx backend and verify the results
np.testing.assert_almost_equal(outputs, backend.run_model(model, inputs))
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Usage - Model quantization&lt;/h3&gt; 
&lt;p&gt;You can get quantized ONNX models by using &lt;a href=&quot;https://github.com/intel/neural-compressor&quot;&gt;Intel® Neural Compressor&lt;/a&gt;. It provides web-based UI service to make quantization easier and supports code-based usage for more abundant quantization settings. Refer to &lt;a href=&quot;https://github.com/intel/neural-compressor/raw/master/docs/bench.md&quot;&gt;bench document&lt;/a&gt; for how to use web-based UI service and &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/resource/docs/INC_code.md&quot;&gt;example document&lt;/a&gt; for a simple code-based demo. &lt;img src=&quot;https://raw.githubusercontent.com/onnx/models/main/resource/images/INC_GUI.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;There are multiple ways to access the ONNX Model Zoo:&lt;/p&gt; 
&lt;h3&gt;Git Clone (Not Recommended)&lt;/h3&gt; 
&lt;p&gt;Cloning the repository using git won&#39;t automatically download the ONNX models due to their size. To manage these files, first, install Git LFS by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git-lfs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To download a specific model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git lfs pull --include=&quot;[path to model].onnx&quot; --exclude=&quot;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To download all models:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git lfs pull --include=&quot;*&quot; --exclude=&quot;&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;GitHub UI&lt;/h3&gt; 
&lt;p&gt;Alternatively, you can download models directly from GitHub. Navigate to the model&#39;s page and click the &quot;Download&quot; button on the top right corner.&lt;/p&gt; 
&lt;h2&gt;Model Visualization&lt;/h2&gt; 
&lt;p&gt;For a graphical representation of each model&#39;s architecture, we recommend using &lt;a href=&quot;https://github.com/lutzroeder/netron&quot;&gt;Netron&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;p&gt;Contributions to the ONNX Model Zoo are welcome! Please check our &lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/contribute.md&quot;&gt;contribution guidelines&lt;/a&gt; for more information on how you can contribute to the growth and improvement of this resource.&lt;/p&gt; 
&lt;p&gt;Thank you for your interest in the ONNX Model Zoo, and we look forward to your participation in our community!&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/onnx/models/main/LICENSE&quot;&gt;Apache License v2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>microsoft/Data-Science-For-Beginners</title>
      <link>https://github.com/microsoft/Data-Science-For-Beginners</link>
      <description>&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/codespaces/new?hide_repo_select=true&amp;amp;ref=main&amp;amp;repo=344191198&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/foundry/forum&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Azure_AI_Foundry_Developer_Forum-blue?style=for-the-badge&amp;amp;logo=github&amp;amp;color=000000&amp;amp;logoColor=fff&quot; alt=&quot;Azure AI Foundry Developer Forum&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&quot;https://github.com/AdityaGarg00&quot;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/alondra-sanchez-molina/&quot;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/ankitasingh007&quot;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/anupam--mishra/&quot;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/arpitadas01/&quot;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&quot;https://www.linkedin.com/in/dibrinsofor&quot;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/dishita-bhasin-7065281bb&quot;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/majd-s/&quot;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/max-blum-6036a1186/&quot;&gt;Max Blum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/miguelmque/&quot;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/iftu119&quot;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/nawrin-tabassum&quot;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/raymond-wp/&quot;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/rty2423&quot;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&quot;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&quot;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/sheena-narua-n/&quot;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/tauqeerahmad5201/&quot;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&quot;https://www.linkedin.com/in/vidushi-gupta07/&quot;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&quot;https://www.linkedin.com/in/jasleen-sondhi/&quot;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&quot; alt=&quot;Sketchnote by @sketchthedocs https://sketchthedocs.dev&quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;🌐 Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href=&quot;https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;Join Our Community&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://aka.ms/ds4beginners/discord&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/kzRShWzttr&quot; alt=&quot;Azure AI Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We have a Discord learn with AI series ongoing, learn more and join us at &lt;a href=&quot;https://aka.ms/learnwithai/discord&quot;&gt;Learn with AI Series&lt;/a&gt; from 18 - 30 September, 2025. You will get tips and tricks of using GitHub Copilot for Data Science.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/images/1.jpg&quot; alt=&quot;Learn with AI series&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Are you a student?&lt;/h1&gt; 
&lt;p&gt;Get started with the following resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-gb/learn/student-hub?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Student Hub page&lt;/a&gt; In this page, you will find beginner resources, Student packs and even ways to get a free cert voucher. This is one page you want to bookmark and check from time to time as we switch out content at least monthly.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://studentambassadors.microsoft.com?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn Student Ambassadors&lt;/a&gt; Join a global community of student ambassadors, this could be your way into Microsoft.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&quot;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&quot;https://github.com/microsoft/Data-Science-For-Beginners/discussions&quot;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://aka.ms/student-page&quot;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&quot;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-77958-bethanycheum&quot;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Meet the Team&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://youtu.be/8mzavjQSMM4&quot; title=&quot;Promo video&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&quot; alt=&quot;Promo video&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&quot;https://www.linkedin.com/in/mohitjaisal&quot;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;🎥 Click the image above for a video about the project the folks who created it!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Pedagogy&lt;/h2&gt; 
&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; 
&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Find our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&quot;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&quot;&gt;Contributing&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&quot;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Each lesson includes:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Optional sketchnote&lt;/li&gt; 
 &lt;li&gt;Optional supplemental video&lt;/li&gt; 
 &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; 
 &lt;li&gt;Written lesson&lt;/li&gt; 
 &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; 
 &lt;li&gt;Knowledge checks&lt;/li&gt; 
 &lt;li&gt;A challenge&lt;/li&gt; 
 &lt;li&gt;Supplemental reading&lt;/li&gt; 
 &lt;li&gt;Assignment&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://ff-quizzes.netlify.app/en/&quot;&gt;Post-lesson quiz&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-App folder, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&quot; alt=&quot; Sketchnote by @sketchthedocs https://sketchthedocs.dev&quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/nitya&quot;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Number&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Topic&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Grouping&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Learning Objectives&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Linked Lesson&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Author&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data Science&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn the basic concepts behind data science and how it’s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/beZ7Mb_oz9I&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science Ethics&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Defining Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;How data is classified and its common sources.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&quot;&gt;Introduction&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/Z5Zy85g4Yjw&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Relational Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced “see-quell”).&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/geektrainer&quot;&gt;Christopher&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with NoSQL Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Working with Python&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&quot;&gt;lesson&lt;/a&gt; &lt;a href=&quot;https://youtu.be/dZjWOGbsN4Y&quot;&gt;video&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Preparation&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&quot;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://www.twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Quantities&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Learn how to use Matplotlib to visualize bird data 🦆&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Distributions of Data&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Proportions&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing Relationships&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Meaningful Visualizations&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&quot;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Analyzing&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/paladique&quot;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Communication&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&quot;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/JalenMcG&quot;&gt;Jalen&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Training models using Low Code tools.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Cloud&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&quot;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/TiffanySouterre&quot;&gt;Tiffany&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/maudstweets&quot;&gt;Maud&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data Science in the Wild&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&quot;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;Data science driven projects in the real world.&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&quot;&gt;lesson&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://twitter.com/nitya&quot;&gt;Nitya&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;GitHub Codespaces&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this sample in a Codespace:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Click the Code drop-down menu and select the Open with Codespaces option.&lt;/li&gt; 
 &lt;li&gt;Select + New codespace at the bottom on the pane. For more info, check out the &lt;a href=&quot;https://docs.github.com/en/codespaces/developing-in-codespaces/creating-a-codespace-for-a-repository#creating-a-codespace&quot;&gt;GitHub documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;VSCode Remote - Containers&lt;/h2&gt; 
&lt;p&gt;Follow these steps to open this repo in a container using your local machine and VSCode using the VS Code Remote - Containers extension:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in &lt;a href=&quot;https://code.visualstudio.com/docs/devcontainers/containers#_getting-started&quot;&gt;the getting started documentation&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;To use this repository, you can either open the repository in an isolated Docker volume:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Under the hood, this will use the Remote-Containers: &lt;strong&gt;Clone Repository in Container Volume...&lt;/strong&gt; command to clone the source code in a Docker volume instead of the local filesystem. &lt;a href=&quot;https://docs.docker.com/storage/volumes/&quot;&gt;Volumes&lt;/a&gt; are the preferred mechanism for persisting container data.&lt;/p&gt; 
&lt;p&gt;Or open a locally cloned or downloaded version of the repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Clone this repository to your local filesystem.&lt;/li&gt; 
 &lt;li&gt;Press F1 and select the &lt;strong&gt;Remote-Containers: Open Folder in Container...&lt;/strong&gt; command.&lt;/li&gt; 
 &lt;li&gt;Select the cloned copy of this folder, wait for the container to start, and try things out.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Offline access&lt;/h2&gt; 
&lt;p&gt;You can run this documentation offline by using &lt;a href=&quot;https://docsify.js.org/#/&quot;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&quot;https://docsify.js.org/#/quickstart&quot;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genaijava&quot;&gt;Generative AI with Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/bash-for-beginners&quot;&gt;Bash for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/xr-dev-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/segment-anything</title>
      <link>https://github.com/facebookresearch/segment-anything</link>
      <description>&lt;p&gt;The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Latest updates -- SAM 2: Segment Anything in Images and Videos&lt;/h2&gt; 
&lt;p&gt;Please check out our new release on &lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;SAM 2 code: &lt;a href=&quot;https://github.com/facebookresearch/segment-anything-2&quot;&gt;https://github.com/facebookresearch/segment-anything-2&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SAM 2 demo: &lt;a href=&quot;https://sam2.metademolab.com/&quot;&gt;https://sam2.metademolab.com/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;SAM 2 paper: &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot;&gt;https://arxiv.org/abs/2408.00714&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/facebookresearch/segment-anything-2/raw/main/assets/model_diagram.png?raw=true&quot; alt=&quot;SAM 2 architecture&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt; is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model-in-the-loop data engine, which improves model and data via user interaction, to collect &lt;a href=&quot;https://ai.meta.com/datasets/segment-anything-video&quot;&gt;&lt;strong&gt;our SA-V dataset&lt;/strong&gt;&lt;/a&gt;, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains.&lt;/p&gt; 
&lt;h1&gt;Segment Anything&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ai.facebook.com/research/&quot;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://alexander-kirillov.github.io/&quot;&gt;Alexander Kirillov&lt;/a&gt;, &lt;a href=&quot;https://ericmintun.github.io/&quot;&gt;Eric Mintun&lt;/a&gt;, &lt;a href=&quot;https://nikhilaravi.com/&quot;&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href=&quot;https://hanzimao.me/&quot;&gt;Hanzi Mao&lt;/a&gt;, Chloe Rolland, Laura Gustafson, &lt;a href=&quot;https://tetexiao.com&quot;&gt;Tete Xiao&lt;/a&gt;, &lt;a href=&quot;https://www.spencerwhitehead.com/&quot;&gt;Spencer Whitehead&lt;/a&gt;, Alex Berg, Wan-Yen Lo, &lt;a href=&quot;https://pdollar.github.io/&quot;&gt;Piotr Dollar&lt;/a&gt;, &lt;a href=&quot;https://www.rossgirshick.info/&quot;&gt;Ross Girshick&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[&lt;a href=&quot;https://ai.facebook.com/research/publications/segment-anything/&quot;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://segment-anything.com/&quot;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://segment-anything.com/demo&quot;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://segment-anything.com/dataset/index.html&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/&quot;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#citing-segment-anything&quot;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png?raw=true&quot; alt=&quot;SAM design&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The &lt;strong&gt;Segment Anything Model (SAM)&lt;/strong&gt; produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a &lt;a href=&quot;https://segment-anything.com/dataset/index.html&quot;&gt;dataset&lt;/a&gt; of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.&lt;/p&gt; 
&lt;p float=&quot;left&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks1.png?raw=true&quot; width=&quot;37.25%&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/masks2.jpg?raw=true&quot; width=&quot;61.5%&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The code requires &lt;code&gt;python&amp;gt;=3.8&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; 
&lt;p&gt;Install Segment Anything:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/facebookresearch/segment-anything.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or clone the repository locally and install with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone git@github.com:facebookresearch/segment-anything.git
cd segment-anything; pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following optional dependencies are necessary for mask post-processing, saving masks in COCO format, the example notebooks, and exporting the model in ONNX format. &lt;code&gt;jupyter&lt;/code&gt; is also required to run the example notebooks.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install opencv-python pycocotools matplotlib onnxruntime onnx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a name=&quot;GettingStarted&quot;&gt;&lt;/a&gt;Getting Started&lt;/h2&gt; 
&lt;p&gt;First download a &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/#model-checkpoints&quot;&gt;model checkpoint&lt;/a&gt;. Then the model can be used in just a few lines to get masks from a given prompt:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import SamPredictor, sam_model_registry
sam = sam_model_registry[&quot;&amp;lt;model_type&amp;gt;&quot;](checkpoint=&quot;&amp;lt;path/to/checkpoint&amp;gt;&quot;)
predictor = SamPredictor(sam)
predictor.set_image(&amp;lt;your_image&amp;gt;)
masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or generate masks for an entire image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
sam = sam_model_registry[&quot;&amp;lt;model_type&amp;gt;&quot;](checkpoint=&quot;&amp;lt;path/to/checkpoint&amp;gt;&quot;)
mask_generator = SamAutomaticMaskGenerator(sam)
masks = mask_generator.generate(&amp;lt;your_image&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Additionally, masks can be generated for images from the command line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/amg.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --input &amp;lt;image_or_folder&amp;gt; --output &amp;lt;path/to/output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the examples notebooks on &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/predictor_example.ipynb&quot;&gt;using SAM with prompts&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/notebooks/automatic_mask_generator_example.ipynb&quot;&gt;automatically generating masks&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p float=&quot;left&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook1.png?raw=true&quot; width=&quot;49.1%&quot; /&gt; &lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/notebook2.png?raw=true&quot; width=&quot;48.9%&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;ONNX Export&lt;/h2&gt; 
&lt;p&gt;SAM&#39;s lightweight mask decoder can be exported to ONNX format so that it can be run in any environment that supports ONNX runtime, such as in-browser as showcased in the &lt;a href=&quot;https://segment-anything.com/demo&quot;&gt;demo&lt;/a&gt;. Export the model with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python scripts/export_onnx_model.py --checkpoint &amp;lt;path/to/checkpoint&amp;gt; --model-type &amp;lt;model_type&amp;gt; --output &amp;lt;path/to/output&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/facebookresearch/segment-anything/raw/main/notebooks/onnx_model_example.ipynb&quot;&gt;example notebook&lt;/a&gt; for details on how to combine image preprocessing via SAM&#39;s backbone with mask prediction using the ONNX model. It is recommended to use the latest stable version of PyTorch for ONNX export.&lt;/p&gt; 
&lt;h3&gt;Web demo&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;demo/&lt;/code&gt; folder has a simple one page React app which shows how to run mask prediction with the exported ONNX model in a web browser with multithreading. Please see &lt;a href=&quot;https://github.com/facebookresearch/segment-anything/raw/main/demo/README.md&quot;&gt;&lt;code&gt;demo/README.md&lt;/code&gt;&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;&lt;a name=&quot;Models&quot;&gt;&lt;/a&gt;Model Checkpoints&lt;/h2&gt; 
&lt;p&gt;Three model versions of the model are available with different backbone sizes. These models can be instantiated by running&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from segment_anything import sam_model_registry
sam = sam_model_registry[&quot;&amp;lt;model_type&amp;gt;&quot;](checkpoint=&quot;&amp;lt;path/to/checkpoint&amp;gt;&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Click the links below to download the checkpoint for the corresponding model type.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;code&gt;default&lt;/code&gt; or &lt;code&gt;vit_h&lt;/code&gt;: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth&quot;&gt;ViT-H SAM model.&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vit_l&lt;/code&gt;: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth&quot;&gt;ViT-L SAM model.&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;vit_b&lt;/code&gt;: &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth&quot;&gt;ViT-B SAM model.&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Dataset&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://ai.facebook.com/datasets/segment-anything/&quot;&gt;here&lt;/a&gt; for an overview of the datastet. The dataset can be downloaded &lt;a href=&quot;https://ai.facebook.com/datasets/segment-anything-downloads/&quot;&gt;here&lt;/a&gt;. By downloading the datasets you agree that you have read and accepted the terms of the SA-1B Dataset Research License.&lt;/p&gt; 
&lt;p&gt;We save masks per image as a json file. It can be loaded as a dictionary in python in the below format.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;{
    &quot;image&quot;                 : image_info,
    &quot;annotations&quot;           : [annotation],
}

image_info {
    &quot;image_id&quot;              : int,              # Image id
    &quot;width&quot;                 : int,              # Image width
    &quot;height&quot;                : int,              # Image height
    &quot;file_name&quot;             : str,              # Image filename
}

annotation {
    &quot;id&quot;                    : int,              # Annotation id
    &quot;segmentation&quot;          : dict,             # Mask saved in COCO RLE format.
    &quot;bbox&quot;                  : [x, y, w, h],     # The box around the mask, in XYWH format
    &quot;area&quot;                  : int,              # The area in pixels of the mask
    &quot;predicted_iou&quot;         : float,            # The model&#39;s own prediction of the mask&#39;s quality
    &quot;stability_score&quot;       : float,            # A measure of the mask&#39;s quality
    &quot;crop_box&quot;              : [x, y, w, h],     # The crop of the image used to generate the mask, in XYWH format
    &quot;point_coords&quot;          : [[x, y]],         # The point coordinates input to the model to generate the mask
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Image ids can be found in sa_images_ids.txt which can be downloaded using the above &lt;a href=&quot;https://ai.facebook.com/datasets/segment-anything-downloads/&quot;&gt;link&lt;/a&gt; as well.&lt;/p&gt; 
&lt;p&gt;To decode a mask in COCO RLE format into binary:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;from pycocotools import mask as mask_utils
mask = mask_utils.decode(annotation[&quot;segmentation&quot;])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/cocodataset/cocoapi/raw/master/PythonAPI/pycocotools/mask.py&quot;&gt;here&lt;/a&gt; for more instructions to manipulate masks stored in RLE format.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The model is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/LICENSE&quot;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/segment-anything/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;The Segment Anything project was made possible with the help of many contributors (alphabetical):&lt;/p&gt; 
&lt;p&gt;Aaron Adcock, Vaibhav Aggarwal, Morteza Behrooz, Cheng-Yang Fu, Ashley Gabriel, Ahuva Goldstand, Allen Goodman, Sumanth Gurram, Jiabo Hu, Somya Jain, Devansh Kukreja, Robert Kuo, Joshua Lane, Yanghao Li, Lilian Luong, Jitendra Malik, Mallika Malhotra, William Ngan, Omkar Parkhi, Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala Varadarajan, Bram Wasti, Zachary Winstrom&lt;/p&gt; 
&lt;h2&gt;Citing Segment Anything&lt;/h2&gt; 
&lt;p&gt;If you use SAM or SA-1B in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\&#39;a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>timeseriesAI/tsai</title>
      <link>https://github.com/timeseriesAI/tsai</link>
      <description>&lt;p&gt;Time series Timeseries Deep Learning Machine Learning Python Pytorch fastai | State-of-the-art Deep Learning library for Time Series and Sequences in Pytorch / fastai&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tsai&lt;/h1&gt; 
&lt;!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! --&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/timeseriesAI/tsai/raw/main/nbs/multimedia/tsai_logo.svg?raw=true&quot; width=&quot;50%&quot; /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/timeseriesai/tsai/workflows/CI/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt; &lt;a href=&quot;https://pypi.org/project/tsai/#description&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/tsai?color=blue&amp;amp;label=pypi%20version.png&quot; alt=&quot;PyPI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://anaconda.org/timeseriesai/tsai&quot;&gt;&lt;img src=&quot;https://img.shields.io/conda/vn/timeseriesai/tsai?color=brightgreen&amp;amp;label=conda%20version.png&quot; alt=&quot;Conda (channel only)&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://zenodo.org/badge/latestdoi/211822289&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/211822289.svg?sanitize=true&quot; alt=&quot;DOI&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&quot; alt=&quot;PRs&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;State-of-the-art Deep Learning library for Time Series and Sequences.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;&lt;code&gt;tsai&lt;/code&gt; is an open-source deep learning package built on top of Pytorch &amp;amp; fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation…&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;tsai&lt;/code&gt; is currently under active development by timeseriesAI.&lt;/p&gt; 
&lt;h2&gt;What’s new:&lt;/h2&gt; 
&lt;p&gt;During the last few releases, here are some of the most significant additions to &lt;code&gt;tsai&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;New models&lt;/strong&gt;: PatchTST (Accepted by ICLR 2023), RNN with Attention (RNNAttention, LSTMAttention, GRUAttention), TabFusionTransformer, …&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New datasets&lt;/strong&gt;: we have increased the number of datasets you can download using &lt;code&gt;tsai&lt;/code&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;128 univariate classification datasets&lt;/li&gt; 
   &lt;li&gt;30 multivariate classification datasets&lt;/li&gt; 
   &lt;li&gt;15 regression datasets&lt;/li&gt; 
   &lt;li&gt;62 forecasting datasets&lt;/li&gt; 
   &lt;li&gt;9 long term forecasting datasets&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New tutorials&lt;/strong&gt;: &lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tutorial_nbs/15_PatchTST_a_new_transformer_for_LTSF.ipynb&quot;&gt;PatchTST&lt;/a&gt;. Based on some of your requests, we are planning to release additional tutorials on data preparation and forecasting.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;New functionality&lt;/strong&gt;: sklearn-type pipeline transforms, walk-foward cross validation, reduced RAM requirements, and a lot of new functionality to perform more accurate time series forecasts.&lt;/li&gt; 
 &lt;li&gt;Pytorch 2.0 support.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Pip install&lt;/h3&gt; 
&lt;p&gt;You can install the &lt;strong&gt;latest stable&lt;/strong&gt; version from pip using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install tsai
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you plan to develop tsai yourself, or want to be on the cutting edge, you can use an editable install. First install PyTorch, and then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;git clone https://github.com/timeseriesAI/tsai
pip install -e &quot;tsai[dev]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note: starting with tsai 0.3.0 tsai will only install hard dependencies. Other soft dependencies (which are only required for selected tasks) will not be installed by default (this is the recommended approach. If you require any of the dependencies that is not installed, tsai will ask you to install it when necessary). If you still want to install tsai with all its dependencies you can do it by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install tsai[extras]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Conda install&lt;/h3&gt; 
&lt;p&gt;You can also install tsai using conda (note that if you replace conda with mamba the install process will be much faster and more reliable):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;conda install -c timeseriesai tsai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;Here’s the link to the &lt;a href=&quot;https://timeseriesai.github.io/tsai/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Available models:&lt;/h2&gt; 
&lt;p&gt;Here’s a list with some of the state-of-the-art models available in &lt;code&gt;tsai&lt;/code&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN.py&quot;&gt;LSTM&lt;/a&gt; (Hochreiter, 1997) (&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963/&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN.py&quot;&gt;GRU&lt;/a&gt; (Cho, 2014) (&lt;a href=&quot;https://arxiv.org/abs/1412.3555&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/MLP.py&quot;&gt;MLP&lt;/a&gt; - Multilayer Perceptron (Wang, 2016) (&lt;a href=&quot;https://arxiv.org/abs/1611.06455&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/FCN.py&quot;&gt;FCN&lt;/a&gt; - Fully Convolutional Network (Wang, 2016) (&lt;a href=&quot;https://arxiv.org/abs/1611.06455&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/ResNet.py&quot;&gt;ResNet&lt;/a&gt; - Residual Network (Wang, 2016) (&lt;a href=&quot;https://arxiv.org/abs/1611.06455&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN_FCN.py&quot;&gt;LSTM-FCN&lt;/a&gt; (Karim, 2017) (&lt;a href=&quot;https://arxiv.org/abs/1709.05206&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN_FCN.py&quot;&gt;GRU-FCN&lt;/a&gt; (Elsayed, 2018) (&lt;a href=&quot;https://arxiv.org/abs/1812.07683&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/mWDN.py&quot;&gt;mWDN&lt;/a&gt; - Multilevel wavelet decomposition network (Wang, 2018) (&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3219819.3220060&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TCN.py&quot;&gt;TCN&lt;/a&gt; - Temporal Convolutional Network (Bai, 2018) (&lt;a href=&quot;https://arxiv.org/abs/1803.01271&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/RNN_FCN.py&quot;&gt;MLSTM-FCN&lt;/a&gt; - Multivariate LSTM-FCN (Karim, 2019) (&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0893608019301200&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/InceptionTime.py&quot;&gt;InceptionTime&lt;/a&gt; (Fawaz, 2019) (&lt;a href=&quot;https://arxiv.org/abs/1909.04939&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/ROCKET.py&quot;&gt;Rocket&lt;/a&gt; (Dempster, 2019) (&lt;a href=&quot;https://arxiv.org/abs/1910.13051&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/XceptionTime.py&quot;&gt;XceptionTime&lt;/a&gt; (Rahimian, 2019) (&lt;a href=&quot;https://arxiv.org/abs/1911.03803&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/ResCNN.py&quot;&gt;ResCNN&lt;/a&gt; - 1D-ResCNN (Zou , 2019) (&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231219311506&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TabModel.py&quot;&gt;TabModel&lt;/a&gt; - modified from fastai’s &lt;a href=&quot;https://docs.fast.ai/tabular.model.html#TabularModel&quot;&gt;TabularModel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/OmniScaleCNN.py&quot;&gt;OmniScale&lt;/a&gt; - Omni-Scale 1D-CNN (Tang, 2020) (&lt;a href=&quot;https://arxiv.org/abs/2002.10061&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TST.py&quot;&gt;TST&lt;/a&gt; - Time Series Transformer (Zerveas, 2020) (&lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3447548.3467401&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TabTransformer.py&quot;&gt;TabTransformer&lt;/a&gt; (Huang, 2020) (&lt;a href=&quot;https://arxiv.org/pdf/2012.06678&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TSiTPlus.py&quot;&gt;TSiT&lt;/a&gt; Adapted from ViT (Dosovitskiy, 2020) (&lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/MINIROCKET.py&quot;&gt;MiniRocket&lt;/a&gt; (Dempster, 2021) (&lt;a href=&quot;https://arxiv.org/abs/2102.00457&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/XCM.py&quot;&gt;XCM&lt;/a&gt; - An Explainable Convolutional Neural Network (Fauvel, 2021) (&lt;a href=&quot;https://hal.inria.fr/hal-03469487/document&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/gMLP.py&quot;&gt;gMLP&lt;/a&gt; - Gated Multilayer Perceptron (Liu, 2021) (&lt;a href=&quot;https://arxiv.org/abs/2105.08050&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TSPerceiver.py&quot;&gt;TSPerceiver&lt;/a&gt; - Adapted from Perceiver IO (Jaegle, 2021) (&lt;a href=&quot;https://arxiv.org/abs/2107.14795&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/GatedTabTransformer.py&quot;&gt;GatedTabTransformer&lt;/a&gt; (Cholakov, 2022) (&lt;a href=&quot;https://arxiv.org/abs/2201.00199&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/TSSequencerPlus.py&quot;&gt;TSSequencerPlus&lt;/a&gt; - Adapted from Sequencer (Tatsunami, 2022) (&lt;a href=&quot;https://arxiv.org/abs/2205.01972&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/tsai/models/PatchTST.py&quot;&gt;PatchTST&lt;/a&gt; - (Nie, 2022) (&lt;a href=&quot;https://arxiv.org/abs/2211.14730&quot;&gt;paper&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;plus other custom models like: TransformerModel, LSTMAttention, GRUAttention, …&lt;/p&gt; 
&lt;h2&gt;How to start using tsai?&lt;/h2&gt; 
&lt;p&gt;To get to know the tsai package, we’d suggest you start with this notebook in Google Colab: &lt;strong&gt;&lt;a href=&quot;https://colab.research.google.com/github/timeseriesAI/tsai/blob/master/tutorial_nbs/01_Intro_to_Time_Series_Classification.ipynb&quot;&gt;01_Intro_to_Time_Series_Classification&lt;/a&gt;&lt;/strong&gt; It provides an overview of a time series classification task.&lt;/p&gt; 
&lt;p&gt;We have also develop many other &lt;a href=&quot;https://github.com/timeseriesAI/tsai/tree/main/tutorial_nbs&quot;&gt;tutorial notebooks&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To use tsai in your own notebooks, the only thing you need to do after you have installed the package is to run this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.all import *
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;These are just a few examples of how you can use &lt;code&gt;tsai&lt;/code&gt;:&lt;/p&gt; 
&lt;h3&gt;Binary, univariate classification&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

X, y, splits = get_classification_data(&#39;ECG200&#39;, split_data=False)
tfms = [None, TSClassification()]
batch_tfms = TSStandardize()
clf = TSClassifier(X, y, splits=splits, path=&#39;models&#39;, arch=&quot;InceptionTimePlus&quot;, tfms=tfms, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())
clf.fit_one_cycle(100, 3e-4)
clf.export(&quot;clf.pkl&quot;) 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

clf = load_learner(&quot;models/clf.pkl&quot;)
probas, target, preds = clf.get_X_preds(X[splits[1]], y[splits[1]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multi-class, multivariate classification&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

X, y, splits = get_classification_data(&#39;LSST&#39;, split_data=False)
tfms = [None, TSClassification()]
batch_tfms = TSStandardize(by_sample=True)
mv_clf = TSClassifier(X, y, splits=splits, path=&#39;models&#39;, arch=&quot;InceptionTimePlus&quot;, tfms=tfms, batch_tfms=batch_tfms, metrics=accuracy, cbs=ShowGraph())
mv_clf.fit_one_cycle(10, 1e-2)
mv_clf.export(&quot;mv_clf.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

mv_clf = load_learner(&quot;models/mv_clf.pkl&quot;)
probas, target, preds = mv_clf.get_X_preds(X[splits[1]], y[splits[1]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Multivariate Regression&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

X, y, splits = get_regression_data(&#39;AppliancesEnergy&#39;, split_data=False)
tfms = [None, TSRegression()]
batch_tfms = TSStandardize(by_sample=True)
reg = TSRegressor(X, y, splits=splits, path=&#39;models&#39;, arch=&quot;TSTPlus&quot;, tfms=tfms, batch_tfms=batch_tfms, metrics=rmse, cbs=ShowGraph(), verbose=True)
reg.fit_one_cycle(100, 3e-4)
reg.export(&quot;reg.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

reg = load_learner(&quot;models/reg.pkl&quot;)
raw_preds, target, preds = reg.get_X_preds(X[splits[1]], y[splits[1]])
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The ROCKETs (RocketClassifier, RocketRegressor, MiniRocketClassifier, MiniRocketRegressor, MiniRocketVotingClassifier or MiniRocketVotingRegressor) are somewhat different models. They are not actually deep learning models (although they use convolutions) and are used in a different way.&lt;/p&gt; 
&lt;p&gt;⚠️ You’ll also need to install sktime to be able to use them. You can install it separately:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install sktime
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or use:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;pip install tsai[extras]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from sklearn.metrics import mean_squared_error, make_scorer
from tsai.data.external import get_Monash_regression_data
from tsai.models.MINIROCKET import MiniRocketRegressor

X_train, y_train, *_ = get_Monash_regression_data(&#39;AppliancesEnergy&#39;)
rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)
reg = MiniRocketRegressor(scoring=rmse_scorer)
reg.fit(X_train, y_train)
reg.save(&#39;MiniRocketRegressor&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from sklearn.metrics import mean_squared_error
from tsai.data.external import get_Monash_regression_data
from tsai.models.MINIROCKET import load_minirocket

*_, X_test, y_test = get_Monash_regression_data(&#39;AppliancesEnergy&#39;)
reg = load_minirocket(&#39;MiniRocketRegressor&#39;)
y_pred = reg.predict(X_test)
mean_squared_error(y_test, y_pred, squared=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Forecasting&lt;/h3&gt; 
&lt;p&gt;You can use tsai for forecast in the following scenarios:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;univariate or multivariate time series input&lt;/li&gt; 
 &lt;li&gt;univariate or multivariate time series output&lt;/li&gt; 
 &lt;li&gt;single or multi-step ahead&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You’ll need to: * prepare X (time series input) and the target y (see &lt;a href=&quot;https://timeseriesai.github.io/tsai/data.preparation.html&quot;&gt;documentation&lt;/a&gt;) * select PatchTST or one of tsai’s models ending in Plus (TSTPlus, InceptionTimePlus, TSiTPlus, etc). The model will auto-configure a head to yield an output with the same shape as the target input y.&lt;/p&gt; 
&lt;h4&gt;Single step&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

ts = get_forecasting_time_series(&quot;Sunspots&quot;).values
X, y = SlidingWindow(60, horizon=1)(ts)
splits = TimeSplitter(235)(y) 
tfms = [None, TSForecasting()]
batch_tfms = TSStandardize()
fcst = TSForecaster(X, y, splits=splits, path=&#39;models&#39;, tfms=tfms, batch_tfms=batch_tfms, bs=512, arch=&quot;TSTPlus&quot;, metrics=mae, cbs=ShowGraph())
fcst.fit_one_cycle(50, 1e-3)
fcst.export(&quot;fcst.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner

fcst = load_learner(&quot;models/fcst.pkl&quot;, cpu=False)
raw_preds, target, preds = fcst.get_X_preds(X[splits[1]], y[splits[1]])
raw_preds.shape
# torch.Size([235, 1])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Multi-step&lt;/h4&gt; 
&lt;p&gt;This example show how to build a 3-step ahead univariate forecast.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.basics import *

ts = get_forecasting_time_series(&quot;Sunspots&quot;).values
X, y = SlidingWindow(60, horizon=3)(ts)
splits = TimeSplitter(235, fcst_horizon=3)(y) 
tfms = [None, TSForecasting()]
batch_tfms = TSStandardize()
fcst = TSForecaster(X, y, splits=splits, path=&#39;models&#39;, tfms=tfms, batch_tfms=batch_tfms, bs=512, arch=&quot;TSTPlus&quot;, metrics=mae, cbs=ShowGraph())
fcst.fit_one_cycle(50, 1e-3)
fcst.export(&quot;fcst.pkl&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from tsai.inference import load_learner
fcst = load_learner(&quot;models/fcst.pkl&quot;, cpu=False)
raw_preds, target, preds = fcst.get_X_preds(X[splits[1]], y[splits[1]])
raw_preds.shape
# torch.Size([235, 3])
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Input data format&lt;/h2&gt; 
&lt;p&gt;The input format for all time series models and image models in tsai is the same. An np.ndarray (or array-like object like zarr, etc) with 3 dimensions:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;[# samples x # variables x sequence length]&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The input format for tabular models in tsai (like TabModel, TabTransformer and TabFusionTransformer) is a pandas dataframe. See &lt;a href=&quot;https://timeseriesai.github.io/tsai/models.TabModel.html&quot;&gt;example&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to contribute to tsai?&lt;/h2&gt; 
&lt;p&gt;We welcome contributions of all kinds. Development of enhancements, bug fixes, documentation, tutorial notebooks, …&lt;/p&gt; 
&lt;p&gt;We have created a guide to help you start contributing to tsai. You can read it &lt;a href=&quot;https://github.com/timeseriesAI/tsai/raw/main/CONTRIBUTING.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Enterprise support and consulting services:&lt;/h2&gt; 
&lt;p&gt;Want to make the most out of timeseriesAI/tsai in a professional setting? Let us help. Send us an email to learn more: &lt;a href=&quot;mailto:info@timeseriesai.co&quot;&gt;info@timeseriesai.co&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Citing tsai&lt;/h2&gt; 
&lt;p&gt;If you use tsai in your research please use the following BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;@Misc{tsai,
    author =       {Ignacio Oguiza},
    title =        {tsai - A state-of-the-art deep learning library for time series and sequential data},
    howpublished = {Github},
    year =         {2023},
    url =          {https://github.com/timeseriesAI/tsai}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>udlbook/udlbook</title>
      <link>https://github.com/udlbook/udlbook</link>
      <description>&lt;p&gt;Understanding Deep Learning - Simon J.D. Prince&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>openai/openai-cookbook</title>
      <link>https://github.com/openai/openai-cookbook</link>
      <description>&lt;p&gt;Examples and guides for using the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://cookbook.openai.com&quot; target=&quot;_blank&quot;&gt; 
 &lt;picture&gt; 
  &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;/images/openai-cookbook-white.png&quot; style=&quot;max-width: 100%; width: 400px; margin-bottom: 20px&quot; /&gt; 
  &lt;img alt=&quot;OpenAI Cookbook Logo&quot; src=&quot;https://raw.githubusercontent.com/openai/openai-cookbook/main/images/openai-cookbook.png&quot; width=&quot;400px&quot; /&gt; 
 &lt;/picture&gt; &lt;/a&gt; 
&lt;h3&gt;&lt;/h3&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;✨ Navigate at &lt;a href=&quot;https://cookbook.openai.com&quot;&gt;cookbook.openai.com&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Example code and guides for accomplishing common tasks with the &lt;a href=&quot;https://platform.openai.com/docs/introduction&quot;&gt;OpenAI API&lt;/a&gt;. To run these examples, you&#39;ll need an OpenAI account and associated API key (&lt;a href=&quot;https://platform.openai.com/signup&quot;&gt;create a free account here&lt;/a&gt;). Set an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; with your API key. Alternatively, in most IDEs such as Visual Studio Code, you can create an &lt;code&gt;.env&lt;/code&gt; file at the root of your repo containing &lt;code&gt;OPENAI_API_KEY=&amp;lt;your API key&amp;gt;&lt;/code&gt;, which will be picked up by the notebooks.&lt;/p&gt; 
&lt;p&gt;Most code examples are written in Python, though the concepts can be applied in any language.&lt;/p&gt; 
&lt;p&gt;For other useful tools, guides and courses, check out these &lt;a href=&quot;https://cookbook.openai.com/related_resources&quot;&gt;related resources from around the web&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;MIT License&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rasbt/LLMs-from-scratch</title>
      <link>https://github.com/rasbt/LLMs-from-scratch</link>
      <description>&lt;p&gt;Implement a ChatGPT-like LLM in PyTorch from scratch, step by step&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Build a Large Language Model (From Scratch)&lt;/h1&gt; 
&lt;p&gt;This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book &lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;Build a Large Language Model (From Scratch)&lt;/a&gt;.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://amzn.to/4fqvn0D&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123&quot; width=&quot;250px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;In &lt;a href=&quot;http://mng.bz/orYv&quot;&gt;&lt;em&gt;Build a Large Language Model (From Scratch)&lt;/em&gt;&lt;/a&gt;, you&#39;ll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I&#39;ll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.&lt;/p&gt; 
&lt;p&gt;The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Link to the official &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;source code repository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://mng.bz/orYv&quot;&gt;Link to the book at Manning (the publisher&#39;s website)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/gp/product/1633437167&quot;&gt;Link to the book page on Amazon.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ISBN 9781633437166&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;http://mng.bz/orYv#reviews&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png&quot; width=&quot;220px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;To download a copy of this repository, click on the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip&quot;&gt;Download ZIP&lt;/a&gt; button or execute the following command in your terminal:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt; 
&lt;p&gt;(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; for the latest updates.)&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;h1&gt;Table of Contents&lt;/h1&gt; 
&lt;p&gt;Please note that this &lt;code&gt;README.md&lt;/code&gt; file is a Markdown (&lt;code&gt;.md&lt;/code&gt;) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven&#39;t installed a Markdown editor yet, &lt;a href=&quot;https://ghostwriter.kde.org&quot;&gt;Ghostwriter&lt;/a&gt; is a good free option.&lt;/p&gt; 
&lt;p&gt;You can alternatively view this and other files on GitHub at &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot;&gt;https://github.com/rasbt/LLMs-from-scratch&lt;/a&gt; in your browser, which renders Markdown automatically.&lt;/p&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Tip:&lt;/strong&gt; If you&#39;re seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/README.md&quot;&gt;README.md&lt;/a&gt; file located in the &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;setup&lt;/a&gt; directory.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;br /&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Linux&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests Windows&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml&quot;&gt;&lt;img src=&quot;https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg?sanitize=true&quot; alt=&quot;Code tests macOS&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;br /&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Chapter Title&lt;/th&gt; 
   &lt;th&gt;Main Code (for Quick Access)&lt;/th&gt; 
   &lt;th&gt;All Code + Supplementary&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup&quot;&gt;Setup recommendations&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 1: Understanding Large Language Models&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 2: Working with Text Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/ch02.ipynb&quot;&gt;ch02.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/dataloader.ipynb&quot;&gt;dataloader.ipynb&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02&quot;&gt;./ch02&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 3: Coding Attention Mechanisms&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/ch03.ipynb&quot;&gt;ch03.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/multihead-attention.ipynb&quot;&gt;multihead-attention.ipynb&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03&quot;&gt;./ch03&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 4: Implementing a GPT Model from Scratch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/ch04.ipynb&quot;&gt;ch04.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/gpt.py&quot;&gt;gpt.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04&quot;&gt;./ch04&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 5: Pretraining on Unlabeled Data&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/ch05.ipynb&quot;&gt;ch05.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_train.py&quot;&gt;gpt_train.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_generate.py&quot;&gt;gpt_generate.py&lt;/a&gt; (summary) &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05&quot;&gt;./ch05&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 6: Finetuning for Text Classification&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/ch06.ipynb&quot;&gt;ch06.ipynb&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/gpt_class_finetune.py&quot;&gt;gpt_class_finetune.py&lt;/a&gt; &lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06&quot;&gt;./ch06&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ch 7: Finetuning to Follow Instructions&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ch07.ipynb&quot;&gt;ch07.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/gpt_instruction_finetuning.py&quot;&gt;gpt_instruction_finetuning.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/ollama_evaluate.py&quot;&gt;ollama_evaluate.py&lt;/a&gt; (summary)&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07&quot;&gt;./ch07&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix A: Introduction to PyTorch&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part1.ipynb&quot;&gt;code-part1.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/code-part2.ipynb&quot;&gt;code-part2.ipynb&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/DDP-script.py&quot;&gt;DDP-script.py&lt;/a&gt;&lt;br /&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;exercise-solutions.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-A&quot;&gt;./appendix-A&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix B: References and Further Reading&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix C: Exercise Solutions&lt;/td&gt; 
   &lt;td&gt;No code&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix D: Adding Bells and Whistles to the Training Loop&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D/01_main-chapter-code/appendix-D.ipynb&quot;&gt;appendix-D.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-D&quot;&gt;./appendix-D&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Appendix E: Parameter-efficient Finetuning with LoRA&lt;/td&gt; 
   &lt;td&gt;- &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E/01_main-chapter-code/appendix-E.ipynb&quot;&gt;appendix-E.ipynb&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/appendix-E&quot;&gt;./appendix-E&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;p&gt;The mental model below summarizes the contents covered in this book.&lt;/p&gt; 
&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg&quot; width=&quot;650px&quot; /&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Prerequisites&lt;/h2&gt; 
&lt;p&gt;The most important prerequisite is a strong foundation in Python programming. With this knowledge, you will be well prepared to explore the fascinating world of LLMs and understand the concepts and code examples presented in this book.&lt;/p&gt; 
&lt;p&gt;If you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.&lt;/p&gt; 
&lt;p&gt;This book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, &lt;a href=&quot;https://sebastianraschka.com/teaching/pytorch-1h/&quot;&gt;PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs&lt;/a&gt;, helpful for learning about the essentials.&lt;/p&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Hardware Requirements&lt;/h2&gt; 
&lt;p&gt;The code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/raw/main/setup/README.md&quot;&gt;setup&lt;/a&gt; doc for additional recommendations.)&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Video Course&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;A 17-hour and 15-minute companion video course&lt;/a&gt; where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book&#39;s structure so that it can be used as a standalone alternative to the book or complementary code-along resource.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123&quot; width=&quot;350px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Companion Book / Sequel&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;&lt;em&gt;Build A Reasoning Model (From Scratch)&lt;/em&gt;&lt;/a&gt;, while a standalone book, can be considered as a sequel to &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;.&lt;/p&gt; 
&lt;p&gt;It starts with a pretrained model and implements different reasoning approaches, including inference-time scaling, reinforcement learning, and distillation, to improve the model&#39;s reasoning capabilities.&lt;/p&gt; 
&lt;p&gt;Similar to &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;, &lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;&lt;em&gt;Build A Reasoning Model (From Scratch)&lt;/em&gt;&lt;/a&gt; takes a hands-on approach implementing these methods from scratch.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/reasoning-from-scratch-images/cover.webp?123&quot; width=&quot;120px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Amazon link (TBD)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://mng.bz/lZ5B&quot;&gt;Manning link&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch&quot;&gt;GitHub repository&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Exercises&lt;/h2&gt; 
&lt;p&gt;Each chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example, &lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/exercise-solutions.ipynb&quot;&gt;./ch02/01_main-chapter-code/exercise-solutions.ipynb&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In addition to the code exercises, you can download a free 170-page PDF titled &lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;Test Yourself On Build a Large Language Model (From Scratch)&lt;/a&gt; from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch&quot;&gt;&lt;img src=&quot;https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123&quot; width=&quot;150px&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Bonus Material&lt;/h2&gt; 
&lt;p&gt;Several folders contain optional materials as a bonus for interested readers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Setup&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/01_optional-python-setup-preferences&quot;&gt;Python Setup Tips&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/02_installing-python-libraries&quot;&gt;Installing Python Packages and Libraries Used In This Book&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/setup/03_optional-docker-environment&quot;&gt;Docker Environment Setup Guide&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 2: Working with text data&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb&quot;&gt;Byte Pair Encoding (BPE) Tokenizer From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/02_bonus_bytepair-encoder&quot;&gt;Comparing Various Byte Pair Encoding (BPE) Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/03_bonus_embedding-vs-matmul&quot;&gt;Understanding the Difference Between Embedding Layers and Linear Layers&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/04_bonus_dataloader-intuition&quot;&gt;Dataloader Intuition with Simple Numbers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 3: Coding attention mechanisms&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb&quot;&gt;Comparing Efficient Multi-Head Attention Implementations&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch03/03_understanding-buffers/understanding-buffers.ipynb&quot;&gt;Understanding PyTorch Buffers&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 4: Implementing a GPT model from scratch&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/02_performance-analysis/flops-analysis.ipynb&quot;&gt;FLOPS Analysis&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch04/03_kv-cache&quot;&gt;KV Cache&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 5: Pretraining on unlabeled data:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/02_alternative_weight_loading/&quot;&gt;Alternative Weight Loading Methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/03_bonus_pretraining_on_gutenberg&quot;&gt;Pretraining GPT on the Project Gutenberg Dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/04_learning_rate_schedulers&quot;&gt;Adding Bells and Whistles to the Training Loop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/05_bonus_hparam_tuning&quot;&gt;Optimizing Hyperparameters for Pretraining&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/06_user_interface&quot;&gt;Building a User Interface to Interact With the Pretrained LLM&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama&quot;&gt;Converting GPT to Llama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/07_gpt_to_llama/standalone-llama32.ipynb&quot;&gt;Llama 3.2 From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/11_qwen3/&quot;&gt;Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/12_gemma3/&quot;&gt;Gemma 3 From Scratch&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb&quot;&gt;Memory-efficient Model Weight Loading&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/09_extending-tokenizers/extend-tiktoken.ipynb&quot;&gt;Extending the Tiktoken BPE Tokenizer with New Tokens&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/10_llm-training-speed&quot;&gt;PyTorch Performance Tips for Faster LLM Training&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 6: Finetuning for classification&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/02_bonus_additional-experiments&quot;&gt;Additional experiments finetuning different layers and using larger models&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/03_bonus_imdb-classification&quot;&gt;Finetuning different models on 50k IMDb movie review dataset&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch06/04_user_interface&quot;&gt;Building a User Interface to Interact With the GPT-based Spam Classifier&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chapter 7: Finetuning to follow instructions&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/02_dataset-utilities&quot;&gt;Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/03_model-evaluation&quot;&gt;Evaluating Instruction Responses Using the OpenAI API and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/llama3-ollama.ipynb&quot;&gt;Generating a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/05_dataset-generation/reflection-gpt4.ipynb&quot;&gt;Improving a Dataset for Instruction Finetuning&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb&quot;&gt;Generating a Preference Dataset with Llama 3.1 70B and Ollama&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&quot;&gt;Direct Preference Optimization (DPO) for LLM Alignment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/06_user_interface&quot;&gt;Building a User Interface to Interact With the Instruction Finetuned GPT Model&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; &amp;nbsp; 
&lt;h2&gt;Questions, Feedback, and Contributing to This Repository&lt;/h2&gt; 
&lt;p&gt;I welcome all sorts of feedback, best shared via the &lt;a href=&quot;https://livebook.manning.com/forum?product=raschka&amp;amp;page=1&quot;&gt;Manning Forum&lt;/a&gt; or &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/discussions&quot;&gt;GitHub Discussions&lt;/a&gt;. Likewise, if you have any questions or just want to bounce ideas off others, please don&#39;t hesitate to post these in the forum as well.&lt;/p&gt; 
&lt;p&gt;Please note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.&lt;/p&gt; 
&lt;p&gt;&amp;nbsp;&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find this book or code useful for your research, please consider citing it.&lt;/p&gt; 
&lt;p&gt;Chicago-style citation:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Raschka, Sebastian. &lt;em&gt;Build A Large Language Model (From Scratch)&lt;/em&gt;. Manning, 2024. ISBN: 978-1633437166.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;BibTeX entry:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@book{build-llms-from-scratch-book,
  author       = {Sebastian Raschka},
  title        = {Build A Large Language Model (From Scratch)},
  publisher    = {Manning},
  year         = {2024},
  isbn         = {978-1633437166},
  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},
  github       = {https://github.com/rasbt/LLMs-from-scratch}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/dinov2</title>
      <link>https://github.com/facebookresearch/dinov2</link>
      <description>&lt;p&gt;PyTorch code and models for the DINOv2 self-supervised learning method.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;span&gt;🆕&lt;/span&gt; [2025-08-14] &lt;em&gt;Please check out the more recent &lt;a href=&quot;https://github.com/facebookresearch/dinov3&quot;&gt;DINOv3&lt;/a&gt; effort continuing this line of work.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[2025-06-11] &lt;em&gt;Added dino.txt inference code, following &lt;a href=&quot;https://arxiv.org/abs/2412.16334&quot;&gt;DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;[2023-10-26] &lt;em&gt;Added DINOv2 backbones with registers, following &lt;a href=&quot;https://arxiv.org/abs/2309.16588&quot;&gt;Vision Transformers Need Registers&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; 
&lt;h1&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ai.facebook.com/research/&quot;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski&lt;/p&gt; 
&lt;p&gt;[&lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;&lt;code&gt;Paper #1&lt;/code&gt;&lt;/a&gt;] &lt;a href=&quot;https://arxiv.org/abs/2309.16588&quot;&gt;&lt;code&gt;Paper #2&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/&quot;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://dinov2.metademolab.com&quot;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/#citing-dinov2&quot;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;PyTorch implementation and pretrained models for DINOv2. For details, see the papers: &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.07193&quot;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.16588&quot;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&quot;&gt;https://github.com/facebookresearch/dinov2/assets/60359573/f168823e-7922-415a-b429-578badf5c356&lt;/a&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt;
  Visualization of the three first principal components of the patch features of all frames, mapped to RGB values. 
&lt;/div&gt; 
&lt;h2&gt;Pretrained models&lt;/h2&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;# of&lt;br /&gt;params&lt;/th&gt; 
   &lt;th&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;ImageNet&lt;br /&gt;k-NN&lt;/th&gt; 
   &lt;th&gt;ImageNet&lt;br /&gt;linear&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;21 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;79.0%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;81.1%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;21 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;79.1%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;80.9%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;82.1%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;82.0%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.6%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;300 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.5%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.3%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;300 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.8%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.7%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1,100 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.5%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1,100 M&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;83.7%&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;87.1%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_pretrain.pth&quot;&gt;backbone only&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Pretrained backbones (via PyTorch Hub)&lt;/h3&gt; 
&lt;p&gt;Please follow the instructions &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.&lt;/p&gt; 
&lt;p&gt;A corresponding &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/MODEL_CARD.md&quot;&gt;model card&lt;/a&gt; is included in the repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch

# DINOv2
dinov2_vits14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14&#39;)
dinov2_vitb14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14&#39;)
dinov2_vitl14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14&#39;)
dinov2_vitg14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14&#39;)

# DINOv2 with registers
dinov2_vits14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg&#39;)
dinov2_vitb14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg&#39;)
dinov2_vitl14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg&#39;)
dinov2_vitg14_reg = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pretrained heads - Image classification&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ImageNet&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; linear head (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_lreg4_inear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;) &lt;/td&gt;
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The (full) classifier models can be loaded via PyTorch Hub:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch

# DINOv2
dinov2_vits14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_lc&#39;)
dinov2_vitb14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_lc&#39;)
dinov2_vitl14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_lc&#39;)
dinov2_vitg14_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_lc&#39;)

# DINOv2 with registers
dinov2_vits14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14_reg_lc&#39;)
dinov2_vitb14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14_reg_lc&#39;)
dinov2_vitl14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg_lc&#39;)
dinov2_vitg14_reg_lc = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14_reg_lc&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Pretrained heads - Depth estimation&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th colspan=&quot;2&quot;&gt;download head&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;NYUd&lt;/th&gt; 
   &lt;th&gt;KITTI&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_nyu_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; linear (&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear_head.pth&quot;&gt;1 layer&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_linear4_head.pth&quot;&gt;4 layers&lt;/a&gt;), &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_kitti_dpt_head.pth&quot;&gt;DPT&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Pretrained heads - Semantic segmentation&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th&gt;download model&lt;/th&gt; 
   &lt;th colspan=&quot;2&quot;&gt;download head&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ADE20K&lt;/th&gt; 
   &lt;th&gt;ADE20K&lt;/th&gt; 
   &lt;th&gt;VOC2012&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_m2f.pth&quot;&gt;Mask2Former&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_ade20k_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_linear_head.pth&quot;&gt;linear&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_voc2012_ms_head.pth&quot;&gt;multi-scale&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Pretrained heads - Zero-shot tasks with dino.txt&lt;/h3&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;backbone&lt;/th&gt; 
   &lt;th rowspan=&quot;2&quot;&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;download&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td&gt; &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_dinotxt_tet1280d20h24l_vision_head.pth&quot;&gt;vision head&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_dinotxt_tet1280d20h24l_text_encoder.pth&quot;&gt;text model&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/thirdparty/bpe_simple_vocab_16e6.txt.gz&quot;&gt;vocabulary&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/thirdparty/LICENSE&quot;&gt;vocabulary license&lt;/a&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The (full) dino.txt model can be loaded via PyTorch Hub:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch

# DINOv2
dinov2_vitl14_reg4_dinotxt_tet1280d20h24l = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14_reg4_dinotxt_tet1280d20h24l&#39;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The training and evaluation code requires PyTorch 2.0 and &lt;a href=&quot;https://github.com/facebookresearch/xformers&quot;&gt;xFormers&lt;/a&gt; 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&quot;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt; - Clone the repository and then create and activate a &lt;code&gt;dinov2&lt;/code&gt; conda environment using the provided environment definition:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda env create -f conda.yaml
conda activate dinov2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pip.pypa.io/en/stable/getting-started/&quot;&gt;pip&lt;/a&gt;&lt;/em&gt; - Clone the repository and then use the provided &lt;code&gt;requirements.txt&lt;/code&gt; to install the dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For dense tasks (depth estimation and semantic segmentation), there are additional dependencies (specific versions of &lt;code&gt;mmcv&lt;/code&gt; and &lt;code&gt;mmsegmentation&lt;/code&gt;) which are captured in the &lt;code&gt;extras&lt;/code&gt; dependency specifications:&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&quot;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;conda env create -f conda-extras.yaml
conda activate dinov2-extras
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pip.pypa.io/en/stable/getting-started/&quot;&gt;pip&lt;/a&gt;&lt;/em&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install -r requirements.txt -r requirements-extras.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Data preparation&lt;/h2&gt; 
&lt;h3&gt;ImageNet-1k&lt;/h3&gt; 
&lt;p&gt;The root directory of the dataset should hold the following contents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00000001.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/[..]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/test/ILSVRC2012_test_00100000.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n01440764/n01440764_10026.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/[...]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/train/n15075141/n15075141_9993.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/[...]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;ROOT&amp;gt;/labels.txt&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The provided dataset implementation expects a few additional metadata files to be present under the extra directory:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-TRAIN.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-ids-VAL.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-TRAIN.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/class-names-VAL.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TEST.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-TRAIN.npy&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;EXTRA&amp;gt;/entries-VAL.npy&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These metadata files can be generated (once) with the following lines of Python code:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from dinov2.data.datasets import ImageNet

for split in ImageNet.Split:
    dataset = ImageNet(split=split, root=&quot;&amp;lt;ROOT&amp;gt;&quot;, extra=&quot;&amp;lt;EXTRA&amp;gt;&quot;)
    dataset.dump_extra()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that the root and extra directories do not have to be distinct directories.&lt;/p&gt; 
&lt;h3&gt;ImageNet-22k&lt;/h3&gt; 
&lt;p&gt;Please adapt the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/dinov2/data/datasets/image_net_22k.py&quot;&gt;dataset class&lt;/a&gt; to match your local setup.&lt;/p&gt; 
&lt;br /&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; To execute the commands provided in the next sections for training and evaluation, the &lt;code&gt;dinov2&lt;/code&gt; package should be included in the Python module search path, i.e. simply prefix the command to run with &lt;code&gt;PYTHONPATH=.&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;h3&gt;Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k&lt;/h3&gt; 
&lt;p&gt;Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/train/train.py \
    --nodes 4 \
    --config-file dinov2/configs/train/vitl16_short.yaml \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \
    train.dataset_path=ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.&lt;/p&gt; 
&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; 
&lt;h3&gt;Long setup: training DINOv2 ViT-L/14 on ImageNet-22k&lt;/h3&gt; 
&lt;p&gt;Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/train/train.py \
    --nodes 12 \
    --config-file dinov2/configs/train/vitl14.yaml \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \
    train.dataset_path=ImageNet22k:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.&lt;/p&gt; 
&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:&lt;/p&gt; 
&lt;h3&gt;k-NN classification on ImageNet-1k&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/knn.py \
    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \
    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/knn \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Logistic regression classification on ImageNet-1k&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/log_regression.py \
    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \
    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/logreg \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Linear classification with data augmentation on ImageNet-1k&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/linear.py \
    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \
    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \
    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/linear \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We release the weights from evaluating the different models:&lt;/p&gt; 
&lt;table style=&quot;margin: auto&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;th&gt;model&lt;/th&gt; 
   &lt;th&gt;with&lt;br /&gt;registers&lt;/th&gt; 
   &lt;th&gt;ImageNet&lt;br /&gt;top-1&lt;/th&gt; 
   &lt;th&gt;linear evaluation&lt;/th&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;81.1%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;80.8%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84.4%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.3%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86.5%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ViT-g/14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;span&gt;✅&lt;/span&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;87.0%&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_reg4_linear_head.pth&quot;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;p&gt;The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python dinov2/run/eval/linear.py \
    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \
    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \
    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \
    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Notebooks&lt;/h2&gt; 
&lt;p&gt;A few notebooks are provided to help the community leverage the models and code:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/raw/main/notebooks/depth_estimation.ipynb&quot;&gt;Depth estimation&lt;/a&gt; - How to load and use the depth heads in combination with a matching backbone via mmcv&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/dinov2/raw/main/notebooks/semantic_segmentation.ipynb&quot;&gt;Semantic segmentation&lt;/a&gt; - How to load and use the segmentation heads in combination with a matching backbone via mmcv, and also how to load and use the Mask2Former-based segmentation model trained on ADE20K&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;DINOv2 code and model weights are released under the Apache License 2.0. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Citing DINOv2&lt;/h2&gt; 
&lt;p&gt;If you find this repository useful, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;🦖&lt;/span&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and Darcet, Timothée and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
  journal={arXiv:2304.07193},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;@misc{darcet2023vitneedreg,
  title={Vision Transformers Need Registers},
  author={Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  journal={arXiv:2309.16588},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code&gt;@misc{jose2024dinov2meetstextunified,
  title={DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment}, 
  author={Cijo Jose and Théo Moutakanni and Dahyun Kang and Federico Baldassarre and Timothée Darcet and Hu Xu and Daniel Li and Marc Szafraniec and Michaël Ramamonjisoa and Maxime Oquab and Oriane Siméoni and Huy V. Vo and Patrick Labatut and Piotr Bojanowski},
  journal={arXiv:2412.16334},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png&quot; width=&quot;400&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 💜 &lt;a href=&quot;https://chat.qwenlm.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤗 &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🤖 &lt;a href=&quot;https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 &lt;a href=&quot;https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list&quot;&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📚 &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks&quot;&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; 🖥️ &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo&quot;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;💬 &lt;a href=&quot;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&quot;&gt;WeChat (微信)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🫨 &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;📑 &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&quot;&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;🖥️ &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl&quot;&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math—causal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text–vision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Full‑frequency allocation over time, width, and height via robust positional embeddings, enhancing long‑horizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multi‑level ViT features to capture fine‑grained details and sharpen image–text alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text–Timestamp Alignment:&lt;/strong&gt; Moves beyond T‑RoPE to precise, timestamp‑grounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&quot;&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&quot;&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune&quot;&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct&quot;&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-vl-32b/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&quot;&gt;3B&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&quot;&gt;7B&lt;/a&gt; , and &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&quot;&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-vl/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/QVQ-72B-Preview&quot;&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qvq-72b-preview/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct&quot;&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ&quot;&gt;AWQ&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8&quot;&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href=&quot;https://arxiv.org/pdf/2409.12191&quot;&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&quot;&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2-vl/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h4&gt;Visual Tasks:&lt;/h4&gt; 
&lt;div style=&quot;display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;&quot;&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg&quot; width=&quot;48%&quot; /&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl.jpg&quot; width=&quot;48%&quot; /&gt; 
&lt;/div&gt; 
&lt;h4&gt;Pure Text Tasks:&lt;/h4&gt; 
&lt;div style=&quot;display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;&quot;&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg&quot; width=&quot;48%&quot; /&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg&quot; width=&quot;48%&quot; /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks&quot;&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb&quot;&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb&quot;&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb&quot;&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb&quot;&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb&quot;&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb&quot;&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb&quot;&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb&quot;&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb&quot;&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model’s precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb&quot;&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb&quot;&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb&quot;&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with 🤖 ModelScope and 🤗 Transformers.&lt;/p&gt; 
&lt;p&gt;The code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/transformers
# pip install transformers==4.57.0 # currently, V4.57.0 is not released
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;🤖 ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using 🤗 Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
#     dtype=torch.bfloat16,
#     attn_implementation=&quot;flash_attention_2&quot;,
#     device_map=&quot;auto&quot;,
# )

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing multiple images and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image1.jpg&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image2.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Identify the similarities between these images.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = &#39;left&#39;

# Sample messages for batch inference
messages1 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image1.jpg&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image2.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What are the common elements in these pictures?&quot;},
        ],
    }
]
messages2 = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are a helpful assistant.&quot;}]},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Who are you?&quot;}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H × W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size[&#39;shortest_edge&#39;]&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt; represents the maximum total number of pixels across all frames in a video — for a video of shape T×H×W, the product T×H×W must not exceed &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size[&#39;shortest_edge&#39;]&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {&quot;longest_edge&quot;: 1280*32*32, &quot;shortest_edge&quot;: 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {&quot;longest_edge&quot;: 16384*32*32, &quot;shortest_edge&quot;: 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors=&quot;pt&quot;,
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install qwen-vl-utils==0.0.14
# It&#39;s highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&quot;image_path_size&quot;: &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;return_video_metadata&quot;(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;📌 Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/your/image.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
## Image URL
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;http://path/to/your/image.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;data:image;base64,/9j/...&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

# resized_height and resized_width
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
                &quot;resized_height&quot;: 280,
                &quot;resized_width&quot;: 420,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
                &quot;min_pixels&quot;: 50176,
                &quot;max_pixels&quot;: 50176,

            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors=&quot;pt&quot;)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: [
                    &quot;file:///path/to/frame1.jpg&quot;,
                    &quot;file:///path/to/frame2.jpg&quot;,
                    &quot;file:///path/to/frame3.jpg&quot;,
                    &quot;file:///path/to/frame4.jpg&quot;,
                ],
                &#39;sample_fps&#39;:&#39;1&#39;, # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;file:///path/to/video1.mp4&quot;,
                &quot;max_pixels&quot;: 360 * 420,
                &quot;fps&quot;: 1.0,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
                &quot;min_pixels&quot;: 4 * 32 * 32,
                &quot;max_pixels&quot;: 256 * 32 * 32,
                &quot;total_pixels&quot;: 20480 * 32 * 32,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
                &quot;min_pixels&quot;: 4 * 32 * 32,
                &quot;max_pixels&quot;: 256 * 32 * 32,
                &quot;total_pixels&quot;: 20480 * 32 * 32,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=&quot;pt&quot;, do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href=&quot;https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source&quot;&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href=&quot;https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec&quot;&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;❌&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
    &lt;td&gt;✅&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it&#39;s helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;conversation = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [{&quot;type&quot;: &quot;image&quot;}, {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Hello, how are you?&quot;}],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;I&#39;m doing well, thank you for asking. How can I assist you today?&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Can you describe these images and video?&quot;},
            {&quot;type&quot;: &quot;image&quot;},
            {&quot;type&quot;: &quot;image&quot;},
            {&quot;type&quot;: &quot;video&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;These are from my vacation.&quot;},
        ],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;I&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;It was a trip to the mountains. Can you see the details in the images and video?&quot;,
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation=&quot;flash_attention_2&quot;&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, 
    torch_dtype=torch.bfloat16, 
    attn_implementation=&quot;flash_attention_2&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href=&quot;https://arxiv.org/abs/2309.00071&quot;&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    &quot;max_position_embeddings&quot;: 1000000,
	...,
    &quot;rope_scaling&quot;: {
        &quot;rope_type&quot;: &quot;yarn&quot;,
        &quot;mrope_section&quot;: [
            24,
            20,
            20
        ],
        &quot;mrope_interleaved&quot;: true,
        &quot;factor&quot;: 3.0,
        &quot;original_max_position_embeddings&quot;: 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling &#39;{&quot;rope_type&quot;:&quot;yarn&quot;,&quot;factor&quot;:3.0,&quot;original_max_position_embeddings&quot;: 262144,&quot;mrope_section&quot;:[24,20,20],&quot;mrope_interleaved&quot;: true}&#39; --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPE’s position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 — not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let&#39;s start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = &quot;&quot;

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
)

completion = client.chat.completions.create(
    model=&quot;qwen3-vl-235b-a22b-instruct&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
        {&quot;type&quot;: &quot;image_url&quot;,
         &quot;image_url&quot;: {&quot;url&quot;: &quot;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&quot;}},
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;这是什么&quot;},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&quot;&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you’ll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model — try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;0.10.2&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker&quot;&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also check &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html&quot;&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-vl-utils==0.0.14
# pip install &#39;vllm&amp;gt;0.10.2&#39; # If this is not working use the below one. 
uv pip install -U vllm \
    --torch-backend=auto \
    --extra-index-url https://wheels.vllm.ai/nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# FP8 requires NVIDIA H100+ and CUDA 12+
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-235B-A22B-Instruct\
  --served-model-name Qwen/Qwen3-VL-235B-A22B-Instruct \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --host 0.0.0.0 \
  --port 22002 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.70 \
  --quantization fp8 \
  --distributed-executor-backend mp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct\
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8 \
   --max-num-batched-tokens 8192 \
   --max-num-seqs 256
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:22002/v1&quot;,
    timeout=3600
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image_url&quot;,
                &quot;image_url&quot;: {
                    &quot;url&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;
                }
            },
            {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: &quot;Read all the text in the image.&quot;
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model=&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
    messages=messages,
    max_tokens=2048
)
print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
print(f&quot;Generated text: {response.choices[0].message.content}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:22002/v1&quot;,
    timeout=3600
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video_url&quot;,
                &quot;video_url&quot;: {
                    &quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;
                }
            },
            {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: &quot;How long is this video?&quot;
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model=&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
    messages=messages,
    max_tokens=2048
)

print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
print(f&quot;Generated text: {response.choices[0].message.content}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ[&#39;VLLM_WORKER_MULTIPROC_METHOD&#39;] = &#39;spawn&#39;

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f&quot;video_kwargs: {video_kwargs}&quot;)

    mm_data = {}
    if image_inputs is not None:
        mm_data[&#39;image&#39;] = image_inputs
    if video_inputs is not None:
        mm_data[&#39;video&#39;] = video_inputs

    return {
        &#39;prompt&#39;: text,
        &#39;multi_modal_data&#39;: mm_data,
        &#39;mm_processor_kwargs&#39;: video_kwargs
    }


if __name__ == &#39;__main__&#39;:
    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                  &quot;type&quot;: &quot;image&quot;,
                  &quot;image&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;,
              },
              {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Read all the text in the image.&quot;},
            ],
        }
    ]

    checkpoint_path = &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        trust_remote_code=True,
        gpu_memory_utilization=0.97,
        enforce_eager=False,
        max_model_len=8192,
        max_num_seqs=8,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print(&#39;=&#39; * 40)
        print(f&quot;Inputs[{i}]: {input_[&#39;prompt&#39;]=!r}&quot;)
    print(&#39;\n&#39; + &#39;&amp;gt;&#39; * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print(&#39;=&#39; * 40)
        print(f&quot;Generated text: {generated_text!r}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == &quot;__main__&quot;:
    # TODO: change to your own checkpoint path
    checkpoint_path = &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                  &quot;type&quot;: &quot;image&quot;,
                  &quot;image&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;,
              },
              {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Read all the text in the image.&quot;},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend=&quot;fa3&quot;,
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {&quot;max_new_tokens&quot;: 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
    print(f&quot;Generated text: {response[&#39;text&#39;]}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;🐳 Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href=&quot;https://hub.docker.com/r/qwenllm/qwenvl&quot;&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-BibTeX&quot;&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model&#39;s Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/self-llm</title>
      <link>https://github.com/datawhalechina/self-llm</link>
      <description>&lt;p&gt;《开源大模型食用指南》针对中国宝宝量身打造的基于Linux环境快速微调（全参数/Lora）、部署国内外开源大模型（LLM）/多模态大模型（MLLM）教程&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/head-img.png&quot; /&gt; 
 &lt;h1&gt;开源大模型食用指南&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;中文 | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;  本项目是一个围绕开源大模型、针对国内初学者、基于 Linux 平台的中国宝宝专属大模型教程，针对各类开源大模型提供包括环境配置、本地部署、高效微调等技能在内的全流程指导，简化开源大模型的部署、使用和应用流程，让更多的普通学生、研究者更好地使用开源大模型，帮助开源、自由的大模型更快融入到普通学习者的生活中。&lt;/p&gt; 
&lt;p&gt;  本项目的主要内容包括：&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;基于 Linux 平台的开源 LLM 环境配置指南，针对不同模型要求提供不同的详细环境配置步骤；&lt;/li&gt; 
 &lt;li&gt;针对国内外主流开源 LLM 的部署使用教程，包括 LLaMA、ChatGLM、InternLM 等；&lt;/li&gt; 
 &lt;li&gt;开源 LLM 的部署应用指导，包括命令行调用、在线 Demo 部署、LangChain 框架集成等；&lt;/li&gt; 
 &lt;li&gt;开源 LLM 的全量微调、高效微调方法，包括分布式全量微调、LoRA、ptuning 等。&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;  &lt;strong&gt;项目的主要内容就是教程，让更多的学生和未来的从业者了解和熟悉开源大模型的食用方法！任何人都可以提出issue或是提交PR，共同构建维护这个项目。&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;  想要深度参与的同学可以联系我们，我们会将你加入到项目的维护者中。&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;  &lt;em&gt;&lt;strong&gt;学习建议：本项目的学习建议是，先学习环境配置，然后再学习模型的部署使用，最后再学习微调。因为环境配置是基础，模型的部署使用是基础，微调是进阶。初学者可以选择Qwen1.5，InternLM2，MiniCPM等模型优先学习。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;  &lt;strong&gt;进阶学习推荐&lt;/strong&gt; ：如果您在学习完本项目后，希望更深入地理解大语言模型的核心原理，并渴望亲手从零开始训练属于自己的大模型，我们强烈推荐关注 Datawhale 的另一个开源项目—— &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;Happy-LLM 从零开始的大语言模型原理与实践教程&lt;/a&gt; 。该项目将带您深入探索大模型的底层机制，掌握完整的训练流程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：如果有同学希望了解大模型的模型构成，以及从零手写RAG、Agent和Eval等任务，可以学习Datawhale的另一个项目&lt;a href=&quot;https://github.com/datawhalechina/tiny-universe&quot;&gt;Tiny-Universe&lt;/a&gt;，大模型是当下深度学习领域的热点，但现有的大部分大模型教程只在于教给大家如何调用api完成大模型的应用，而很少有人能够从原理层面讲清楚模型结构、RAG、Agent 以及 Eval。所以该仓库会提供全部手写，不采用调用api的形式，完成大模型的 RAG 、 Agent 、Eval 任务。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：考虑到有同学希望在学习本项目之前，希望学习大模型的理论部分，如果想要进一步深入学习 LLM 的理论基础，并在理论的基础上进一步认识、应用 LLM，可以参考 Datawhale 的 &lt;a href=&quot;https://github.com/datawhalechina/so-large-lm.git&quot;&gt;so-large-llm&lt;/a&gt;课程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：如果有同学在学习本课程之后，想要自己动手开发大模型应用。同学们可以参考 Datawhale 的 &lt;a href=&quot;https://github.com/datawhalechina/llm-universe&quot;&gt;动手学大模型应用开发&lt;/a&gt; 课程，该项目是一个面向小白开发者的大模型应用开发教程，旨在基于阿里云服务器，结合个人知识库助手项目，向同学们完整的呈现大模型应用开发流程。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;项目意义&lt;/h2&gt; 
&lt;p&gt;  什么是大模型？&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;大模型（LLM）狭义上指基于深度学习算法进行训练的自然语言处理（NLP）模型，主要应用于自然语言理解和生成等领域，广义上还包括机器视觉（CV）大模型、多模态大模型和科学计算大模型等。&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;  百模大战正值火热，开源 LLM 层出不穷。如今国内外已经涌现了众多优秀开源 LLM，国外如 LLaMA、Alpaca，国内如 ChatGLM、BaiChuan、InternLM（书生·浦语）等。开源 LLM 支持用户本地部署、私域微调，每一个人都可以在开源 LLM 的基础上打造专属于自己的独特大模型。&lt;/p&gt; 
&lt;p&gt;  然而，当前普通学生和用户想要使用这些大模型，需要具备一定的技术能力，才能完成模型的部署和使用。对于层出不穷又各有特色的开源 LLM，想要快速掌握一个开源 LLM 的应用方法，是一项比较有挑战的任务。&lt;/p&gt; 
&lt;p&gt;  本项目旨在首先基于核心贡献者的经验，实现国内外主流开源 LLM 的部署、使用与微调教程；在实现主流 LLM 的相关部分之后，我们希望充分聚集共创者，一起丰富这个开源 LLM 的世界，打造更多、更全面特色 LLM 的教程。星火点点，汇聚成海。&lt;/p&gt; 
&lt;p&gt;  &lt;em&gt;&lt;strong&gt;我们希望成为 LLM 与普罗大众的阶梯，以自由、平等的开源精神，拥抱更恢弘而辽阔的 LLM 世界。&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;项目受众&lt;/h2&gt; 
&lt;p&gt;  本项目适合以下学习者：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;想要使用或体验 LLM，但无条件获得或使用相关 API；&lt;/li&gt; 
 &lt;li&gt;希望长期、低成本、大量应用 LLM；&lt;/li&gt; 
 &lt;li&gt;对开源 LLM 感兴趣，想要亲自上手开源 LLM；&lt;/li&gt; 
 &lt;li&gt;NLP 在学，希望进一步学习 LLM；&lt;/li&gt; 
 &lt;li&gt;希望结合开源 LLM，打造领域特色的私域 LLM；&lt;/li&gt; 
 &lt;li&gt;以及最广大、最普通的学生群体。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;项目规划及进展&lt;/h2&gt; 
&lt;p&gt;   本项目拟围绕开源 LLM 应用全流程组织，包括环境配置及使用、部署应用、微调等，每个部分覆盖主流及特点开源 LLM：&lt;/p&gt; 
&lt;h3&gt;Example 系列&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Chat-%E5%AC%9B%E5%AC%9B/readme.md&quot;&gt;Chat-嬛嬛&lt;/a&gt;： Chat-甄嬛是利用《甄嬛传》剧本中所有关于甄嬛的台词和语句，基于LLM进行LoRA微调得到的模仿甄嬛语气的聊天语言模型。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/Tianji-%E5%A4%A9%E6%9C%BA/readme.md&quot;&gt;Tianji-天机&lt;/a&gt;：天机是一款基于人情世故社交场景，涵盖提示词工程 、智能体制作、 数据获取与模型微调、RAG 数据清洗与使用等全流程的大语言模型系统应用教程。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/AMchat-%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/readme.md&quot;&gt;AMChat&lt;/a&gt;: AM (Advanced Mathematics) chat 是一个集成了数学知识和高等数学习题及其解答的大语言模型。该模型使用 Math 和高等数学习题及其解析融合的数据集，基于 InternLM2-Math-7B 模型，通过 xtuner 微调，专门设计用于解答高等数学问题。&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/examples/%E6%95%B0%E5%AD%97%E7%94%9F%E5%91%BD/readme.md&quot;&gt;数字生命&lt;/a&gt;: 本项目将以我为原型，利用特制的数据集对大语言模型进行微调，致力于创造一个能够真正反映我的个性特征的AI数字人——包括但不限于我的语气、表达方式和思维模式等等，因此无论是日常聊天还是分享心情，它都以一种既熟悉又舒适的方式交流，仿佛我在他们身边一样。整个流程是可迁移复制的，亮点是数据集的制作。&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;已支持模型&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/BAAI/bge-m3&quot;&gt;BGE-M3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BGE-M3-finetune-embedding-with-valid/README.md&quot;&gt;代码检索场景微调实战 微调BGE-M3 embedding模型&lt;/a&gt; @李秀奇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/openai/gpt-oss-20b&quot;&gt;gpt-oss-20b&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/1-gpt-oss-20b%20vllm%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gpt-oss-20b vllm 部署调用&lt;/a&gt;@郭宣伯&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/2-gpt-oss-20b%20Evalscope%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md&quot;&gt;gpt-oss-20b EvalScope 并发评测&lt;/a&gt; @郭宣伯&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/3-gpt-oss-20b%20lmstudio%20%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gpt-oss-20b lmstudio 本地部署调用&lt;/a&gt; @郭宣伯&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/4-gpt-oss-20b%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;gpt-oss-20b Lora 微调及 SwanLab 可视化记录&lt;/a&gt; @郭宣伯&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/gpt-oss/5-gpt-oss-20b%20DPO%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;gpt-oss-20b DPO 微调及 SwanLab 可视化记录&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.1V-Thinking&quot;&gt;GLM-4.1-Thinking&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/01-GLM-4%201V-Thinking%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.1V-Thinking vLLM 部署调用&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/02-GLM-4%201V-Thinking%20Gradio%E9%83%A8%E7%BD%B2.md&quot;&gt;GLM-4.1V-Thinking Gradio部署&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.1V-Thinking/03-GLM-4%201V-Thinking%20LoRA%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;GLM-4.1V-Thinking Lora 微调及 SwanLab 可视化记录&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/GLM4.1V-Thinking-lora&quot;&gt;GLM-4.1V-Thinking Docker 镜像&lt;/a&gt; @林恒宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zai-org/GLM-4.5&quot;&gt;GLM-4.5-Air&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/01-GLM-4.5-Air-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4.5-Air vLLM 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/02-GLM-4.5-Air%20EvalScope%20%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95.md&quot;&gt;GLM-4.5-Air EvalScope 智商情商 &amp;amp;&amp;amp; 并发评测&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4.5-Air/03-GLM-4.5-Air-Lora%20%E5%8F%8A%20Swanlab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4.5-Air Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.compshare.cn/images/lUQhKDCeCdZW?referral_code=ELukJdQS3vvCwYIfgsQf2C&amp;amp;ytag=GPU_yy_github_selfllm&quot;&gt;GLM-4.5-Air Ucloud Docker 镜像&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT&quot;&gt;ERNIE-4.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ERNIE-4.5/01-ERNIE-4.5-0.3B-PT%20Lora%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;ERNIE-4.5-0.3B-PT Lora 微调及 SwanLab 可视化记录&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/ERNIE-4.5-lora&quot;&gt;ERNIE-4.5-0.3B-PT Lora Docker 镜像&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/Tencent-Hunyuan/Hunyuan-A13B&quot;&gt;Hunyuan-A13B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/01-Hunyuan-A13B-Instruct%20%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90%20Blog.md&quot;&gt;Hunyuan-A13B-Instruct 模型架构解析 Blog&lt;/a&gt; @卓堂越&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/03-Hunyuan-A13B-Instruct-SGLang%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan-A13B-Instruct SGLang 部署调用&lt;/a&gt; @fancy&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan-A13B-Instruct/05-Hunyuan-A13B-Instruct-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Hunyuan-A13B-Instruct Lora SwanLab 可视化微调&lt;/a&gt; @谢好冉&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan-A13B-Instruct-lora&quot;&gt;Hunyuan-A13B-Instruct Lora Docker 镜像&lt;/a&gt; @谢好冉&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3&quot;&gt;Qwen3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/01-Qwen3-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90-Blog.md&quot;&gt;Qwen3 模型结构解析 Blog&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/02-Qwen3-8B-vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen3-8B vllm 部署调用&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/03-Qwen3-7B-Instruct%20Windows%20LMStudio%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen3-8B Windows LMStudio 部署调用&lt;/a&gt; @王熠明&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/04-Qwen3-8B%20EvalScope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;Qwen3-8B Evalscope 智商情商评测&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/05-Qwen3-8B-LoRA%E5%8F%8ASwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-8B Lora 微调及SwanLab 可视化记录&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/06-Qwen3-30B-A3B%20%E5%BE%AE%E8%B0%83%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-30B-A3B 微调及SwanLab 可视化记录&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/07-Qwen3-Think-%E8%A7%A3%E5%AF%86-Blog.md&quot;&gt;Qwen3 Think 解密 Blog&lt;/a&gt; @樊奇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Qwen3&quot;&gt;Qwen3-8B Docker 镜像&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models//Qwen3/08-Qwen3_0_6B%E7%9A%84%E5%B0%8F%E6%A8%A1%E5%9E%8B%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8.md&quot;&gt;Qwen3-0.6B 的小模型有什么用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/09-Qwen3-1.7B-%E5%8C%BB%E5%AD%A6%E6%8E%A8%E7%90%86%E5%BC%8F%E5%AF%B9%E8%AF%9D%E5%BE%AE%E8%B0%83%20%E5%8F%8A%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95.md&quot;&gt;Qwen3-1.7B 医学推理式对话微调 及 SwanLab 可视化记录&lt;/a&gt; @林泽毅&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen3/10-Qwen3-8B%20GRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;Qwen3-8B GRPO微调及通过swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/MoonshotAI/Kimi-VL&quot;&gt;Kimi&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/02-Kimi-VL-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A%E8%A7%A3%E8%AF%BB.md&quot;&gt;Kimi-VL-A3B 技术报告解读&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Kimi-VL/01-Kimi-VL-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Kimi-VL-A3B-Thinking WebDemo 部署（网页对话助手）&lt;/a&gt; @姜舒凡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct&quot;&gt;Llama4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama4/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B/01-Llama4-%E5%AF%B9%E8%AF%9D%E5%8A%A9%E6%89%8B.md&quot;&gt;Llama4 对话助手&lt;/a&gt; @姜舒凡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/manycore-research/SpatialLM&quot;&gt;SpatialLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/SpatialLM/readme.md&quot;&gt;SpatialLM 3D点云理解与目标检测模型部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/tencent/Hunyuan3D-2&quot;&gt;Hunyuan3D-2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/01-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 系列模型部署&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/02-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8.md&quot;&gt;Hunyuan3D-2 系列模型代码调用&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/03-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BGradio%E9%83%A8%E7%BD%B2.md&quot;&gt;Hunyuan3D-2 系列模型Gradio部署&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Hunyuan3D-2/04-Hunyuan3D-2%20%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8BAPI%20Server.md&quot;&gt;Hunyuan3D-2 系列模型API Server&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/Hunyuan3D-2&quot;&gt;Hunyuan3D-2 Docker 镜像&lt;/a&gt; @林恒宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-3-4b-it&quot;&gt;Gemma3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/01-gemma-3-4b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-3-4b-it FastApi 部署调用&lt;/a&gt; @杜森&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/03-gemma-3-4b-it-ollama%20+%20open-webui%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-3-4b-it ollama + open-webui部署&lt;/a&gt; @孙超&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/04-Gemma3-4b%20%20evalscope%E6%99%BA%E5%95%86%E6%83%85%E5%95%86%E8%AF%84%E6%B5%8B.md&quot;&gt;gemma-3-4b-it evalscope 智商情商评测&lt;/a&gt; @张龙斐&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/05-gemma-3-4b-it%20LoRA.md&quot;&gt;gemma-3-4b-it Lora 微调&lt;/a&gt; @荞麦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://www.codewithgpu.com/i/datawhalechina/self-llm/self-llm-gemma3&quot;&gt;gemma-3-4b-it Docker 镜像&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma3/6-gemma3-4B-itGRPO%E5%BE%AE%E8%B0%83%E5%8F%8A%E9%80%9A%E8%BF%87swanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;gemma-3-4b-it GRPO微调及通过swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B&quot;&gt;DeepSeek-R1-Distill&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/01-DeepSeek-R1-Distill-Qwen-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B FastApi 部署调用&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/02-DeepSeek-R1-Distill-Qwen-7B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B Langchain 接入&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/03-DeepSeek-R1-Distill-Qwen-7B%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B WebDemo 部署&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/04-DeepSeek-R1-Distill-Qwen-7B%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-R1-Distill-Qwen-7B vLLM 部署调用&lt;/a&gt; @骆秀韬&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-R1-Distill-Qwen/05-DeepSeek-R1-0528-Qwen3-8B-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;DeepSeek-R1-0528-Qwen3-8B-GRPO及swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-o&quot;&gt;MiniCPM-o-2_6&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/01MiniCPM-o%202%206%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8%20.md&quot;&gt;minicpm-o-2.6 FastApi 部署调用&lt;/a&gt; @林恒宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/02minicpm-o-2.6WebDemo_streamlit.py&quot;&gt;minicpm-o-2.6 WebDemo 部署&lt;/a&gt; @程宏&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/03-MiniCPM-o-2.6%20%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E9%9F%B3%E8%83%BD%E5%8A%9B.md&quot;&gt;minicpm-o-2.6 多模态语音能力&lt;/a&gt; @邓恺俊&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM-o/04-MiniCPM-0-2.6%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;minicpm-o-2.6 可视化 LaTeX_OCR Lora 微调&lt;/a&gt; @林泽毅&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/01-InternLM3-8B-Instruct%20FastAPI.md&quot;&gt;internlm3-8b-instruct FastApi 部署调用&lt;/a&gt; @苏向标&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/02-internlm3-8b-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;internlm3-8b-instruct Langchian接入&lt;/a&gt; @赵文恺&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/03-InternLM3-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;internlm3-8b-instruct WebDemo 部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/04-InternLM3-8B-Instruct%20LoRA.md&quot;&gt;internlm3-8b-instruct Lora 微调&lt;/a&gt; @程宏&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM3/05-internlm3-8b-instruct%20%E4%B8%8Eo1%20.md&quot;&gt;internlm3-8b-instruct o1-like推理链实现&lt;/a&gt; @陈睿&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/phi-4&quot;&gt;phi4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/01-Phi-4%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;phi4 FastApi 部署调用&lt;/a&gt; @杜森&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/02-Phi-4-Langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;phi4 langchain 接入&lt;/a&gt; @小罗&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/03-Phi-4%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;phi4 WebDemo 部署&lt;/a&gt; @杜森&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/04-Phi-4-Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;phi4 Lora 微调&lt;/a&gt; @郑远婧&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/05-Phi-4-Lora%20%E5%BE%AE%E8%B0%83%20%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB.md&quot;&gt;phi4 Lora 微调 NER任务 SwanLab 可视化记录版&lt;/a&gt; @林泽毅&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi4/06-Phi-4-GRPO%E5%8F%8Aswanlab%E5%8F%AF%E8%A7%86%E5%8C%96.md&quot;&gt;phi4 GRPO微调及通过swanlab可视化&lt;/a&gt; @郭宣伯&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Coder&quot;&gt;Qwen2.5-Coder&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/01-Qwen2.5-Coder-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct FastApi部署调用&lt;/a&gt; @赵文恺&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Langchian接入&lt;/a&gt; @杨晨旭&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/03-Qwen2.5-Coder-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-Coder-7B-Instruct WebDemo 部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/04-Qwen2.5-Coder-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-Coder-7B-Instruct vLLM 部署&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora 微调&lt;/a&gt; @荞麦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5-Coder/05-Qwen2.5-Coder-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-Coder-7B-Instruct Lora 微调 SwanLab 可视化记录版&lt;/a&gt; @杨卓&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2-VL&quot;&gt;Qwen2-vl&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/01-Qwen2-VL-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B FastApi 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/02-Qwen2-VL-2B-Instruct%20Web%20Demo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-vl-2B WebDemo 部署&lt;/a&gt; @赵伟&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/03-Qwen2-VL-2B-Instruct%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-vl-2B vLLM 部署&lt;/a&gt; @荞麦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/04-Qwen2-VL-2B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-vl-2B Lora 微调&lt;/a&gt; @李柯辰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/05-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%20%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2-vl-2B Lora 微调 SwanLab 可视化记录版&lt;/a&gt; @林泽毅&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2-VL/06-Qwen2-VL-2B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%E6%A1%88%E4%BE%8B%20-%20LaTexOCR.md&quot;&gt;Qwen2-vl-2B Lora 微调案例 - LaTexOCR&lt;/a&gt; @林泽毅&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5&quot;&gt;Qwen2.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/01-Qwen2.5-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct FastApi 部署调用&lt;/a&gt; @娄天奥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/02-Qwen2.5-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2.5-7B-Instruct langchain 接入&lt;/a&gt; @娄天奥&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/03-Qwen2.5-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2.5-7B-Instruct vLLM 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/04-Qwen2_5-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2.5-7B-Instruct WebDemo 部署&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/05-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2.5-7B-Instruct Lora 微调&lt;/a&gt; @左春生&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/06-Qwen2.5-7B-Instruct%20o1-like%20%E6%8E%A8%E7%90%86%E9%93%BE%E5%AE%9E%E7%8E%B0.md&quot;&gt;Qwen2.5-7B-Instruct o1-like 推理链实现&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2.5/07-Qwen2.5-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83%20SwanLab%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AE%B0%E5%BD%95%E7%89%88.md&quot;&gt;Qwen2.5-7B-Instruct Lora 微调 SwanLab 可视化记录版&lt;/a&gt; @林泽毅&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://machinelearning.apple.com/research/openelm&quot;&gt;Apple OpenELM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/01-OpenELM-3B-Instruct%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;OpenELM-3B-Instruct FastApi 部署调用&lt;/a&gt; @王泽宇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/OpenELM/02-OpenELM-3B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;OpenELM-3B-Instruct Lora 微调&lt;/a&gt; @王泽宇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&quot;&gt;Llama3_1-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/01-Llama3_1-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Llama3_1-8B-Instruct FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/02-Llama3_1-8B-Instruct%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;Llama3_1-8B-Instruct langchain 接入&lt;/a&gt; @张晋&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/03-Llama3_1-8B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Llama3_1-8B-Instruct WebDemo 部署&lt;/a&gt; @张晋&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/04-Llama3_1-8B--Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Llama3_1-8B-Instruct Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Llama3_1/%E5%8A%A8%E6%89%8B%E8%BD%AC%E6%8D%A2GGUF%E6%A8%A1%E5%9E%8B%E5%B9%B6%E4%BD%BF%E7%94%A8Ollama%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2.md&quot;&gt;动手转换GGUF模型并使用Ollama本地部署&lt;/a&gt; @Gaoboy&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-2-9b-it&quot;&gt;Gemma-2-9b-it&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/01-Gemma-2-9b-it%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Gemma-2-9b-it FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/02-Gemma-2-9b-it%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Gemma-2-9b-it langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/03-Gemma-2-9b-it%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;Gemma-2-9b-it WebDemo 部署&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma2/04-Gemma-2-9b-it%20peft%20lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Gemma-2-9b-it Peft Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan-2.0&quot;&gt;Yuan2.0&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/01-Yuan2.0-2B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B FastApi 部署调用&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/02-Yuan2.0-2B%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-2B Langchain 接入&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/03-Yuan2.0-2B%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-2B WebDemo部署&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/04-Yuan2.0-2B%20vLLM%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-2B vLLM部署调用&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0/05-Yuan2.0-2B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;Yuan2.0-2B Lora微调&lt;/a&gt; @张帆&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/IEIT-Yuan/Yuan2.0-M32&quot;&gt;Yuan2.0-M32&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/01-Yuan2.0-M32%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yuan2.0-M32 FastApi 部署调用&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/02-Yuan2.0-M32%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Yuan2.0-M32 Langchain 接入&lt;/a&gt; @张帆&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yuan2.0-M32/03-Yuan2.0-M32%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Yuan2.0-M32 WebDemo部署&lt;/a&gt; @张帆&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-Coder-V2&quot;&gt;DeepSeek-Coder-V2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/01-DeepSeek-Coder-V2-Lite-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct FastApi 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/02-DeepSeek-Coder-V2-Lite-Instruct%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct langchain 接入&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/03-DeepSeek-Coder-V2-Lite-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct WebDemo 部署&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek-Coder-V2/04-DeepSeek-Coder-V2-Lite-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-Coder-V2-Lite-Instruct Lora 微调&lt;/a&gt; @余洋&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/bilibili/Index-1.9B&quot;&gt;哔哩哔哩 Index-1.9B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/01-Index-1.9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Index-1.9B-Chat FastApi 部署调用&lt;/a&gt; @邓恺俊&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/02-Index-1.9B-Chat%20%E6%8E%A5%E5%85%A5%20LangChain.md&quot;&gt;Index-1.9B-Chat langchain 接入&lt;/a&gt; @张友东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/03-Index-1.9B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Index-1.9B-Chat WebDemo 部署&lt;/a&gt; @程宏&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/bilibili_Index-1.9B/04-Index-1.9B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Index-1.9B-Chat Lora 微调&lt;/a&gt; @姜舒凡&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2&quot;&gt;Qwen2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/01-Qwen2-7B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct FastApi 部署调用&lt;/a&gt; @康婧淇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/02-Qwen2-7B-Instruct%20Langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Qwen2-7B-Instruct langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/03-Qwen2-7B-Instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Qwen2-7B-Instruct WebDemo 部署&lt;/a&gt; @三水&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/04-Qwen2-7B-Instruct%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen2-7B-Instruct vLLM 部署调用&lt;/a&gt; @姜舒凡&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen2/05-Qwen2-7B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen2-7B-Instruct Lora 微调&lt;/a&gt; @散步&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/GLM-4.git&quot;&gt;GLM-4&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/01-GLM-4-9B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat FastApi 部署调用&lt;/a&gt; @张友东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/02-GLM-4-9B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;GLM-4-9B-chat langchain 接入&lt;/a&gt; @谭逸珂&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/03-GLM-4-9B-Chat%20WebDemo.md&quot;&gt;GLM-4-9B-chat WebDemo 部署&lt;/a&gt; @何至轩&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/04-GLM-4-9B-Chat%20vLLM%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;GLM-4-9B-chat vLLM 部署&lt;/a&gt; @王熠明&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat Lora 微调&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/GLM-4/05-GLM-4-9B-chat-hf%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;GLM-4-9B-chat-hf Lora 微调&lt;/a&gt; @付志远&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen1.5.git&quot;&gt;Qwen 1.5&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/01-Qwen1.5-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat FastApi 部署调用&lt;/a&gt; @颜鑫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/02-Qwen1.5-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen1.5-7B-chat langchain 接入&lt;/a&gt; @颜鑫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/03-Qwen1.5-7B-Chat%20WebDemo.md&quot;&gt;Qwen1.5-7B-chat WebDemo 部署&lt;/a&gt; @颜鑫&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/04-Qwen1.5-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen1.5-7B-chat Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/05-Qwen1.5-7B-Chat-GPTQ-Int4%20%20WebDemo.md&quot;&gt;Qwen1.5-72B-chat-GPTQ-Int4 部署环境&lt;/a&gt; @byx020119&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/06-Qwen1.5-MoE-A2.7B.md&quot;&gt;Qwen1.5-MoE-chat Transformers 部署调用&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/07-Qwen1.5-7B-Chat%20vLLM%20%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen1.5-7B-chat vLLM推理部署&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen1.5/08-Qwen1.5-7B-chat%20LoRA%E5%BE%AE%E8%B0%83%E6%8E%A5%E5%85%A5%E5%AE%9E%E9%AA%8C%E7%AE%A1%E7%90%86.md&quot;&gt;Qwen1.5-7B-chat Lora 微调 接入SwanLab实验管理平台&lt;/a&gt; @黄柏特&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/google/gemma-7b-it&quot;&gt;谷歌-Gemma&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/01-Gemma-2B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;gemma-2b-it FastApi 部署调用 &lt;/a&gt; @东东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/02-Gemma-2B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;gemma-2b-it langchain 接入 &lt;/a&gt; @东东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/03-Gemma-2B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;gemma-2b-it WebDemo 部署 &lt;/a&gt; @东东&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Gemma/04-Gemma-2B-Instruct%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;gemma-2b-it Peft Lora 微调 &lt;/a&gt; @东东&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct&quot;&gt;phi-3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/01-Phi-3-mini-4k-instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Phi-3-mini-4k-instruct FastApi 部署调用&lt;/a&gt; @郑皓桦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/02-Phi-3-mini-4k-instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;Phi-3-mini-4k-instruct langchain 接入&lt;/a&gt; @郑皓桦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/03-Phi-3-mini-4k-instruct%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;Phi-3-mini-4k-instruct WebDemo 部署&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/phi-3/04-Phi-3-mini-4k-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Phi-3-mini-4k-instruct Lora 微调&lt;/a&gt; @丁悦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/thu-coai/CharacterGLM-6B&quot;&gt;CharacterGLM-6B&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/01-CharacterGLM-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B Transformers 部署调用&lt;/a&gt; @孙健壮&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/02-CharacterGLM-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;CharacterGLM-6B FastApi 部署调用&lt;/a&gt; @孙健壮&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/03-CharacterGLM-6B-chat.md&quot;&gt;CharacterGLM-6B webdemo 部署&lt;/a&gt; @孙健壮&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/CharacterGLM/04-CharacterGLM-6B%20Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;CharacterGLM-6B Lora 微调&lt;/a&gt; @孙健壮&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/meta-llama/llama3.git&quot;&gt;LLaMA3-8B-Instruct&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;LLaMA3-8B-Instruct FastApi 部署调用&lt;/a&gt; @高立业&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/02-LLaMA3-8B-Instruct%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;LLaMA3-8B-Instruct langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/03-LLaMA3-8B-Instruct%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;LLaMA3-8B-Instruct WebDemo 部署&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/LLaMA3/04-LLaMA3-8B-Instruct%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;LLaMA3-8B-Instruct Lora 微调&lt;/a&gt; @高立业&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://modelscope.cn/models/xverse/XVERSE-7B-Chat/summary&quot;&gt;XVERSE-7B-Chat&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/01-XVERSE-7B-chat%20Transformers%E6%8E%A8%E7%90%86.md&quot;&gt;XVERSE-7B-Chat transformers 部署调用&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/02-XVERSE-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat FastApi 部署调用&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/03-XVERSE-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;XVERSE-7B-Chat langchain 接入&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/04-XVERSE-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;XVERSE-7B-Chat WebDemo 部署&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/XVERSE/05-XVERSE-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;XVERSE-7B-Chat Lora 微调&lt;/a&gt; @郭志航&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenNLPLab/TransnormerLLM.git&quot;&gt;TransNormerLLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/01-TransNormer-7B%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;TransNormerLLM-7B-Chat FastApi 部署调用&lt;/a&gt; @王茂霖&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/02-TransNormer-7B%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;TransNormerLLM-7B-Chat langchain 接入&lt;/a&gt; @王茂霖&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/03-TransNormer-7B%20WebDemo.md&quot;&gt;TransNormerLLM-7B-Chat WebDemo 部署&lt;/a&gt; @王茂霖&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/TransNormer/04-TrasnNormer-7B%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;TransNormerLLM-7B-Chat Lora 微调&lt;/a&gt; @王茂霖&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/vivo-ai-lab/BlueLM.git&quot;&gt;BlueLM Vivo 蓝心大模型&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/01-BlueLM-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat FatApi 部署调用&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/02-BlueLM-7B-Chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;BlueLM-7B-Chat langchain 接入&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/03-BlueLM-7B-Chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;BlueLM-7B-Chat WebDemo 部署&lt;/a&gt; @郭志航&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BlueLM/04-BlueLM-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;BlueLM-7B-Chat Lora 微调&lt;/a&gt; @郭志航&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM&quot;&gt;InternLM2&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/01-InternLM2-7B-chat%20FastAPI%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/02-InternLM2-7B-chat%20langchain%20%E6%8E%A5%E5%85%A5.md&quot;&gt;InternLM2-7B-chat langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/03-InternLM2-7B-chat%20WebDemo%20%E9%83%A8%E7%BD%B2.md&quot;&gt;InternLM2-7B-chat WebDemo 部署&lt;/a&gt; @郑皓桦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM2/04-InternLM2-7B-chat%20Xtuner%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;InternLM2-7B-chat Xtuner Qlora 微调&lt;/a&gt; @郑皓桦&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-LLM&quot;&gt;DeepSeek 深度求索&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/01-DeepSeek-7B-chat%20FastApi.md&quot;&gt;DeepSeek-7B-chat FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/02-DeepSeek-7B-chat%20langchain.md&quot;&gt;DeepSeek-7B-chat langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/03-DeepSeek-7B-chat%20WebDemo.md&quot;&gt;DeepSeek-7B-chat WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/04-DeepSeek-7B-chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/05-DeepSeek-7B-chat%204bits%E9%87%8F%E5%8C%96%20Qlora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;DeepSeek-7B-chat 4bits量化 Qlora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;DeepSeek-MoE-16b-chat Transformers 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/06-DeepSeek-MoE-16b-chat%20FastApi.md&quot;&gt;DeepSeek-MoE-16b-chat FastApi 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/07-deepseek_fine_tune.ipynb&quot;&gt;DeepSeek-coder-6.7b finetune colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/DeepSeek/08-deepseek_web_demo.ipynb&quot;&gt;Deepseek-coder-6.7b webdemo colab&lt;/a&gt; @Swiftie&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM.git&quot;&gt;MiniCPM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat transformers 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;MiniCPM-2B-chat FastApi 部署调用&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20langchain%E6%8E%A5%E5%85%A5.md&quot;&gt;MiniCPM-2B-chat langchain 接入&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20WebDemo%E9%83%A8%E7%BD%B2.md&quot;&gt;MiniCPM-2B-chat webdemo 部署&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/MiniCPM/MiniCPM-2B-chat%20Lora%20&amp;amp;&amp;amp;%20Full%20%E5%BE%AE%E8%B0%83.md&quot;&gt;MiniCPM-2B-chat Lora &amp;amp;&amp;amp; Full 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; 官方友情链接：&lt;a href=&quot;https://modelbest.feishu.cn/wiki/D2tFw8Pcsi5CIzkaHNacLK64npg&quot;&gt;面壁小钢炮MiniCPM教程&lt;/a&gt; @OpenBMB&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; 官方友情链接：&lt;a href=&quot;https://github.com/OpenBMB/MiniCPM-CookBook&quot;&gt;MiniCPM-Cookbook&lt;/a&gt; @OpenBMB&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen-Audio.git&quot;&gt;Qwen-Audio&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/01-Qwen-Audio-chat%20FastApi.md&quot;&gt;Qwen-Audio FastApi 部署调用&lt;/a&gt; @陈思州&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen-Audio/02-Qwen-Audio-chat%20WebDemo.md&quot;&gt;Qwen-Audio WebDemo&lt;/a&gt; @陈思州&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen.git&quot;&gt;Qwen&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/01-Qwen-7B-Chat%20Transformers%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat Transformers 部署调用&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/02-Qwen-7B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Qwen-7B-chat FastApi 部署调用&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/03-Qwen-7B-Chat%20WebDemo.md&quot;&gt;Qwen-7B-chat WebDemo&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/04-Qwen-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat Lora 微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/05-Qwen-7B-Chat%20Ptuning%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat ptuning 微调&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/06-Qwen-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat 全量微调&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/07-Qwen-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Qwen-7B-Chat 接入langchain搭建知识库助手&lt;/a&gt; @李娇娇&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/08-Qwen-7B-Chat%20Lora%20%E4%BD%8E%E7%B2%BE%E5%BA%A6%E5%BE%AE%E8%B0%83.md&quot;&gt;Qwen-7B-chat 低精度训练&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Qwen/09-Qwen-1_8B-chat%20CPU%20%E9%83%A8%E7%BD%B2%20.md&quot;&gt;Qwen-1_8B-chat CPU 部署&lt;/a&gt; @散步&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/01-ai/Yi.git&quot;&gt;Yi 零一万物&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/01-Yi-6B-Chat%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Yi-6B-chat FastApi 部署调用&lt;/a&gt; @李柯辰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/02-Yi-6B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Yi-6B-chat langchain接入&lt;/a&gt; @李柯辰&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/03-Yi-6B-chat%20WebDemo.md&quot;&gt;Yi-6B-chat WebDemo&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Yi/04-Yi-6B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Yi-6B-chat Lora 微调&lt;/a&gt; @李娇娇&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.baichuan-ai.com/home&quot;&gt;Baichuan 百川智能&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/BaiChuan/01-Baichuan2-7B-chat%2BFastApi%2B%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;Baichuan2-7B-chat FastApi 部署调用&lt;/a&gt; @惠佳豪&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/02-Baichuan-7B-chat%2BWebDemo.md&quot;&gt;Baichuan2-7B-chat WebDemo&lt;/a&gt; @惠佳豪&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/03-Baichuan2-7B-chat%E6%8E%A5%E5%85%A5LangChain%E6%A1%86%E6%9E%B6.md&quot;&gt;Baichuan2-7B-chat 接入 LangChain 框架&lt;/a&gt; @惠佳豪&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/BaiChuan/04-Baichuan2-7B-chat%2Blora%2B%E5%BE%AE%E8%B0%83.md&quot;&gt;Baichuan2-7B-chat Lora 微调&lt;/a&gt; @惠佳豪&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/InternLM/InternLM.git&quot;&gt;InternLM&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/01-InternLM-Chat-7B%20Transformers%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;InternLM-Chat-7B Transformers 部署调用&lt;/a&gt; @小罗&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/02-internLM-Chat-7B%20FastApi.md&quot;&gt;InternLM-Chat-7B FastApi 部署调用&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/03-InternLM-Chat-7B.md&quot;&gt;InternLM-Chat-7B WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/04-Lagent+InternLM-Chat-7B-V1.1.md&quot;&gt;Lagent+InternLM-Chat-7B-V1.1 WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/05-%E6%B5%A6%E8%AF%AD%E7%81%B5%E7%AC%94%E5%9B%BE%E6%96%87%E7%90%86%E8%A7%A3&amp;amp;%E5%88%9B%E4%BD%9C.md&quot;&gt;浦语灵笔图文理解&amp;amp;创作 WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/InternLM/06-InternLM%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;InternLM-Chat-7B 接入 LangChain 框架&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://hf-mirror.com/FlagAlpha/Atom-7B-Chat&quot;&gt;Atom (llama2)&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/01-Atom-7B-chat-WebDemo.md&quot;&gt;Atom-7B-chat WebDemo&lt;/a&gt; @Kailigithub&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/02-Atom-7B-Chat%20Lora%20%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat Lora 微调&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/03-Atom-7B-Chat%20%E6%8E%A5%E5%85%A5langchain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;Atom-7B-Chat 接入langchain搭建知识库助手&lt;/a&gt; @陈思州&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/Atom/04-Atom-7B-chat%20%E5%85%A8%E9%87%8F%E5%BE%AE%E8%B0%83.md&quot;&gt;Atom-7B-chat 全量微调&lt;/a&gt; @Logan Zou&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/THUDM/ChatGLM3.git&quot;&gt;ChatGLM3&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/01-ChatGLM3-6B%20Transformer%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B Transformers 部署调用&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/02-ChatGLM3-6B%20FastApi%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md&quot;&gt;ChatGLM3-6B FastApi 部署调用&lt;/a&gt; @丁悦&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/03-ChatGLM3-6B-chat.md&quot;&gt;ChatGLM3-6B chat WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/04-ChatGLM3-6B-Code-Interpreter.md&quot;&gt;ChatGLM3-6B Code Interpreter WebDemo&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/05-ChatGLM3-6B%E6%8E%A5%E5%85%A5LangChain%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.md&quot;&gt;ChatGLM3-6B 接入 LangChain 框架&lt;/a&gt; @Logan Zou&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/ChatGLM/06-ChatGLM3-6B-Lora%E5%BE%AE%E8%B0%83.md&quot;&gt;ChatGLM3-6B Lora 微调&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;通用环境配置&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/01-pip%E3%80%81conda%E6%8D%A2%E6%BA%90.md&quot;&gt;pip、conda 换源&lt;/a&gt; @不要葱姜蒜&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/02-AutoDL%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3.md&quot;&gt;AutoDL 开放端口&lt;/a&gt; @不要葱姜蒜&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;模型下载&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;hugging face&lt;/a&gt; 镜像下载 @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;modelscope&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;git-lfs&lt;/a&gt; @不要葱姜蒜&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/03-%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD.md&quot;&gt;Openxlab&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Issue &amp;amp;&amp;amp; PR&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;Issue 提交&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;PR 提交&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
   &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/models/General-Setting/04-Issue&amp;amp;PR&amp;amp;update.md&quot;&gt;fork更新&lt;/a&gt; @肖鸿儒&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;致谢&lt;/h2&gt; 
&lt;h3&gt;核心贡献者&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;宋志学(不要葱姜蒜)-项目负责人&lt;/a&gt; （Datawhale成员-中国矿业大学(北京)）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;邹雨衡-项目负责人&lt;/a&gt; （Datawhale成员-对外经济贸易大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Tsumugii24&quot;&gt;姜舒凡&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;肖鸿儒&lt;/a&gt; （Datawhale成员-同济大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/acwwt&quot;&gt;郭志航&lt;/a&gt;（内容创作者）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Zeyi-Lin&quot;&gt;林泽毅&lt;/a&gt;（内容创作者-SwanLab产品负责人）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LINHYYY&quot;&gt;林恒宇&lt;/a&gt;（内容创作者-广东东软学院-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Twosugar666&quot;&gt;郭宣伯&lt;/a&gt;（内容创作者-北京航空航天大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhangfanTJU&quot;&gt;张帆&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/moyitech&quot;&gt;王泽宇&lt;/a&gt;（内容创作者-太原理工大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aphasia0515&quot;&gt;李娇娇&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/0-yy-0&quot;&gt;高立业&lt;/a&gt;（内容创作者-DataWhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dingyue772&quot;&gt;丁悦&lt;/a&gt; （Datawhale-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/L4HeyXiao&quot;&gt;惠佳豪&lt;/a&gt; （Datawhale-宣传大使）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mlw67&quot;&gt;王茂霖&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Caleb-Sun-jz&quot;&gt;孙健壮&lt;/a&gt;（内容创作者-对外经济贸易大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LucaChen&quot;&gt;东东&lt;/a&gt;（内容创作者-谷歌开发者机器学习技术专家）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yeyeyeyeeeee&quot;&gt;荞麦&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kailigithub&quot;&gt;Kailigithub&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/BaiYu96&quot;&gt;郑皓桦&lt;/a&gt; （内容创作者）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Joe-2002&quot;&gt;李柯辰&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chg0901&quot;&gt;程宏&lt;/a&gt;（内容创作者-Datawhale意向成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anine09&quot;&gt;骆秀韬&lt;/a&gt;（内容创作者-Datawhale成员-似然实验室）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ilovexsir&quot;&gt;谢好冉&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jjyaoao&quot;&gt;陈思州&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sanbuphy&quot;&gt;散步&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/thomas-yanxin&quot;&gt;颜鑫&lt;/a&gt; （Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/study520ai520&quot;&gt;杜森&lt;/a&gt;（内容创作者-Datawhale成员-南阳理工学院）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cswangxiaowei&quot;&gt;Swiftie&lt;/a&gt; （小米NLP算法工程师）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KashiwaByte&quot;&gt;黄柏特&lt;/a&gt;（内容创作者-西安电子科技大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AXYZdong&quot;&gt;张友东&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YangYu-NUAA&quot;&gt;余洋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Jin-Zhang-Yaoguang&quot;&gt;张晋&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lta155&quot;&gt;娄天奥&lt;/a&gt;（内容创作者-中国科学院大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LinChentang&quot;&gt;左春生&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/little1d&quot;&gt;杨卓&lt;/a&gt;（内容创作者-西安电子科技大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lyj11111111&quot;&gt;小罗&lt;/a&gt; （内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kedreamix&quot;&gt;邓恺俊&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/XiLinky&quot;&gt;赵文恺&lt;/a&gt;（内容创作者-太原理工大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/comfzy&quot;&gt;付志远&lt;/a&gt;（内容创作者-海南大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/isaacahahah&quot;&gt;郑远婧&lt;/a&gt;（内容创作者-鲸英助教-福州大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bald0Wang&quot;&gt;王熠明&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LikeGiver&quot;&gt;谭逸珂&lt;/a&gt;（内容创作者-对外经济贸易大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pod2c&quot;&gt;何至轩&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jodie-kang&quot;&gt;康婧淇&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sssanssss&quot;&gt;三水&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/langlibai66&quot;&gt;杨晨旭&lt;/a&gt;（内容创作者-太原理工大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/2710932616&quot;&gt;赵伟&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gzhuuser&quot;&gt;苏向标&lt;/a&gt;（内容创作者-广州大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/riannyway&quot;&gt;陈睿&lt;/a&gt;（内容创作者-西交利物浦大学-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Feimike09&quot;&gt;张龙斐&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/anarchysaiko&quot;&gt;孙超&lt;/a&gt;（内容创作者-Datawhale成员）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fanqiNO1&quot;&gt;樊奇&lt;/a&gt;（内容创作者-上海交通大学）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nusakom&quot;&gt;卓堂越&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fancyboi999&quot;&gt;fancy&lt;/a&gt;（内容创作者-鲸英助教）&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/li-xiu-qi&quot;&gt;李秀奇&lt;/a&gt;（内容创作者-DataWhale意向成员）&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;注：排名根据贡献程度排序&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;其他&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;特别感谢&lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt;对本项目的帮助与支持&lt;/li&gt; 
 &lt;li&gt;部分lora代码和讲解参考仓库：&lt;a href=&quot;https://github.com/zyds/transformers-code.git&quot;&gt;https://github.com/zyds/transformers-code.git&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue&lt;/li&gt; 
 &lt;li&gt;特别感谢以下为教程做出贡献的同学！&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/self-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/self-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h3&gt;Star History&lt;/h3&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/self-llm/master/images/star-history-202572.png&quot; /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>microsoft/ai-agents-for-beginners</title>
      <link>https://github.com/microsoft/ai-agents-for-beginners</link>
      <description>&lt;p&gt;12 Lessons to Get Started Building AI Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Agents for Beginners - A Course&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/images/repo-thumbnailv2.png&quot; alt=&quot;Generative AI For Beginners&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;A course teaching everything you need to know to start building AI Agents&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/raw/master/LICENSE?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/issues/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/pulls/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/ai-agents-for-beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;🌐 Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href=&quot;https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/watchers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/network/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/ai-agents-for-beginners/stargazers/?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/ai-agents-for-beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/kzRShWzttr&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/kzRShWzttr&quot; alt=&quot;Azure AI Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🌱 Getting Started&lt;/h2&gt; 
&lt;p&gt;This course has lessons covering the fundamentals of building AI Agents. Each lesson covers its own topic so start wherever you like!&lt;/p&gt; 
&lt;p&gt;There is multi-language support for this course. Go to our &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/#-multi-language-support&quot;&gt;available languages here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If this is your first time building with Generative AI models, check out our &lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI For Beginners&lt;/a&gt; course, which includes 21 lessons on building with GenAI.&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to &lt;a href=&quot;https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst&quot;&gt;star (🌟) this repo&lt;/a&gt; and &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/fork&quot;&gt;fork this repo&lt;/a&gt; to run the code.&lt;/p&gt; 
&lt;h3&gt;Meet Other Learners, Get Your Questions Answered&lt;/h3&gt; 
&lt;p&gt;If you get stuck or have any questions about building AI Agents, join our dedicated Discord Channel in the &lt;a href=&quot;https://aka.ms/ai-agents/discord&quot;&gt;Azure AI Foundry Community Discord&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;What You Need&lt;/h3&gt; 
&lt;p&gt;Each lesson in this course includes code examples, which can be found in the code_samples folder. You can &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/fork&quot;&gt;fork this repo&lt;/a&gt; to create your own copy.&lt;/p&gt; 
&lt;p&gt;The code example in these exercises, utilize Azure AI Foundry and GitHub Model Catalogs for interacting with Language Models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/github-models&quot;&gt;Github Models&lt;/a&gt; - Free / Limited&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/ai-foundry&quot;&gt;Azure AI Foundry&lt;/a&gt; - Azure Account Required&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;This course also uses the following AI Agent frameworks and services from Microsoft:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/ai-agent-service&quot;&gt;Azure AI Agent Service&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/semantic-kernel&quot;&gt;Semantic Kernel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-agents/autogen&quot;&gt;AutoGen&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more information on running the code for this course, go to the &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/00-course-setup/README.md&quot;&gt;Course Setup&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;🙏 Want to help?&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/issues?WT.mc_id=academic-105485-koreyst&quot;&gt;Raise an issue&lt;/a&gt; or &lt;a href=&quot;https://github.com/microsoft/ai-agents-for-beginners/pulls?WT.mc_id=academic-105485-koreyst&quot;&gt;Create a pull request&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📂 Each lesson includes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A written lesson located in the README and a short video&lt;/li&gt; 
 &lt;li&gt;Python code samples supporting Azure AI Foundry and Github Models (Free)&lt;/li&gt; 
 &lt;li&gt;Links to extra resources to continue your learning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🗃️ Lessons&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Lesson&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Text &amp;amp; Code&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Video&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Extra Learning&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Intro to AI Agents and Agent Use Cases&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/01-intro-to-ai-agents/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/3zgm60bXmQk?si=z8QygFvYQv-9WtO1&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Exploring AI Agentic Frameworks&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/02-explore-agentic-frameworks/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/ODwF-EZo_O8?si=Vawth4hzVaHv-u0H&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Understanding AI Agentic Design Patterns&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/03-agentic-design-patterns/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/m9lM8qqoOEA?si=BIzHwzstTPL8o9GF&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Tool Use Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/04-tool-use/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/vieRiPRx-gI?si=2z6O2Xu2cu_Jz46N&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Agentic RAG&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/05-agentic-rag/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/WcjAARvdL7I?si=gKPWsQpKiIlDH9A3&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Trustworthy AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/06-building-trustworthy-agents/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/iZKkMEGBCUQ?si=jZjpiMnGFOE9L8OK&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Planning Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/07-planning-design/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/kPfJ2BrBCMY?si=6SC_iv_E5-mzucnC&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi-Agent Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/08-multi-agent/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/V6HpE9hZEx0?si=rMgDhEu7wXo2uo6g&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Metacognition Design Pattern&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/09-metacognition/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/His9R6gw6Ec?si=8gck6vvdSNCt6OcF&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;AI Agents in Production&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/10-ai-agents-production/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/l4TP6IyJxmQ?si=31dnhexRo6yLRJDl&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Using Agentic Protocols (MCP, A2A and NLWeb)&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/11-agentic-protocols/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/X-Dh9R3Opn8&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Context Engineering for AI Agents&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/12-context-engineering/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://youtu.be/F5zqRV7gEag&quot;&gt;Video&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://aka.ms/ai-agents-beginners/collection?WT.mc_id=academic-105485-koreyst&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Managing Agentic Memory&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/ai-agents-for-beginners/main/13-agent-memory/README.md&quot;&gt;Link&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Evaluating AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 18th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Building Computer Use Agents (CUA)&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Deploying Scalable Agents&lt;/td&gt; 
   &lt;td&gt;Coming - September 25th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Creating Local AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 3rd&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Securing AI Agents&lt;/td&gt; 
   &lt;td&gt;Coming - October 10th&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🎒 Other Courses&lt;/h2&gt; 
&lt;p&gt;Our team produces other courses! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;&lt;strong&gt;NEW&lt;/strong&gt; Model Context Protocol (MCP) For Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst&quot;&gt;Generative AI for Beginners using Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101??WT.mc_id=academic-96948-sayoung&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for AI Paired Programming&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🌟 Community Thanks&lt;/h2&gt; 
&lt;p&gt;Thanks to &lt;a href=&quot;https://www.linkedin.com/in/shivam2003/&quot;&gt;Shivam Goyal&lt;/a&gt; for contributing important code samples demonstrating Agentic RAG.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&quot;https://cla.opensource.microsoft.com&quot;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; 
&lt;p&gt;This project has adopted the &lt;a href=&quot;https://opensource.microsoft.com/codeofconduct/&quot;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&quot;https://opensource.microsoft.com/codeofconduct/faq/&quot;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&quot;mailto:opencode@microsoft.com&quot;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; 
&lt;h2&gt;Trademarks&lt;/h2&gt; 
&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&quot;https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general&quot;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos is subject to those third-parties&#39; policies.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ageron/handson-ml3</title>
      <link>https://github.com/ageron/handson-ml3</link>
      <description>&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks, 3rd edition&lt;/h1&gt; 
&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the third edition of my O&#39;Reilly book &lt;a href=&quot;https://homl.info/er3&quot;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow (3rd edition)&lt;/a&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://homl.info/er3&quot;&gt;&lt;img src=&quot;https://learning.oreilly.com/library/cover/9781098125967/300w/&quot; title=&quot;book&quot; width=&quot;150&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the second edition notebooks, check out &lt;a href=&quot;https://github.com/ageron/handson-ml2&quot;&gt;ageron/handson-ml2&lt;/a&gt;. For the first edition, see &lt;a href=&quot;https://github.com/ageron/handson-ml&quot;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/ageron/handson-ml3/blob/main/&quot; target=&quot;_parent&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Open In Colab&quot; /&gt;&lt;/a&gt; (recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;⚠ &lt;em&gt;Colab provides a temporary environment: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; 
&lt;details&gt; 
 &lt;p&gt;Other services may work as well, but I have not fully tested them:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://homl.info/kaggle3/&quot;&gt;&lt;img src=&quot;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&quot; alt=&quot;Open in Kaggle&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://mybinder.org/v2/gh/ageron/handson-ml3/HEAD?filepath=%2Findex.ipynb&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg?sanitize=true&quot; alt=&quot;Launch binder&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://homl.info/deepnote3/&quot;&gt;&lt;img src=&quot;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&quot; alt=&quot;Launch in Deepnote&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/ageron/handson-ml3/blob/main/index.ipynb&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&quot; alt=&quot;Render nbviewer&quot; /&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/ageron/handson-ml3/raw/main/index.ipynb&quot;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; 
&lt;p&gt;Read the &lt;a href=&quot;https://github.com/ageron/handson-ml3/tree/main/docker&quot;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; 
&lt;p&gt;Start by installing &lt;a href=&quot;https://www.anaconda.com/products/distribution&quot;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&quot;https://docs.conda.io/en/latest/miniconda.html&quot;&gt;Miniconda&lt;/a&gt;), &lt;a href=&quot;https://git-scm.com/downloads&quot;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&quot;https://www.nvidia.com/Download/index.aspx&quot;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; 
&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml3.git
$ cd handson-ml3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Next, run the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml
$ conda activate homl3
$ python -m ipykernel install --user --name=python3
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you need further instructions, read the &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&quot;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;FAQ&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;I recommend Python 3.10. If you follow the installation instructions above, that&#39;s the version you will get. Any version ≥3.7 should work as well.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration. If it&#39;s an SSL error, see the next question.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&quot;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&quot;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.10/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.10&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&quot;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ageron/handson-ml3/main/INSTALL.md&quot;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;I would like to thank everyone &lt;a href=&quot;https://github.com/ageron/handson-ml3/graphs/contributors&quot;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions. Thanks a lot to Victor Khaustov who submitted plenty of excellent PRs, fixing many errors. And lastly, thanks to Google ML Developer Programs team who supported this work by providing Google Cloud Credit.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
