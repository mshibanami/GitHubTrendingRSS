<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Jupyter Notebook Weekly Trending</title>
    <description>Weekly Trending of Jupyter Notebook in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:44:00 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>pyannote/pyannote-audio</title>
      <link>https://github.com/pyannote/pyannote-audio</link>
      <description>&lt;p&gt;Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding&lt;/p&gt;&lt;hr&gt;&lt;p&gt;Using &lt;code&gt;pyannote.audio&lt;/code&gt; open-source toolkit in production?&lt;br /&gt; Consider switching to &lt;a href=&quot;https://www.pyannote.ai&quot;&gt;pyannoteAI&lt;/a&gt; for better and faster options.&lt;/p&gt; 
&lt;h1&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; speaker diarization toolkit&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;pyannote.audio&lt;/code&gt; is an open-source toolkit written in Python for speaker diarization. Based on &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/pytorch.org&quot;&gt;PyTorch&lt;/a&gt; machine learning framework, it comes with state-of-the-art &lt;a href=&quot;https://hf.co/pyannote&quot;&gt;pretrained models and pipelines&lt;/a&gt;, that can be further finetuned to your own data for even better performance.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=37R_R82lfwA&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/37R_R82lfwA/0.jpg&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;TL;DR&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://github.com/pyannote/pyannote-audio&quot;&gt;&lt;code&gt;pyannote.audio&lt;/code&gt;&lt;/a&gt; with &lt;code&gt;pip install pyannote.audio&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Accept &lt;a href=&quot;https://hf.co/pyannote/segmentation-3.0&quot;&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/a&gt; user conditions&lt;/li&gt; 
 &lt;li&gt;Accept &lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;&lt;code&gt;pyannote/speaker-diarization-3.1&lt;/code&gt;&lt;/a&gt; user conditions&lt;/li&gt; 
 &lt;li&gt;Create access token at &lt;a href=&quot;https://hf.co/settings/tokens&quot;&gt;&lt;code&gt;hf.co/settings/tokens&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from pyannote.audio import Pipeline
pipeline = Pipeline.from_pretrained(
    &quot;pyannote/speaker-diarization-3.1&quot;,
    use_auth_token=&quot;HUGGINGFACE_ACCESS_TOKEN_GOES_HERE&quot;)

# send pipeline to GPU (when available)
import torch
pipeline.to(torch.device(&quot;cuda&quot;))

# apply pretrained pipeline
diarization = pipeline(&quot;audio.wav&quot;)

# print the result
for turn, _, speaker in diarization.itertracks(yield_label=True):
    print(f&quot;start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}&quot;)
# start=0.2s stop=1.5s speaker_0
# start=1.8s stop=3.9s speaker_1
# start=4.2s stop=5.7s speaker_0
# ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Highlights&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;span&gt;ü§ó&lt;/span&gt; pretrained &lt;a href=&quot;https://hf.co/models?other=pyannote-audio-pipeline&quot;&gt;pipelines&lt;/a&gt; (and &lt;a href=&quot;https://hf.co/models?other=pyannote-audio-model&quot;&gt;models&lt;/a&gt;) on &lt;a href=&quot;https://huggingface.co/pyannote&quot;&gt;&lt;span&gt;ü§ó&lt;/span&gt; model hub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;ü§Ø&lt;/span&gt; state-of-the-art performance (see &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/#benchmark&quot;&gt;Benchmark&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;üêç&lt;/span&gt; Python-first API&lt;/li&gt; 
 &lt;li&gt;&lt;span&gt;‚ö°&lt;/span&gt; multi-GPU training with &lt;a href=&quot;https://pytorchlightning.ai/&quot;&gt;pytorch-lightning&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/CHANGELOG.md&quot;&gt;Changelog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/FAQ.md&quot;&gt;Frequently asked questions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Models 
  &lt;ul&gt; 
   &lt;li&gt;Available tasks explained&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/applying_a_model.ipynb&quot;&gt;Applying a pretrained model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/training_a_model.ipynb&quot;&gt;Training, fine-tuning, and transfer learning&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Pipelines 
  &lt;ul&gt; 
   &lt;li&gt;Available pipelines explained&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/applying_a_pipeline.ipynb&quot;&gt;Applying a pretrained pipeline&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/adapting_pretrained_pipeline.ipynb&quot;&gt;Adapting a pretrained pipeline to your own data&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/voice_activity_detection.ipynb&quot;&gt;Training a pipeline&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Contributing 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/add_your_own_model.ipynb&quot;&gt;Adding a new model&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/add_your_own_task.ipynb&quot;&gt;Adding a new task&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Adding a new pipeline&lt;/li&gt; 
   &lt;li&gt;Sharing pretrained models and pipelines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Blog 
  &lt;ul&gt; 
   &lt;li&gt;2022-12-02 &amp;gt; &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/adapting_pretrained_pipeline.ipynb&quot;&gt;&quot;How I reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022 speaker diarization challenges&quot;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2022-10-23 &amp;gt; &lt;a href=&quot;https://herve.niderb.fr/fastpages/2022/10/23/One-speaker-segmentation-model-to-rule-them-all&quot;&gt;&quot;One speaker segmentation model to rule them all&quot;&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;2021-08-05 &amp;gt; &lt;a href=&quot;https://herve.niderb.fr/fastpages/2021/08/05/Streaming-voice-activity-detection-with-pyannote.html&quot;&gt;&quot;Streaming voice activity detection with pyannote.audio&quot;&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Videos 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://umotion.univ-lemans.fr/video/9513-speech-segmentation-and-speaker-diarization/&quot;&gt;Introduction to speaker diarization&lt;/a&gt; / JSALT 2023 summer school / 90 min&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wDH2rvkjymY&quot;&gt;Speaker segmentation model&lt;/a&gt; / Interspeech 2021 / 3 min&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=37R_R82lfwA&quot;&gt;First release of pyannote.audio&lt;/a&gt; / ICASSP 2020 / 8 min&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Community contributions (not maintained by the core team) 
  &lt;ul&gt; 
   &lt;li&gt;2024-04-05 &amp;gt; &lt;a href=&quot;https://raw.githubusercontent.com/pyannote/pyannote-audio/main/tutorials/community/offline_usage_speaker_diarization.ipynb&quot;&gt;Offline speaker diarization (speaker-diarization-3.1)&lt;/a&gt; by &lt;a href=&quot;https://github.com/simonottenhauskenbun&quot;&gt;Simon Ottenhaus&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark&lt;/h2&gt; 
&lt;p&gt;Out of the box, &lt;code&gt;pyannote.audio&lt;/code&gt; speaker diarization &lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;pipeline&lt;/a&gt; v3.1 is expected to be much better (and faster) than v2.x. Those numbers are diarization error rates (in %):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-2.1&quot;&gt;v2.1&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://hf.co/pyannote/speaker-diarization-3.1&quot;&gt;v3.1&lt;/a&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;a href=&quot;https://www.pyannote.ai&quot;&gt;pyannoteAI&lt;/a&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.03603&quot;&gt;AISHELL-4&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;14.1&lt;/td&gt; 
   &lt;td&gt;12.2&lt;/td&gt; 
   &lt;td&gt;11.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.openslr.org/119/&quot;&gt;AliMeeting&lt;/a&gt; (channel 1)&lt;/td&gt; 
   &lt;td&gt;27.4&lt;/td&gt; 
   &lt;td&gt;24.4&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;AMI&lt;/a&gt; (IHM)&lt;/td&gt; 
   &lt;td&gt;18.9&lt;/td&gt; 
   &lt;td&gt;18.8&lt;/td&gt; 
   &lt;td&gt;16.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://groups.inf.ed.ac.uk/ami/corpus/&quot;&gt;AMI&lt;/a&gt; (SDM)&lt;/td&gt; 
   &lt;td&gt;27.1&lt;/td&gt; 
   &lt;td&gt;22.4&lt;/td&gt; 
   &lt;td&gt;20.9&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.14448&quot;&gt;AVA-AVD&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;66.3&lt;/td&gt; 
   &lt;td&gt;50.0&lt;/td&gt; 
   &lt;td&gt;39.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2001S97&quot;&gt;CALLHOME&lt;/a&gt; (&lt;a href=&quot;https://github.com/BUTSpeechFIT/CALLHOME_sublists/issues/1&quot;&gt;part 2&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;31.6&lt;/td&gt; 
   &lt;td&gt;28.4&lt;/td&gt; 
   &lt;td&gt;22.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC2022S14&quot;&gt;DIHARD 3&lt;/a&gt; (&lt;a href=&quot;https://arxiv.org/abs/2012.01477&quot;&gt;full&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;26.9&lt;/td&gt; 
   &lt;td&gt;21.7&lt;/td&gt; 
   &lt;td&gt;17.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/revdotcom/speech-datasets&quot;&gt;Earnings21&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;17.0&lt;/td&gt; 
   &lt;td&gt;9.4&lt;/td&gt; 
   &lt;td&gt;9.0&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.07058&quot;&gt;Ego4D&lt;/a&gt; (dev.)&lt;/td&gt; 
   &lt;td&gt;61.5&lt;/td&gt; 
   &lt;td&gt;51.2&lt;/td&gt; 
   &lt;td&gt;43.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/X-LANCE/MSDWILD&quot;&gt;MSDWild&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;32.8&lt;/td&gt; 
   &lt;td&gt;25.3&lt;/td&gt; 
   &lt;td&gt;19.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.openslr.org/123/&quot;&gt;RAMC&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;22.5&lt;/td&gt; 
   &lt;td&gt;22.2&lt;/td&gt; 
   &lt;td&gt;18.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.islrn.org/resources/360-758-359-485-0/&quot;&gt;REPERE&lt;/a&gt; (phase2)&lt;/td&gt; 
   &lt;td&gt;8.2&lt;/td&gt; 
   &lt;td&gt;7.8&lt;/td&gt; 
   &lt;td&gt;7.6&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/joonson/voxconverse&quot;&gt;VoxConverse&lt;/a&gt; (v0.3)&lt;/td&gt; 
   &lt;td&gt;11.2&lt;/td&gt; 
   &lt;td&gt;11.3&lt;/td&gt; 
   &lt;td&gt;9.4&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;&lt;a href=&quot;http://pyannote.github.io/pyannote-metrics/reference.html#diarization&quot;&gt;Diarization error rate&lt;/a&gt; (in %)&lt;/p&gt; 
&lt;h2&gt;Citations&lt;/h2&gt; 
&lt;p&gt;If you use &lt;code&gt;pyannote.audio&lt;/code&gt; please use the following citations:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Plaquet23,
  author={Alexis Plaquet and Herv√© Bredin},
  title={{Powerset multi-class cross entropy loss for neural speaker diarization}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inproceedings{Bredin23,
  author={Herv√© Bredin},
  title={{pyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development&lt;/h2&gt; 
&lt;p&gt;The commands below will setup pre-commit hooks and packages needed for developing the &lt;code&gt;pyannote.audio&lt;/code&gt; library.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e .[dev,testing]
pre-commit install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Test&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pytest
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>microsoft/AI-For-Beginners</title>
      <link>https://github.com/microsoft/AI-For-Beginners</link>
      <description>&lt;p&gt;12 Weeks, 24 Lessons, AI for All!&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/issues/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/pulls/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg?sanitize=true&quot; alt=&quot;GitHub pull-requests&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;http://makeapullrequest.com&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&quot; alt=&quot;PRs Welcome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/watchers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Watch&quot; alt=&quot;GitHub watchers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/network/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Fork&quot; alt=&quot;GitHub forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://GitHub.com/microsoft/AI-For-Beginners/stargazers/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&amp;amp;label=Star&quot; alt=&quot;GitHub stars&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD&quot;&gt;&lt;img src=&quot;https://mybinder.org/badge_logo.svg?sanitize=true&quot; alt=&quot;Binder&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/Microsoft/ai-for-beginners.svg?sanitize=true&quot; alt=&quot;Gitter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/zxKYvhSnVp?WT.mc_id=academic-000002-leestott&quot;&gt;&lt;img src=&quot;https://dcbadge.vercel.app/api/server/ByRwuEEgH4&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;Artificial Intelligence for Beginners - A Curriculum&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/sketchnotes/ai-overview.png&quot; alt=&quot;Sketchnote by @girlie_mac https://twitter.com/girlie_mac&quot; /&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;AI For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&quot;https://twitter.com/girlie_mac&quot;&gt;@girlie_mac&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Explore the world of &lt;strong&gt;Artificial Intelligence&lt;/strong&gt; (AI) with our 12-week, 24-lesson curriculum! It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI&lt;/p&gt; 
&lt;h3&gt;üåê Multi-Language Support&lt;/h3&gt; 
&lt;h4&gt;Supported via GitHub Action (Automated &amp;amp; Always Up-to-Date)&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/fr/README.md&quot;&gt;French&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/es/README.md&quot;&gt;Spanish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/de/README.md&quot;&gt;German&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ru/README.md&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ar/README.md&quot;&gt;Arabic&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/fa/README.md&quot;&gt;Persian (Farsi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ur/README.md&quot;&gt;Urdu&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/zh/README.md&quot;&gt;Chinese (Simplified)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/mo/README.md&quot;&gt;Chinese (Traditional, Macau)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/hk/README.md&quot;&gt;Chinese (Traditional, Hong Kong)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/tw/README.md&quot;&gt;Chinese (Traditional, Taiwan)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ja/README.md&quot;&gt;Japanese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ko/README.md&quot;&gt;Korean&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/hi/README.md&quot;&gt;Hindi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/bn/README.md&quot;&gt;Bengali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/mr/README.md&quot;&gt;Marathi&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ne/README.md&quot;&gt;Nepali&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/pa/README.md&quot;&gt;Punjabi (Gurmukhi)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/pt/README.md&quot;&gt;Portuguese (Portugal)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/br/README.md&quot;&gt;Portuguese (Brazil)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/it/README.md&quot;&gt;Italian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/pl/README.md&quot;&gt;Polish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/tr/README.md&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/el/README.md&quot;&gt;Greek&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/th/README.md&quot;&gt;Thai&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/sv/README.md&quot;&gt;Swedish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/da/README.md&quot;&gt;Danish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/no/README.md&quot;&gt;Norwegian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/fi/README.md&quot;&gt;Finnish&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/nl/README.md&quot;&gt;Dutch&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/he/README.md&quot;&gt;Hebrew&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/vi/README.md&quot;&gt;Vietnamese&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/id/README.md&quot;&gt;Indonesian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ms/README.md&quot;&gt;Malay&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/tl/README.md&quot;&gt;Tagalog (Filipino)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/sw/README.md&quot;&gt;Swahili&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/hu/README.md&quot;&gt;Hungarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/cs/README.md&quot;&gt;Czech&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/sk/README.md&quot;&gt;Slovak&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/ro/README.md&quot;&gt;Romanian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/bg/README.md&quot;&gt;Bulgarian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/sr/README.md&quot;&gt;Serbian (Cyrillic)&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/hr/README.md&quot;&gt;Croatian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/sl/README.md&quot;&gt;Slovenian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/uk/README.md&quot;&gt;Ukrainian&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/translations/my/README.md&quot;&gt;Burmese (Myanmar)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;If you wish to have additional translations languages supported are listed &lt;a href=&quot;https://github.com/Azure/co-op-translator/raw/main/getting_started/supported-languages.md&quot;&gt;here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Join the Community&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/kzRShWzttr&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/kzRShWzttr&quot; alt=&quot;Azure AI Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What you will learn&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://soshnikov.com/courses/ai-for-beginners/mindmap.html&quot;&gt;Mindmap of the Course&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;In this curriculum, you will learn:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Different approaches to Artificial Intelligence, including the &quot;good old&quot; symbolic approach with &lt;strong&gt;Knowledge Representation&lt;/strong&gt; and reasoning (&lt;a href=&quot;https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence&quot;&gt;GOFAI&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Networks&lt;/strong&gt; and &lt;strong&gt;Deep Learning&lt;/strong&gt;, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - &lt;a href=&quot;http://Tensorflow.org&quot;&gt;TensorFlow&lt;/a&gt; and &lt;a href=&quot;http://pytorch.org&quot;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Architectures&lt;/strong&gt; for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.&lt;/li&gt; 
 &lt;li&gt;Less popular AI approaches, such as &lt;strong&gt;Genetic Algorithms&lt;/strong&gt; and &lt;strong&gt;Multi-Agent Systems&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;What we will not cover in this curriculum:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Find all additional resources for this course in our Microsoft Learn collection&lt;/a&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;ul&gt; 
 &lt;li&gt;Business cases of using &lt;strong&gt;AI in Business&lt;/strong&gt;. Consider taking &lt;a href=&quot;https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Introduction to AI for business users&lt;/a&gt; learning path on Microsoft Learn, or &lt;a href=&quot;https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;AI Business School&lt;/a&gt;, developed in cooperation with &lt;a href=&quot;https://www.insead.edu/&quot;&gt;INSEAD&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Classic Machine Learning&lt;/strong&gt;, which is well described in our &lt;a href=&quot;http://github.com/Microsoft/ML-for-Beginners&quot;&gt;Machine Learning for Beginners Curriculum&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Practical AI applications built using &lt;strong&gt;&lt;a href=&quot;https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Cognitive Services&lt;/a&gt;&lt;/strong&gt;. For this, we recommend that you start with modules Microsoft Learn for &lt;a href=&quot;https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;vision&lt;/a&gt;, &lt;a href=&quot;https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;natural language processing&lt;/a&gt;, &lt;strong&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Generative AI with Azure OpenAI Service&lt;/a&gt;&lt;/strong&gt; and others.&lt;/li&gt; 
 &lt;li&gt;Specific ML &lt;strong&gt;Cloud Frameworks&lt;/strong&gt;, such as &lt;a href=&quot;https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Azure Machine Learning&lt;/a&gt;, &lt;a href=&quot;https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Microsoft Fabric&lt;/a&gt;, or &lt;a href=&quot;https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Azure Databricks&lt;/a&gt;. Consider using &lt;a href=&quot;https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Build and operate machine learning solutions with Azure Machine Learning&lt;/a&gt; and &lt;a href=&quot;https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Build and Operate Machine Learning Solutions with Azure Databricks&lt;/a&gt; learning paths.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conversational AI&lt;/strong&gt; and &lt;strong&gt;Chat Bots&lt;/strong&gt;. There is a separate &lt;a href=&quot;https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Create conversational AI solutions&lt;/a&gt; learning path, and you can also refer to &lt;a href=&quot;https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/&quot;&gt;this blog post&lt;/a&gt; for more detail.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Mathematics&lt;/strong&gt; behind deep learning. For this, we would recommend &lt;a href=&quot;https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618&quot;&gt;Deep Learning&lt;/a&gt; by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;https://www.deeplearningbook.org/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For a gentle introduction to &lt;em&gt;AI in the Cloud&lt;/em&gt; topics you may consider taking the &lt;a href=&quot;https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Get started with artificial intelligence on Azure&lt;/a&gt; Learning Path.&lt;/p&gt; 
&lt;h1&gt;Content&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Lesson Link&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;PyTorch/Keras/TensorFlow&lt;/th&gt; 
   &lt;th&gt;Lab&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/setup.md&quot;&gt;Course Setup&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/how-to-run.md&quot;&gt;Setup Your Development Environment&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;I&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&quot;&gt;&lt;strong&gt;Introduction to AI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;01&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/1-Intro/README.md&quot;&gt;Introduction and History of AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;-&lt;/td&gt; 
   &lt;td&gt;-&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;II&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Symbolic AI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;02&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/README.md&quot;&gt;Knowledge Representation and Expert Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/Animals.ipynb&quot;&gt;Expert Systems&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/FamilyOntology.ipynb&quot;&gt;Ontology&lt;/a&gt; /&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/2-Symbolic/MSConceptGraph.ipynb&quot;&gt;Concept Graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;III&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/README.md&quot;&gt;&lt;strong&gt;Introduction to Neural Networks&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;03&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/README.md&quot;&gt;Perceptron&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/03-Perceptron/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;04&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/README.md&quot;&gt;Multi-Layered Perceptron and Creating our own Framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;05&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/README.md&quot;&gt;Intro to Frameworks (PyTorch/TensorFlow) and Overfitting&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb&quot;&gt;Keras&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;IV&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/README.md&quot;&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-77998-cacaste&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-77998-cacaste&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Explore Computer Vision on Microsoft Azure&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;06&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/README.md&quot;&gt;Intro to Computer Vision. OpenCV&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/06-IntroCV/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;07&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/README.md&quot;&gt;Convolutional Neural Networks&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md&quot;&gt;CNN Architectures&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/07-ConvNets/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;08&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/README.md&quot;&gt;Pre-trained Networks and Transfer Learning&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md&quot;&gt;Training Tricks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/08-TransferLearning/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;09&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/README.md&quot;&gt;Autoencoders and VAEs&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;10&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/README.md&quot;&gt;Generative Adversarial Networks &amp;amp; Artistic Style Transfer&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/10-GANs/GANTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;11&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/README.md&quot;&gt;Object Detection&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/11-ObjectDetection/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;12&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/README.md&quot;&gt;Semantic Segmentation. U-Net&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;V&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/README.md&quot;&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-77998-cacaste&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;Explore Natural Language Processing on Microsoft Azure&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;13&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/13-TextRep/README.md&quot;&gt;Text Representation. Bow/TF-IDF&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;14&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/14-Embeddings/README.md&quot;&gt;Semantic word embeddings. Word2Vec and GloVe&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;15&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/README.md&quot;&gt;Language Modeling. Training your own embeddings&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/15-LanguageModeling/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;16&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/16-RNN/README.md&quot;&gt;Recurrent Neural Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/16-RNN/RNNTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;17&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/README.md&quot;&gt;Generative Recurrent Networks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; / &lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/17-GenerativeNetworks/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;18&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/18-Transformers/README.md&quot;&gt;Transformers. BERT.&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://github.com/microsoft/AI-For-Beginners/raw/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;19&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/README.md&quot;&gt;Named Entity Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/19-NER/NER-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/19-NER/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;20&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/5-NLP/20-LangModels/README.md&quot;&gt;Large Language Models, Prompt Programming and Few-Shot Tasks&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;VI&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Other AI Techniques&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;21&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/README.md&quot;&gt;Genetic Algorithms&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;22&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/README.md&quot;&gt;Deep Reinforcement Learning&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb&quot;&gt;PyTorch&lt;/a&gt; /&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb&quot;&gt;TensorFlow&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/22-DeepRL/lab/README.md&quot;&gt;Lab&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;23&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/6-Other/23-MultiagentSystems/README.md&quot;&gt;Multi-Agent Systems&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;VII&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;AI Ethics&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;24&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/7-Ethics/README.md&quot;&gt;AI Ethics and Responsible AI&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-77998-cacaste&quot;&gt;Microsoft Learn: Responsible AI Principles&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;IX&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;strong&gt;Extras&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;25&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/README.md&quot;&gt;Multi-Modal Networks, CLIP and VQGAN&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/X-Extras/X1-MultiModal/Clip.ipynb&quot;&gt;Notebook&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Each lesson contains&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pre-reading material&lt;/li&gt; 
 &lt;li&gt;Executable Jupyter Notebooks, which are often specific to the framework (&lt;strong&gt;PyTorch&lt;/strong&gt; or &lt;strong&gt;TensorFlow&lt;/strong&gt;). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Labs&lt;/strong&gt; available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.&lt;/li&gt; 
 &lt;li&gt;Some sections contain links to &lt;a href=&quot;https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum&quot;&gt;&lt;strong&gt;MS Learn&lt;/strong&gt;&lt;/a&gt; modules that cover related topics.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;We have created a &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/setup.md&quot;&gt;setup lesson&lt;/a&gt; to help you with setting up your development environment. - For Educators, we have created a &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/for-teachers.md&quot;&gt;curricula setup lesson&lt;/a&gt; for you too!&lt;/li&gt; 
 &lt;li&gt;How to &lt;a href=&quot;https://raw.githubusercontent.com/microsoft/AI-For-Beginners/main/lessons/0-course-setup/how-to-run.md&quot;&gt;Run the code in a VSCode or a Codepace&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Follow these steps:&lt;/p&gt; 
&lt;p&gt;Fork the Repository: Click on the &quot;Fork&quot; button at the top-right corner of this page.&lt;/p&gt; 
&lt;p&gt;Clone the Repository: &lt;code&gt;git clone https://github.com/microsoft/AI-For-Beginners.git&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Don&#39;t forget to star (üåü) this repo to find it easier later.&lt;/p&gt; 
&lt;h2&gt;Meet other Learners&lt;/h2&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://aka.ms/genai-discord?WT.mc_id=academic-105485-bethanycheum&quot;&gt;official AI Discord server&lt;/a&gt; to meet and network with other learners taking this course and get support.&lt;/p&gt; 
&lt;p&gt;If you have product feedback or questions whilst building visit our &lt;a href=&quot;https://aka.ms/foundry/forum&quot;&gt;Azure AI Foundry Developer Forum&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Quizzes&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained in the Quiz-app folder in etc\quiz-app, or &lt;a href=&quot;https://ff-quizzes.netlify.app/&quot;&gt;Online Here&lt;/a&gt; They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Help Wanted&lt;/h2&gt; 
&lt;p&gt;Do you have suggestions or found spelling or code errors? Raise an issue or create a pull request.&lt;/p&gt; 
&lt;h2&gt;Special Thanks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;‚úçÔ∏è Primary Author:&lt;/strong&gt; &lt;a href=&quot;http://soshnikov.com&quot;&gt;Dmitry Soshnikov&lt;/a&gt;, PhD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üî• Editor:&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/jenlooper&quot;&gt;Jen Looper&lt;/a&gt;, PhD&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üé® Sketchnote illustrator:&lt;/strong&gt; &lt;a href=&quot;https://twitter.com/girlie_mac&quot;&gt;Tomomi Imura&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;‚úÖ Quiz Creator:&lt;/strong&gt; &lt;a href=&quot;https://github.com/CinnamonXI&quot;&gt;Lateefah Bello&lt;/a&gt;, &lt;a href=&quot;https://studentambassadors.microsoft.com/&quot;&gt;MLSA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;üôè Core Contributors:&lt;/strong&gt; &lt;a href=&quot;https://github.com/Pe4enIks&quot;&gt;Evgenii Pishchik&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Other Curricula&lt;/h2&gt; 
&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/genai-beginners&quot;&gt;Generative AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-dotnet&quot;&gt;Generative AI for Beginners .NET&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/generative-ai-with-javascript&quot;&gt;Generative AI with JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Generative-AI-for-beginners-java&quot;&gt;Generative AI with Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ai-beginners&quot;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/datascience-beginners&quot;&gt;Data Science for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/ml-beginners&quot;&gt;ML for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Security-101&quot;&gt;Cybersecurity for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/webdev-beginners&quot;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://aka.ms/iot-beginners&quot;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/xr-development-for-beginners&quot;&gt;XR Development for Beginners&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming&quot;&gt;Mastering GitHub Copilot for Agentic use&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers&quot;&gt;Mastering GitHub Copilot for C#/.NET Developers&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/CopilotAdventures&quot;&gt;Choose Your Own Copilot Adventure&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>udlbook/udlbook</title>
      <link>https://github.com/udlbook/udlbook</link>
      <description>&lt;p&gt;Understanding Deep Learning - Simon J.D. Prince&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/llm-cookbook</title>
      <link>https://github.com/datawhalechina/llm-cookbook</link>
      <description>&lt;p&gt;Èù¢ÂêëÂºÄÂèëËÄÖÁöÑ LLM ÂÖ•Èó®ÊïôÁ®ãÔºåÂê¥ÊÅ©ËææÂ§ßÊ®°ÂûãÁ≥ªÂàóËØæÁ®ã‰∏≠ÊñáÁâà&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/readme.jpg&quot; alt=&quot;figures/readme.jpg&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Èù¢ÂêëÂºÄÂèëËÄÖÁöÑÂ§ßÊ®°ÂûãÊâãÂÜå - LLM Cookbook&lt;/h1&gt; 
&lt;h2&gt;È°πÁõÆÁÆÄ‰ªã&lt;/h2&gt; 
&lt;p&gt;Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™Èù¢ÂêëÂºÄÂèëËÄÖÁöÑÂ§ßÊ®°ÂûãÊâãÂÜåÔºåÈíàÂØπÂõΩÂÜÖÂºÄÂèëËÄÖÁöÑÂÆûÈôÖÈúÄÊ±ÇÔºå‰∏ªÊâì LLM ÂÖ®Êñπ‰ΩçÂÖ•Èó®ÂÆûË∑µ„ÄÇÊú¨È°πÁõÆÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏àÂ§ßÊ®°ÂûãÁ≥ªÂàóËØæÁ®ãÂÜÖÂÆπÔºåÂØπÂéüËØæÁ®ãÂÜÖÂÆπËøõË°åÁ≠õÈÄâ„ÄÅÁøªËØë„ÄÅÂ§çÁé∞ÂíåË∞É‰ºòÔºåË¶ÜÁõñ‰ªé Prompt Engineering Âà∞ RAG ÂºÄÂèë„ÄÅÊ®°ÂûãÂæÆË∞ÉÁöÑÂÖ®ÈÉ®ÊµÅÁ®ãÔºåÁî®ÊúÄÈÄÇÂêàÂõΩÂÜÖÂ≠¶‰π†ËÄÖÁöÑÊñπÂºèÔºåÊåáÂØºÂõΩÂÜÖÂºÄÂèëËÄÖÂ¶Ç‰ΩïÂ≠¶‰π†„ÄÅÂÖ•Èó® LLM Áõ∏ÂÖ≥È°πÁõÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÈíàÂØπ‰∏çÂêåÂÜÖÂÆπÁöÑÁâπÁÇπÔºåÊàë‰ª¨ÂØπÂÖ±ËÆ° 11 Èó®Âê¥ÊÅ©ËææËÄÅÂ∏àÁöÑÂ§ßÊ®°ÂûãËØæÁ®ãËøõË°å‰∫ÜÁøªËØëÂ§çÁé∞ÔºåÂπ∂ÁªìÂêàÂõΩÂÜÖÂ≠¶‰π†ËÄÖÁöÑÂÆûÈôÖÊÉÖÂÜµÔºåÂØπ‰∏çÂêåËØæÁ®ãËøõË°å‰∫ÜÂàÜÁ∫ßÂíåÊéíÂ∫èÔºåÂàùÂ≠¶ËÄÖÂèØ‰ª•ÂÖàÁ≥ªÁªüÂ≠¶‰π†Êàë‰ª¨ÁöÑÂøÖ‰øÆÁ±ªËØæÁ®ãÔºåÊéåÊè°ÂÖ•Èó® LLM ÊâÄÊúâÊñπÂêëÈÉΩÈúÄË¶ÅÊéåÊè°ÁöÑÂü∫Á°ÄÊäÄËÉΩÂíåÊ¶ÇÂøµÔºåÂÜçÈÄâÊã©ÊÄßÂú∞Â≠¶‰π†Êàë‰ª¨ÁöÑÈÄâ‰øÆÁ±ªËØæÁ®ãÔºåÂú®Ëá™Â∑±ÊÑüÂÖ¥Ë∂£ÁöÑÊñπÂêë‰∏ä‰∏çÊñ≠Êé¢Á¥¢ÂíåÂ≠¶‰π†„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûúÊúâ‰Ω†ÈùûÂ∏∏ÂñúÊ¨¢‰ΩÜÊàë‰ª¨ËøòÊ≤°ÊúâËøõË°åÂ§çÁé∞ÁöÑÂê¥ÊÅ©ËææËÄÅÂ∏àÂ§ßÊ®°ÂûãËØæÁ®ãÔºåÊàë‰ª¨Ê¨¢ËøéÊØè‰∏Ä‰ΩçÂºÄÂèëËÄÖÂèÇËÄÉÊàë‰ª¨Â∑≤ÊúâËØæÁ®ãÁöÑÊ†ºÂºèÂíåÂÜôÊ≥ïÊù•ÂØπËØæÁ®ãËøõË°åÂ§çÁé∞Âπ∂Êèê‰∫§ PRÔºåÂú® PR ÂÆ°Ê†∏ÈÄöËøáÂêéÔºåÊàë‰ª¨‰ºöÊ†πÊçÆËØæÁ®ãÂÜÖÂÆπÂ∞ÜËØæÁ®ãËøõË°åÂàÜÁ∫ßÂêàÂπ∂„ÄÇÊ¨¢ËøéÊØè‰∏Ä‰ΩçÂºÄÂèëËÄÖÁöÑË¥°ÁåÆÔºÅ&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Âú®Á∫øÈòÖËØªÂú∞ÂùÄÔºö&lt;a href=&quot;https://datawhalechina.github.io/llm-cookbook/&quot;&gt;Èù¢ÂêëÂºÄÂèëËÄÖÁöÑ LLM ÂÖ•Èó®ËØæÁ®ã - Âú®Á∫øÈòÖËØª&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;PDF ‰∏ãËΩΩÂú∞ÂùÄÔºö&lt;a href=&quot;https://github.com/datawhalechina/llm-cookbook/releases/tag/v1%2C0%2C0&quot;&gt;Èù¢ÂêëÂºÄÂèëËÄÖÁöÑ LLM ÂÖ•Èó®ÊïôÁ®ã - PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Ëã±ÊñáÂéüÁâàÂú∞ÂùÄÔºö&lt;a href=&quot;https://learn.deeplearning.ai&quot;&gt;Âê¥ÊÅ©ËææÂÖ≥‰∫éÂ§ßÊ®°ÂûãÁöÑÁ≥ªÂàóËØæÁ®ã&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;È°πÁõÆÊÑè‰πâ&lt;/h2&gt; 
&lt;p&gt;LLM Ê≠£Âú®ÈÄêÊ≠•ÊîπÂèò‰∫∫‰ª¨ÁöÑÁîüÊ¥ªÔºåËÄåÂØπ‰∫éÂºÄÂèëËÄÖÔºåÂ¶Ç‰ΩïÂü∫‰∫é LLM Êèê‰æõÁöÑ API Âø´ÈÄü„ÄÅ‰æøÊç∑Âú∞ÂºÄÂèë‰∏Ä‰∫õÂÖ∑Â§áÊõ¥Âº∫ËÉΩÂäõ„ÄÅÈõÜÊàê LLM ÁöÑÂ∫îÁî®ÔºåÊù•‰æøÊç∑Âú∞ÂÆûÁé∞‰∏Ä‰∫õÊõ¥Êñ∞È¢ñ„ÄÅÊõ¥ÂÆûÁî®ÁöÑËÉΩÂäõÔºåÊòØ‰∏Ä‰∏™ÊÄ•ÈúÄÂ≠¶‰π†ÁöÑÈáçË¶ÅËÉΩÂäõ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Áî±Âê¥ÊÅ©ËææËÄÅÂ∏à‰∏é OpenAI Âêà‰ΩúÊé®Âá∫ÁöÑÂ§ßÊ®°ÂûãÁ≥ªÂàóÊïôÁ®ãÔºå‰ªéÂ§ßÊ®°ÂûãÊó∂‰ª£ÂºÄÂèëËÄÖÁöÑÂü∫Á°ÄÊäÄËÉΩÂá∫ÂèëÔºåÊ∑±ÂÖ•ÊµÖÂá∫Âú∞‰ªãÁªç‰∫ÜÂ¶Ç‰ΩïÂü∫‰∫éÂ§ßÊ®°Âûã API„ÄÅLangChain Êû∂ÊûÑÂø´ÈÄüÂºÄÂèëÁªìÂêàÂ§ßÊ®°ÂûãÂº∫Â§ßËÉΩÂäõÁöÑÂ∫îÁî®„ÄÇÂÖ∂‰∏≠Ôºå„ÄäPrompt Engineering for Developers„ÄãÊïôÁ®ãÈù¢ÂêëÂÖ•Èó® LLM ÁöÑÂºÄÂèëËÄÖÔºåÊ∑±ÂÖ•ÊµÖÂá∫Âú∞‰ªãÁªç‰∫ÜÂØπ‰∫éÂºÄÂèëËÄÖÔºåÂ¶Ç‰ΩïÊûÑÈÄ† Prompt Âπ∂Âü∫‰∫é OpenAI Êèê‰æõÁöÑ API ÂÆûÁé∞ÂåÖÊã¨ÊÄªÁªì„ÄÅÊé®Êñ≠„ÄÅËΩ¨Êç¢Á≠âÂ§öÁßçÂ∏∏Áî®ÂäüËÉΩÔºåÊòØÂÖ•Èó® LLM ÂºÄÂèëÁöÑÁªèÂÖ∏ÊïôÁ®ãÔºõ„ÄäBuilding Systems with the ChatGPT API„ÄãÊïôÁ®ãÈù¢ÂêëÊÉ≥Ë¶ÅÂü∫‰∫é LLM ÂºÄÂèëÂ∫îÁî®Á®ãÂ∫èÁöÑÂºÄÂèëËÄÖÔºåÁÆÄÊ¥ÅÊúâÊïàËÄåÂèàÁ≥ªÁªüÂÖ®Èù¢Âú∞‰ªãÁªç‰∫ÜÂ¶Ç‰ΩïÂü∫‰∫é ChatGPT API ÊâìÈÄ†ÂÆåÊï¥ÁöÑÂØπËØùÁ≥ªÁªüÔºõ„ÄäLangChain for LLM Application Development„ÄãÊïôÁ®ãÁªìÂêàÁªèÂÖ∏Â§ßÊ®°ÂûãÂºÄÊ∫êÊ°ÜÊû∂ LangChainÔºå‰ªãÁªç‰∫ÜÂ¶Ç‰ΩïÂü∫‰∫é LangChain Ê°ÜÊû∂ÂºÄÂèëÂÖ∑Â§áÂÆûÁî®ÂäüËÉΩ„ÄÅËÉΩÂäõÂÖ®Èù¢ÁöÑÂ∫îÁî®Á®ãÂ∫èÔºå„ÄäLangChain Chat With Your Data„ÄãÊïôÁ®ãÂàôÂú®Ê≠§Âü∫Á°Ä‰∏äËøõ‰∏ÄÊ≠•‰ªãÁªç‰∫ÜÂ¶Ç‰Ωï‰ΩøÁî® LangChain Êû∂ÊûÑÁªìÂêà‰∏™‰∫∫ÁßÅÊúâÊï∞ÊçÆÂºÄÂèë‰∏™ÊÄßÂåñÂ§ßÊ®°ÂûãÂ∫îÁî®Ôºõ„ÄäBuilding Generative AI Applications with Gradio„Äã„ÄÅ„ÄäEvaluating and Debugging Generative AI„ÄãÊïôÁ®ãÂàÜÂà´‰ªãÁªç‰∫Ü‰∏§‰∏™ÂÆûÁî®Â∑•ÂÖ∑ Gradio ‰∏é W&amp;amp;BÔºåÊåáÂØºÂºÄÂèëËÄÖÂ¶Ç‰ΩïÁªìÂêàËøô‰∏§‰∏™Â∑•ÂÖ∑Êù•ÊâìÈÄ†„ÄÅËØÑ‰º∞ÁîüÊàêÂºè AI Â∫îÁî®„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰∏äËø∞ÊïôÁ®ãÈùûÂ∏∏ÈÄÇÁî®‰∫éÂºÄÂèëËÄÖÂ≠¶‰π†‰ª•ÂºÄÂêØÂü∫‰∫é LLM ÂÆûÈôÖÊê≠Âª∫Â∫îÁî®Á®ãÂ∫è‰πãË∑Ø„ÄÇÂõ†Ê≠§ÔºåÊàë‰ª¨Â∞ÜËØ•Á≥ªÂàóËØæÁ®ãÁøªËØë‰∏∫‰∏≠ÊñáÔºåÂπ∂Â§çÁé∞ÂÖ∂ËåÉ‰æã‰ª£Á†ÅÔºå‰πü‰∏∫ÂÖ∂‰∏≠‰∏Ä‰∏™ËßÜÈ¢ëÂ¢ûÂä†‰∫Ü‰∏≠ÊñáÂ≠óÂπïÔºåÊîØÊåÅÂõΩÂÜÖ‰∏≠ÊñáÂ≠¶‰π†ËÄÖÁõ¥Êé•‰ΩøÁî®Ôºå‰ª•Â∏ÆÂä©‰∏≠ÊñáÂ≠¶‰π†ËÄÖÊõ¥Â•ΩÂú∞Â≠¶‰π† LLM ÂºÄÂèëÔºõÊàë‰ª¨‰πüÂêåÊó∂ÂÆûÁé∞‰∫ÜÊïàÊûúÂ§ßËá¥Áõ∏ÂΩìÁöÑ‰∏≠Êñá PromptÔºåÊîØÊåÅÂ≠¶‰π†ËÄÖÊÑüÂèó‰∏≠ÊñáËØ≠Â¢É‰∏ã LLM ÁöÑÂ≠¶‰π†‰ΩøÁî®ÔºåÂØπÊØîÊéåÊè°Â§öËØ≠Ë®ÄËØ≠Â¢É‰∏ãÁöÑ Prompt ËÆæËÆ°‰∏é LLM ÂºÄÂèë„ÄÇÊú™Êù•ÔºåÊàë‰ª¨‰πüÂ∞ÜÂä†ÂÖ•Êõ¥Â§ö Prompt È´òÁ∫ßÊäÄÂ∑ßÔºå‰ª•‰∏∞ÂØåÊú¨ËØæÁ®ãÂÜÖÂÆπÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÊéåÊè°Êõ¥Â§ö„ÄÅÊõ¥Â∑ßÂ¶ôÁöÑ Prompt ÊäÄËÉΩ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;È°πÁõÆÂèó‰ºó&lt;/h2&gt; 
&lt;p&gt;ÊâÄÊúâÂÖ∑Â§áÂü∫Á°Ä Python ËÉΩÂäõÔºåÊÉ≥Ë¶ÅÂÖ•Èó® LLM ÁöÑÂºÄÂèëËÄÖ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;È°πÁõÆ‰∫ÆÁÇπ&lt;/h2&gt; 
&lt;p&gt;„ÄäChatGPT Prompt Engineering for Developers„Äã„ÄÅ„ÄäBuilding Systems with the ChatGPT API„ÄãÁ≠âÊïôÁ®ã‰Ωú‰∏∫Áî±Âê¥ÊÅ©ËææËÄÅÂ∏à‰∏é OpenAI ËÅîÂêàÊé®Âá∫ÁöÑÂÆòÊñπÊïôÁ®ãÔºåÂú®ÂèØÈ¢ÑËßÅÁöÑÊú™Êù•‰ºöÊàê‰∏∫ LLM ÁöÑÈáçË¶ÅÂÖ•Èó®ÊïôÁ®ãÔºå‰ΩÜÊòØÁõÆÂâçËøòÂè™ÊîØÊåÅËã±ÊñáÁâà‰∏îÂõΩÂÜÖËÆøÈóÆÂèóÈôêÔºåÊâìÈÄ†‰∏≠ÊñáÁâà‰∏îÂõΩÂÜÖÊµÅÁïÖËÆøÈóÆÁöÑÊïôÁ®ãÂÖ∑ÊúâÈáçË¶ÅÊÑè‰πâÔºõÂêåÊó∂ÔºåGPT ÂØπ‰∏≠Êñá„ÄÅËã±ÊñáÂÖ∑Êúâ‰∏çÂêåÁöÑÁêÜËß£ËÉΩÂäõÔºåÊú¨ÊïôÁ®ãÂú®Â§öÊ¨°ÂØπÊØî„ÄÅÂÆûÈ™å‰πãÂêéÁ°ÆÂÆö‰∫ÜÊïàÊûúÂ§ßËá¥Áõ∏ÂΩìÁöÑ‰∏≠Êñá PromptÔºåÊîØÊåÅÂ≠¶‰π†ËÄÖÁ†îÁ©∂Â¶Ç‰ΩïÊèêÂçá ChatGPT Âú®‰∏≠ÊñáËØ≠Â¢É‰∏ãÁöÑÁêÜËß£‰∏éÁîüÊàêËÉΩÂäõ„ÄÇ&lt;/p&gt; 
&lt;h2&gt;Â≠¶‰π†ÊåáÂçó&lt;/h2&gt; 
&lt;p&gt;Êú¨ÊïôÁ®ãÈÄÇÁî®‰∫éÊâÄÊúâÂÖ∑Â§áÂü∫Á°Ä Python ËÉΩÂäõÔºåÊÉ≥Ë¶ÅÂÖ•Èó® LLM ÁöÑÂºÄÂèëËÄÖ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅÂºÄÂßãÂ≠¶‰π†Êú¨ÊïôÁ®ãÔºå‰Ω†ÈúÄË¶ÅÊèêÂâçÂÖ∑Â§áÔºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ëá≥Â∞ë‰∏Ä‰∏™ LLM APIÔºàÊúÄÂ•ΩÊòØ OpenAIÔºåÂ¶ÇÊûúÊòØÂÖ∂‰ªñ APIÔºå‰Ω†ÂèØËÉΩÈúÄË¶ÅÂèÇËÄÉ &lt;a href=&quot;https://github.com/datawhalechina/llm-universe&quot;&gt;ÂÖ∂‰ªñÊïôÁ®ã&lt;/a&gt; ÂØπ API Ë∞ÉÁî®‰ª£Á†ÅËøõË°å‰øÆÊîπÔºâ&lt;/li&gt; 
 &lt;li&gt;ËÉΩÂ§ü‰ΩøÁî® Python Jupyter Notebook&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Êú¨ÊïôÁ®ãÂÖ±ÂåÖÊã¨ 11 Èó®ËØæÁ®ãÔºåÂàÜ‰∏∫ÂøÖ‰øÆÁ±ª„ÄÅÈÄâ‰øÆÁ±ª‰∏§‰∏™Á±ªÂà´„ÄÇÂøÖ‰øÆÁ±ªËØæÁ®ãÊòØÊàë‰ª¨ËÆ§‰∏∫ÊúÄÈÄÇÂêàÂàùÂ≠¶ËÄÖÂ≠¶‰π†‰ª•ÂÖ•Èó® LLM ÁöÑËØæÁ®ãÔºåÂåÖÊã¨‰∫ÜÂÖ•Èó® LLM ÊâÄÊúâÊñπÂêëÈÉΩÈúÄË¶ÅÊéåÊè°ÁöÑÂü∫Á°ÄÊäÄËÉΩÂíåÊ¶ÇÂøµÔºåÊàë‰ª¨‰πüÈíàÂØπÂøÖ‰øÆÁ±ªËØæÁ®ãÂà∂‰Ωú‰∫ÜÈÄÇÂêàÈòÖËØªÁöÑÂú®Á∫øÈòÖËØªÂíå PDF ÁâàÊú¨ÔºåÂú®Â≠¶‰π†ÂøÖ‰øÆÁ±ªËØæÁ®ãÊó∂ÔºåÊàë‰ª¨Âª∫ËÆÆÂ≠¶‰π†ËÄÖÊåâÁÖßÊàë‰ª¨ÂàóÂá∫ÁöÑÈ°∫Â∫èËøõË°åÂ≠¶‰π†ÔºõÈÄâ‰øÆÁ±ªËØæÁ®ãÊòØÂú®ÂøÖ‰øÆÁ±ªËØæÁ®ã‰∏äÁöÑÊãìÂ±ïÂª∂‰º∏ÔºåÂåÖÊã¨‰∫Ü RAG ÂºÄÂèë„ÄÅÊ®°ÂûãÂæÆË∞É„ÄÅÊ®°ÂûãËØÑ‰º∞Á≠âÂ§ö‰∏™ÊñπÈù¢ÔºåÈÄÇÂêàÂ≠¶‰π†ËÄÖÂú®ÊéåÊè°‰∫ÜÂøÖ‰øÆÁ±ªËØæÁ®ã‰πãÂêéÈÄâÊã©Ëá™Â∑±ÊÑüÂÖ¥Ë∂£ÁöÑÊñπÂêëÂíåËØæÁ®ãËøõË°åÂ≠¶‰π†„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÂøÖ‰øÆÁ±ªËØæÁ®ãÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Èù¢ÂêëÂºÄÂèëËÄÖÁöÑ Prompt Engineering„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäChatGPT Prompt Engineering for Developers„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÈù¢ÂêëÂÖ•Èó® LLM ÁöÑÂºÄÂèëËÄÖÔºåÊ∑±ÂÖ•ÊµÖÂá∫Âú∞‰ªãÁªç‰∫ÜÂØπ‰∫éÂºÄÂèëËÄÖÔºåÂ¶Ç‰ΩïÊûÑÈÄ† Prompt Âπ∂Âü∫‰∫é OpenAI Êèê‰æõÁöÑ API ÂÆûÁé∞ÂåÖÊã¨ÊÄªÁªì„ÄÅÊé®Êñ≠„ÄÅËΩ¨Êç¢Á≠âÂ§öÁßçÂ∏∏Áî®ÂäüËÉΩÔºåÊòØÂÖ•Èó® LLM ÂºÄÂèëÁöÑÁ¨¨‰∏ÄÊ≠•„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Êê≠Âª∫Âü∫‰∫é ChatGPT ÁöÑÈóÆÁ≠îÁ≥ªÁªü„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäBuilding Systems with the ChatGPT API„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÊåáÂØºÂºÄÂèëËÄÖÂ¶Ç‰ΩïÂü∫‰∫é ChatGPT Êèê‰æõÁöÑ API ÂºÄÂèë‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ„ÄÅÂÖ®Èù¢ÁöÑÊô∫ËÉΩÈóÆÁ≠îÁ≥ªÁªü„ÄÇÈÄöËøá‰ª£Á†ÅÂÆûË∑µÔºåÂÆûÁé∞‰∫ÜÂü∫‰∫é ChatGPT ÂºÄÂèëÈóÆÁ≠îÁ≥ªÁªüÁöÑÂÖ®ÊµÅÁ®ãÔºå‰ªãÁªç‰∫ÜÂü∫‰∫éÂ§ßÊ®°ÂûãÂºÄÂèëÁöÑÊñ∞ËåÉÂºèÔºåÊòØÂ§ßÊ®°ÂûãÂºÄÂèëÁöÑÂÆûË∑µÂü∫Á°Ä„ÄÇ&lt;/li&gt; 
 &lt;li&gt;‰ΩøÁî® LangChain ÂºÄÂèëÂ∫îÁî®Á®ãÂ∫è„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäLangChain for LLM Application Development„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÂØπ LangChain Â±ïÂºÄÊ∑±ÂÖ•‰ªãÁªçÔºåÂ∏ÆÂä©Â≠¶‰π†ËÄÖ‰∫ÜËß£Â¶Ç‰Ωï‰ΩøÁî® LangChainÔºåÂπ∂Âü∫‰∫é LangChain ÂºÄÂèëÂÆåÊï¥ÁöÑ„ÄÅÂÖ∑Â§áÂº∫Â§ßËÉΩÂäõÁöÑÂ∫îÁî®Á®ãÂ∫è„ÄÇ&lt;/li&gt; 
 &lt;li&gt;‰ΩøÁî® LangChain ËÆøÈóÆ‰∏™‰∫∫Êï∞ÊçÆ„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäLangChain Chat with Your Data„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÊ∑±ÂÖ•ÊãìÂ±ï LangChain Êèê‰æõÁöÑ‰∏™‰∫∫Êï∞ÊçÆËÆøÈóÆËÉΩÂäõÔºåÊåáÂØºÂºÄÂèëËÄÖÂ¶Ç‰Ωï‰ΩøÁî® LangChain ÂºÄÂèëËÉΩÂ§üËÆøÈóÆÁî®Êà∑‰∏™‰∫∫Êï∞ÊçÆ„ÄÅÊèê‰æõ‰∏™ÊÄßÂåñÊúçÂä°ÁöÑÂ§ßÊ®°ÂûãÂ∫îÁî®„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ÈÄâ‰øÆÁ±ªËØæÁ®ãÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;‰ΩøÁî® Gradio Êê≠Âª∫ÁîüÊàêÂºè AI Â∫îÁî®„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäBuilding Generative AI Applications with Gradio„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÊåáÂØºÂºÄÂèëËÄÖÂ¶Ç‰Ωï‰ΩøÁî® Gradio ÈÄöËøá Python Êé•Âè£Á®ãÂ∫èÂø´ÈÄü„ÄÅÈ´òÊïàÂú∞‰∏∫ÁîüÊàêÂºè AI ÊûÑÂª∫Áî®Êà∑ÁïåÈù¢„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ËØÑ‰º∞ÊîπËøõÁîüÊàêÂºè AI„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäEvaluating and Debugging Generative AI„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÁªìÂêà wandbÔºåÊèê‰æõ‰∏ÄÂ•óÁ≥ªÁªüÂåñÁöÑÊñπÊ≥ïÂíåÂ∑•ÂÖ∑ÔºåÂ∏ÆÂä©ÂºÄÂèëËÄÖÊúâÊïàÂú∞Ë∑üË∏™ÂíåË∞ÉËØïÁîüÊàêÂºè AI Ê®°Âûã„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂæÆË∞ÉÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäFinetuning Large Language Model„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÁªìÂêà lamini Ê°ÜÊû∂ÔºåËÆ≤Ëø∞Â¶Ç‰Ωï‰æøÊç∑È´òÊïàÂú∞Âú®Êú¨Âú∞Âü∫‰∫é‰∏™‰∫∫Êï∞ÊçÆÂæÆË∞ÉÂºÄÊ∫êÂ§ßËØ≠Ë®ÄÊ®°Âûã„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Â§ßÊ®°Âûã‰∏éËØ≠‰πâÊ£ÄÁ¥¢„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäLarge Language Models with Semantic Search„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÈíàÂØπÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºåËÆ≤Ëø∞‰∫ÜÂ§öÁßçÈ´òÁ∫ßÊ£ÄÁ¥¢ÊäÄÂ∑ß‰ª•ÂÆûÁé∞Êõ¥ÂáÜÁ°Æ„ÄÅÈ´òÊïàÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ LLM ÁîüÊàêÊïàÊûú„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Âü∫‰∫é Chroma ÁöÑÈ´òÁ∫ßÊ£ÄÁ¥¢„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäAdvanced Retrieval for AI with Chroma„ÄãËØæÁ®ãÊâìÈÄ†ÔºåÊó®Âú®‰ªãÁªçÂü∫‰∫é Chroma ÁöÑÈ´òÁ∫ßÊ£ÄÁ¥¢ÊäÄÊúØÔºåÊèêÂçáÊ£ÄÁ¥¢ÁªìÊûúÁöÑÂáÜÁ°ÆÊÄß„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Êê≠Âª∫ÂíåËØÑ‰º∞È´òÁ∫ß RAG Â∫îÁî®„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäBuilding and Evaluating Advanced RAG Applications„ÄãËØæÁ®ãÊâìÈÄ†Ôºå‰ªãÁªçÊûÑÂª∫ÂíåÂÆûÁé∞È´òË¥®Èáè RAG Á≥ªÁªüÊâÄÈúÄÁöÑÂÖ≥ÈîÆÊäÄÊúØÂíåËØÑ‰º∞Ê°ÜÊû∂„ÄÇ&lt;/li&gt; 
 &lt;li&gt;LangChain ÁöÑ Functions„ÄÅTools Âíå Agents„ÄÇÂü∫‰∫éÂê¥ÊÅ©ËææËÄÅÂ∏à„ÄäFunctions, Tools and Agents with LangChain„ÄãËØæÁ®ãÊâìÈÄ†Ôºå‰ªãÁªçÂ¶Ç‰ΩïÂü∫‰∫é LangChain ÁöÑÊñ∞ËØ≠Ê≥ïÊûÑÂª∫ Agent„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Prompt È´òÁ∫ßÊäÄÂ∑ß„ÄÇÂåÖÊã¨ CoT„ÄÅËá™Êàë‰∏ÄËá¥ÊÄßÁ≠âÂ§öÁßç Prompt È´òÁ∫ßÊäÄÂ∑ßÁöÑÂü∫Á°ÄÁêÜËÆ∫‰∏é‰ª£Á†ÅÂÆûÁé∞„ÄÇ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;ÂÖ∂‰ªñËµÑÊñôÂåÖÊã¨Ôºö&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ÂèåËØ≠Â≠óÂπïËßÜÈ¢ëÂú∞ÂùÄÔºö&lt;a href=&quot;https://www.bilibili.com/video/BV1Bo4y1A7FU/?share_source=copy_web&quot;&gt;Âê¥ÊÅ©Ëææ x OpenAI ÁöÑ Prompt Engineering ËØæÁ®ã‰∏ì‰∏öÁøªËØëÁâà&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‰∏≠Ëã±ÂèåËØ≠Â≠óÂπï‰∏ãËΩΩÔºö&lt;a href=&quot;https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese&quot;&gt;„ÄäChatGPT ÊèêÁ§∫Â∑•Á®ã„ÄãÈùûÂÆòÊñπÁâà‰∏≠Ëã±ÂèåËØ≠Â≠óÂπï&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;ËßÜÈ¢ëËÆ≤Ëß£Ôºö&lt;a href=&quot;https://www.bilibili.com/video/BV1PN4y1k7y2/?spm_id_from=333.999.0.0&quot;&gt;Èù¢ÂêëÂºÄÂèëËÄÖÁöÑ Prompt Engineering ËÆ≤Ëß£ÔºàÊï∞Â≠óÊ∏∏Ê∞ëÂ§ß‰ºöÔºâ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;ÁõÆÂΩïÁªìÊûÑËØ¥ÊòéÔºö&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;contentÔºöÂü∫‰∫éÂéüËØæÁ®ãÂ§çÁé∞ÁöÑÂèåËØ≠Áâà‰ª£Á†ÅÔºåÂèØËøêË°åÁöÑ NotebookÔºåÊõ¥Êñ∞È¢ëÁéáÊúÄÈ´òÔºåÊõ¥Êñ∞ÈÄüÂ∫¶ÊúÄÂø´„ÄÇ

docsÔºöÂøÖ‰øÆÁ±ªËØæÁ®ãÊñáÂ≠óÊïôÁ®ãÁâàÂú®Á∫øÈòÖËØªÊ∫êÁ†ÅÔºåÈÄÇÂêàÈòÖËØªÁöÑ Markdown„ÄÇ

figuresÔºöÂõæÁâáÊñá‰ª∂„ÄÇ
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Ê†∏ÂøÉË¥°ÁåÆËÄÖ&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;ÈÇπÈõ®Ë°°-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt;ÔºàDatawhaleÊàêÂëò-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶Á†îÁ©∂ÁîüÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LinChentang&quot;&gt;Â∑¶Êò•Áîü-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://yam.gift/&quot;&gt;ÈïøÁê¥-È°πÁõÆÂèëËµ∑‰∫∫&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëò-AIÁÆóÊ≥ïÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Sophia-Huang&quot;&gt;ÁéâÁê≥-È°πÁõÆÂèëËµ∑‰∫∫&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xuhu0115&quot;&gt;ÂæêËôé-ÊïôÁ®ãÁºñÊí∞ËÄÖ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Weihong-Liu&quot;&gt;Âàò‰ºüÈ∏ø-ÊïôÁ®ãÁºñÊí∞ËÄÖ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Ê±üÂçóÂ§ßÂ≠¶ÈùûÂÖ®Á†îÁ©∂ÁîüÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://Joyenjoye.com&quot;&gt;Joye-ÊïôÁ®ãÁºñÊí∞ËÄÖ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Êï∞ÊçÆÁßëÂ≠¶ÂÆ∂Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/0-yy-0&quot;&gt;È´òÁ´ã‰∏ö&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DataWhaleÊàêÂëò-ÁÆóÊ≥ïÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/GKDGKD&quot;&gt;ÈÇìÂÆáÊñá&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wisdom-pan&quot;&gt;È≠ÇÂÖÆ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ÂâçÁ´ØÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;ÂÆãÂøóÂ≠¶&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YikunHan42&quot;&gt;Èü©È¢êÂ†É&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/6forwater29&quot;&gt;ÈôàÈÄ∏Ê∂µ&lt;/a&gt; (ÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊÑèÂêëÊàêÂëò-AIÁà±Â•ΩËÄÖ)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ztgg0228&quot;&gt;‰ª≤Ê≥∞&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/leason-wan&quot;&gt;‰∏áÁ§ºË°å&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ËßÜÈ¢ëÁøªËØëËÄÖÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Bald0Wang&quot;&gt;ÁéãÁÜ†Êòé&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://yetingyun.blog.csdn.net&quot;&gt;ÊõæÊµ©Èæô&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-Datawhale ÊÑèÂêëÊàêÂëò-JLU AI Á†îÁ©∂ÁîüÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinqi-fan&quot;&gt;Â∞èÈ•≠ÂêåÂ≠¶&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sunhanyu714&quot;&gt;Â≠ôÈü©Áéâ&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ÁÆóÊ≥ïÈáèÂåñÈÉ®ÁΩ≤Â∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YinHan-Zhang&quot;&gt;Âº†Èì∂Êôó&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Jin-Zhang-Yaoguang&quot;&gt;Âº†Êôã&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Aphasia0515&quot;&gt;ÊùéÂ®áÂ®á&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Kedreamix&quot;&gt;ÈÇìÊÅ∫‰øä&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Zhiyuan-Fan&quot;&gt;ËåÉËá¥Ëøú&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Beyondzjl&quot;&gt;Âë®ÊôØÊûó&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-DatawhaleÊàêÂëòÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/very-very-very&quot;&gt;ËØ∏‰∏ñÁ∫™&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ÁÆóÊ≥ïÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YixinZ-NUS&quot;&gt;Zhang Yixin&lt;/a&gt;ÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-ITÁà±Â•ΩËÄÖÔºâ&lt;/li&gt; 
 &lt;li&gt;SaraiÔºàÂÜÖÂÆπÂàõ‰ΩúËÄÖ-AIÂ∫îÁî®Áà±Â•ΩËÄÖÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;ÂÖ∂‰ªñ&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;ÁâπÂà´ÊÑüË∞¢ &lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt;„ÄÅ&lt;a href=&quot;https://github.com/LSGOMYP&quot;&gt;@LSGOMYP&lt;/a&gt; ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅÔºõ&lt;/li&gt; 
 &lt;li&gt;ÊÑüË∞¢ &lt;a href=&quot;https://github.com/GitHubDaily&quot;&gt;GithubDaily&lt;/a&gt; Êèê‰æõÁöÑÂèåËØ≠Â≠óÂπïÔºõ&lt;/li&gt; 
 &lt;li&gt;Â¶ÇÊûúÊúâ‰ªª‰ΩïÊÉ≥Ê≥ïÂèØ‰ª•ËÅîÁ≥ªÊàë‰ª¨ Datawhale ‰πüÊ¨¢ËøéÂ§ßÂÆ∂Â§öÂ§öÊèêÂá∫ IssuesÔºõ&lt;/li&gt; 
 &lt;li&gt;ÁâπÂà´ÊÑüË∞¢‰ª•‰∏ã‰∏∫ÊïôÁ®ãÂÅöÂá∫Ë¥°ÁåÆÁöÑÂêåÂ≠¶ÔºÅ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;a href=&quot;https://datawhalechina.github.io/llm-cookbook/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/llm-cookbook&quot; /&gt; &lt;/a&gt; 
&lt;p&gt;Made with &lt;a href=&quot;https://contrib.rocks&quot;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://star-history.com/#datawhalechina/llm-cookbook&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=datawhalechina/llm-cookbook&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;ÂÖ≥Ê≥®Êàë‰ª¨&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;Êâ´Êèè‰∏ãÊñπ‰∫åÁª¥Á†ÅÂÖ≥Ê≥®ÂÖ¨‰ºóÂè∑ÔºöDatawhale&lt;/p&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/llm-cookbook/main/figures/qrcode.jpeg&quot; width=&quot;180&quot; height=&quot;180&quot; /&gt; 
&lt;/div&gt; Datawhale ÊòØ‰∏Ä‰∏™‰∏ìÊ≥®‰∫éÊï∞ÊçÆÁßëÂ≠¶‰∏é AI È¢ÜÂüüÁöÑÂºÄÊ∫êÁªÑÁªáÔºåÊ±áÈõÜ‰∫Ü‰ºóÂ§öÈ¢ÜÂüüÈô¢Ê†°ÂíåÁü•Âêç‰ºÅ‰∏öÁöÑ‰ºòÁßÄÂ≠¶‰π†ËÄÖÔºåËÅöÂêà‰∫Ü‰∏ÄÁæ§ÊúâÂºÄÊ∫êÁ≤æÁ•ûÂíåÊé¢Á¥¢Á≤æÁ•ûÁöÑÂõ¢ÈòüÊàêÂëò„ÄÇÂæÆ‰ø°ÊêúÁ¥¢ÂÖ¨‰ºóÂè∑ Datawhale ÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨„ÄÇ 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;&lt;img alt=&quot;Áü•ËØÜÂÖ±‰∫´ËÆ∏ÂèØÂçèËÆÆ&quot; style=&quot;border-width:0&quot; src=&quot;https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey&quot; /&gt;&lt;/a&gt;&lt;br /&gt;Êú¨‰ΩúÂìÅÈááÁî®&lt;a rel=&quot;license&quot; href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç - ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî® - Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ&lt;/a&gt;ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>datawhalechina/happy-llm</title>
      <link>https://github.com/datawhalechina/happy-llm</link>
      <description>&lt;p&gt;üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/head.jpg&quot; alt=&quot;alt text&quot; width=&quot;100%&quot; /&gt; 
 &lt;h1&gt;Happy-LLM&lt;/h1&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub stars&quot; /&gt; 
 &lt;img src=&quot;https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub forks&quot; /&gt; 
 &lt;img src=&quot;https://img.shields.io/badge/language-Chinese-brightgreen?style=flat&quot; alt=&quot;Language&quot; /&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/happy-llm&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/GitHub-Project-blue?style=flat&amp;amp;logo=github&quot; alt=&quot;GitHub Project&quot; /&gt;&lt;/a&gt; 
 &lt;a href=&quot;https://swanlab.cn/@kmno4/Happy-LLM/overview&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg?sanitize=true&quot; alt=&quot;SwanLab&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://trendshift.io/repositories/14175&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14175&quot; alt=&quot;datawhalechina%2Fhappy-llm | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;&lt;/a&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/README_en.md&quot;&gt;English&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://datawhalechina.github.io/happy-llm/&quot;&gt;üìö Âú®Á∫øÈòÖËØªÂú∞ÂùÄ&lt;/a&gt;&lt;/p&gt; 
 &lt;h3&gt;üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã&lt;/h3&gt; 
 &lt;p&gt;&lt;em&gt;Ê∑±ÂÖ•ÁêÜËß£ LLM Ê†∏ÂøÉÂéüÁêÜÔºåÂä®ÊâãÂÆûÁé∞‰Ω†ÁöÑÁ¨¨‰∏Ä‰∏™Â§ßÊ®°Âûã&lt;/em&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üéØ È°πÁõÆ‰ªãÁªç&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;ÂæàÂ§öÂ∞è‰ºô‰º¥Âú®ÁúãÂÆå DatawhaleÂºÄÊ∫êÈ°πÁõÆÔºö &lt;a href=&quot;https://github.com/datawhalechina/self-llm&quot;&gt;self-llm ÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó&lt;/a&gt; ÂêéÔºåÊÑüËßâÊÑèÁäπÊú™Â∞ΩÔºåÊÉ≥Ë¶ÅÊ∑±ÂÖ•‰∫ÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁêÜÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇ‰∫éÊòØÊàë‰ª¨ÔºàDatawhaleÔºâÂÜ≥ÂÆöÊé®Âá∫„ÄäHappy-LLM„ÄãÈ°πÁõÆÔºåÊó®Âú®Â∏ÆÂä©Â§ßÂÆ∂Ê∑±ÂÖ•ÁêÜËß£Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂéüÁêÜÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇ&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÊòØ‰∏Ä‰∏™&lt;strong&gt;Á≥ªÁªüÊÄßÁöÑ LLM Â≠¶‰π†ÊïôÁ®ã&lt;/strong&gt;ÔºåÂ∞Ü‰ªé NLP ÁöÑÂü∫Êú¨Á†îÁ©∂ÊñπÊ≥ïÂá∫ÂèëÔºåÊ†πÊçÆ LLM ÁöÑÊÄùË∑ØÂèäÂéüÁêÜÈÄêÂ±ÇÊ∑±ÂÖ•Ôºå‰æùÊ¨°‰∏∫ËØªËÄÖÂâñÊûê LLM ÁöÑÊû∂ÊûÑÂü∫Á°ÄÂíåËÆ≠ÁªÉËøáÁ®ã„ÄÇÂêåÊó∂ÔºåÊàë‰ª¨‰ºöÁªìÂêàÁõÆÂâç LLM È¢ÜÂüüÊúÄ‰∏ªÊµÅÁöÑ‰ª£Á†ÅÊ°ÜÊû∂ÔºåÊºîÁªÉÂ¶Ç‰Ωï‰∫≤ÊâãÊê≠Âª∫„ÄÅËÆ≠ÁªÉ‰∏Ä‰∏™ LLMÔºåÊúü‰ª•ÂÆûÁé∞Êéà‰πã‰ª•È±ºÔºåÊõ¥Êéà‰πã‰ª•Ê∏î„ÄÇÂ∏åÊúõÂ§ßÂÆ∂ËÉΩ‰ªéËøôÊú¨‰π¶ÂºÄÂßãËµ∞ÂÖ• LLM ÁöÑÊµ©ÁÄö‰∏ñÁïåÔºåÊé¢Á¥¢ LLM ÁöÑÊó†Â∞ΩÂèØËÉΩ„ÄÇ&lt;/p&gt; 
&lt;h3&gt;‚ú® ‰Ω†Â∞ÜÊî∂Ëé∑‰ªÄ‰πàÔºü&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;üìö &lt;strong&gt;Datawhale ÂºÄÊ∫êÂÖçË¥π&lt;/strong&gt; ÂÆåÂÖ®ÂÖçË¥πÁöÑÂ≠¶‰π†Êú¨È°πÁõÆÊâÄÊúâÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üîç &lt;strong&gt;Ê∑±ÂÖ•ÁêÜËß£&lt;/strong&gt; Transformer Êû∂ÊûÑÂíåÊ≥®ÊÑèÂäõÊú∫Âà∂&lt;/li&gt; 
 &lt;li&gt;üìö &lt;strong&gt;ÊéåÊè°&lt;/strong&gt; È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Êú¨ÂéüÁêÜ&lt;/li&gt; 
 &lt;li&gt;üß† &lt;strong&gt;‰∫ÜËß£&lt;/strong&gt; Áé∞ÊúâÂ§ßÊ®°ÂûãÁöÑÂü∫Êú¨ÁªìÊûÑ&lt;/li&gt; 
 &lt;li&gt;üèóÔ∏è &lt;strong&gt;Âä®ÊâãÂÆûÁé∞&lt;/strong&gt; ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑ LLaMA2 Ê®°Âûã&lt;/li&gt; 
 &lt;li&gt;‚öôÔ∏è &lt;strong&gt;ÊéåÊè°ËÆ≠ÁªÉ&lt;/strong&gt; ‰ªéÈ¢ÑËÆ≠ÁªÉÂà∞ÂæÆË∞ÉÁöÑÂÖ®ÊµÅÁ®ã&lt;/li&gt; 
 &lt;li&gt;üöÄ &lt;strong&gt;ÂÆûÊàòÂ∫îÁî®&lt;/strong&gt; RAG„ÄÅAgent Á≠âÂâçÊ≤øÊäÄÊúØ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üìñ ÂÜÖÂÆπÂØºËà™&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Á´†ËäÇ&lt;/th&gt; 
   &lt;th&gt;ÂÖ≥ÈîÆÂÜÖÂÆπ&lt;/th&gt; 
   &lt;th&gt;Áä∂ÊÄÅ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/%E5%89%8D%E8%A8%80.md&quot;&gt;ÂâçË®Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Êú¨È°πÁõÆÁöÑÁºòËµ∑„ÄÅËÉåÊôØÂèäËØªËÄÖÂª∫ËÆÆ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter1/%E7%AC%AC%E4%B8%80%E7%AB%A0%20NLP%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5.md&quot;&gt;Á¨¨‰∏ÄÁ´† NLP Âü∫Á°ÄÊ¶ÇÂøµ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ªÄ‰πàÊòØ NLP„ÄÅÂèëÂ±ïÂéÜÁ®ã„ÄÅ‰ªªÂä°ÂàÜÁ±ª„ÄÅÊñáÊú¨Ë°®Á§∫ÊºîËøõ&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter2/%E7%AC%AC%E4%BA%8C%E7%AB%A0%20Transformer%E6%9E%B6%E6%9E%84.md&quot;&gt;Á¨¨‰∫åÁ´† Transformer Êû∂ÊûÑ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÅEncoder-Decoder„ÄÅÊâãÊääÊâãÊê≠Âª∫ Transformer&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter3/%E7%AC%AC%E4%B8%89%E7%AB%A0%20%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&quot;&gt;Á¨¨‰∏âÁ´† È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Encoder-only„ÄÅEncoder-Decoder„ÄÅDecoder-Only Ê®°ÂûãÂØπÊØî&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter4/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.md&quot;&gt;Á¨¨ÂõõÁ´† Â§ßËØ≠Ë®ÄÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;LLM ÂÆö‰πâ„ÄÅËÆ≠ÁªÉÁ≠ñÁï•„ÄÅÊ∂åÁé∞ËÉΩÂäõÂàÜÊûê&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter5/%E7%AC%AC%E4%BA%94%E7%AB%A0%20%E5%8A%A8%E6%89%8B%E6%90%AD%E5%BB%BA%E5%A4%A7%E6%A8%A1%E5%9E%8B.md&quot;&gt;Á¨¨‰∫îÁ´† Âä®ÊâãÊê≠Âª∫Â§ßÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;ÂÆûÁé∞ LLaMA2„ÄÅËÆ≠ÁªÉ Tokenizer„ÄÅÈ¢ÑËÆ≠ÁªÉÂ∞èÂûã LLM&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter6/%E7%AC%AC%E5%85%AD%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E5%AE%9E%E8%B7%B5.md&quot;&gt;Á¨¨ÂÖ≠Á´† Â§ßÊ®°ÂûãËÆ≠ÁªÉÂÆûË∑µ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;È¢ÑËÆ≠ÁªÉ„ÄÅÊúâÁõëÁù£ÂæÆË∞É„ÄÅLoRA/QLoRA È´òÊïàÂæÆË∞É&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/chapter7/%E7%AC%AC%E4%B8%83%E7%AB%A0%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8.md&quot;&gt;Á¨¨‰∏ÉÁ´† Â§ßÊ®°ÂûãÂ∫îÁî®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Ê®°ÂûãËØÑÊµã„ÄÅRAG Ê£ÄÁ¥¢Â¢ûÂº∫„ÄÅAgent Êô∫ËÉΩ‰Ωì&lt;/td&gt; 
   &lt;td&gt;‚úÖ&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&quot;&gt;Extra Chapter LLM Blog&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;‰ºòÁßÄÁöÑÂ§ßÊ®°Âûã Â≠¶‰π†Á¨îËÆ∞/Blog ÔºåÊ¨¢ËøéÂ§ßÂÆ∂Êù• PR ÔºÅ&lt;/td&gt; 
   &lt;td&gt;üöß&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Extra Chapter LLM Blog&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/why-fine-tune-small-large-language-models/readme.md&quot;&gt;Â§ßÊ®°ÂûãÈÉΩËøô‰πàÂéâÂÆ≥‰∫ÜÔºåÂæÆË∞É0.6BÁöÑÂ∞èÊ®°ÂûãÊúâ‰ªÄ‰πàÊÑè‰πâÔºü&lt;/a&gt; @&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;‰∏çË¶ÅËë±ÂßúËíú&lt;/a&gt; 2025-7-11&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/transformer-architecture/&quot;&gt;Transformer Êï¥‰ΩìÊ®°ÂùóËÆæËÆ°Ëß£ËØª&lt;/a&gt; @&lt;a href=&quot;https://github.com/ditingdapeng&quot;&gt;ditingdapeng&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/text-data-processing/readme.md&quot;&gt;ÊñáÊú¨Êï∞ÊçÆÂ§ÑÁêÜËØ¶Ëß£&lt;/a&gt; @&lt;a href=&quot;https://github.com/xinala-781&quot;&gt;Ëî°ÈãÜÊç∑&lt;/a&gt; 2025-7-14&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/vlm-concatenation-finetune/README.md&quot;&gt;Qwen3-&quot;VL&quot;‚Äî‚ÄîË∂ÖÂ∞è‰∏≠ÊñáÂ§öÊ®°ÊÄÅÊ®°ÂûãÁöÑ‚ÄúÊãºÊé•ÂæÆË∞É‚Äù‰πãË∑Ø&lt;/a&gt; @&lt;a href=&quot;https://github.com/ShaohonChen&quot;&gt;ShaohonChen&lt;/a&gt; 2025-7-30&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/s1-vllm-thinking-budget/readme.md&quot;&gt;S1: Thinking Budget with vLLM&lt;/a&gt; @&lt;a href=&quot;https://github.com/kmno4-zx&quot;&gt;kmno4-zx&lt;/a&gt; 2025-8-03&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/CDDRS/readme.md&quot;&gt;CDDRS: ‰ΩøÁî®ÁªÜÁ≤íÂ∫¶ËØ≠‰πâ‰ø°ÊÅØÊåáÂØºÂ¢ûÂº∫ÁöÑRAGÊ£ÄÁ¥¢ÊñπÊ≥ï&lt;/a&gt; @&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;Hongru0306&lt;/a&gt; 2025-8-21&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;Â¶ÇÊûúÂ§ßÂÆ∂Âú®Â≠¶‰π† Happy-LLM È°πÁõÆÊàñ LLM Áõ∏ÂÖ≥Áü•ËØÜ‰∏≠ÊúâËá™Â∑±Áã¨Âà∞ÁöÑËßÅËß£„ÄÅËÆ§Áü•„ÄÅÂÆûË∑µÔºåÊ¨¢ËøéÂ§ßÂÆ∂ PR Âú® &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/&quot;&gt;Extra Chapter LLM Blog&lt;/a&gt; ‰∏≠„ÄÇËØ∑ÈÅµÂÆà Extra Chapter LLM Blog ÁöÑ &lt;a href=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/Extra-Chapter/Readme.md&quot;&gt;PR ËßÑËåÉ&lt;/a&gt;ÔºåÊàë‰ª¨‰ºöËßÜ PR ÂÜÖÂÆπÁöÑË¥®ÈáèÂíå‰ª∑ÂÄºÊù•ÂÜ≥ÂÆöÊòØÂê¶ÂêàÂπ∂ÊàñË°•ÂÖÖÂà∞ Happy-LLM Ê≠£Êñá‰∏≠Êù•„ÄÇ&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Ê®°Âûã‰∏ãËΩΩ&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ê®°ÂûãÂêçÁß∞&lt;/th&gt; 
   &lt;th&gt;‰∏ãËΩΩÂú∞ÂùÄ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-Base-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-base&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Happy-LLM-Chapter5-SFT-215M&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-sft&quot;&gt;ü§ñ ModelScope&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;ModelScope ÂàõÁ©∫Èó¥‰ΩìÈ™åÂú∞ÂùÄÔºö&lt;a href=&quot;https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft&quot;&gt;ü§ñ ÂàõÁ©∫Èó¥&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;PDF ÁâàÊú¨‰∏ãËΩΩ&lt;/h3&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉ&lt;em&gt;&lt;strong&gt;Êú¨ Happy-LLM PDF ÊïôÁ®ãÂÆåÂÖ®ÂºÄÊ∫êÂÖçË¥π„ÄÇ‰∏∫Èò≤Ê≠¢ÂêÑÁ±ªËê•ÈîÄÂè∑Âä†Ê∞¥Âç∞ÂêéË¥©ÂçñÁªôÂ§ßÊ®°ÂûãÂàùÂ≠¶ËÄÖÔºåÊàë‰ª¨ÁâπÂú∞Âú® PDF Êñá‰ª∂‰∏≠È¢ÑÂÖàÊ∑ªÂä†‰∫Ü‰∏çÂΩ±ÂìçÈòÖËØªÁöÑ Datawhale ÂºÄÊ∫êÊ†áÂøóÊ∞¥Âç∞ÔºåÊï¨ËØ∑Ë∞ÖËß£ÔΩû&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;em&gt;Happy-LLM PDF : &lt;a href=&quot;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&quot;&gt;https://github.com/datawhalechina/happy-llm/releases/tag/v1.0.1&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üí° Â¶Ç‰ΩïÂ≠¶‰π†&lt;/h2&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÈÄÇÂêàÂ§ßÂ≠¶Áîü„ÄÅÁ†îÁ©∂‰∫∫Âëò„ÄÅLLM Áà±Â•ΩËÄÖ„ÄÇÂú®Â≠¶‰π†Êú¨È°πÁõÆ‰πãÂâçÔºåÂª∫ËÆÆÂÖ∑Â§á‰∏ÄÂÆöÁöÑÁºñÁ®ãÁªèÈ™åÔºåÂ∞§ÂÖ∂ÊòØË¶ÅÂØπ Python ÁºñÁ®ãËØ≠Ë®ÄÊúâ‰∏ÄÂÆöÁöÑ‰∫ÜËß£„ÄÇÊúÄÂ•ΩÂÖ∑Â§áÊ∑±Â∫¶Â≠¶‰π†ÁöÑÁõ∏ÂÖ≥Áü•ËØÜÔºåÂπ∂‰∫ÜËß£ NLP È¢ÜÂüüÁöÑÁõ∏ÂÖ≥Ê¶ÇÂøµÂíåÊúØËØ≠Ôºå‰ª•‰æøÊõ¥ËΩªÊùæÂú∞Â≠¶‰π†Êú¨È°πÁõÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊú¨È°πÁõÆÂàÜ‰∏∫‰∏§ÈÉ®ÂàÜ‚Äî‚ÄîÂü∫Á°ÄÁü•ËØÜ‰∏éÂÆûÊàòÂ∫îÁî®„ÄÇÁ¨¨1Á´†ÔΩûÁ¨¨4Á´†ÊòØÂü∫Á°ÄÁü•ËØÜÈÉ®ÂàÜÔºå‰ªéÊµÖÂÖ•Ê∑±‰ªãÁªç LLM ÁöÑÂü∫Êú¨ÂéüÁêÜ„ÄÇÂÖ∂‰∏≠ÔºåÁ¨¨1Á´†ÁÆÄÂçï‰ªãÁªç NLP ÁöÑÂü∫Êú¨‰ªªÂä°ÂíåÂèëÂ±ïÔºå‰∏∫Èùû NLP È¢ÜÂüüÁ†îÁ©∂ËÄÖÊèê‰æõÂèÇËÄÉÔºõÁ¨¨2Á´†‰ªãÁªç LLM ÁöÑÂü∫Êú¨Êû∂ÊûÑ‚Äî‚ÄîTransformerÔºåÂåÖÊã¨ÂéüÁêÜ‰ªãÁªçÂèä‰ª£Á†ÅÂÆûÁé∞Ôºå‰Ωú‰∏∫ LLM ÊúÄÈáçË¶ÅÁöÑÁêÜËÆ∫Âü∫Á°ÄÔºõÁ¨¨3Á´†Êï¥‰Ωì‰ªãÁªçÁªèÂÖ∏ÁöÑ PLMÔºåÂåÖÊã¨ Encoder-Only„ÄÅEncoder-Decoder Âíå Decoder-Only ‰∏âÁßçÊû∂ÊûÑÔºå‰πüÂêåÊó∂‰ªãÁªç‰∫ÜÂΩìÂâç‰∏Ä‰∫õ‰∏ªÊµÅ LLM ÁöÑÊû∂ÊûÑÂíåÊÄùÊÉ≥ÔºõÁ¨¨4Á´†ÂàôÊ≠£ÂºèËøõÂÖ• LLM ÈÉ®ÂàÜÔºåËØ¶ÁªÜ‰ªãÁªç LLM ÁöÑÁâπÁÇπ„ÄÅËÉΩÂäõÂíåÊï¥‰ΩìËÆ≠ÁªÉËøáÁ®ã„ÄÇÁ¨¨5Á´†ÔΩûÁ¨¨7Á´†ÊòØÂÆûÊàòÂ∫îÁî®ÈÉ®ÂàÜÔºåÂ∞ÜÈÄêÊ≠•Â∏¶È¢ÜÂ§ßÂÆ∂Ê∑±ÂÖ• LLM ÁöÑÂ∫ïÂ±ÇÁªÜËäÇ„ÄÇÂÖ∂‰∏≠ÔºåÁ¨¨5Á´†Â∞ÜÂ∏¶È¢ÜÂ§ßÂÆ∂ËÄÖÂü∫‰∫é PyTorch Â±Ç‰∫≤ÊâãÊê≠Âª∫‰∏Ä‰∏™ LLMÔºåÂπ∂ÂÆûÁé∞È¢ÑËÆ≠ÁªÉ„ÄÅÊúâÁõëÁù£ÂæÆË∞ÉÁöÑÂÖ®ÊµÅÁ®ãÔºõÁ¨¨6Á´†Â∞ÜÂºïÂÖ•ÁõÆÂâç‰∏öÁïå‰∏ªÊµÅÁöÑ LLM ËÆ≠ÁªÉÊ°ÜÊû∂ TransformersÔºåÂ∏¶È¢ÜÂ≠¶‰π†ËÄÖÂü∫‰∫éËØ•Ê°ÜÊû∂Âø´ÈÄü„ÄÅÈ´òÊïàÂú∞ÂÆûÁé∞ LLM ËÆ≠ÁªÉËøáÁ®ãÔºõÁ¨¨7Á´†ÂàôÂ∞Ü‰ªãÁªç Âü∫‰∫é LLM ÁöÑÂêÑÁßçÂ∫îÁî®ÔºåË°•ÂÖ®Â≠¶‰π†ËÄÖÂØπ LLM ‰ΩìÁ≥ªÁöÑËÆ§Áü•ÔºåÂåÖÊã¨ LLM ÁöÑËØÑÊµã„ÄÅÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRetrieval-Augmented GenerationÔºåRAGÔºâ„ÄÅÊô∫ËÉΩ‰ΩìÔºàAgentÔºâÁöÑÊÄùÊÉ≥ÂíåÁÆÄÂçïÂÆûÁé∞„ÄÇ‰Ω†ÂèØ‰ª•Ê†πÊçÆ‰∏™‰∫∫ÂÖ¥Ë∂£ÂíåÈúÄÊ±ÇÔºåÈÄâÊã©ÊÄßÂú∞ÈòÖËØªÁõ∏ÂÖ≥Á´†ËäÇ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÂú®ÈòÖËØªÊú¨‰π¶ÁöÑËøáÁ®ã‰∏≠ÔºåÂª∫ËÆÆ‰Ω†Â∞ÜÁêÜËÆ∫ÂíåÂÆûÈôÖÁõ∏ÁªìÂêà„ÄÇLLM ÊòØ‰∏Ä‰∏™Âø´ÈÄüÂèëÂ±ï„ÄÅÊ≥®ÈáçÂÆûË∑µÁöÑÈ¢ÜÂüüÔºåÊàë‰ª¨Âª∫ËÆÆ‰Ω†Â§öÊäïÂÖ•ÂÆûÊàòÔºåÂ§çÁé∞Êú¨‰π¶Êèê‰æõÁöÑÂêÑÁßç‰ª£Á†ÅÔºåÂêåÊó∂ÁßØÊûÅÂèÇÂä† LLM Áõ∏ÂÖ≥ÁöÑÈ°πÁõÆ‰∏éÊØîËµõÔºåÁúüÊ≠£ÊäïÂÖ•Âà∞ LLM ÂºÄÂèëÁöÑÊµ™ÊΩÆ‰∏≠„ÄÇÊàë‰ª¨ÈºìÂä±‰Ω†ÂÖ≥Ê≥® Datawhale ÂèäÂÖ∂‰ªñ LLM Áõ∏ÂÖ≥ÂºÄÊ∫êÁ§æÂå∫ÔºåÂΩìÈÅáÂà∞ÈóÆÈ¢òÊó∂Ôºå‰Ω†ÂèØ‰ª•ÈöèÊó∂Âú®Êú¨È°πÁõÆÁöÑ issue Âå∫ÊèêÈóÆ„ÄÇ&lt;/p&gt; 
&lt;p&gt;‚ÄÉ‚ÄÉÊúÄÂêéÔºåÊ¨¢ËøéÊØè‰∏Ä‰ΩçËØªËÄÖÂú®Â≠¶‰π†ÂÆåÊú¨È°πÁõÆÂêéÂä†ÂÖ•Âà∞ LLM ÂºÄÂèëËÄÖÁöÑË°åÂàó„ÄÇ‰Ωú‰∏∫ÂõΩÂÜÖ AI ÂºÄÊ∫êÁ§æÂå∫ÔºåÊàë‰ª¨Â∏åÊúõÂÖÖÂàÜËÅöÈõÜÂÖ±ÂàõËÄÖÔºå‰∏ÄËµ∑‰∏∞ÂØåËøô‰∏™ÂºÄÊ∫ê LLM ÁöÑ‰∏ñÁïåÔºåÊâìÈÄ†Êõ¥Â§ö„ÄÅÊõ¥ÂÖ®Èù¢ÁâπËâ≤ LLM ÁöÑÊïôÁ®ã„ÄÇÊòüÁÅ´ÁÇπÁÇπÔºåÊ±áËÅöÊàêÊµ∑„ÄÇÊàë‰ª¨Â∏åÊúõÊàê‰∏∫ LLM ‰∏éÊôÆÁΩóÂ§ß‰ºóÁöÑÈò∂Ê¢ØÔºå‰ª•Ëá™Áî±„ÄÅÂπ≥Á≠âÁöÑÂºÄÊ∫êÁ≤æÁ•ûÔºåÊã•Êä±Êõ¥ÊÅ¢ÂºòËÄåËæΩÈòîÁöÑ LLM ‰∏ñÁïå„ÄÇ&lt;/p&gt; 
&lt;h2&gt;ü§ù Â¶Ç‰ΩïË¥°ÁåÆ&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨Ê¨¢Ëøé‰ªª‰ΩïÂΩ¢ÂºèÁöÑË¥°ÁåÆÔºÅ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;üêõ &lt;strong&gt;Êä•Âëä Bug&lt;/strong&gt; - ÂèëÁé∞ÈóÆÈ¢òËØ∑Êèê‰∫§ Issue&lt;/li&gt; 
 &lt;li&gt;üí° &lt;strong&gt;ÂäüËÉΩÂª∫ËÆÆ&lt;/strong&gt; - ÊúâÂ•ΩÊÉ≥Ê≥ïÂ∞±ÂëäËØâÊàë‰ª¨&lt;/li&gt; 
 &lt;li&gt;üìù &lt;strong&gt;ÂÜÖÂÆπÂÆåÂñÑ&lt;/strong&gt; - Â∏ÆÂä©ÊîπËøõÊïôÁ®ãÂÜÖÂÆπ&lt;/li&gt; 
 &lt;li&gt;üîß &lt;strong&gt;‰ª£Á†Å‰ºòÂåñ&lt;/strong&gt; - Êèê‰∫§ Pull Request&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üôè Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;h3&gt;Ê†∏ÂøÉË¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KMnO4-zx&quot;&gt;ÂÆãÂøóÂ≠¶-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (DatawhaleÊàêÂëò-‰∏≠ÂõΩÁüø‰∏öÂ§ßÂ≠¶(Âåó‰∫¨))&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/logan-zou&quot;&gt;ÈÇπÈõ®Ë°°-È°πÁõÆË¥üË¥£‰∫∫&lt;/a&gt; (DatawhaleÊàêÂëò-ÂØπÂ§ñÁªèÊµéË¥∏ÊòìÂ§ßÂ≠¶)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://xinzhongzhu.github.io/&quot;&gt;Êú±‰ø°Âø†-ÊåáÂØº‰∏ìÂÆ∂&lt;/a&gt;ÔºàDatawhaleÈ¶ñÂ∏≠ÁßëÂ≠¶ÂÆ∂-ÊµôÊ±üÂ∏àËåÉÂ§ßÂ≠¶Êù≠Â∑û‰∫∫Â∑•Êô∫ËÉΩÁ†îÁ©∂Èô¢ÊïôÊéàÔºâ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Extra-Chapter Ë¥°ÁåÆËÄÖ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ditingdapeng&quot;&gt;ditingdapeng&lt;/a&gt;ÔºàÂÜÖÂÆπË¥°ÁåÆËÄÖ-‰∫ëÂéüÁîüÂü∫Á°ÄÊû∂ÊûÑÂ∑•Á®ãÂ∏àÔºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xinala-781&quot;&gt;Ëî°ÈãÜÊç∑&lt;/a&gt;ÔºàÂÜÖÂÆπË¥°ÁåÆËÄÖ-Á¶èÂ∑ûÂ§ßÂ≠¶Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ShaohonChen&quot;&gt;ShaohonChen&lt;/a&gt; ÔºàÊÉÖÊÑüÊú∫Âô®ÂÆûÈ™åÂÆ§Á†îÁ©∂Âëò-Ë•øÂÆâÁîµÂ≠êÁßëÊäÄÂ§ßÂ≠¶Âú®ËØªÁ°ïÂ£´Ôºâ&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Hongru0306&quot;&gt;ËÇñÈ∏øÂÑí, Â∫ÑÂÅ•Áê®&lt;/a&gt; (ÂÜÖÂÆπË¥°ÁåÆËÄÖ-ÂêåÊµéÂ§ßÂ≠¶)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÁâπÂà´ÊÑüË∞¢&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊÑüË∞¢ &lt;a href=&quot;https://github.com/Sm1les&quot;&gt;@Sm1les&lt;/a&gt; ÂØπÊú¨È°πÁõÆÁöÑÂ∏ÆÂä©‰∏éÊîØÊåÅ&lt;/li&gt; 
 &lt;li&gt;ÊÑüË∞¢ÊâÄÊúâ‰∏∫Êú¨È°πÁõÆÂÅöÂá∫Ë¥°ÁåÆÁöÑÂºÄÂèëËÄÖ‰ª¨ ‚ù§Ô∏è&lt;/li&gt; 
&lt;/ul&gt; 
&lt;div align=&quot;center&quot; style=&quot;margin-top: 30px;&quot;&gt; 
 &lt;a href=&quot;https://github.com/datawhalechina/happy-llm/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=datawhalechina/happy-llm&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/star-history-2025710.png&quot; alt=&quot;Datawhale&quot; width=&quot;90%&quot; /&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;‚≠ê Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåËØ∑ÁªôÊàë‰ª¨‰∏Ä‰∏™ StarÔºÅ&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;ÂÖ≥‰∫é Datawhale&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/datawhalechina/happy-llm/main/images/datawhale.png&quot; alt=&quot;Datawhale&quot; width=&quot;30%&quot; /&gt; 
 &lt;p&gt;Êâ´Êèè‰∫åÁª¥Á†ÅÂÖ≥Ê≥® Datawhale ÂÖ¨‰ºóÂè∑ÔºåËé∑ÂèñÊõ¥Â§ö‰ºòË¥®ÂºÄÊ∫êÂÜÖÂÆπ&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;üìú ÂºÄÊ∫êÂçèËÆÆ&lt;/h2&gt; 
&lt;p&gt;Êú¨‰ΩúÂìÅÈááÁî®&lt;a href=&quot;http://creativecommons.org/licenses/by-nc-sa/4.0/&quot;&gt;Áü•ËØÜÂÖ±‰∫´ÁΩ≤Âêç-ÈùûÂïÜ‰∏öÊÄß‰ΩøÁî®-Áõ∏ÂêåÊñπÂºèÂÖ±‰∫´ 4.0 ÂõΩÈôÖËÆ∏ÂèØÂçèËÆÆ&lt;/a&gt;ËøõË°åËÆ∏ÂèØ„ÄÇ&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>alphacep/vosk-api</title>
      <link>https://github.com/alphacep/vosk-api</link>
      <description>&lt;p&gt;Offline speech recognition API for Android, iOS, Raspberry Pi and servers with Python, Java, C# and Node&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Vosk Speech Recognition Toolkit&lt;/h1&gt; 
&lt;p&gt;Vosk is an offline open source speech recognition toolkit. It enables speech recognition for 20+ languages and dialects - English, Indian English, German, French, Spanish, Portuguese, Chinese, Russian, Turkish, Vietnamese, Italian, Dutch, Catalan, Arabic, Greek, Farsi, Filipino, Ukrainian, Kazakh, Swedish, Japanese, Esperanto, Hindi, Czech, Polish. More to come.&lt;/p&gt; 
&lt;p&gt;Vosk models are small (50 Mb) but provide continuous large vocabulary transcription, zero-latency response with streaming API, reconfigurable vocabulary and speaker identification.&lt;/p&gt; 
&lt;p&gt;Speech recognition bindings implemented for various programming languages like Python, Java, Node.JS, C#, C++, Rust, Go and others.&lt;/p&gt; 
&lt;p&gt;Vosk supplies speech recognition for chatbots, smart home appliances, virtual assistants. It can also create subtitles for movies, transcription for lectures and interviews.&lt;/p&gt; 
&lt;p&gt;Vosk scales from small devices like Raspberry Pi or Android smartphone to big clusters.&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;For installation instructions, examples and documentation visit &lt;a href=&quot;https://alphacephei.com/vosk&quot;&gt;Vosk Website&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen3-VL</title>
      <link>https://github.com/QwenLM/Qwen3-VL</link>
      <description>&lt;p&gt;Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen3-VL&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vllogo.png&quot; width=&quot;400&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; üíú &lt;a href=&quot;https://chat.qwenlm.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&quot;https://modelscope.cn/collections/Qwen3-VL-5c7a94c8cb144b&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list&quot;&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìö &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks&quot;&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë Paper is coming&amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen3-VL-Demo&quot;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&quot;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&quot;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&quot;&gt;API&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üñ•Ô∏è &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl&quot;&gt;PAI-DSW&lt;/a&gt; &lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Meet Qwen3-VL ‚Äî the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; 
&lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; 
&lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‚Äëenhanced Thinking editions for flexible, on‚Äëdemand deployment.&lt;/p&gt; 
&lt;h4&gt;Key Enhancements:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs‚Äîrecognizes elements, understands functions, invokes tools, completes tasks.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates Draw.io/HTML/CSS/JS from images/videos.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math‚Äîcausal analysis and logical, evidence-based answers.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to ‚Äúrecognize everything‚Äù‚Äîcelebrities, anime, products, landmarks, flora/fauna, etc.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 10); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text‚Äìvision fusion for lossless, unified comprehension.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Model Architecture Updates:&lt;/h4&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interleaved-MRoPE&lt;/strong&gt;: Full‚Äëfrequency allocation over time, width, and height via robust positional embeddings, enhancing long‚Äëhorizon video reasoning.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;DeepStack&lt;/strong&gt;: Fuses multi‚Äëlevel ViT features to capture fine‚Äëgrained details and sharpen image‚Äìtext alignment.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text‚ÄìTimestamp Alignment:&lt;/strong&gt; Moves beyond T‚ÄëRoPE to precise, timestamp‚Äëgrounded event localization for stronger video temporal modeling.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.09.23: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&quot;&gt;Qwen3-VL-235B-A22B-Instruct&lt;/a&gt; and &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&quot;&gt;Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.04.08: We provide the &lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune&quot;&gt;code&lt;/a&gt; for fine-tuning Qwen2-VL and Qwen2.5-VL.&lt;/li&gt; 
 &lt;li&gt;2025.03.25: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct&quot;&gt;Qwen2.5-VL-32B&lt;/a&gt;. It is smarter and its responses align more closely with human preferences. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-vl-32b/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2025.02.20: we have released the &lt;a href=&quot;https://arxiv.org/abs/2502.13923&quot;&gt;Qwen2.5-VL Technical Report&lt;/a&gt;. Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ&quot;&gt;3B&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ&quot;&gt;7B&lt;/a&gt; , and &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ&quot;&gt;72B&lt;/a&gt; parameters.&lt;/li&gt; 
 &lt;li&gt;2025.01.28: We have released the &lt;a href=&quot;https://huggingface.co/Qwen&quot;&gt;Qwen2.5-VL series&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-vl/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.12.25: We have released the &lt;a href=&quot;https://huggingface.co/Qwen/QVQ-72B-Preview&quot;&gt;QvQ-72B-Preview&lt;/a&gt;. QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qvq-72b-preview/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;2024.09.19: The instruction-tuned &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct&quot;&gt;Qwen2-VL-72B model&lt;/a&gt; and its quantized version [&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ&quot;&gt;AWQ&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8&quot;&gt;GPTQ-Int8&lt;/a&gt;] are now available. We have also released the &lt;a href=&quot;https://arxiv.org/pdf/2409.12191&quot;&gt;Qwen2-VL paper&lt;/a&gt; simultaneously.&lt;/li&gt; 
 &lt;li&gt;2024.08.30: We have released the &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&quot;&gt;Qwen2-VL series&lt;/a&gt;. The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2-vl/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Performance&lt;/h2&gt; 
&lt;h4&gt;Visual Tasks:&lt;/h4&gt; 
&lt;div style=&quot;display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;&quot;&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_vl.jpg&quot; width=&quot;48%&quot; /&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_vl.jpg&quot; width=&quot;48%&quot; /&gt; 
&lt;/div&gt; 
&lt;h4&gt;Pure Text Tasks:&lt;/h4&gt; 
&lt;div style=&quot;display: flex; justify-content: center; gap: 16px; flex-wrap: wrap;&quot;&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_nothinking_text.jpg&quot; width=&quot;48%&quot; /&gt; 
 &lt;img src=&quot;https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/table_thinking_text.jpg&quot; width=&quot;48%&quot; /&gt; 
&lt;/div&gt; 
&lt;h2&gt;Cookbooks&lt;/h2&gt; 
&lt;p&gt;We are preparing &lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks&quot;&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/omni_recognition.ipynb&quot;&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/document_parsing.ipynb&quot;&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/2d_grounding.ipynb&quot;&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/ocr.ipynb&quot;&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/video_understanding.ipynb&quot;&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mobile_agent.ipynb&quot;&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for mobile phone control.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/computer_use.ipynb&quot;&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Locate and think for controlling computers and Web.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/3d_grounding.ipynb&quot;&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/think_with_images.ipynb&quot;&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model‚Äôs precise comprehension of fine-grained visual details within images.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/mmcode.ipynb&quot;&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/long_document_understanding.ipynb&quot;&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen3-VL/raw/main/cookbooks/spatial_understanding.ipynb&quot;&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;See, understand and reason about the spatial information&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen3-VL with ü§ñ ModelScope and ü§ó Transformers.&lt;/p&gt; 
&lt;p&gt;The code of Qwen3-VL has been in the latest Hugging face transformers and we advise you to build from source with command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/huggingface/transformers
# pip install transformers==4.57.0 # currently, V4.57.0 is not released
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;Using ü§ó Transformers to Chat&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor

# default: Load the model on the available device(s)
model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.
# model = AutoModelForImageTextToText.from_pretrained(
#     &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
#     dtype=torch.bfloat16,
#     attn_implementation=&quot;flash_attention_2&quot;,
#     device_map=&quot;auto&quot;,
# )

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;!-- &lt;details&gt;
&lt;summary&gt;Minimum VRAM requirements&lt;/summary&gt;

| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |
|-----------|------------| --------- | -------- |
| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |
| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |
| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |
| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |

Note: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).
&lt;/details&gt; --&gt; 
&lt;details&gt; 
 &lt;summary&gt;Multi image inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing multiple images and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image1.jpg&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image2.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Identify the similarities between these images.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing a video url(or a local path) and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# for batch generation, padding_side should be set to left!
processor.tokenizer.padding_side = &#39;left&#39;

# Sample messages for batch inference
messages1 = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image1.jpg&quot;},
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/image2.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What are the common elements in these pictures?&quot;},
        ],
    }
]
messages2 = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are a helpful assistant.&quot;}]},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Who are you?&quot;}]},
]
# Combine messages for batch processing
messages = [messages1, messages2]

# Preparation for inference
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
    padding=True # padding should be set for batch generation!
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Pixel Control via Official Processor&lt;/summary&gt; 
 &lt;p&gt;Using the official HF processor, we can conveniently control the budget of visual tokens. Since the Qwen3-VL processor separates image and video processing, we can independently configure the pixel budget for each modality.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the image processor&lt;/strong&gt;:&lt;br /&gt; The parameter &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt; originally corresponds to &lt;code&gt;max_pixels&lt;/code&gt;, which defines the maximum number of pixels allowed for an image (i.e., for an image of height H and width W, H √ó W must not exceed &lt;code&gt;max_pixels&lt;/code&gt;; image channels are ignored for simplicity).&lt;br /&gt; Similarly, &lt;code&gt;size[&#39;shortest_edge&#39;]&lt;/code&gt; corresponds to &lt;code&gt;min_pixels&lt;/code&gt;, specifying the minimum allowable pixel count for an image.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;&lt;strong&gt;For the video processor&lt;/strong&gt;:&lt;br /&gt; The interpretation differs slightly. &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt; represents the maximum total number of pixels across all frames in a video ‚Äî for a video of shape T√óH√óW, the product T√óH√óW must not exceed &lt;code&gt;size[&#39;longest_edge&#39;]&lt;/code&gt;.&lt;br /&gt; Similarly, &lt;code&gt;size[&#39;shortest_edge&#39;]&lt;/code&gt; sets the minimum total pixel budget for the video.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

# budget for image processor, since the compression ratio is 32 for Qwen3-VL, we can set the number of visual tokens of a single image to 256-1280
processor.image_processor.size = {&quot;longest_edge&quot;: 1280*32*32, &quot;shortest_edge&quot;: 256*32*32}

# budget for video processor, we can set the number of visual tokens of a single video to 256-16384
processor.video_processor.size = {&quot;longest_edge&quot;: 16384*32*32, &quot;shortest_edge&quot;: 256*32*32}
&lt;/code&gt;&lt;/pre&gt; 
 &lt;ul&gt; 
  &lt;li&gt;You can further control the &lt;strong&gt;sample fps&lt;/strong&gt; or &lt;strong&gt;sample frames&lt;/strong&gt; of video, as shown below.&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# for video input, we can further control the fps or num_frames. \
# defaultly, fps is set to 2

# set fps = 4
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors=&quot;pt&quot;,
    fps=4
)
inputs = inputs.to(model.device)

# set num_frames = 128 and overwrite the fps to None!
# inputs = processor.apply_chat_template(
#     messages,
#     tokenize=True,
#     add_generation_prompt=True,
#     return_dict=True,
#     return_tensors=&quot;pt&quot;,
#     num_frames=128,
#     fps=None,
# )
# inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;New &lt;code&gt;qwen-vl-utils&lt;/code&gt; Usage&lt;/h3&gt; 
&lt;p&gt;With the latest &lt;code&gt;qwen-vl-utils&lt;/code&gt; toolkit (backward compatible with Qwen2.5-VL), you can control pixel constraints per visual input.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install qwen-vl-utils==0.0.14
# It&#39;s highly recommended to use `[decord]` feature for faster video loading.
# pip install qwen-vl-utils[decord]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compared to previous version, the new &lt;code&gt;qwen-vl-utils&lt;/code&gt; introduces:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&quot;image_path_size&quot;: &lt;code&gt;14&lt;/code&gt; for Qwen2.5-VL and &lt;code&gt;16&lt;/code&gt; for Qwen3-VL. Default set to &lt;code&gt;14&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&quot;return_video_metadata&quot;(Qwen3-VL only): Due to the new video processor, if True, each video returns as (video_tensor, video_metadata). Default set to &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# for Qwen2.5VL, you can simply call 
images, videos, video_kwargs = process_vision_info(messages, return_video_kwargs=True)

# For Qwen3VL series, you should call 
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;üìå Note: Since &lt;code&gt;qwen-vl-utils&lt;/code&gt; already resizes images/videos, pass &lt;code&gt;do_resize=False&lt;/code&gt; to the processor to avoid duplicate resizing.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Images&lt;/summary&gt; 
 &lt;p&gt;For input images, we support local files, base64, and URLs.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.
## Local file path
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;file:///path/to/your/image.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
## Image URL
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;http://path/to/your/image.jpg&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
## Base64 encoded image
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;data:image;base64,/9j/...&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We provide two methods for fine-grained control over the image size input to the model:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;Specify exact dimensions: Directly set resized_height and resized_width. These values will be rounded to the nearest multiple of 32 (32 for Qwen3VL, 28 for Qwen2.5VL).&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

# resized_height and resized_width
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
                &quot;resized_height&quot;: 280,
                &quot;resized_width&quot;: 420,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# min_pixels and max_pixels
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image&quot;,
                &quot;image&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;,
                &quot;min_pixels&quot;: 50176,
                &quot;max_pixels&quot;: 50176,

            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this image.&quot;},
        ],
    }
]

# Preparation for inference with qwen-vl-utils
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos = process_vision_info(messages, image_patch_size=16)

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, do_resize=False, return_tensors=&quot;pt&quot;)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Process Videos&lt;/summary&gt; 
 &lt;p&gt;For input videos, we support images lists, local path and url.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Messages containing a images list as a video and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: [
                    &quot;file:///path/to/frame1.jpg&quot;,
                    &quot;file:///path/to/frame2.jpg&quot;,
                    &quot;file:///path/to/frame3.jpg&quot;,
                    &quot;file:///path/to/frame4.jpg&quot;,
                ],
                &#39;sample_fps&#39;:&#39;1&#39;, # sample_fps: frame sampling rate (frames per second), used to determine timestamps for each frame
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Messages containing a local video path and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;file:///path/to/video1.mp4&quot;,
                &quot;max_pixels&quot;: 360 * 420,
                &quot;fps&quot;: 1.0,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

# Messages containing a video url and a text query
messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
                &quot;min_pixels&quot;: 4 * 32 * 32,
                &quot;max_pixels&quot;: 256 * 32 * 32,
                &quot;total_pixels&quot;: 20480 * 32 * 32,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;We recommend setting appropriate values for the &lt;code&gt;min_pixels&lt;/code&gt; and &lt;code&gt;max_pixels&lt;/code&gt; parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video.&lt;/p&gt; 
 &lt;p&gt;Alternatively, you can use the &lt;code&gt;total_pixels&lt;/code&gt; parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 32 * 32 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the &lt;code&gt;fetch_video&lt;/code&gt; function in &lt;code&gt;qwen_vl_utils/vision_process.py&lt;/code&gt;.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import AutoModelForImageTextToText, AutoProcessor
from qwen_vl_utils import process_vision_info

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, dtype=&quot;auto&quot;, device_map=&quot;auto&quot;
)

processor = AutoProcessor.from_pretrained(&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video&quot;,
                &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;,
                &quot;min_pixels&quot;: 4 * 32 * 32,
                &quot;max_pixels&quot;: 256 * 32 * 32,
                &quot;total_pixels&quot;: 20480 * 32 * 32,
            },
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Describe this video.&quot;},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
images, videos, video_kwargs = process_vision_info(messages, image_patch_size=16, return_video_kwargs=True, return_video_metadata=True)

# split the videos and according metadatas
if videos is not None:
    videos, video_metadatas = zip(*videos)
    videos, video_metadatas = list(videos), list(video_metadatas)
else:
    video_metadatas = None

# since qwen-vl-utils has resize the images/videos, \
# we should pass do_resize=False to avoid duplicate operation in processor!
inputs = processor(text=text, images=images, videos=videos, video_metadata=video_metadatas, return_tensors=&quot;pt&quot;, do_resize=False, **video_kwargs)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video Backends and URL Compatibility&lt;/summary&gt; 
 &lt;p&gt;Currently, &lt;code&gt;qwen-vl-utils&lt;/code&gt; supports three video decoding backends: &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, and &lt;code&gt;torchcodec&lt;/code&gt;. While &lt;code&gt;decord&lt;/code&gt; and &lt;code&gt;torchcodec&lt;/code&gt; generally offer significantly faster decoding speeds compared to &lt;code&gt;torchvision&lt;/code&gt;, we recommend using &lt;code&gt;torchcodec&lt;/code&gt;. This is because &lt;code&gt;decord&lt;/code&gt; has known issues, such as decoding hangs, and its project is no longer actively maintained.&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt; &lt;p&gt;For &lt;code&gt;decord&lt;/code&gt;, if you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-vl-utils&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href=&quot;https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source&quot;&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; &lt;/li&gt; 
  &lt;li&gt; &lt;p&gt;To use &lt;code&gt;torchcodec&lt;/code&gt; as the backend for video decoding, follow the installation instructions provided in the official &lt;a href=&quot;https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec&quot;&gt;torchcodec repository&lt;/a&gt; and install it manually. Note that &lt;code&gt;torchcodec&lt;/code&gt; depends on FFmpeg for decoding functionality.&lt;/p&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Video URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting &lt;code&gt;FORCE_QWENVL_VIDEO_READER&lt;/code&gt; to &lt;code&gt;torchvision&lt;/code&gt;, &lt;code&gt;decord&lt;/code&gt;, or &lt;code&gt;torchcodec&lt;/code&gt;.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchcodec&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h3&gt;More Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Add ids for Multiple Visual Inputs&lt;/h4&gt; 
&lt;p&gt;By default, images and video content are directly included in the conversation. When handling multiple images, it&#39;s helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Add vision ids&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;conversation = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [{&quot;type&quot;: &quot;image&quot;}, {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Hello, how are you?&quot;}],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;I&#39;m doing well, thank you for asking. How can I assist you today?&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Can you describe these images and video?&quot;},
            {&quot;type&quot;: &quot;image&quot;},
            {&quot;type&quot;: &quot;image&quot;},
            {&quot;type&quot;: &quot;video&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;These are from my vacation.&quot;},
        ],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;I&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;It was a trip to the mountains. Can you see the details in the images and video?&quot;,
    },
]

# default:
prompt_without_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True
)
# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;&amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;


# add ids
prompt_with_id = processor.apply_chat_template(
    conversation, add_generation_prompt=True, add_vision_id=True
)
# Excepted output: &#39;&amp;lt;|im_start|&amp;gt;system\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nPicture 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Hello, how are you?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;m doing well, thank you for asking. How can I assist you today?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nCan you describe these images and video?Picture 2: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Picture 3: &amp;lt;|vision_start|&amp;gt;&amp;lt;|image_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;Video 1: &amp;lt;|vision_start|&amp;gt;&amp;lt;|video_pad|&amp;gt;&amp;lt;|vision_end|&amp;gt;These are from my vacation.&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\nI&#39;d be happy to describe the images and video for you. Could you please provide more context about your vacation?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\nIt was a trip to the mountains. Can you see the details in the images and video?&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using Flash Attention-2, simply add &lt;code&gt;attn_implementation=&quot;flash_attention_2&quot;&lt;/code&gt; when loading the model as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from transformers import AutoModelForImageTextToText

model = AutoModelForImageTextToText.from_pretrained(
    &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;, 
    torch_dtype=torch.bfloat16, 
    attn_implementation=&quot;flash_attention_2&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Processing Long Texts&lt;/h4&gt; 
&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 256K tokens. To handle extensive inputs exceeding 256K tokens, we utilize &lt;a href=&quot;https://arxiv.org/abs/2309.00071&quot;&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; 
&lt;p&gt;For supported frameworks (currently transformers and vLLM), you could modify &lt;code&gt;max_position_embeddings&lt;/code&gt; and &lt;code&gt;rope_scaling&lt;/code&gt; in &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    &quot;max_position_embeddings&quot;: 1000000,
	...,
    &quot;rope_scaling&quot;: {
        &quot;rope_type&quot;: &quot;yarn&quot;,
        &quot;mrope_section&quot;: [
            24,
            20,
            20
        ],
        &quot;mrope_interleaved&quot;: true,
        &quot;factor&quot;: 3.0,
        &quot;original_max_position_embeddings&quot;: 262144
    },
    ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When using vLLM for serving, you can also enable YaRN by adding the additional arguments &lt;code&gt;--rope-scaling&lt;/code&gt; and &lt;code&gt;--max-model-len&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-235B-A22B-Instruct --rope-scaling &#39;{&quot;rope_type&quot;:&quot;yarn&quot;,&quot;factor&quot;:3.0,&quot;original_max_position_embeddings&quot;: 262144,&quot;mrope_section&quot;:[24,20,20],&quot;mrope_interleaved&quot;: true}&#39; --max-model-len 1000000
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Because Interleaved-MRoPE‚Äôs position IDs grow more slowly than vanilla RoPE, use a &lt;strong&gt;smaller scaling factor&lt;/strong&gt;. For example, to support 1M context with 256K context length, set factor=2 or 3 ‚Äî not 4.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Try Qwen3-VL-235B-A22 with API!&lt;/h3&gt; 
&lt;p&gt;To explore Qwen3-VL-235B-A22, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let&#39;s start the exciting journey right now!&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openai import OpenAI

# set your DASHSCOPE_API_KEY here
DASHSCOPE_API_KEY = &quot;&quot;

client = OpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
)

completion = client.chat.completions.create(
    model=&quot;qwen3-vl-235b-a22b-instruct&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
        {&quot;type&quot;: &quot;image_url&quot;,
         &quot;image_url&quot;: {&quot;url&quot;: &quot;https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg&quot;}},
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;ËøôÊòØ‰ªÄ‰πà&quot;},
    ]}]
)
print(completion.model_dump_json())
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more usage, please refer to the tutorial at &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api&quot;&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Web UI Example&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.&lt;/p&gt; 
&lt;p&gt;Install the required dependencies by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Launch a browser-based UI to interact with the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;python web_demo_mm.py -c /your/path/to/qwen3vl/weight
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open the link in your browser to interact with the model ‚Äî try text, images, or other features. For a quick start, you can also use our pre-built Docker image:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cd docker &amp;amp;&amp;amp; bash run_web_demo.sh -c /your/path/to/qwen3vl/weight --port 8881
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen3-VL deployment and inference. You need to install &lt;code&gt;vllm&amp;gt;0.10.2&lt;/code&gt; to enable Qwen3-VL support. You can also use our &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen3-VL/main/#-docker&quot;&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also check &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html&quot;&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-vl-utils==0.0.14
# pip install &#39;vllm&amp;gt;0.10.2&#39; # If this is not working use the below one. 
uv pip install -U vllm \
    --torch-backend=auto \
    --extra-index-url https://wheels.vllm.ai/nightly
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;You can start either a vLLM or SGLang server to serve LLMs efficiently, and then access it using an OpenAI-style API.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# FP8 requires NVIDIA H100+ and CUDA 12+
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-VL-235B-A22B-Instruct\
  --served-model-name Qwen/Qwen3-VL-235B-A22B-Instruct \
  --tensor-parallel-size 8 \
  --mm-encoder-tp-mode data \
  --enable-expert-parallel \
  --host 0.0.0.0 \
  --port 22002 \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.70 \
  --quantization fp8 \
  --distributed-executor-backend mp
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang server:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;python -m sglang.launch_server \
   --model-path Qwen/Qwen3-VL-235B-A22B-Instruct\
   --host 0.0.0.0 \
   --port 22002 \
   --tp 8 \
   --max-num-batched-tokens 8192 \
   --max-num-seqs 256
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Image Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:22002/v1&quot;,
    timeout=3600
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image_url&quot;,
                &quot;image_url&quot;: {
                    &quot;url&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;
                }
            },
            {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: &quot;Read all the text in the image.&quot;
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model=&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
    messages=messages,
    max_tokens=2048
)
print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
print(f&quot;Generated text: {response.choices[0].message.content}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;Video Request Example&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from openai import OpenAI

client = OpenAI(
    api_key=&quot;EMPTY&quot;,
    base_url=&quot;http://127.0.0.1:22002/v1&quot;,
    timeout=3600
)

messages = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;video_url&quot;,
                &quot;video_url&quot;: {
                    &quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4&quot;
                }
            },
            {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: &quot;How long is this video?&quot;
            }
        ]
    }
]

start = time.time()
response = client.chat.completions.create(
    model=&quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;,
    messages=messages,
    max_tokens=2048
)

print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
print(f&quot;Generated text: {response.choices[0].message.content}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Offline Inference&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM or SGLang to inference Qwen3-VL locally:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;vLLM Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# -*- coding: utf-8 -*-
import torch
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor
from vllm import LLM, SamplingParams

import os
os.environ[&#39;VLLM_WORKER_MULTIPROC_METHOD&#39;] = &#39;spawn&#39;

def prepare_inputs_for_vllm(messages, processor):
    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    # qwen_vl_utils 0.0.14+ reqired
    image_inputs, video_inputs, video_kwargs = process_vision_info(
        messages,
        image_patch_size=processor.image_processor.patch_size,
        return_video_kwargs=True,
        return_video_metadata=True
    )
    print(f&quot;video_kwargs: {video_kwargs}&quot;)

    mm_data = {}
    if image_inputs is not None:
        mm_data[&#39;image&#39;] = image_inputs
    if video_inputs is not None:
        mm_data[&#39;video&#39;] = video_inputs

    return {
        &#39;prompt&#39;: text,
        &#39;multi_modal_data&#39;: mm_data,
        &#39;mm_processor_kwargs&#39;: video_kwargs
    }


if __name__ == &#39;__main__&#39;:
    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                  &quot;type&quot;: &quot;image&quot;,
                  &quot;image&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;,
              },
              {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Read all the text in the image.&quot;},
            ],
        }
    ]

    checkpoint_path = &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;
    processor = AutoProcessor.from_pretrained(checkpoint_path)
    inputs = [prepare_inputs_for_vllm(message, processor) for message in [messages]]

    llm = LLM(
        model=checkpoint_path,
        trust_remote_code=True,
        gpu_memory_utilization=0.97,
        enforce_eager=False,
        max_model_len=8192,
        max_num_seqs=8,
        tensor_parallel_size=torch.cuda.device_count(),
        seed=0
    )

    sampling_params = SamplingParams(
        temperature=0,
        max_tokens=1024,
        top_k=-1,
        stop_token_ids=[],
    )

    for i, input_ in enumerate(inputs):
        print()
        print(&#39;=&#39; * 40)
        print(f&quot;Inputs[{i}]: {input_[&#39;prompt&#39;]=!r}&quot;)
    print(&#39;\n&#39; + &#39;&amp;gt;&#39; * 40)

    outputs = llm.generate(inputs, sampling_params=sampling_params)
    for i, output in enumerate(outputs):
        generated_text = output.outputs[0].text
        print()
        print(&#39;=&#39; * 40)
        print(f&quot;Generated text: {generated_text!r}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;SGLang Examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import time
from PIL import Image
from sglang import Engine
from qwen_vl_utils import process_vision_info
from transformers import AutoProcessor, AutoConfig


if __name__ == &quot;__main__&quot;:
    # TODO: change to your own checkpoint path
    checkpoint_path = &quot;Qwen/Qwen3-VL-235B-A22B-Instruct&quot;
    processor = AutoProcessor.from_pretrained(checkpoint_path)

    messages = [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                  &quot;type&quot;: &quot;image&quot;,
                  &quot;image&quot;: &quot;https://ofasys-multimodal-wlcb-3-toshanghai.oss-accelerate.aliyuncs.com/wpf272043/keepme/image/receipt.png&quot;,
              },
              {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Read all the text in the image.&quot;},
            ],
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    image_inputs, _ = process_vision_info(messages, image_patch_size=processor.image_processor.patch_size)

    llm = Engine(
        model_path=checkpoint_path,
        enable_multimodal=True,
        mem_fraction_static=0.8,
        tp_size=4,
        attention_backend=&quot;fa3&quot;,
        context_length=10240,
        disable_cuda_graph=True,
    )

    start = time.time()
    sampling_params = {&quot;max_new_tokens&quot;: 1024}
    response = llm.generate(prompt=text, image_data=image_inputs, sampling_params=sampling_params)
    print(f&quot;Response costs: {time.time() - start:.2f}s&quot;)
    print(f&quot;Generated text: {response[&#39;text&#39;]}&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;üê≥ Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href=&quot;https://hub.docker.com/r/qwenllm/qwenvl&quot;&gt;qwenllm/qwenvl&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen3vl -it qwenllm/qwenvl:qwen3vl-cu128 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-BibTeX&quot;&gt;
@article{Qwen2.5-VL,
  title={Qwen2.5-VL Technical Report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{Qwen2-VL,
  title={Qwen2-VL: Enhancing Vision-Language Model&#39;s Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{Qwen-VL,
  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>Lifelong-Robot-Learning/LIBERO</title>
      <link>https://github.com/Lifelong-Robot-Learning/LIBERO</link>
      <description>&lt;p&gt;Benchmarking Knowledge Transfer in Lifelong Robot Learning&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/raw/master/images/libero_logo.png&quot; width=&quot;360&quot; /&gt; 
 &lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/actions&quot;&gt; &lt;img alt=&quot;Tests Passing&quot; src=&quot;https://github.com/anuraghazra/github-readme-stats/workflows/Test/badge.svg?sanitize=true&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/graphs/contributors&quot;&gt; &lt;img alt=&quot;GitHub Contributors&quot; src=&quot;https://img.shields.io/github/contributors/Lifelong-Robot-Learning/LIBERO&quot; /&gt; &lt;/a&gt; &lt;a href=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&quot;&gt; &lt;img alt=&quot;Issues&quot; src=&quot;https://img.shields.io/github/issues/Lifelong-Robot-Learning/LIBERO?color=0088ff&quot; /&gt; &lt;/a&gt;&lt;/p&gt;
 &lt;h2&gt;&lt;a href=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&quot;&gt;&lt;strong&gt;Benchmarking Knowledge Transfer for Lifelong Robot Learning&lt;/strong&gt;&lt;/a&gt;&lt;/h2&gt;
 &lt;a href=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&quot;&gt; &lt;p&gt;Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone&lt;/p&gt; &lt;/a&gt;
 &lt;p&gt;&lt;a href=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/issues&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://libero-project.github.io&quot;&gt;[Website]&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/pdf/2306.03310.pdf&quot;&gt;[Paper]&lt;/a&gt; &lt;a href=&quot;https://lifelong-robot-learning.github.io/LIBERO/&quot;&gt;[Docs]&lt;/a&gt;&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;p&gt;&lt;img src=&quot;https://github.com/Lifelong-Robot-Learning/LIBERO/raw/master/images//fig1.png&quot; alt=&quot;pull_figure&quot; /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;LIBERO&lt;/strong&gt; is designed for studying knowledge transfer in multitask and lifelong robot learning problems. Successfully resolving these problems require both declarative knowledge about objects/spatial relationships and procedural knowledge about motion/behaviors. &lt;strong&gt;LIBERO&lt;/strong&gt; provides:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a procedural generation pipeline that could in principle generate an infinite number of manipulation tasks.&lt;/li&gt; 
 &lt;li&gt;130 tasks grouped into four task suites: &lt;strong&gt;LIBERO-Spatial&lt;/strong&gt;, &lt;strong&gt;LIBERO-Object&lt;/strong&gt;, &lt;strong&gt;LIBERO-Goal&lt;/strong&gt;, and &lt;strong&gt;LIBERO-100&lt;/strong&gt;. The first three task suites have controlled distribution shifts, meaning that they require the transfer of a specific type of knowledge. In contrast, &lt;strong&gt;LIBERO-100&lt;/strong&gt; consists of 100 manipulation tasks that require the transfer of entangled knowledge. &lt;strong&gt;LIBERO-100&lt;/strong&gt; is further splitted into &lt;strong&gt;LIBERO-90&lt;/strong&gt; for pretraining a policy and &lt;strong&gt;LIBERO-10&lt;/strong&gt; for testing the agent&#39;s downstream lifelong learning performance.&lt;/li&gt; 
 &lt;li&gt;five research topics.&lt;/li&gt; 
 &lt;li&gt;three visuomotor policy network architectures.&lt;/li&gt; 
 &lt;li&gt;three lifelong learning algorithms with the sequential finetuning and multitask learning baselines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Contents&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Dataset&quot;&gt;Datasets&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Getting-Started&quot;&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Task&quot;&gt;Task&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Training&quot;&gt;Training&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Evaluation&quot;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#Citation&quot;&gt;Citation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/#License&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installtion&lt;/h1&gt; 
&lt;p&gt;Please run the following commands in the given order to install the dependency for &lt;strong&gt;LIBERO&lt;/strong&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda create -n libero python=3.8.13
conda activate libero
git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git
cd LIBERO
pip install -r requirements.txt
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then install the &lt;code&gt;libero&lt;/code&gt; package:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Datasets&lt;/h1&gt; 
&lt;p&gt;We provide high-quality human teleoperation demonstrations for the four task suites in &lt;strong&gt;LIBERO&lt;/strong&gt;. To download the demonstration dataset, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;python benchmark_scripts/download_libero_datasets.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, the dataset will be stored under the &lt;code&gt;LIBERO&lt;/code&gt; folder and all four datasets will be downloaded. To download a specific dataset, use&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;python benchmark_scripts/download_libero_datasets.py --datasets DATASET
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;where &lt;code&gt;DATASET&lt;/code&gt; is chosen from &lt;code&gt;[libero_spatial, libero_object, libero_100, libero_goal&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NEW!!!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Alternatively, you can download the dataset from HuggingFace by using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;python benchmark_scripts/download_libero_datasets.py --use-huggingface
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This option can also be combined with the specific dataset selection:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;python benchmark_scripts/download_libero_datasets.py --datasets DATASET --use-huggingface
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The datasets hosted on HuggingFace are available at &lt;a href=&quot;https://huggingface.co/datasets/yifengzhu-hf/LIBERO-datasets&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Getting Started&lt;/h1&gt; 
&lt;p&gt;For a detailed walk-through, please either refer to the documentation or the notebook examples provided under the &lt;code&gt;notebooks&lt;/code&gt; folder. In the following, we provide example scripts for retrieving a task, training and evaluation.&lt;/p&gt; 
&lt;h2&gt;Task&lt;/h2&gt; 
&lt;p&gt;The following is a minimal example of retrieving a specific task from a specific task suite.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from libero.libero import benchmark
from libero.libero.envs import OffScreenRenderEnv


benchmark_dict = benchmark.get_benchmark_dict()
task_suite_name = &quot;libero_10&quot; # can also choose libero_spatial, libero_object, etc.
task_suite = benchmark_dict[task_suite_name]()

# retrieve a specific task
task_id = 0
task = task_suite.get_task(task_id)
task_name = task.name
task_description = task.language
task_bddl_file = os.path.join(get_libero_path(&quot;bddl_files&quot;), task.problem_folder, task.bddl_file)
print(f&quot;[info] retrieving task {task_id} from suite {task_suite_name}, the &quot; + \
      f&quot;language instruction is {task_description}, and the bddl file is {task_bddl_file}&quot;)

# step over the environment
env_args = {
    &quot;bddl_file_name&quot;: task_bddl_file,
    &quot;camera_heights&quot;: 128,
    &quot;camera_widths&quot;: 128
}
env = OffScreenRenderEnv(**env_args)
env.seed(0)
env.reset()
init_states = task_suite.get_task_init_states(task_id) # for benchmarking purpose, we fix the a set of initial states
init_state_id = 0
env.set_init_state(init_states[init_state_id])

dummy_action = [0.] * 7
for step in range(10):
    obs, reward, done, info = env.step(dummy_action)
env.close()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Currently, we only support sparse reward function (i.e., the agent receives &lt;code&gt;+1&lt;/code&gt; when the task is finished). As sparse-reward RL is extremely hard to learn, currently we mainly focus on lifelong imitation learning.&lt;/p&gt; 
&lt;h2&gt;Training&lt;/h2&gt; 
&lt;p&gt;To start a lifelong learning experiment, please choose:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;BENCHMARK&lt;/code&gt; from &lt;code&gt;[LIBERO_SPATIAL, LIBERO_OBJECT, LIBERO_GOAL, LIBERO_90, LIBERO_10]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;POLICY&lt;/code&gt; from &lt;code&gt;[bc_rnn_policy, bc_transformer_policy, bc_vilt_policy]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;ALGO&lt;/code&gt; from &lt;code&gt;[base, er, ewc, packnet, multitask]&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;then run the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;export CUDA_VISIBLE_DEVICES=GPU_ID &amp;amp;&amp;amp; \
export MUJOCO_EGL_DEVICE_ID=GPU_ID &amp;amp;&amp;amp; \
python libero/lifelong/main.py seed=SEED \
                               benchmark_name=BENCHMARK \
                               policy=POLICY \
                               lifelong=ALGO
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see the documentation for the details of reproducing the study results.&lt;/p&gt; 
&lt;h2&gt;Evaluation&lt;/h2&gt; 
&lt;p&gt;By default the policies will be evaluated on the fly during training. If you have limited computing resource of GPUs, we offer an evaluation script for you to evaluate models separately.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;python libero/lifelong/evaluate.py --benchmark BENCHMARK_NAME \
                                   --task_id TASK_ID \ 
                                   --algo ALGO_NAME \
                                   --policy POLICY_NAME \
                                   --seed SEED \
                                   --ep EPOCH \
                                   --load_task LOAD_TASK \
                                   --device_id CUDA_ID
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find &lt;strong&gt;LIBERO&lt;/strong&gt; to be useful in your own research, please consider citing our paper:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{liu2023libero,
  title={LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning},
  author={Liu, Bo and Zhu, Yifeng and Gao, Chongkai and Feng, Yihao and Liu, Qiang and Zhu, Yuke and Stone, Peter},
  journal={arXiv preprint arXiv:2306.03310},
  year={2023}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;License&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Codebase&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Lifelong-Robot-Learning/LIBERO/master/LICENSE&quot;&gt;MIT License&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Datasets&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/legalcode&quot;&gt;Creative Commons Attribution 4.0 International (CC BY 4.0)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>panaversity/learn-agentic-ai</title>
      <link>https://github.com/panaversity/learn-agentic-ai</link>
      <description>&lt;p&gt;Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern and Agent-Native Cloud Technologies: OpenAI Agents SDK, Memory, MCP, A2A, Knowledge Graphs, Dapr, Rancher Desktop, and Kubernetes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern: From Start to Scale&lt;/h1&gt; 
&lt;p&gt;This repo is part of the &lt;a href=&quot;https://panaversity.org/&quot;&gt;Panaversity Certified Agentic &amp;amp; Robotic AI Engineer&lt;/a&gt; program. You can also review the certification and course details in the &lt;a href=&quot;https://docs.google.com/document/d/1BygAckkfc_NFQnTfEM6qqUvPdlIHpNItmRtvfRMGp38/edit?usp=sharing&quot;&gt;program guide&lt;/a&gt;. This repo provides learning material for Agentic AI and Cloud courses.&lt;/p&gt; 
&lt;p&gt;Here‚Äôs a polished, professional rewrite you can use as a one-pager or slide‚Äîtight on wording, clear on stakes, and just a touch playful so it doesn‚Äôt read like it was written by a committee (no offense to committees üòÑ).&lt;/p&gt; 
&lt;h1&gt;Our Agentic Strategy for Pakistan: Four Working Hypotheses&lt;/h1&gt; 
&lt;p&gt;Pakistan must place smart, early bets on the technologies and talent that will define the agentic AI era‚Äîbecause we intend to train &lt;strong&gt;millions&lt;/strong&gt; of agentic-AI developers across the country and abroad, and launch startups at scale (ambitious, yes‚Äîbut coffee is cheaper than regret).&lt;/p&gt; 
&lt;h2&gt;Hypothesis 1 ‚Äî Agentic AI is the trajectory&lt;/h2&gt; 
&lt;p&gt;We believe the future of AI is &lt;strong&gt;agentic&lt;/strong&gt;: systems that plan, coordinate tools, and take actions to deliver outcomes, not just answers (aka ‚Äúfrom chat to getting things done‚Äù‚Äîand ideally without breaking anything valuable). This hypothesis guides our curriculum design, tooling choices, and venture focus.&lt;/p&gt; 
&lt;h2&gt;Hypothesis 2 ‚Äî Cloud-native rails: Kubernetes √ó Dapr √ó Ray&lt;/h2&gt; 
&lt;p&gt;Our bet for large-scale agentic systems is a cloud-native stack: &lt;strong&gt;Kubernetes&lt;/strong&gt; for orchestration, &lt;strong&gt;Dapr&lt;/strong&gt; (Actors, Workflows, and Agents) for reliable micro-primitives, and &lt;strong&gt;Ray&lt;/strong&gt; for elastic distributed compute. Together, these provide the building blocks for durable, observable, horizontally scalable agent swarms.&lt;/p&gt; 
&lt;h2&gt;Hypothesis 3 ‚Äî The real blocker is the &lt;strong&gt;learning gap&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;Most AI pilots fail not because the models are incapable, but because teams don‚Äôt know &lt;strong&gt;how&lt;/strong&gt; to integrate AI into workflows, controls, and economics. Recent coverage of an MIT study reports that &lt;strong&gt;~95%&lt;/strong&gt; of enterprise gen-AI implementations show no measurable P&amp;amp;L impact‚Äîlargely due to poor problem selection and integration practices, not model quality. Our program is designed to close this gap with workflow design, safety guardrails, and ROI-first delivery. &lt;a href=&quot;https://fortune.com/2025/08/21/an-mit-report-that-95-of-ai-pilots-fail-spooked-investors-but-the-reason-why-those-pilots-failed-is-what-should-make-the-c-suite-anxious/&quot;&gt;An MIT report that 95% of AI pilots fail spooked investors. But it‚Äôs the reason why those pilots failed that should make the C-suite anxious&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Hypothesis 4 ‚Äî The web is becoming &lt;strong&gt;agentic and interoperable&lt;/strong&gt;&lt;/h2&gt; 
&lt;p&gt;The next web is a fabric of interoperable agents coordinating via open protocols‚Äî&lt;strong&gt;MCP&lt;/strong&gt; for standardized tool/context access, &lt;strong&gt;A2A&lt;/strong&gt; for authenticated agent-to-agent collaboration, and &lt;strong&gt;NANDA&lt;/strong&gt; for identity, authorization, and verifiable audit. These emerging standards enable composable automation across apps, devices, and clouds‚Äîshifting the browser from a tab list to an &lt;strong&gt;outcome orchestrator&lt;/strong&gt; with trust and consent built in (finally, fewer tabs, more results).&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;What this means for execution&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Talent engine:&lt;/strong&gt; hands-on training in agentic patterns (planning, tools, memory, evaluation), workflow design, and safety‚Äîtied to real industry use-cases (because ‚ÄúHello, World‚Äù doesn‚Äôt move P&amp;amp;L).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Reference stack:&lt;/strong&gt; Kubernetes + Dapr + Ray blueprints with observability, guardrails, and cost controls‚Äîshippable by small teams (and auditable by large ones).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Protocol readiness:&lt;/strong&gt; MCP/A2A/NANDA-aware agent designs to ensure our solutions interoperate as the standards mature (future-proof beats future-guess).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If any hypothesis is wrong, we‚Äôll measure, publish, and pivot fast‚Äîbecause the only unforgivable error is not learning.&lt;/p&gt; 
&lt;h2&gt;This Panaversity Initiative Tackles the Critical Challenge:&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;‚ÄúHow do we design AI Agents that can handle 10 million concurrent AI Agents without failing?‚Äù&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note: The challenge is intensified as we must guide our students to solve this issue with minimal financial resources available during training.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/cover.png&quot; width=&quot;600&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;Kubernetes with Dapr can theoretically handle 10 million concurrent agents in an agentic AI system without failing, but achieving this requires extensive optimization, significant infrastructure, and careful engineering. While direct evidence at this scale is limited, logical extrapolation from existing benchmarks, Kubernetes‚Äô scalability, and Dapr‚Äôs actor model supports feasibility, especially with rigorous tuning and resource allocation.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Condensed Argument with Proof and Logic&lt;/strong&gt;:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Kubernetes Scalability&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Kubernetes supports up to 5,000 nodes and 150,000 pods per cluster (Kubernetes docs), with real-world examples like PayPal scaling to 4,000 nodes and 200,000 pods (InfoQ, 2023) and KubeEdge managing 100,000 edge nodes and 1 million pods (KubeEdge case studies). OpenAI‚Äôs 2,500-node cluster for AI workloads (OpenAI blog, 2022) shows Kubernetes can handle compute-intensive tasks.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: For 10 million users, a cluster of 5,000‚Äì10,000 nodes (e.g., AWS g5 instances with GPUs) can distribute workloads. Each node can run hundreds of pods, and Kubernetes‚Äô horizontal pod autoscaling (HPA) dynamically adjusts to demand. Bottlenecks (e.g., API server, networking) can be mitigated by tuning etcd, using high-performance CNIs like Cilium, and optimizing DNS.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dapr‚Äôs Efficiency for Agentic AI&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Dapr‚Äôs actor model supports thousands of virtual actors per CPU core with double-digit millisecond latency (Dapr docs, 2024). Case studies show Dapr handling millions of events, e.g., Tempestive‚Äôs IoT platform processing billions of messages (Dapr blog, 2023) and DeFacto‚Äôs system managing 3,700 events/second (320 million daily) on Kubernetes with Kafka (Microsoft case study, 2022).&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Agentic AI relies on stateful, low-latency agents. Dapr Agents, built on the actor model, can represent 10 million users as actors, distributed across a Kubernetes cluster. Dapr‚Äôs state management (e.g., Redis) and pub/sub messaging (e.g., Kafka) ensure efficient coordination and resilience, with automatic retries preventing failures. Sharding state stores and message brokers scales to millions of operations/second.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Handling AI Workloads&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: LLM inference frameworks like vLLM and TGI serve thousands of requests/second per GPU (vLLM benchmarks, 2024). Kubernetes orchestrates GPU workloads effectively, as seen Kubernetes manages GPU workloads, as seen in NVIDIA‚Äôs AI platform scaling to thousands of GPUs (NVIDIA case study, 2023).&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Assuming each user generates 1 request/second requiring 0.01 GPU, 10 million users need ~100,000 GPUs. Batching, caching, and model parallelism reduce this to a feasible ~10,000‚Äì20,000 GPUs, achievable in hyperscale clouds (e.g., AWS). Kubernetes‚Äô resource scheduling ensures optimal GPU utilization.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking and Storage&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: EMQX on Kubernetes handled 1 million concurrent connections with tuning (EMQX blog, 2024). C10M benchmarks (2013) achieved 10 million connections using optimized stacks. Dapr‚Äôs state stores (e.g., Redis) support millions of operations/second (Redis benchmarks, 2024).&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: 10 million connections require ~100‚Äì1,000 Gbps bandwidth, supported by modern clouds. High-throughput databases (e.g., CockroachDB) and caching (e.g., Redis Cluster) handle 10 TB of state data for 10 million users (1 KB/user). Kernel bypass (e.g., DPDK) and eBPF-based CNIs (e.g., Cilium) minimize networking latency.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Resilience and Monitoring&lt;/strong&gt;:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Dapr‚Äôs resiliency policies (retries, circuit breakers) and Kubernetes‚Äô self-healing (pod restarts) ensure reliability (Dapr docs, 2024). Dapr‚Äôs OpenTelemetry integration scales monitoring for millions of agents (Prometheus case studies, 2023).&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Real-time metrics (e.g., latency, error rates) and distributed tracing prevent cascading failures. Kubernetes‚Äô liveness probes and Dapr‚Äôs workflow engine recover from crashes, ensuring 99.999% uptime.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Feasibility with Constraints&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: No direct benchmark exists for 10 million concurrent users with Dapr/Kubernetes in an agentic AI context. Infrastructure costs (e.g., $10M‚Äì$100M for 10,000 nodes) are prohibitive for low-budget scenarios.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Use open-source tools (e.g., Minikube, kind) for local testing and cloud credits (e.g., AWS Educate) for students. Simulate 10 million users with tools like Locust on smaller clusters (e.g., 100 nodes), extrapolating results. Optimize Dapr‚Äôs actor placement and Kubernetes‚Äô resource quotas to maximize efficiency on limited hardware. Leverage free-tier databases (e.g., MongoDB Atlas) and message brokers (e.g., RabbitMQ).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Kubernetes with Dapr can handle 10 million concurrent users in an agentic AI system, supported by their proven scalability, real-world case studies, and logical extrapolation. For students with minimal budgets, small-scale simulations, open-source tools, and cloud credits make the problem tractable, though production-scale deployment requires hyperscale resources and expertise.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Agentic AI Top Trend of 2025&lt;/strong&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/toptrend.webp&quot; width=&quot;200&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;The Dapr Agentic Cloud Ascent (DACA) Design Pattern Addresses 10 Million AI Agents Challenge&lt;/h2&gt; 
&lt;p&gt;Let&#39;s understand and learn about &quot;Dapr Agentic Cloud Ascent (DACA)&quot;, our winning design pattern for developing and deploying planet scale multi-agent systems.&lt;/p&gt; 
&lt;h3&gt;Executive Summary: Dapr Agentic Cloud Ascent (DACA)&lt;/h3&gt; 
&lt;p&gt;The Dapr Agentic Cloud Ascent (DACA) guide introduces a strategic design pattern for building and deploying sophisticated, scalable, and resilient agentic AI systems. Addressing the complexities of modern AI development, DACA integrates the OpenAI Agents SDK for core agent logic with the Model Context Protocol (MCP) for standardized tool use and the Agent2Agent (A2A) protocol for seamless inter-agent communication, all underpinned by the distributed capabilities of Dapr. &lt;strong&gt;Grounded in AI-first and cloud-first principles&lt;/strong&gt;, DACA promotes the use of stateless, containerized applications deployed on platforms like Azure Container Apps (Serverless Containers) or Kubernetes, enabling efficient scaling from local development to planetary-scale production, potentially leveraging free-tier cloud services and self-hosted LLMs for cost optimization. The pattern emphasizes modularity, context-awareness, and standardized communication, envisioning an &lt;strong&gt;Agentia World&lt;/strong&gt; where diverse AI agents collaborate intelligently. Ultimately, DACA offers a robust, flexible, and cost-effective framework for developers and architects aiming to create complex, cloud-native agentic AI applications that are built for scalability and resilience from the ground up.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/panaversity/learn-agentic-ai/raw/main/comprehensive_guide_daca.md&quot;&gt;Comprehensive Guide to Dapr Agentic Cloud Ascent (DACA) Design Pattern&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/ascent.png&quot; width=&quot;500&quot; /&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/architecture1.png&quot; width=&quot;400&quot; /&gt; &lt;/p&gt; 
&lt;h3&gt;Target User&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Agentic AI Developer and AgentOps Professionals&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Why OpenAI Agents SDK should be the main framework for agentic development for most use cases?&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Table 1: Comparison of Abstraction Levels in AI Agent Frameworks&lt;/strong&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Framework&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Abstraction Level&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Key Characteristics&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Learning Curve&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Control Level&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Minimal&lt;/td&gt; 
   &lt;td&gt;Python-first, core primitives (Agents, Handoffs, Guardrails), direct control&lt;/td&gt; 
   &lt;td&gt;Low&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;Role-based agents, crews, tasks, focus on collaboration&lt;/td&gt; 
   &lt;td&gt;Low-Medium&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;AutoGen&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;High&lt;/td&gt; 
   &lt;td&gt;Conversational agents, flexible conversation patterns, human-in-the-loop support&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Google ADK&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;Multi-agent hierarchies, Google Cloud integration (Gemini, Vertex AI), rich tool ecosystem, bidirectional streaming&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Medium-High&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Low-Moderate&lt;/td&gt; 
   &lt;td&gt;Graph-based workflows, nodes, edges, explicit state management&lt;/td&gt; 
   &lt;td&gt;Very High&lt;/td&gt; 
   &lt;td&gt;Very High&lt;/td&gt; 
   &lt;td&gt;Low&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Dapr Agents&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Moderate&lt;/td&gt; 
   &lt;td&gt;Stateful virtual actors, event-driven multi-agent workflows, Kubernetes integration, 50+ data connectors, built-in resiliency&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
   &lt;td&gt;Medium-High&lt;/td&gt; 
   &lt;td&gt;Medium&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The table clearly identifies why OpenAI Agents SDK should be the main framework for agentic development for most use cases:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It excels in &lt;strong&gt;simplicity&lt;/strong&gt; and &lt;strong&gt;ease of use&lt;/strong&gt;, making it the best choice for rapid development and broad accessibility.&lt;/li&gt; 
 &lt;li&gt;It offers &lt;strong&gt;high control&lt;/strong&gt; with &lt;strong&gt;minimal abstraction&lt;/strong&gt;, providing the flexibility needed for agentic development without the complexity of frameworks like LangGraph.&lt;/li&gt; 
 &lt;li&gt;It outperforms most alternatives (CrewAI, AutoGen, Google ADK, Dapr Agents) in balancing usability and power, and while LangGraph offers more control, its complexity makes it less practical for general use.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your priority is ease of use, flexibility, and quick iteration in agentic development, OpenAI Agents SDK is the clear winner based on the table. However, if your project requires enterprise-scale features (e.g., Dapr Agents) or maximum control for complex workflows (e.g., LangGraph), you might consider those alternatives despite their added complexity.&lt;/p&gt; 
&lt;h2&gt;Core DACA Agentic AI Courses:&lt;/h2&gt; 
&lt;h3&gt;AI-201: Fundamentals of Agentic AI and DACA AI-First Development (14 weeks)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚Å†Agentic &amp;amp; DACA Theory - 1 week&lt;/li&gt; 
 &lt;li&gt;UV &amp;amp; ‚Å†OpenAI Agents SDK - 5 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Agentic Design Patterns - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Memory [LangMem &amp;amp; mem0] 1 week&lt;/li&gt; 
 &lt;li&gt;Postgres/Redis (Managed Cloud) - 1 week&lt;/li&gt; 
 &lt;li&gt;FastAPI (Basic) - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Containerization (Rancher Desktop) - 1 week&lt;/li&gt; 
 &lt;li&gt;Hugging Face Docker Spaces - 1 week&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6&quot;&gt;AI-201 Video Playlist&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Note: These videos are for additional learning, and do not cover all the material taught in the onsite classes.&lt;/p&gt; 
&lt;p&gt;Prerequisite: Successful completion of &lt;a href=&quot;https://github.com/panaversity/learn-modern-ai-python&quot;&gt;AI-101: Modern AI Python Programming - Your Launchpad into Intelligent Systems&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;AI-202: DACA Cloud-First Agentic AI Development (14 weeks)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Rancher Desktop with Local Kubernetes - 4 weeks&lt;/li&gt; 
 &lt;li&gt;Advanced FastAPI with Kubernetes - 2 weeks&lt;/li&gt; 
 &lt;li&gt;Dapr [workflows, state, pubsub, secrets] - 3 Week&lt;/li&gt; 
 &lt;li&gt;CockRoachdb &amp;amp; RabbitMQ Managed Services - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Model Context Protocol - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Serverless Containers Deployment (ACA) - 2 weeks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Prerequisite: Successful completion of AI-201&lt;/p&gt; 
&lt;h3&gt;AI-301 DACA Planet-Scale Distributed AI Agents (14 Weeks)&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;‚Å†Certified Kubernetes Application Developer (CKAD) - 4 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†A2A Protocol - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Voice Agents - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Dapr Agents/Google ADK - 2 weeks&lt;/li&gt; 
 &lt;li&gt;‚Å†Self-LLMs Hosting - 1 week&lt;/li&gt; 
 &lt;li&gt;Finetuning LLMs - 3 weeks&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Prerequisite: Successful completion of AI-201 &amp;amp; AI-202&lt;/p&gt; 
&lt;h2&gt;Evaluations&lt;/h2&gt; 
&lt;p&gt;Quizzes + Hackathons (Everything is Onsite)&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Advanced Modern Python (including asyncio) [Q1]&lt;/li&gt; 
 &lt;li&gt;OpenAI Agents SDK (48 MCQ in 2 hour) [01_ai_agents_first]&lt;/li&gt; 
 &lt;li&gt;Protocols &amp;amp; Design Patterns (A2A and MCP) [05_ai_protocols]&lt;/li&gt; 
 &lt;li&gt;Hackathon1 - 8 Hours (Using Above Quiz Stack)&lt;/li&gt; 
 &lt;li&gt;Containerization + FastAPI [05_daca_agent_native_dev = 01 + 02 ]&lt;/li&gt; 
 &lt;li&gt;Kubernetes (Rancher Desktop) [Stimulations] [05_daca_agent_native_dev = 02 ]&lt;/li&gt; 
 &lt;li&gt;Dapr-1 - State, PubSub, Bindings, Invocation [05_daca_agent_native_dev = 03 ]&lt;/li&gt; 
 &lt;li&gt;Dapr-2 - Workflows, Virtual Actors [04_agent_native = 04, 05, 06]&lt;/li&gt; 
 &lt;li&gt;Hackathon2 - 8 Hours (Agent Native Startup)&lt;/li&gt; 
 &lt;li&gt;CKAD + DAPR + ArgoCD (Simulations) [06_daca_deployment_guide + 07_ckad]&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Quiz Details&lt;/h2&gt; 
&lt;h3&gt;Fundamentals of Agentic AI Quiz&lt;/h3&gt; 
&lt;p&gt;Total Questions: 48 MCQs&lt;/p&gt; 
&lt;p&gt;Duration: 120 Minutes&lt;/p&gt; 
&lt;p&gt;Difficulty Level: Intermediate or Advanced (NOT beginner-level)&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PL0vKVrkG4hWr4V2I4P6GaDzMG_LijlGTm&quot;&gt;Quiz Preparation Playlist&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This is a well-constructed, comprehensive quiz that accurately tests deep knowledge of the OpenAI Agents SDK. However, it&#39;s significantly more challenging than typical beginner-level assessments.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Difficulty Level for Beginners&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The quiz is challenging for beginners due to the following factors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Technical Depth&lt;/strong&gt;: Questions require understanding the OpenAI Agents SDK‚Äôs architecture (e.g., Agents, Tools, Handoffs, Runner), Pydantic models, async programming, and prompt engineering. These are advanced topics for someone new to AI or Python.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Conceptual Complexity&lt;/strong&gt;: Topics like dynamic instructions, context management, error handling, and Chain-of-Thought prompting require familiarity with both theoretical and practical aspects of agentic AI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code Analysis&lt;/strong&gt;: Many questions involve analyzing code snippets, understanding execution paths, and predicting outcomes, which demand strong Python and debugging skills. Domain Knowledge: Questions on Markdown are simpler, but the majority focus on niche SDK features, making the quiz specialized.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Beginner Challenges&lt;/strong&gt;: Beginners (e.g., those with basic Python knowledge and minimal AI experience) would struggle with SDK-specific concepts like Runner.run_sync, tool_choice, and Pydantic validation, as well as async programming and multi-agent workflows.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Difficulty Rating&lt;/strong&gt;: Advanced (not beginner-friendly). Beginners would need foundational knowledge in Python, async programming, and LLMs, plus specific training on the OpenAI Agents SDK to perform well.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To excel in this quiz, focus on understanding the core components and philosophy of the OpenAI Agents SDK, such as its &quot;Python-first&quot; design for orchestration, the roles of Agents and Tools, and how primitives like &quot;Handoffs&quot; facilitate multi-agent collaboration. Pay close attention to how the SDK manages the agent loop, handles tool calls and Pydantic models for typed inputs/outputs, and uses context objects. Review concepts like dynamic instructions, agent cloning, error handling during tool execution, and the nuances of Runner.run_sync() versus streaming. Additionally, refresh your knowledge of prompt engineering techniques, including crafting clear instructions, guiding the agent&#39;s reasoning (e.g., Chain-of-Thought), and managing sensitive data through persona and careful prompting. Finally, ensure you&#39;re comfortable with basic Markdown syntax for links and images.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Preparation Guide for Beginner Students&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;This OpenAI Agents SDK quiz is designed for intermediate to advanced learners and requires substantial preparation to succeed. Before attempting this assessment, ensure you have a solid foundation in Python programming, including object-oriented concepts, async/await patterns, decorators, and error handling. You&#39;ll need to thoroughly study Pydantic models for data validation, understanding field definitions, default values, and validation behavior. Dedicate significant time to the OpenAI Agents SDK documentation (&lt;a href=&quot;https://openai.github.io/openai-agents-python/&quot;&gt;https://openai.github.io/openai-agents-python/&lt;/a&gt;), focusing on core concepts like Agents, Tools, Handoffs, context management, and the agent execution loop. Practice writing and analyzing code that uses the @function_tool decorator, Runner.run_sync(), agent cloning, and multi-agent orchestration patterns. Review prompt engineering techniques from the OpenAI cookbook, particularly Chain-of-Thought prompting, system message design, and handling sensitive data. Finally, familiarize yourself with basic Markdown syntax for links and images. Plan to spend at least 2-3 weeks studying these materials, complete hands-on coding exercises with the SDK. Consider this quiz a capstone assessment that requires comprehensive understanding rather than a beginner-level introduction to the concepts.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Quiz Covers&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://openai.github.io/openai-agents-python/&quot;&gt;https://openai.github.io/openai-agents-python/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://cookbook.openai.com/examples/gpt4-1_prompting_guide&quot;&gt;https://cookbook.openai.com/examples/gpt4-1_prompting_guide&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.markdownguide.org/basic-syntax/&quot;&gt;https://www.markdownguide.org/basic-syntax/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.markdownguide.org/cheat-sheet/&quot;&gt;https://www.markdownguide.org/cheat-sheet/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/panaversity/learn-agentic-ai/tree/main/01_ai_agents_first&quot;&gt;https://github.com/panaversity/learn-agentic-ai/tree/main/01_ai_agents_first&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;You Can Generate Mock Quizzes for Practice using LLMs from this Prompt:&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Create a comprehensive quiz covering OpenAI Agents SDK. It should include as many MCQ Quiz Questions as required to test the material, the questions should be difficult and at the graduate level and should test both concepts and include code were required. From the following following documentation:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://openai.github.io/openai-agents-python/&quot;&gt;https://openai.github.io/openai-agents-python/&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yandexdataschool/nlp_course</title>
      <link>https://github.com/yandexdataschool/nlp_course</link>
      <description>&lt;p&gt;YSDA course in Natural Language Processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YSDA Natural Language Processing course&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;This is the 2025 iteration of the course, materials are added as we prepare them. &lt;strong&gt;For full 2025 course materials, go to &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/tree/2024&quot;&gt;this branch&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Lecture and seminar materials for each week are in ./week* folders, see README.md for materials and instructions&lt;/li&gt; 
 &lt;li&gt;Any technical issues, ideas, bugs in course materials, contribution ideas - add an &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/issues&quot;&gt;issue&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Installing libraries and troubleshooting: &lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/issues/1&quot;&gt;this thread&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Syllabus&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2025/week01_embeddings&quot;&gt;&lt;strong&gt;week01&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Word Embeddings&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Word embeddings. Distributional semantics. Count-based (pre-neural) methods. Word2Vec: learn vectors. GloVe: count, then learn. Evaluation: intrinsic vs extrinsic. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_word_emb&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Playing with word and sentence embeddings&lt;/li&gt; 
   &lt;li&gt;Homework: Embedding-based machine translation system&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/yandexdataschool/nlp_course/2025/week02_lm&quot;&gt;&lt;strong&gt;week02&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;Language Modeling&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Language Modeling: what does it mean? Left-to-right framework. N-gram language models. Neural Language Models: General View, Recurrent Models, Convolutional Models. Evaluation. Practical Tips: Weight Tying. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_lang_models&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Build a N-gram language model from scratch&lt;/li&gt; 
   &lt;li&gt;Homework: Neural LMs &amp;amp; smoothing in count-based models.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TBU&lt;/strong&gt; &lt;code&gt;./week03_attention&lt;/code&gt; &lt;strong&gt;Seq2seq and Attention&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Seq2seq Basics: Encoder-Decoder framework, Training, Simple Models, Inference (e.g., beam search). Attention: general, score functions, models. Transformer: self-attention, masked self-attention, multi-head attention; model architecture. Subword Segmentation (BPE). Analysis and Interpretability: functions of attention heads; probing for linguistic structure. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_seq2seq_attn&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Seminar: Basic sequence to sequence model&lt;/li&gt; 
   &lt;li&gt;Homework: Machine translation with attention&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TBU&lt;/strong&gt; &lt;code&gt;./week04_transfer&lt;/code&gt; &lt;strong&gt;Transfer Learning&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: What is Transfer Learning? Great idea 1: From Words to Words-in-Context (CoVe, ELMo). Great idea 2: From Replacing Embeddings to Replacing Models (GPT, BERT). (A Bit of) Adaptors. Analysis and Interpretability. &lt;a href=&quot;https://lena-voita.github.io/nlp_course.html#preview_transfer&quot;&gt;Interactive lecture materials and more.&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Homework: fine-tuning a pre-trained BERT model&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TBU&lt;/strong&gt; &lt;code&gt;./week06_llm&lt;/code&gt; &lt;strong&gt;Large Language Models&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Lecture: Scaling laws. Emergent abilities. Prompting (aka &quot;in-context learning&quot;): techiques that work; questioning whether model &quot;understands&quot; prompts. Hypotheses for why and how in-context learning works. Analysis and Interpretability.&lt;/li&gt; 
   &lt;li&gt;Homework: manual prompt engneering and chain-of-thought reasoning&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Additional lectures to be announced!&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Contributors &amp;amp; course staff&lt;/h1&gt; 
&lt;p&gt;Course materials and teaching performed by&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lena-voita.github.io&quot;&gt;Elena Voita&lt;/a&gt; - course author&lt;/li&gt; 
 &lt;li&gt;[Mikhail Diskin] [Ignat Romanov] [Ruslan Svirschevski] - lectures&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.hse.ru/org/persons/831207784/?_gl=1%2a1hz2yht%2a_ga%2aMTg3MTM2ODIwMS4xNjk4NTEyODg5%2a_ga_D145P1R4PL%2aMTY5ODUxMjg4OC4xLjAuMTY5ODUxMjg4OC42MC4wLjA.&quot;&gt;Valentina Broner&lt;/a&gt; - course admin for on-campus students&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kovarsky&quot;&gt;Boris Kovarsky&lt;/a&gt;, &lt;a href=&quot;https://github.com/drt7&quot;&gt;David Talbot&lt;/a&gt;, &lt;a href=&quot;https://github.com/esgv&quot;&gt;Sergey Gubanov&lt;/a&gt;, &lt;a href=&quot;https://github.com/justheuristic&quot;&gt;Just Heuristic&lt;/a&gt; - help build course materials and/or held some classes&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yandexdataschool/nlp_course/graphs/contributors&quot;&gt;30+ volunteers&lt;/a&gt; who contributed and refined the notebooks and course materials. Without their help, the course would not be what it is today&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lk.yandexdataschool.ru/courses/2023-autumn/7.1171-avtomaticheskaia-obrabotka-tekstov/&quot;&gt;A mighty host of TAs&lt;/a&gt; who stoically grade hundreds of homework submissions from on-campus students each year&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>wilsonfreitas/awesome-quant</title>
      <link>https://github.com/wilsonfreitas/awesome-quant</link>
      <description>&lt;p&gt;A curated list of insanely awesome libraries, packages and resources for Quants (Quantitative Finance)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Quant&lt;/h1&gt; 
&lt;p&gt;A curated list of insanely awesome libraries, packages and resources for Quants (Quantitative Finance).&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://awesome.re&quot;&gt;&lt;img src=&quot;https://awesome.re/badge.svg?sanitize=true&quot; alt=&quot;&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Languages&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#python&quot;&gt;Python&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#r&quot;&gt;R&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#matlab&quot;&gt;Matlab&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#julia&quot;&gt;Julia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#java&quot;&gt;Java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#javascript&quot;&gt;JavaScript&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#haskell&quot;&gt;Haskell&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#scala&quot;&gt;Scala&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#ruby&quot;&gt;Ruby&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#elixirerlang&quot;&gt;Elixir/Erlang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#golang&quot;&gt;Golang&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#cpp&quot;&gt;CPP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#csharp&quot;&gt;CSharp&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#rust&quot;&gt;Rust&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#frameworks&quot;&gt;Frameworks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/#reproducing-works-training--books&quot;&gt;Reproducing Works, Training &amp;amp; Books&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Python&lt;/h2&gt; 
&lt;h3&gt;Numerical Libraries &amp;amp; Data Structures&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.numpy.org&quot;&gt;numpy&lt;/a&gt; - NumPy is the fundamental package for scientific computing with Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.scipy.org&quot;&gt;scipy&lt;/a&gt; - SciPy (pronounced ‚ÄúSigh Pie‚Äù) is a Python-based ecosystem of open-source software for mathematics, science, and engineering.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pandas.pydata.org&quot;&gt;pandas&lt;/a&gt; - pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.pola.rs/&quot;&gt;polars&lt;/a&gt; - Polars is a blazingly fast DataFrame library for manipulating structured data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/johnbywater/quantdsl&quot;&gt;quantdsl&lt;/a&gt; - Domain specific language for quantitative analytics in finance and trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/statistics.html&quot;&gt;statistics&lt;/a&gt; - Builtin Python library for all basic statistical calculations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.sympy.org/&quot;&gt;sympy&lt;/a&gt; - SymPy is a Python library for symbolic mathematics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.pymc.io/&quot;&gt;pymc3&lt;/a&gt; - Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with Theano.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.modelx.io/&quot;&gt;modelx&lt;/a&gt; - Python reimagination of spreadsheets as formula-centric objects that are interoperable with pandas.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/man-group/ArcticDB&quot;&gt;ArcticDB&lt;/a&gt; - High performance datastore for time series and tick data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Financial Instruments and Pricing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OpenBB-finance/OpenBBTerminal&quot;&gt;OpenBB Terminal&lt;/a&gt; - Terminal for investment research for everyone.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Fincept-Corporation/FinceptTerminal&quot;&gt;Fincept Terminal&lt;/a&gt; - Advance Data Based A.I Terminal for all Types of Financial Asset Research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/enthought/pyql&quot;&gt;PyQL&lt;/a&gt; - QuantLib&#39;s Python port.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/opendoor-labs/pyfin&quot;&gt;pyfin&lt;/a&gt; - Basic options pricing in Python. &lt;em&gt;ARCHIVED&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vollib/vollib&quot;&gt;vollib&lt;/a&gt; - vollib is a python library for calculating option prices, implied volatility and greeks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jsmidt/QuantPy&quot;&gt;QuantPy&lt;/a&gt; - A framework for quantitative finance In python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alpha-miner/Finance-Python&quot;&gt;Finance-Python&lt;/a&gt; - Python tools for Finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pmorissette/ffn&quot;&gt;ffn&lt;/a&gt; - A financial function library for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/GriffinAustin/pynance&quot;&gt;pynance&lt;/a&gt; - Lightweight Python library for assembling and analyzing financial data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bpsmith/tia&quot;&gt;tia&lt;/a&gt; - Toolkit for integration and analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://platform.hasura.io/hub/projects/hasura/base-python-dash&quot;&gt;hasura/base-python-dash&lt;/a&gt; - Hasura quick start to deploy Dash framework. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://platform.hasura.io/hub/projects/hasura/base-python-bokeh&quot;&gt;hasura/base-python-bokeh&lt;/a&gt; - Hasura quick start to visualize data with bokeh library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ynouri/pysabr&quot;&gt;pysabr&lt;/a&gt; - SABR model Python implementation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/domokane/FinancePy&quot;&gt;FinancePy&lt;/a&gt; - A Python Finance Library that focuses on the pricing and risk-management of Financial Derivatives, including fixed-income, equity, FX and credit derivatives.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/goldmansachs/gs-quant&quot;&gt;gs-quant&lt;/a&gt; - Python toolkit for quantitative finance&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/federicomariamassari/willowtree&quot;&gt;willowtree&lt;/a&gt; - Robust and flexible Python implementation of the willow tree lattice for derivatives pricing.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/federicomariamassari/financial-engineering&quot;&gt;financial-engineering&lt;/a&gt; - Applications of Monte Carlo methods to financial engineering projects, in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dbrojas/optlib&quot;&gt;optlib&lt;/a&gt; - A library for financial options pricing written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/google/tf-quant-finance&quot;&gt;tf-quant-finance&lt;/a&gt; - High-performance TensorFlow library for quantitative finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RomanMichaelPaolucci/Q-Fin&quot;&gt;Q-Fin&lt;/a&gt; - A Python library for mathematical finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantsbin/Quantsbin&quot;&gt;Quantsbin&lt;/a&gt; - Tools for pricing and plotting of vanilla option prices, greeks and various other analysis around them.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bbcho/finoptions-dev&quot;&gt;finoptions&lt;/a&gt; - Complete python implementation of R package fOptions with partial implementation of fExoticOptions for pricing various options.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ymyke/pypme&quot;&gt;pypme&lt;/a&gt; - PME (Public Market Equivalent) calculation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yellowbean/AbsBox&quot;&gt;AbsBox&lt;/a&gt; - A Python based library to model cashflow for structured product like Asset-backed securities (ABS) and Mortgage-backed securities (MBS).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/akashaero/Intrinsic-Value-Calculator&quot;&gt;Intrinsic-Value-Calculator&lt;/a&gt; - A Python tool for quick calculations of a stock&#39;s fair value using Discounted Cash Flow analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/deltaray-io/kelly-criterion&quot;&gt;Kelly-Criterion&lt;/a&gt; - Kelly Criterion implemented in Python to size portfolios based on J. L. Kelly Jr&#39;s formula.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/attack68/rateslib&quot;&gt;rateslib&lt;/a&gt; - A fixed income library for pricing bonds and bond futures, and derivatives such as IRS, cross-currency and FX swaps.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jkirkby3/fypy&quot;&gt;fypy&lt;/a&gt; - Vanilla and exotic option pricing library to support quantitative R&amp;amp;D. Focus on pricing interesting/useful models and contracts (including and beyond Black-Scholes), as well as calibration of financial models to market data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Indicators&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/femtotrader/pandas_talib&quot;&gt;pandas_talib&lt;/a&gt; - A Python Pandas implementation of technical analysis indicators.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/peerchemist/finta&quot;&gt;finta&lt;/a&gt; - Common financial technical analysis indicators implemented in Pandas.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cirla/tulipy&quot;&gt;Tulipy&lt;/a&gt; - Financial Technical Analysis Indicator Library (Python bindings for &lt;a href=&quot;https://github.com/TulipCharts/tulipindicators&quot;&gt;tulipindicators&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Boulder-Investment-Technologies/lppls&quot;&gt;lppls&lt;/a&gt; - A Python module for fitting the &lt;a href=&quot;https://en.wikipedia.org/wiki/Didier_Sornette#The_JLS_and_LPPLS_models&quot;&gt;Log-Periodic Power Law Singularity (LPPLS)&lt;/a&gt; model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nardew/talipp&quot;&gt;talipp&lt;/a&gt; - Incremental technical analysis library for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mr-easy/streaming_indicators&quot;&gt;streaming_indicators&lt;/a&gt; - A python library for computing technical analysis indicators on streaming data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trading &amp;amp; Backtesting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/skfolio/skfolio&quot;&gt;skfolio&lt;/a&gt; - Python library for portfolio optimization built on top of scikit-learn. It provides a unified interface and sklearn compatible tools to build, tune and cross-validate portfolio models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/coding-kitties/investing-algorithm-framework&quot;&gt;Investing algorithm framework&lt;/a&gt; - Framework for developing, backtesting, and deploying automated trading algorithms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mhallsmoore/qstrader&quot;&gt;QSTrader&lt;/a&gt; - QSTrader backtesting simulation engine.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Blankly-Finance/Blankly&quot;&gt;Blankly&lt;/a&gt; - Fully integrated backtesting, paper trading, and live deployment.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mrjbq7/ta-lib&quot;&gt;TA-Lib&lt;/a&gt; - Python wrapper for TA-Lib (&lt;a href=&quot;http://ta-lib.org/&quot;&gt;http://ta-lib.org/&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantopian/zipline&quot;&gt;zipline&lt;/a&gt; - Pythonic algorithmic trading library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/stefan-jansen/zipline-reloaded&quot;&gt;zipline-reloaded&lt;/a&gt; - Zipline, a Pythonic Algorithmic Trading Library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/QuantSoftware/QuantSoftwareToolkit&quot;&gt;QuantSoftware Toolkit&lt;/a&gt; - Python-based open source software framework designed to support portfolio construction and management.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jeffrey-liang/quantitative&quot;&gt;quantitative&lt;/a&gt; - Quantitative finance, and backtesting library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/llazzaro/analyzer&quot;&gt;analyzer&lt;/a&gt; - Python framework for real-time financial and backtesting trading strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pmorissette/bt&quot;&gt;bt&lt;/a&gt; - Flexible Backtesting for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/backtrader/backtrader&quot;&gt;backtrader&lt;/a&gt; - Python Backtesting library for trading strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/thalesians/pythalesians&quot;&gt;pythalesians&lt;/a&gt; - Python library to backtest trading strategies, plot charts, seamlessly download market data, analyze market patterns etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ematvey/pybacktest&quot;&gt;pybacktest&lt;/a&gt; - Vectorized backtesting framework in Python / pandas, designed to make your backtesting easier.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gbeced/pyalgotrade&quot;&gt;pyalgotrade&lt;/a&gt; - Python Algorithmic Trading Library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gbeced/basana&quot;&gt;basana&lt;/a&gt; - A Python async and event driven framework for algorithmic trading, with a focus on crypto currencies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/tradingWithPython/&quot;&gt;tradingWithPython&lt;/a&gt; - A collection of functions and classes for Quantitative trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/twopirllc/pandas-ta&quot;&gt;Pandas TA&lt;/a&gt; - Pandas TA is an easy to use Python 3 Pandas Extension with 115+ Indicators. Easily build Custom Strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bukosabino/ta&quot;&gt;ta&lt;/a&gt; - Technical Analysis Library using Pandas (Python)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joequant/algobroker&quot;&gt;algobroker&lt;/a&gt; - This is an execution engine for algo trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/pysentosa/&quot;&gt;pysentosa&lt;/a&gt; - Python API for sentosa trading system.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cuemacro/finmarketpy&quot;&gt;finmarketpy&lt;/a&gt; - Python library for backtesting trading strategies and analyzing financial markets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/metaperl/binary-martingale&quot;&gt;binary-martingale&lt;/a&gt; - Computer program to automatically trade binary options martingale style.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/foolcage/fooltrader&quot;&gt;fooltrader&lt;/a&gt; - the project using big-data technology to provide an uniform way to analyze the whole market.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zvtvz/zvt&quot;&gt;zvt&lt;/a&gt; - the project using sql, pandas to provide an uniform and extendable way to record data, computing factors, select securities, backtesting, realtime trading and it could show all of them in clearly charts in realtime.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alpacahq/pylivetrader&quot;&gt;pylivetrader&lt;/a&gt; - zipline-compatible live trading library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alpacahq/pipeline-live&quot;&gt;pipeline-live&lt;/a&gt; - zipline&#39;s pipeline capability with IEX for live trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantrocket-llc/zipline-extensions&quot;&gt;zipline-extensions&lt;/a&gt; - Zipline extensions and adapters for QuantRocket.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantrocket-llc/moonshot&quot;&gt;moonshot&lt;/a&gt; - Vectorized backtester and trading engine for QuantRocket based on Pandas.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/robertmartin8/PyPortfolioOpt&quot;&gt;PyPortfolioOpt&lt;/a&gt; - Financial portfolio optimization in python, including classical efficient frontier and advanced methods.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tradytics/eiten&quot;&gt;Eiten&lt;/a&gt; - Eiten is an open source toolkit by Tradytics that implements various statistical and algorithmic investing strategies such as Eigen Portfolios, Minimum Variance Portfolios, Maximum Sharpe Ratio Portfolios, and Genetic Algorithms based Portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dppalomar/riskparity.py&quot;&gt;riskparity.py&lt;/a&gt; - fast and scalable design of risk parity portfolios with TensorFlow 2.0&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hudson-and-thames/mlfinlab&quot;&gt;mlfinlab&lt;/a&gt; - Implementations regarding &quot;Advances in Financial Machine Learning&quot; by Marcos Lopez de Prado. (Feature Engineering, Financial Data Structures, Meta-Labeling)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/abbass2/pyqstrat&quot;&gt;pyqstrat&lt;/a&gt; - A fast, extensible, transparent python library for backtesting quantitative strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/edouardpoitras/NowTrade&quot;&gt;NowTrade&lt;/a&gt; - Python library for backtesting technical/mechanical strategies in the stock and currency markets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fja05680/pinkfish&quot;&gt;pinkfish&lt;/a&gt; - A backtester and spreadsheet library for security analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timkpaine/aat&quot;&gt;aat&lt;/a&gt; - Async Algorithmic Trading Engine&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://kernc.github.io/backtesting.py/&quot;&gt;Backtesting.py&lt;/a&gt; - Backtest trading strategies in Python&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/enigmampc/catalyst&quot;&gt;catalyst&lt;/a&gt; - An Algorithmic Trading Library for Crypto-Assets in Python&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ranaroussi/quantstats&quot;&gt;quantstats&lt;/a&gt; - Portfolio analytics for quants, written in Python&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ranaroussi/qtpylib&quot;&gt;qtpylib&lt;/a&gt; - QTPyLib, Pythonic Algorithmic Trading &lt;a href=&quot;http://qtpylib.io&quot;&gt;http://qtpylib.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/constverum/Quantdom&quot;&gt;Quantdom&lt;/a&gt; - Python-based framework for backtesting trading strategies &amp;amp; analyzing financial markets [GUI &lt;img alt=&quot;neckbeard&quot; src=&quot;https://github.githubassets.com/images/icons/emoji/neckbeard.png?v8&quot; /&gt;)]&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/freqtrade/freqtrade&quot;&gt;freqtrade&lt;/a&gt; - Free, open source crypto trading bot&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chrisconlan/algorithmic-trading-with-python&quot;&gt;algorithmic-trading-with-python&lt;/a&gt; - Free &lt;code&gt;pandas&lt;/code&gt; and &lt;code&gt;scikit-learn&lt;/code&gt; resources for trading simulation, backtesting, and machine learning on financial data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jankrepl/deepdow&quot;&gt;DeepDow&lt;/a&gt; - Portfolio optimization with deep learning&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/qlib&quot;&gt;Qlib&lt;/a&gt; - An AI-oriented Quantitative Investment Platform by Microsoft. Full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/stefan-jansen/machine-learning-for-trading&quot;&gt;machine-learning-for-trading&lt;/a&gt; - Code and resources for Machine Learning for Algorithmic Trading&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ScottfreeLLC/AlphaPy&quot;&gt;AlphaPy&lt;/a&gt; - Automated Machine Learning [AutoML] with Python, scikit-learn, Keras, XGBoost, LightGBM, and CatBoost&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jesse-ai/jesse&quot;&gt;jesse&lt;/a&gt; - An advanced crypto trading bot written in Python&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ricequant/rqalpha&quot;&gt;rqalpha&lt;/a&gt; - A extendable, replaceable Python algorithmic backtest &amp;amp;&amp;amp; trading framework supporting multiple securities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AI4Finance-LLC/FinRL-Library&quot;&gt;FinRL-Library&lt;/a&gt; - A Deep Reinforcement Learning Library for Automated Trading in Quantitative Finance. NeurIPS 2020.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/achillesrasquinha/bulbea&quot;&gt;bulbea&lt;/a&gt; - Deep Learning based Python Library for Stock Market Prediction and Modelling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ajhpark/ib_nope&quot;&gt;ib_nope&lt;/a&gt; - Automated trading system for NOPE strategy over IBKR TWS.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Drakkar-Software/OctoBot&quot;&gt;OctoBot&lt;/a&gt; - Open source cryptocurrency trading bot for high frequency, arbitrage, TA and social trading with an advanced web interface.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mementum/bta-lib&quot;&gt;bta-lib&lt;/a&gt; - Technical Analysis library in pandas for backtesting algotrading and quantitative analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/huseinzol05/Stock-Prediction-Models&quot;&gt;Stock-Prediction-Models&lt;/a&gt; - Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jmrichardson/tuneta&quot;&gt;TuneTA&lt;/a&gt; - TuneTA optimizes technical indicators using a distance correlation measure to a user defined target feature such as next day return.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/kieran-mackle/AutoTrader&quot;&gt;AutoTrader&lt;/a&gt; - A Python-based development platform for automated trading systems - from backtesting to optimization to livetrading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jrmeier/fast-trade&quot;&gt;fast-trade&lt;/a&gt; - A library built with backtest portability and performance in mind for backtest trading strategies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quarkfin/qf-lib&quot;&gt;qf-lib&lt;/a&gt; - QF-Lib is a Python library that provides high quality tools for quantitative finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alexgolec/tda-api&quot;&gt;tda-api&lt;/a&gt; - Gather data and trade equities, options, and ETFs via TDAmeritrade.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/polakowo/vectorbt&quot;&gt;vectorbt&lt;/a&gt; - Find your trading edge, using a powerful toolkit for backtesting, algorithmic trading, and research.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/QuantConnect/Lean&quot;&gt;Lean&lt;/a&gt; - Lean Algorithmic Trading Engine by QuantConnect (Python, C#).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jrmeier/fast-trade&quot;&gt;fast-trade&lt;/a&gt; - Low code backtesting library utilizing pandas and technical analysis indicators.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/robcarver17/pysystemtrade&quot;&gt;pysystemtrade&lt;/a&gt; - pysystemtrade is the open source version of Robert Carver&#39;s backtesting and trading engine that implements systems according to the framework outlined in his book &quot;Systematic Trading&quot;, which is further developed on his &lt;a href=&quot;https://qoppac.blogspot.com/&quot;&gt;blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rafa-rod/pytrendseries&quot;&gt;pytrendseries&lt;/a&gt; - Detect trend in time series, drawdown, drawdown within a constant look-back window , maximum drawdown, time underwater.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/DrAshBooth/PyLOB&quot;&gt;PyLOB&lt;/a&gt; - Fully functioning fast Limit Order Book written in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/edtechre/pybroker&quot;&gt;PyBroker&lt;/a&gt; - Algorithmic Trading with Machine Learning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Drakkar-Software/OctoBot-Script&quot;&gt;OctoBot Script&lt;/a&gt; - A quant framework to create cryptocurrencies strategies - from backtesting to optimization to livetrading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nkaz001/hftbacktest&quot;&gt;hftbacktest&lt;/a&gt; - A high-frequency trading and market-making backtesting tool accounts for limit orders, queue positions, and latencies, utilizing full tick data for trades and order books.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vnpy/vnpy&quot;&gt;vnpy&lt;/a&gt; - VeighNa is a Python-based open source quantitative trading system development framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/asavinov/intelligent-trading-bot&quot;&gt;Intelligent Trading Bot&lt;/a&gt; - Automatically generating signals and trading based on machine learning and feature engineering&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/enzoampil/fastquant&quot;&gt;fastquant&lt;/a&gt; - fastquant allows you to easily backtest investment strategies with as few as 3 lines of python code.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nautechsystems/nautilus_trader&quot;&gt;nautilus_trader&lt;/a&gt; - A high-performance algorithmic trading platform and event-driven backtester.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bsdz/yabte&quot;&gt;YABTE&lt;/a&gt; - Yet Another (Python) BackTesting Engine.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tradingstrategy-ai/getting-started&quot;&gt;Trading Strategy&lt;/a&gt; - TradingStrategy.ai is a market data, backtesting, live trading and investor management framework for decentralised finance&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fasiondog/hikyuu&quot;&gt;Hikyuu&lt;/a&gt; - A base on Python/C++ open source high-performance quant framework for faster analysis and backtesting, contains the complete trading system components for reuse and combination.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jensnesten/rust_bt&quot;&gt;rust_bt&lt;/a&gt; - A high performance, low-latency backtesting engine for testing quantitative trading strategies on historical and live data in Rust.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Risk Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/auto-differentiation/QuantLib-Risks-Py&quot;&gt;QuantLibRisks&lt;/a&gt; - Fast risks with QuantLib&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/auto-differentiation/xad-py&quot;&gt;XAD&lt;/a&gt; - Automatic Differentation (AAD) Library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantopian/pyfolio&quot;&gt;pyfolio&lt;/a&gt; - Portfolio and risk analytics in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantopian/empyrical&quot;&gt;empyrical&lt;/a&gt; - Common financial risk and performance metrics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rsvp/fecon235&quot;&gt;fecon235&lt;/a&gt; - Computational tools for financial economics include: Gaussian Mixture model of leptokurtotic risk, adaptive Boltzmann portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/finance/&quot;&gt;finance&lt;/a&gt; - Financial Risk Calculations. Optimized for ease of use through class construction and operator overload.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/qfrm/&quot;&gt;qfrm&lt;/a&gt; - Quantitative Financial Risk Management: awesome OOP tools for measuring, managing and visualizing risk of financial instruments and portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/benjaminmgross/visualize-wealth&quot;&gt;visualize-wealth&lt;/a&gt; - Portfolio construction and quantitative analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wegamekinglc/VisualPortfolio&quot;&gt;VisualPortfolio&lt;/a&gt; - This tool is used to visualize the performance of a portfolio.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Marigold/universal-portfolios&quot;&gt;universal-portfolios&lt;/a&gt; - Collection of algorithms for online portfolio selection.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fmilthaler/FinQuant&quot;&gt;FinQuant&lt;/a&gt; - A program for financial portfolio management, analysis and optimization.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ssantoshp/Empyrial&quot;&gt;Empyrial&lt;/a&gt; - Portfolio&#39;s risk and performance analytics and returns predictions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bbcho/risktools-dev&quot;&gt;risktools&lt;/a&gt; - Risk tools for use within the crude and crude products trading space with partial implementation of R&#39;s PerformanceAnalytics.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dcajasn/Riskfolio-Lib&quot;&gt;Riskfolio-Lib&lt;/a&gt; - Portfolio Optimization and Quantitative Strategic Asset Allocation in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/stefan-jansen/empyrical-reloaded&quot;&gt;empyrical-reloaded&lt;/a&gt; - Common financial risk and performance metrics. &lt;a href=&quot;https://github.com/quantopian/empyrical&quot;&gt;empyrical&lt;/a&gt; fork.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/stefan-jansen/pyfolio-reloaded&quot;&gt;pyfolio-reloaded&lt;/a&gt; - Portfolio and risk analytics in Python. &lt;a href=&quot;https://github.com/quantopian/pyfolio&quot;&gt;pyfolio&lt;/a&gt; fork.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fortitudo-tech/fortitudo.tech&quot;&gt;fortitudo.tech&lt;/a&gt; - Conditional Value-at-Risk (CVaR) portfolio optimization and Entropy Pooling views / stress-testing in Python.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Factor Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantopian/alphalens&quot;&gt;alphalens&lt;/a&gt; - Performance analysis of predictive alpha factors.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/stefan-jansen/alphalens-reloaded&quot;&gt;alphalens-reloaded&lt;/a&gt; - Performance analysis of predictive (alpha) stock factors.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Heerozh/spectre&quot;&gt;Spectre&lt;/a&gt; - GPU-accelerated Factors analysis library and Backtester&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sentiment Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/KVignesh122/AssetNewsSentimentAnalyzer&quot;&gt;Asset News Sentiment Analyzer&lt;/a&gt; - Sentiment analysis and report generation package for financial assets and securities utilizing GPT models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quant Research Environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gnzsnz/jupyter-quant&quot;&gt;Jupyter Quant&lt;/a&gt; - A dockerized Jupyter quant research environment with preloaded tools for quant analysis, statsmodels, pymc, arch, py_vollib, zipline-reloaded, PyPortfolioOpt, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Time Series&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bashtage/arch&quot;&gt;ARCH&lt;/a&gt; - ARCH models in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://statsmodels.sourceforge.net&quot;&gt;statsmodels&lt;/a&gt; - Python module that allows users to explore data, estimate statistical models, and perform statistical tests.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/quantmind/dynts&quot;&gt;dynts&lt;/a&gt; - Python package for timeseries analysis and manipulation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RJT1990/pyflux&quot;&gt;PyFlux&lt;/a&gt; - Python library for timeseries modelling and inference (frequentist and Bayesian) on models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/blue-yonder/tsfresh&quot;&gt;tsfresh&lt;/a&gt; - Automatic extraction of relevant features from time series.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://platform.hasura.io/hub/projects/anirudhm/quandl-metabase-time-series&quot;&gt;hasura/quandl-metabase&lt;/a&gt; - Hasura quickstart to visualize Quandl&#39;s timeseries datasets with Metabase.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/facebook/prophet&quot;&gt;Facebook Prophet&lt;/a&gt; - Tool for producing high quality forecasts for time series data that has multiple seasonality with linear or non-linear growth.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cerlymarco/tsmoothie&quot;&gt;tsmoothie&lt;/a&gt; - A python library for time-series smoothing and outlier detection in a vectorized way.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alkaline-ml/pmdarima&quot;&gt;pmdarima&lt;/a&gt; - A statistical library designed to fill the void in Python&#39;s time series analysis capabilities, including the equivalent of R&#39;s auto.arima function.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/gluon-ts&quot;&gt;gluon-ts&lt;/a&gt; - vProbabilistic time series modeling in Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/functime-org/functime&quot;&gt;functime&lt;/a&gt; - Time-series machine learning at scale. Built with Polars for embarrassingly parallel feature extraction and forecasts on panel data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Calendars&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gerrymanoim/exchange_calendars&quot;&gt;exchange_calendars&lt;/a&gt; - Stock Exchange Trading Calendars.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wilsonfreitas/python-bizdays&quot;&gt;bizdays&lt;/a&gt; - Business days calculations and utilities.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rsheftel/pandas_market_calendars&quot;&gt;pandas_market_calendars&lt;/a&gt; - Exchange calendars to use with pandas for trading applications.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data Sources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ranaroussi/yfinance&quot;&gt;yfinance&lt;/a&gt; - Yahoo! Finance market data downloader (+faster Pandas Datareader)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cuemacro/findatapy&quot;&gt;findatapy&lt;/a&gt; - Python library to download market data via Bloomberg, Quandl, Yahoo etc.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hongtaocai/googlefinance&quot;&gt;googlefinance&lt;/a&gt; - Python module to get real-time stock data from Google Finance API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lukaszbanasiak/yahoo-finance&quot;&gt;yahoo-finance&lt;/a&gt; - Python module to get stock data from Yahoo! Finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pydata/pandas-datareader&quot;&gt;pandas-datareader&lt;/a&gt; - Python module to get data from various sources (Google Finance, Yahoo Finance, FRED, OECD, Fama/French, World Bank, Eurostat...) into Pandas datastructures such as DataFrame, Panel with a caching mechanism.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/davidastephens/pandas-finance&quot;&gt;pandas-finance&lt;/a&gt; - High level API for access to and analysis of financial data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/innes213/pyhoofinance&quot;&gt;pyhoofinance&lt;/a&gt; - Rapidly queries Yahoo Finance for multiple tickers and returns typed data for analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Karthik005/yfinanceapi&quot;&gt;yfinanceapi&lt;/a&gt; - Finance API for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/slawek87/yql-finance&quot;&gt;yql-finance&lt;/a&gt; - yql-finance is simple and fast. API returns stock closing prices for current period of time and current stock ticker (i.e. APPL, GOOGL).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cgoldberg/ystockquote&quot;&gt;ystockquote&lt;/a&gt; - Retrieve stock quote data from Yahoo Finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mcdallas/wallstreet&quot;&gt;wallstreet&lt;/a&gt; - Real time stock and option data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ZachLiuGIS/stock_extractor&quot;&gt;stock_extractor&lt;/a&gt; - General Purpose Stock Extractors from Online Resources.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cttn/Stockex&quot;&gt;Stockex&lt;/a&gt; - Python wrapper for Yahoo! Finance API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/skillachie/finsymbols&quot;&gt;finsymbols&lt;/a&gt; - Obtains stock symbols and relating information for SP500, AMEX, NYSE, and NASDAQ.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/avelkoski/FRB&quot;&gt;FRB&lt;/a&gt; - Python Client for FRED¬Æ API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/econdb/inquisitor&quot;&gt;inquisitor&lt;/a&gt; - Python Interface to Econdb.com API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/nickelkr/yfi&quot;&gt;yfi&lt;/a&gt; - Yahoo! YQL library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/chinesestockapi/&quot;&gt;chinesestockapi&lt;/a&gt; - Python API to get Chinese stock price.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/akarat/exchange&quot;&gt;exchange&lt;/a&gt; - Get current exchange rate.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jamescnowell/ticks&quot;&gt;ticks&lt;/a&gt; - Simple command line tool to get stock ticker data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bpsmith/pybbg&quot;&gt;pybbg&lt;/a&gt; - Python interface to Bloomberg COM APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lsbardel/ccy&quot;&gt;ccy&lt;/a&gt; - Python module for currencies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/tushare/&quot;&gt;tushare&lt;/a&gt; - A utility for crawling historical and Real-time Quotes data of China stocks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/jsm/&quot;&gt;jsm&lt;/a&gt; - Get the japanese stock market data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jealous/cn_stock_src&quot;&gt;cn_stock_src&lt;/a&gt; - Utility for retrieving basic China stock data from different sources.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/barnumbirr/coinmarketcap&quot;&gt;coinmarketcap&lt;/a&gt; - Python API for coinmarketcap.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/datawrestler/after-hours&quot;&gt;after-hours&lt;/a&gt; - Obtain pre market and after hours stock prices for a given symbol.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/bronto-python/&quot;&gt;bronto-python&lt;/a&gt; - Bronto API Integration for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rainx/pytdx&quot;&gt;pytdx&lt;/a&gt; - Python Interface for retrieving chinese stock realtime quote data from TongDaXin Nodes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/matthewgilbert/pdblp&quot;&gt;pdblp&lt;/a&gt; - A simple interface to integrate pandas and the Bloomberg Open API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/hydrosquall/tiingo-python&quot;&gt;tiingo&lt;/a&gt; - Python interface for daily composite prices/OHLC/Volume + Real-time News Feeds, powered by the Tiingo Data Platform.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/addisonlynch/iexfinance&quot;&gt;iexfinance&lt;/a&gt; - Python Interface for retrieving real-time and historical prices and equities data from The Investor&#39;s Exchange.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/timkpaine/pyEX&quot;&gt;pyEX&lt;/a&gt; - Python interface to IEX with emphasis on pandas, support for streaming data, premium data, points data (economic, rates, commodities), and technical indicators.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alpacahq/alpaca-trade-api-python&quot;&gt;alpaca-trade-api&lt;/a&gt; - Python interface for retrieving real-time and historical prices from Alpaca API as well as trade execution.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pypi.org/project/MetaTrader5/&quot;&gt;metatrader5&lt;/a&gt; - API Connector to MetaTrader 5 Terminal&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jindaxiang/akshare&quot;&gt;akshare&lt;/a&gt; - AkShare is an elegant and simple financial data interface library for Python, built for human beings! &lt;a href=&quot;https://akshare.readthedocs.io&quot;&gt;https://akshare.readthedocs.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dpguthrie/yahooquery&quot;&gt;yahooquery&lt;/a&gt; - Python interface for retrieving data through unofficial Yahoo Finance API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alvarobartt/investpy&quot;&gt;investpy&lt;/a&gt; - Financial Data Extraction from Investing.com with Python! &lt;a href=&quot;https://investpy.readthedocs.io/&quot;&gt;https://investpy.readthedocs.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yahoofinancelive/yliveticker&quot;&gt;yliveticker&lt;/a&gt; - Live stream of market data from Yahoo Finance websocket.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ran404/bbgbridge&quot;&gt;bbgbridge&lt;/a&gt; - Easy to use Bloomberg Desktop API wrapper for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/polygon-io/client-python&quot;&gt;polygon.io&lt;/a&gt; - A python library for Polygon.io financial data APIs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RomelTorres/alpha_vantage&quot;&gt;alpha_vantage&lt;/a&gt; - A python wrapper for Alpha Vantage API for financial data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/FinanceData/FinanceDataReader&quot;&gt;FinanceDataReader&lt;/a&gt; - Open Source Financial data reader for U.S, Korean, Japanese, Chinese, Vietnamese Stocks&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/TomasKoutek/pystlouisfed&quot;&gt;pystlouisfed&lt;/a&gt; - Python client for Federal Reserve Bank of St. Louis API - FRED, ALFRED, GeoFRED and FRASER.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wilsonfreitas/python-bcb&quot;&gt;python-bcb&lt;/a&gt; - Python interface to Brazilian Central Bank web services.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maread99/market_prices&quot;&gt;market-prices&lt;/a&gt; - Create meaningful OHLCV datasets from knowledge of &lt;a href=&quot;https://github.com/gerrymanoim/exchange_calendars&quot;&gt;exchange-calendars&lt;/a&gt; (works out-the-box with data from Yahoo Finance).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tardis-dev/tardis-python&quot;&gt;tardis-python&lt;/a&gt; - Python interface for Tardis.dev high frequency crypto market data&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/crypto-lake/lake-api&quot;&gt;lake-api&lt;/a&gt; - Python interface for Crypto Lake high frequency crypto market data&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ymyke/tessa&quot;&gt;tessa&lt;/a&gt; - simple, hassle-free access to price information of financial assets (currently based on yfinance and pycoingecko), including search and a symbol class.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dr-leo/pandaSDMX&quot;&gt;pandaSDMX&lt;/a&gt; - Python package that implements SDMX 2.1 (ISO 17369:2013), a format for exchange of statistical data and metadata used by national statistical agencies, central banks, and international organisations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LenkaV/CIF&quot;&gt;cif&lt;/a&gt; - Python package that include few composite indicators, which summarize multidimensional relationships between individual economic indicators.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/theOGognf/finagg&quot;&gt;finagg&lt;/a&gt; - finagg is a Python package that provides implementations of popular and free financial APIs, tools for aggregating historical data from those APIs into SQL databases, and tools for transforming aggregated data into features useful for analysis and AI/ML.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JerBouma/FinanceDatabase&quot;&gt;FinanceDatabase&lt;/a&gt; - This is a database of 300.000+ symbols containing Equities, ETFs, Funds, Indices, Currencies, Cryptocurrencies and Money Markets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tradingstrategy-ai/trading-strategy/&quot;&gt;Trading Strategy&lt;/a&gt; - download price data for decentralised exchanges and lending protocols (DeFi)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/john-friedman/datamule-python&quot;&gt;datamule-python&lt;/a&gt; - A package to work with SEC data. Incorporates datamule endpoints.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://financialdata.net/&quot;&gt;Financial Data&lt;/a&gt; - Stock Market and Financial Data API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.developer.saxo/&quot;&gt;SaxoOpenAPI&lt;/a&gt; - Saxo Bank financial data API.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Excel Integration&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.xlwings.org/&quot;&gt;xlwings&lt;/a&gt; - Make Excel fly with Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://openpyxl.readthedocs.io/en/latest/&quot;&gt;openpyxl&lt;/a&gt; - Read/Write Excel 2007 xlsx/xlsm files.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/python-excel/xlrd&quot;&gt;xlrd&lt;/a&gt; - Library for developers to extract data from Microsoft Excel spreadsheet files.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://xlsxwriter.readthedocs.io/&quot;&gt;xlsxwriter&lt;/a&gt; - Write files in the Excel 2007+ XLSX file format.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/python-excel/xlwt&quot;&gt;xlwt&lt;/a&gt; - Library to create spreadsheet files compatible with MS Excel 97/2000/XP/2003 XLS files, on any platform.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://datanitro.com/&quot;&gt;DataNitro&lt;/a&gt; - DataNitro also offers full-featured Python-Excel integration, including UDFs. Trial downloads are available, but users must purchase a license.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://xlloop.sourceforge.net&quot;&gt;xlloop&lt;/a&gt; - XLLoop is an open source framework for implementing Excel user-defined functions (UDFs) on a centralised server (a function server).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.bnikolic.co.uk/expy/expy.html&quot;&gt;expy&lt;/a&gt; - The ExPy add-in allows easy use of Python directly from within an Microsoft Excel spreadsheet, both to execute arbitrary code and to define new Excel functions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.pyxll.com&quot;&gt;pyxll&lt;/a&gt; - PyXLL is an Excel add-in that enables you to extend Excel using nothing but Python code.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Visualization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/man-group/dtale&quot;&gt;D-Tale&lt;/a&gt; - Visualizer for pandas dataframes and xarray datasets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/matplotlib/mplfinance&quot;&gt;mplfinance&lt;/a&gt; - matplotlib utilities for the visualization, and visual analysis, of financial data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/highfestiva/finplot&quot;&gt;finplot&lt;/a&gt; - Performant and effortless finance plotting for Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lit26/finvizfinance&quot;&gt;finvizfinance&lt;/a&gt; - Finviz analysis python library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/maread99/market_analy&quot;&gt;market-analy&lt;/a&gt; - Analysis and interactive charting using &lt;a href=&quot;https://github.com/maread99/market_prices&quot;&gt;market-prices&lt;/a&gt; and bqplot.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ArturSepp/QuantInvestStrats&quot;&gt;QuantInvestStrats&lt;/a&gt; - Quantitative Investment Strategies (QIS) package implements Python analytics for visualisation of financial data, performance reporting, analysis of quantitative strategies.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;R&lt;/h2&gt; 
&lt;h3&gt;Numerical Libraries &amp;amp; Data Structures&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joshuaulrich/xts&quot;&gt;xts&lt;/a&gt; - eXtensible Time Series: Provide for uniform handling of R&#39;s different time-based data classes by extending zoo, maximizing native format information preservation and allowing for user level customization and extension, while simplifying cross-class interoperability.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Rdatatable/data.table&quot;&gt;data.table&lt;/a&gt; - Extension of data.frame: Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns and a fast file reader (fread). Offers a natural and flexible syntax, for faster development.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dppalomar/sparseEigen&quot;&gt;sparseEigen&lt;/a&gt; - Sparse principal component analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://tsdbi.r-forge.r-project.org/&quot;&gt;TSdbi&lt;/a&gt; - Provides a common interface to time series databases.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tseries/index.html&quot;&gt;tseries&lt;/a&gt; - Time Series Analysis and Computational Finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/zoo/index.html&quot;&gt;zoo&lt;/a&gt; - S3 Infrastructure for Regular and Irregular Time Series (Z&#39;s Ordered Observations).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tis/index.html&quot;&gt;tis&lt;/a&gt; - Functions and S3 classes for time indexes and time indexed series, which are compatible with FAME frequencies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tfplot/index.html&quot;&gt;tfplot&lt;/a&gt; - Utilities for simple manipulation and quick plotting of time series data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tframe/index.html&quot;&gt;tframe&lt;/a&gt; - A kernel of functions for programming time series methods in a way that is relatively independently of the representation of time.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data Sources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/IBrokers/index.html&quot;&gt;IBrokers&lt;/a&gt; - Provides native R access to Interactive Brokers Trader Workstation API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Rblp/Rblpapi&quot;&gt;Rblpapi&lt;/a&gt; - An R Interface to &#39;Bloomberg&#39; is provided via the &#39;Blp API&#39;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.quandl.com/tools/r&quot;&gt;Quandl&lt;/a&gt; - Get Financial Data Directly Into R.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jangorecki/Rbitcoin&quot;&gt;Rbitcoin&lt;/a&gt; - Unified markets API interface (bitstamp, kraken, btce, bitmarket).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/msperlin/GetTDData&quot;&gt;GetTDData&lt;/a&gt; - Downloads and aggregates data for Brazilian government issued bonds directly from the website of Tesouro Direto.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/msperlin/GetHFData&quot;&gt;GetHFData&lt;/a&gt; - Downloads and aggregates high frequency trading data for Brazilian instruments directly from Bovespa ftp site.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dashboard.nbshare.io/apps/reddit/api/&quot;&gt;Reddit WallstreetBets API&lt;/a&gt; - Provides daily top 50 stocks from reddit (subreddit) Wallstreetbets and their sentiments via the API.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eddelbuettel/td&quot;&gt;td&lt;/a&gt; - Interfaces the &#39;twelvedata&#39; API for stocks and (digital and standard) currencies.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wilsonfreitas/rbcb&quot;&gt;rbcb&lt;/a&gt; - R interface to Brazilian Central Bank web services.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ropensci/rb3&quot;&gt;rb3&lt;/a&gt; - A bunch of downloaders and parsers for data delivered from B3.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/matthiasgomolka/simfinapi&quot;&gt;simfinapi&lt;/a&gt; - Makes &#39;SimFin&#39; data (&lt;a href=&quot;https://simfin.com/&quot;&gt;https://simfin.com/&lt;/a&gt;) easily accessible in R.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/tidy-finance/r-tidyfinance&quot;&gt;tidyfinance&lt;/a&gt; - Tidy Finance helper functions to download financial data and process the raw data into a structured Format (tidy data), including date conversion, scaling factor values, and filtering by the specified date.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Financial Instruments and Pricing&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/eddelbuettel/rquantlib&quot;&gt;RQuantLib&lt;/a&gt; - RQuantLib connects GNU R with QuantLib.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/quantmod/index.html&quot;&gt;quantmod&lt;/a&gt; - Quantitative Financial Modelling Framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.rmetrics.org&quot;&gt;Rmetrics&lt;/a&gt; - The premier open source software solution for teaching and training quantitative finance. 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fAsianOptions/index.html&quot;&gt;fAsianOptions&lt;/a&gt; - EBM and Asian Option Valuation.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fAssets/index.html&quot;&gt;fAssets&lt;/a&gt; - Analysing and Modelling Financial Assets.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fBasics/index.html&quot;&gt;fBasics&lt;/a&gt; - Markets and Basic Statistics.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fBonds/index.html&quot;&gt;fBonds&lt;/a&gt; - Bonds and Interest Rate Models.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fExoticOptions/index.html&quot;&gt;fExoticOptions&lt;/a&gt; - Exotic Option Valuation.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fOptions/index.html&quot;&gt;fOptions&lt;/a&gt; - Pricing and Evaluating Basic Options.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fPortfolio/index.html&quot;&gt;fPortfolio&lt;/a&gt; - Portfolio Selection and Optimization.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dgerlanc/portfolio&quot;&gt;portfolio&lt;/a&gt; - Analysing equity portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dppalomar/sparseIndexTracking&quot;&gt;sparseIndexTracking&lt;/a&gt; - Portfolio design to track an index.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dppalomar/covFactorModel&quot;&gt;covFactorModel&lt;/a&gt; - Covariance matrix estimation via factor models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dppalomar/riskParityPortfolio&quot;&gt;riskParityPortfolio&lt;/a&gt; - Blazingly fast design of risk parity portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/sde/index.html&quot;&gt;sde&lt;/a&gt; - Simulation and Inference for Stochastic Differential Equations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/YieldCurve/index.html&quot;&gt;YieldCurve&lt;/a&gt; - Modelling and estimation of the yield curve.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/SmithWilsonYieldCurve/index.html&quot;&gt;SmithWilsonYieldCurve&lt;/a&gt; - Constructs a yield curve by the Smith-Wilson method from a table of LIBOR and SWAP rates.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/ycinterextra/index.html&quot;&gt;ycinterextra&lt;/a&gt; - Yield curve or zero-coupon prices interpolation and extrapolation.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/AmericanCallOpt/index.html&quot;&gt;AmericanCallOpt&lt;/a&gt; - This package includes pricing function for selected American call options with underlying assets that generate payouts.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/VarSwapPrice/index.html&quot;&gt;VarSwapPrice&lt;/a&gt; - Pricing a variance swap on an equity index.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/RND/index.html&quot;&gt;RND&lt;/a&gt; - Risk Neutral Density Extraction Package.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/LSMonteCarlo/index.html&quot;&gt;LSMonteCarlo&lt;/a&gt; - American options pricing with Least Squares Monte Carlo method.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/OptHedging/index.html&quot;&gt;OptHedging&lt;/a&gt; - Estimation of value and hedging strategy of call and put options.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tvm/index.html&quot;&gt;tvm&lt;/a&gt; - Time Value of Money Functions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/OptionPricing/index.html&quot;&gt;OptionPricing&lt;/a&gt; - Option Pricing with Efficient Simulation Algorithms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/blenezet/credule&quot;&gt;credule&lt;/a&gt; - Credit Default Swap Functions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/derivmkts/index.html&quot;&gt;derivmkts&lt;/a&gt; - Functions and R Code to Accompany Derivatives Markets.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/felixfan/FinCal&quot;&gt;FinCal&lt;/a&gt; - Package for time value of money calculation, time series analysis and computational finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/artyyouth/r-quant&quot;&gt;r-quant&lt;/a&gt; - R code for quantitative analysis in finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/taylorizing/options.studies&quot;&gt;options.studies&lt;/a&gt; - options trading studies functions for use with options.data package and shiny.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/braverock/PortfolioAnalytics&quot;&gt;PortfolioAnalytics&lt;/a&gt; - Portfolio Analysis, Including Numerical Methods for Optimizationof Portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/imanuelcostigan/fmbasics&quot;&gt;fmbasics&lt;/a&gt; - Financial Market Building Blocks.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wilsonfreitas/R-fixedincome&quot;&gt;R-fixedincome&lt;/a&gt; - Fixed income tools for R.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Trading&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/backtest/index.html&quot;&gt;backtest&lt;/a&gt; - Exploring Portfolio-Based Conjectures About Financial Instruments.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/pa/index.html&quot;&gt;pa&lt;/a&gt; - Performance Attribution for Equity Portfolios.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/joshuaulrich/TTR&quot;&gt;TTR&lt;/a&gt; - Technical Trading Rules.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://quanttools.bitbucket.io/_site/index.html&quot;&gt;QuantTools&lt;/a&gt; - Enhanced Quantitative Trading Modelling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/braverock/blotter&quot;&gt;blotter&lt;/a&gt; - Transaction infrastructure for defining instruments, transactions, portfolios and accounts for trading systems and simulation. Provides portfolio support for multi-asset class and multi-currency portfolios. Actively maintained and developed.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Backtesting&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/braverock/quantstrat&quot;&gt;quantstrat&lt;/a&gt; - Transaction-oriented infrastructure for constructing trading systems and simulation. Provides support for multi-asset class and multi-currency portfolios for backtesting and other financial research.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Risk Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/braverock/PerformanceAnalytics&quot;&gt;PerformanceAnalytics&lt;/a&gt; - Econometric tools for performance and risk analysis.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Factor Analysis&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/braverock/FactorAnalytics&quot;&gt;FactorAnalytics&lt;/a&gt; - The FactorAnalytics package contains fitting and analysis methods for the three main types of factor models used in conjunction with portfolio construction, optimization and risk management, namely fundamental factor models, time series factor models and statistical factor models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JustinMShea/ExpectedReturns&quot;&gt;Expected Returns&lt;/a&gt; - Solutions for enhancing portfolio diversification and replications of seminal papers with R, most of which are discussed in one of the best investment references of the recent decade, Expected Returns: An Investors Guide to Harvesting Market Rewards by Antti Ilmanen.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Time Series&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/tseries/index.html&quot;&gt;tseries&lt;/a&gt; - Time Series Analysis and Computational Finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/fGarch/index.html&quot;&gt;fGarch&lt;/a&gt; - Rmetrics - Autoregressive Conditional Heteroskedastic Modelling.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/timeSeries/index.html&quot;&gt;timeSeries&lt;/a&gt; - Rmetrics - Financial Time Series Objects.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alexiosg/rugarch&quot;&gt;rugarch&lt;/a&gt; - Univariate GARCH Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alexiosg/rmgarch&quot;&gt;rmgarch&lt;/a&gt; - Multivariate GARCH Models.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/edgararuiz/tidypredict&quot;&gt;tidypredict&lt;/a&gt; - Run predictions inside the database &lt;a href=&quot;https://tidypredict.netlify.com/&quot;&gt;https://tidypredict.netlify.com/&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/business-science/tidyquant&quot;&gt;tidyquant&lt;/a&gt; - Bringing financial analysis to the tidyverse.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/business-science/timetk&quot;&gt;timetk&lt;/a&gt; - A toolkit for working with time series in R.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/business-science/tibbletime&quot;&gt;tibbletime&lt;/a&gt; - Built on top of the tidyverse, tibbletime is an extension that allows for the creation of time aware tibbles through the setting of a time index.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/matrix-profile-foundation/matrixprofile&quot;&gt;matrixprofile&lt;/a&gt; - Time series data mining library built on top of the novel Matrix Profile data structure and algorithms.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/AlbertoAlmuinha/garchmodels&quot;&gt;garchmodels&lt;/a&gt; - A parsnip backend for GARCH models.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Calendars&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cran.r-project.org/web/packages/timeDate/index.html&quot;&gt;timeDate&lt;/a&gt; - Chronological and Calendar Objects&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/wilsonfreitas/R-bizdays&quot;&gt;bizdays&lt;/a&gt; - Business days calculations and utilities&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Matlab&lt;/h2&gt; 
&lt;h3&gt;FrameWorks&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yutiansut/quantaxis&quot;&gt;QUANTAXIS&lt;/a&gt; - Integrated Quantitative Toolbox with Matlab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jkirkby3/PROJ_Option_Pricing_Matlab&quot;&gt;PROJ_Option_Pricing_Matlab&lt;/a&gt; - Quant Option Pricing - Exotic/Vanilla: Barrier, Asian, European, American, Parisian, Lookback, Cliquet, Variance Swap, Swing, Forward Starting, Step, Fader&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Julia&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/oliviermilla/Lucky.jl&quot;&gt;Lucky.jl&lt;/a&gt; - Modular, asynchronous trading engine in pure Julia.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pazzo83/QuantLib.jl&quot;&gt;QuantLib.jl&lt;/a&gt; - Quantlib implementation in pure Julia.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/aviks/Ito.jl&quot;&gt;Ito.jl&lt;/a&gt; - A Julia package for quantitative finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/femtotrader/TALib.jl&quot;&gt;TALib.jl&lt;/a&gt; - A Julia wrapper for TA-Lib.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/femtotrader/IncTA.jl&quot;&gt;IncTA.jl&lt;/a&gt; - Julia Incremental Technical Analysis Indicators&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JuliaComputing/Miletus.jl&quot;&gt;Miletus.jl&lt;/a&gt; - A financial contract definition, modeling language, and valuation framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dysonance/Temporal.jl&quot;&gt;Temporal.jl&lt;/a&gt; - Flexible and efficient time series class &amp;amp; methods.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dysonance/Indicators.jl&quot;&gt;Indicators.jl&lt;/a&gt; - Financial market technical analysis &amp;amp; indicators on top of Temporal.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dysonance/Strategems.jl&quot;&gt;Strategems.jl&lt;/a&gt; - Quantitative systematic trading strategy development and backtesting.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JuliaStats/TimeSeries.jl&quot;&gt;TimeSeries.jl&lt;/a&gt; - Time series toolkit for Julia.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JuliaQuant/MarketTechnicals.jl&quot;&gt;MarketTechnicals.jl&lt;/a&gt; - Technical analysis of financial time series on top of TimeSeries.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JuliaQuant/MarketData.jl&quot;&gt;MarketData.jl&lt;/a&gt; - Time series market data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/femtotrader/TimeFrames.jl&quot;&gt;TimeFrames.jl&lt;/a&gt; - A Julia library that defines TimeFrame (essentially for resampling TimeSeries).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JuliaData/DataFrames.jl&quot;&gt;DataFrames.jl&lt;/a&gt; - In-memory tabular data in Julia&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xKDR/TSFrames.jl&quot;&gt;TSFrames.jl&lt;/a&gt; - Handle timeseries data on top of the powerful and mature DataFrames.jl&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Java&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://strata.opengamma.io/&quot;&gt;Strata&lt;/a&gt; - Modern open-source analytics and market risk library designed and written in Java.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/frgomes/jquantlib&quot;&gt;JQuantLib&lt;/a&gt; - JQuantLib is a free, open-source, comprehensive framework for quantitative finance, written in 100% Java.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://finmath.net&quot;&gt;finmath.net&lt;/a&gt; - Java library with algorithms and methodologies related to mathematical finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lsgro/quantcomponents&quot;&gt;quantcomponents&lt;/a&gt; - Free Java components for Quantitative Finance and Algorithmic Trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://lakshmidrip.github.io/DRIP&quot;&gt;DRIP&lt;/a&gt; - Fixed Income, Asset Allocation, Transaction Cost Analysis, XVA Metrics Libraries.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ta4j/ta4j&quot;&gt;ta4j&lt;/a&gt; - A Java library for technical analysis.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;JavaScript&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ebradyjobory/finance.js&quot;&gt;finance.js&lt;/a&gt; - A JavaScript library for common financial calculations.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lequant40/portfolio_allocation_js&quot;&gt;portfolio-allocation&lt;/a&gt; - PortfolioAllocation is a JavaScript library designed to help constructing financial portfolios made of several assets: bonds, commodities, cryptocurrencies, currencies, exchange traded funds (ETFs), mutual funds, stocks...&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ghostfolio/ghostfolio&quot;&gt;Ghostfolio&lt;/a&gt; - Wealth management software to keep track of financial assets like stocks, ETFs or cryptocurrencies and make solid, data-driven investment decisions.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cinar/indicatorts&quot;&gt;IndicatorTS&lt;/a&gt; - Indicator is a TypeScript module providing various stock technical analysis indicators, strategies, and a backtest framework for trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/focus1691/chart-patterns&quot;&gt;chart-patterns&lt;/a&gt; - Technical analysis library for Market Profile, Volume Profile, Stacked Imbalances and High Volume Node indicators.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/focus1691/orderflow&quot;&gt;orderflow&lt;/a&gt; - Orderflow trade aggregator for building Footprint Candles from exchange websocket data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ccxt/ccxt&quot;&gt;ccxt&lt;/a&gt; - A JavaScript / Python / PHP cryptocurrency trading API with support for more than 100 bitcoin/altcoin exchanges.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/CompendiumFi/PENDAX-SDK&quot;&gt;PENDAX&lt;/a&gt; - Javascript SDK for Trading/Data API and Websockets for FTX, FTXUS, OKX, Bybit, &amp;amp; More.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data Visualization&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yutiansut/QUANTAXIS_Webkit&quot;&gt;QUANTAXIS_Webkit&lt;/a&gt; - An awesome visualization center based on quantaxis.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Haskell&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/boundedvariation/quantfin&quot;&gt;quantfin&lt;/a&gt; - quant finance in pure haskell.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MarcusRainbow/Haxcel&quot;&gt;Haxcel&lt;/a&gt; - Excel Addin for Haskell.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MarcusRainbow/Ffinar&quot;&gt;Ffinar&lt;/a&gt; - A financial maths library in Haskell.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Scala&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/choucrifahed/quantscale&quot;&gt;QuantScale&lt;/a&gt; - Scala Quantitative Finance Library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/frankcash/Scala-Quant&quot;&gt;Scala Quant&lt;/a&gt; - Scala library for working with stock data from IFTTT recipes or Google Finance.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ruby&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/unageanu/jiji2&quot;&gt;Jiji&lt;/a&gt; - Open Source Forex algorithmic trading framework using OANDA REST API.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Elixir/Erlang&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fremantle-capital/tai&quot;&gt;Tai&lt;/a&gt; - Open Source composable, real time, market data and trade execution toolkit.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fremantle-industries/workbench&quot;&gt;Workbench&lt;/a&gt; - From Idea to Execution - Manage your trading operation across a globally distributed cluster&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fremantle-industries/prop&quot;&gt;Prop&lt;/a&gt; - An open and opinionated trading platform using productive &amp;amp; familiar open source libraries and tools for strategy research, execution and operation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Golang&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/stellar/kelp&quot;&gt;Kelp&lt;/a&gt; - Kelp is an open-source Golang algorithmic cryptocurrency trading bot that runs on centralized exchanges and Stellar DEX (command-line usage and desktop GUI).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/alpacahq/marketstore&quot;&gt;marketstore&lt;/a&gt; - DataFrame Server for Financial Timeseries Data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cinar/indicator&quot;&gt;IndicatorGo&lt;/a&gt; - IndicatorGo is a Golang module providing various stock technical analysis indicators, strategies, and a backtest framework for trading.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;CPP&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lballabio/QuantLib&quot;&gt;QuantLib&lt;/a&gt; - The QuantLib project is aimed at providing a comprehensive software framework for quantitative finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/auto-differentiation/QuantLib-Risks-Cpp&quot;&gt;QuantLibRisks&lt;/a&gt; - Fast risks with QuantLib in C++&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/auto-differentiation/xad&quot;&gt;XAD&lt;/a&gt; - Automatic Differentation (AAD) Library&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rburkholder/trade-frame&quot;&gt;TradeFrame&lt;/a&gt; - C++ 17 based framework/library (with sample applications) for testing options based automated trading ideas using DTN IQ real time data feed and Interactive Brokers (TWS API) for trade execution. Comes with built-in &lt;a href=&quot;https://github.com/rburkholder/trade-frame/tree/master/lib/TFOptions&quot;&gt;Option Greeks/IV&lt;/a&gt; calculation library.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/fasiondog/hikyuu&quot;&gt;Hikyuu&lt;/a&gt; - A base on Python/C++ open source high-performance quant framework for faster analysis and backtesting, contains the complete trading system components for reuse and combination. You can use python or c++ freely.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Frameworks&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/lballabio/QuantLib&quot;&gt;QuantLib&lt;/a&gt; - The QuantLib project is aimed at providing a comprehensive software framework for quantitative finance.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;QuantLibRisks - Fast risks with QuantLib in &lt;a href=&quot;https://pypi.org/project/QuantLib-Risks/&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://github.com/auto-differentiation/QuantLib-Risks-Cpp&quot;&gt;C++&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;XAD - Automatic Differentiation (AAD) Library in &lt;a href=&quot;https://pypi.org/project/xad/&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://github.com/auto-differentiation/xad/&quot;&gt;C++&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/frgomes/jquantlib&quot;&gt;JQuantLib&lt;/a&gt; - Java port.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/eddelbuettel/rquantlib&quot;&gt;RQuantLib&lt;/a&gt; - R port.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.quantlib.org/quantlibaddin/&quot;&gt;QuantLibAddin&lt;/a&gt; - Excel support.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://www.quantlib.org/quantlibxl/&quot;&gt;QuantLibXL&lt;/a&gt; - Excel support.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/amaggiulli/qlnet&quot;&gt;QLNet&lt;/a&gt; - .Net port.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/enthought/pyql&quot;&gt;PyQL&lt;/a&gt; - Python port.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/pazzo83/QuantLib.jl&quot;&gt;QuantLib.jl&lt;/a&gt; - Julia port.&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://quantlib-python-docs.readthedocs.io/&quot;&gt;QuantLib-Python Documentation&lt;/a&gt; - Documentation for the Python bindings for the QuantLib library&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://ta-lib.org&quot;&gt;TA-Lib&lt;/a&gt; - perform technical analysis of financial market data.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/TA-Lib/ta-lib-python&quot;&gt;ta-lib-python&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/TA-Lib/ta-lib&quot;&gt;ta-lib&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://portfoliooptimizer.io/&quot;&gt;Portfolio Optimizer&lt;/a&gt; - Portfolio Optimizer is a Web API for portfolio analysis and optimization.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;XAD: Automatic Differentation (AAD) Library for &lt;a href=&quot;https://pypi.org/project/xad/&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://github.com/auto-differentiation/xad&quot;&gt;C++&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;CSharp&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/QuantConnect/Lean&quot;&gt;QuantConnect&lt;/a&gt; - Lean Engine is an open-source fully managed C# algorithmic trading engine built for desktop and cloud usage.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/StockSharp/StockSharp&quot;&gt;StockSharp&lt;/a&gt; - Algorithmic trading and quantitative trading open source platform to develop trading robots (stock markets, forex, crypto, bitcoins, and options).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/NVentimiglia/TDAmeritrade.DotNetCore&quot;&gt;TDAmeritrade.DotNetCore&lt;/a&gt; - Free, open-source .NET Client for the TD Ameritrade Trading Platform. Helps developers integrate TD Ameritrade API into custom trading solutions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Rust&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MarcusRainbow/QuantMath&quot;&gt;QuantMath&lt;/a&gt; - Financial maths library for risk-neutral pricing and risk&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/barter-rs/barter-rs&quot;&gt;Barter&lt;/a&gt; - Open-source Rust framework for building event-driven live-trading &amp;amp; backtesting systems&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MathisWellmann/lfest-rs&quot;&gt;LFEST&lt;/a&gt; - Simulated perpetual futures exchange to trade your strategy against.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MathisWellmann/trade_aggregation-rs&quot;&gt;TradeAggregation&lt;/a&gt; - Aggregate trades into user-defined candles using information driven rules.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MathisWellmann/sliding_features-rs&quot;&gt;SlidingFeatures&lt;/a&gt; - Chainable tree-like sliding windows for signal processing and technical analysis.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/avhz/RustQuant&quot;&gt;RustQuant&lt;/a&gt; - Quantitative finance library written in Rust.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Nnamdi-sys/finalytics&quot;&gt;finalytics&lt;/a&gt; - A rust library for financial data analysis.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Reproducing Works, Training &amp;amp; Books&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://auto-differentiation.github.io/&quot;&gt;Auto-Differentiation Website&lt;/a&gt; - Background and resources on Automatic Differentiation (AD) / Adjoint Algorithmic Differentitation (AAD).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/MarcosCarreira/DermanPapers&quot;&gt;Derman Papers&lt;/a&gt; - Notebooks that replicate original quantitative finance papers from Emanuel Derman.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.ml-quant.com/&quot;&gt;ML-Quant&lt;/a&gt; - Top Quant resources like ArXiv (sanity), SSRN, RePec, Journals, Podcasts, Videos, and Blogs.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jasonstrimpel/volatility-trading&quot;&gt;volatility-trading&lt;/a&gt; - A complete set of volatility estimators based on Euan Sinclair&#39;s Volatility Trading.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/paulperry/quant&quot;&gt;quant&lt;/a&gt; - Quantitative Finance and Algorithmic Trading exhaust; mostly ipython notebooks based on Quantopian, Zipline, or Pandas.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/rsvp/fecon235&quot;&gt;fecon235&lt;/a&gt; - Open source project for software tools in financial economics. Many jupyter notebook to verify theoretical ideas and practical methods interactively.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LongOnly/Quantitative-Notebooks&quot;&gt;Quantitative-Notebooks&lt;/a&gt; - Educational notebooks on quantitative finance, algorithmic trading, financial modelling and investment strategy&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://quantecon.org/&quot;&gt;QuantEcon&lt;/a&gt; - Lecture series on economics, finance, econometrics and data science; QuantEcon.py, QuantEcon.jl, notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Finance-Hub/FinanceHub&quot;&gt;FinanceHub&lt;/a&gt; - Resources for Quantitative Finance&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dedwards25/Python_Option_Pricing&quot;&gt;Python_Option_Pricing&lt;/a&gt; - An library to price financial options written in Python. Includes: Black Scholes, Black 76, Implied Volatility, American, European, Asian, Spread Options.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jpmorganchase/python-training&quot;&gt;python-training&lt;/a&gt; - J.P. Morgan&#39;s Python training for business analysts and traders.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LastAncientOne/Stock_Analysis_For_Quant&quot;&gt;Stock_Analysis_For_Quant&lt;/a&gt; - Different Types of Stock Analysis in Excel, Matlab, Power BI, Python, R, and Tableau.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/chrisconlan/algorithmic-trading-with-python&quot;&gt;algorithmic-trading-with-python&lt;/a&gt; - Source code for Algorithmic Trading with Python (2020) by Chris Conlan.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/cerlymarco/MEDIUM_NoteBook&quot;&gt;MEDIUM_NoteBook&lt;/a&gt; - Repository containing notebooks of &lt;a href=&quot;https://github.com/cerlymarco&quot;&gt;cerlymarco&lt;/a&gt;&#39;s posts on Medium.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/PythonCharmers/QuantFinance&quot;&gt;QuantFinance&lt;/a&gt; - Training materials in quantitative finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mgroncki/IPythonScripts&quot;&gt;IPythonScripts&lt;/a&gt; - Tutorials about Quantitative Finance in Python and QuantLib: Pricing, xVAs, Hedging, Portfolio Optimisation, Machine Learning and Deep Learning.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LechGrzelak/Computational-Finance-Course&quot;&gt;Computational-Finance-Course&lt;/a&gt; - Materials for the course of Computational Finance.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/emoen/Machine-Learning-for-Asset-Managers&quot;&gt;Machine-Learning-for-Asset-Managers&lt;/a&gt; - Implementation of code snippets, exercises and application to live data from Machine Learning for Asset Managers (Elements in Quantitative Finance) written by Prof. Marcos L√≥pez de Prado.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/PacktPublishing/Python-for-Finance-Cookbook&quot;&gt;Python-for-Finance-Cookbook&lt;/a&gt; - Python for Finance Cookbook, published by Packt.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ysaporito/modelos_vol_derivativos&quot;&gt;modelos_vol_derivativos&lt;/a&gt; - &quot;Modelos de Volatilidade para Derivativos&quot; book&#39;s Jupyter notebooks&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/enricoschumann/NMOF&quot;&gt;NMOF&lt;/a&gt; - Functions, examples and data from the first and the second edition of &quot;Numerical Methods and Optimization in Finance&quot; by M. Gilli, D. Maringer and E. Schumann (2019, ISBN:978-0128150658).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhilpisch/py4fi2nd&quot;&gt;py4fi2nd&lt;/a&gt; - Jupyter Notebooks and code for Python for Finance (2nd ed., O&#39;Reilly) by Yves Hilpisch.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhilpisch/aiif&quot;&gt;aiif&lt;/a&gt; - Jupyter Notebooks and code for the book Artificial Intelligence in Finance (O&#39;Reilly) by Yves Hilpisch.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhilpisch/py4at&quot;&gt;py4at&lt;/a&gt; - Jupyter Notebooks and code for the book Python for Algorithmic Trading (O&#39;Reilly) by Yves Hilpisch.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhilpisch/dawp&quot;&gt;dawp&lt;/a&gt; - Jupyter Notebooks and code for Derivatives Analytics with Python (Wiley Finance) by Yves Hilpisch.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yhilpisch/dx&quot;&gt;dx&lt;/a&gt; - DX Analytics | Financial and Derivatives Analytics with Python.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LechGrzelak/QuantFinanceBook&quot;&gt;QuantFinanceBook&lt;/a&gt; - Quantitative Finance book.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ryanmccrickerd/rough_bergomi&quot;&gt;rough_bergomi&lt;/a&gt; - A Python implementation of the rough Bergomi model.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ryanmccrickerd/frh-fx&quot;&gt;frh-fx&lt;/a&gt; - A python implementation of the fast-reversion Heston model of Mechkov for FX purposes.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/euclidjda/value-investing-studies&quot;&gt;Value Investing Studies&lt;/a&gt; - A collection of data analysis studies that examine the performance and characteristics of value investing over long periods of time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/firmai/machine-learning-asset-management&quot;&gt;Machine Learning Asset Management&lt;/a&gt; - Machine Learning in Asset Management (by @firmai).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/LastAncientOne/Deep-Learning-Machine-Learning-Stock&quot;&gt;Deep Learning Machine Learning Stock&lt;/a&gt; - Deep Learning and Machine Learning stocks represent a promising long-term or short-term opportunity for investors and traders.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jo-cho/Technical_Analysis_and_Feature_Engineering&quot;&gt;Technical Analysis and Feature Engineering&lt;/a&gt; - Feature Engineering and Feature Importance of Machine Learning in Financial Market.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/differential-machine-learning/notebooks&quot;&gt;Differential Machine Learning and Axes that matter by Brian Huge and Antoine Savine&lt;/a&gt; - Implement, demonstrate, reproduce and extend the results of the Risk articles &#39;Differential Machine Learning&#39; (2020) and &#39;PCA with a Difference&#39; (2021) by Huge and Savine, and cover implementation details left out from the papers.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/robcarver17/systematictradingexamples&quot;&gt;systematictradingexamples&lt;/a&gt; - Examples of code related to book &lt;a href=&quot;https://raw.githubusercontent.com/wilsonfreitas/awesome-quant/master/www.systematictrading.org&quot;&gt;Systematic Trading&lt;/a&gt; and &lt;a href=&quot;http://qoppac.blogspot.com&quot;&gt;blog&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/robcarver17/pysystemtrade_examples&quot;&gt;pysystemtrade_examples&lt;/a&gt; - Examples using pysystemtrade for Robert Carver&#39;s &lt;a href=&quot;http://qoppac.blogspot.com&quot;&gt;blog&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/mfrdixon/ML_Finance_Codes&quot;&gt;ML_Finance_Codes&lt;/a&gt; - Machine Learning in Finance: From Theory to Practice Book&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/packtpublishing/hands-on-machine-learning-for-algorithmic-trading&quot;&gt;Hands-On Machine Learning for Algorithmic Trading&lt;/a&gt; - Hands-On Machine Learning for Algorithmic Trading, published by Packt&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/financialnoob/misc&quot;&gt;financialnoob-misc&lt;/a&gt; - Codes from @financialnoob&#39;s posts&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/deltaray-io/strategy-library&quot;&gt;MesoSim Options Trading Strategy Library&lt;/a&gt; - Free and public Options Trading strategy library for MesoSim.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/lingyixu/Quant-Finance-With-Python-Code&quot;&gt;Quant-Finance-With-Python-Code&lt;/a&gt; - Repo for code examples in Quantitative Finance with Python by Chris Kelliher&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/JoaoJungblut/QuantFinanceTraining&quot;&gt;QuantFinanceTraining&lt;/a&gt; - This repository contains codes that were executed during my training in the CQF (Certificate in Quantitative Finance). The codes are organized by class, facilitating navigation and reference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/YannickKae/Statistical-Learning-based-Portfolio-Optimization&quot;&gt;Statistical-Learning-based-Portfolio-Optimization&lt;/a&gt; - This R Shiny App utilizes the Hierarchical Equal Risk Contribution (HERC) approach, a modern portfolio optimization method developed by Raffinot (2018).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/attack68/book_irds3&quot;&gt;book_irds3&lt;/a&gt; - Code repository for Pricing and Trading Interest Rate Derivatives.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/RichardS0268/Autoencoder-Asset-Pricing-Models&quot;&gt;Autoencoder-Asset-Pricing-Models&lt;/a&gt; - Reimplementation of Autoencoder Asset Pricing Models (&lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3335536&quot;&gt;GKX, 2019&lt;/a&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/shashankvemuri/Finance&quot;&gt;Finance&lt;/a&gt; - 150+ quantitative finance Python programs to help you gather, manipulate, and analyze stock market data.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ram-ki/101_formulaic_alphas&quot;&gt;101_formulaic_alphas&lt;/a&gt; - Implementation of &lt;a href=&quot;https://arxiv.org/ftp/arxiv/papers/1601/1601.00991.pdf&quot;&gt;101 formulaic alphas&lt;/a&gt; using qstrader.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.tidy-finance.org/&quot;&gt;Tidy Finance&lt;/a&gt; - An opinionated approach to empirical research in financial economics - a fully transparent, open-source code base in multiple programming languages (Python and R) to enable the reproducible implementation of financial research projects for students and practitioners.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jgatheral/RoughVolatilityWorkshop&quot;&gt;RoughVolatilityWorkshop&lt;/a&gt; - 2024 QuantMind&#39;s Rough Volatility Workshop lectures.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/boyboi86/AFML&quot;&gt;AFML&lt;/a&gt; - All the answers for exercises from Advances in Financial Machine Learning by Dr Marco Lopez de Parodo.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>fchollet/deep-learning-with-python-notebooks</title>
      <link>https://github.com/fchollet/deep-learning-with-python-notebooks</link>
      <description>&lt;p&gt;Jupyter notebooks for the code samples of the book &quot;Deep Learning with Python&quot;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Companion notebooks for Deep Learning with Python&lt;/h1&gt; 
&lt;p&gt;This repository contains Jupyter notebooks implementing the code samples found in the book &lt;a href=&quot;https://www.manning.com/books/deep-learning-with-python-third-edition?a_aid=keras&amp;amp;a_bid=76564dff&quot;&gt;Deep Learning with Python, third edition (2025)&lt;/a&gt; by Francois Chollet and Matthew Watson. In addition, you will also find the legacy notebooks for the &lt;a href=&quot;https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&amp;amp;a_bid=76564dff&quot;&gt;second edition (2021)&lt;/a&gt; and the &lt;a href=&quot;https://www.manning.com/books/deep-learning-with-python?a_aid=keras&amp;amp;a_bid=76564dff&quot;&gt;first edition (2017)&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For readability, these notebooks only contain runnable code blocks and section titles, and omit everything else in the book: text paragraphs, figures, and pseudocode. &lt;strong&gt;If you want to be able to follow what&#39;s going on, I recommend reading the notebooks side by side with your copy of the book.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Running the code&lt;/h2&gt; 
&lt;p&gt;We recommend running these notebooks on &lt;a href=&quot;https://colab.google&quot;&gt;Colab&lt;/a&gt;, which provides a hosted runtime with all the dependencies you will need. You can also, run these notebooks locally, either by setting up your own Jupyter environment, or using Colab&#39;s instructions for &lt;a href=&quot;https://research.google.com/colaboratory/local-runtimes.html&quot;&gt;running locally&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;By default, all notebooks will run on Colab&#39;s free tier GPU runtime, which is sufficient to run all code in this book. Chapter 8-18 chapters will benefit from a faster GPU if you have a Colab Pro subscription. You can change your runtime type using &lt;strong&gt;Runtime -&amp;gt; Change runtime type&lt;/strong&gt; in Colab&#39;s dropdown menus.&lt;/p&gt; 
&lt;h2&gt;Choosing a backend&lt;/h2&gt; 
&lt;p&gt;The code for third edition is written using Keras 3. As such, it can be run with JAX, TensorFlow or PyTorch as a backend. To set the backend, update the backend in the cell at the top of the colab that looks like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import os
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;jax&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This must be done only once per session before importing Keras. If you are in the middle running a notebook, you will need to restart the notebook session and rerun all relevant notebook cells. This can be done in using &lt;strong&gt;Runtime -&amp;gt; Restart Session&lt;/strong&gt; in Colab&#39;s dropdown menus.&lt;/p&gt; 
&lt;h2&gt;Using Kaggle data&lt;/h2&gt; 
&lt;p&gt;This book uses datasets and model weights provided by Kaggle, an online Machine Learning community and platform. You will need to create a Kaggle login to run Kaggle code in this book; instructions are given in Chapter 8.&lt;/p&gt; 
&lt;p&gt;For chapters that need Kaggle data, you can login to Kaggle once per session when you hit the notebook cell with &lt;code&gt;kagglehub.login()&lt;/code&gt;. Alternately, you can set up your Kaggle login information once as Colab secrets:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Go to &lt;a href=&quot;https://www.kaggle.com/&quot;&gt;https://www.kaggle.com/&lt;/a&gt; and sign in.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;a href=&quot;https://www.kaggle.com/settings&quot;&gt;https://www.kaggle.com/settings&lt;/a&gt; and generate a Kaggle API key.&lt;/li&gt; 
 &lt;li&gt;Open the secrets tab in Colab by clicking the key icon on the left.&lt;/li&gt; 
 &lt;li&gt;Add two secrets, &lt;code&gt;KAGGLE_USERNAME&lt;/code&gt; and &lt;code&gt;KAGGLE_KEY&lt;/code&gt; with the username and key you just created.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Following this approach you will only need to copy your Kaggle secret key once, though you will need to allow each notebook to access your secrets when running the relevant Kaggle code.&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter02_mathematical-building-blocks.ipynb&quot;&gt;Chapter 2: The mathematical building blocks of neural networks&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter03_introduction-to-ml-frameworks.ipynb&quot;&gt;Chapter 3: Introduction to TensorFlow, PyTorch, JAX, and Keras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter04_classification-and-regression.ipynb&quot;&gt;Chapter 4: Classification and regression&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter05_fundamentals-of-ml.ipynb&quot;&gt;Chapter 5: Fundamentals of machine learning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter07_deep-dive-keras.ipynb&quot;&gt;Chapter 7: A deep dive on Keras&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter08_image-classification.ipynb&quot;&gt;Chapter 8: Image Classification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter09_convnet-architecture-patterns.ipynb&quot;&gt;Chapter 9: Convnet architecture patterns&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter10_interpreting-what-convnets-learn.ipynb&quot;&gt;Chapter 10: Interpreting what ConvNets learn&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_image-segmentation.ipynb&quot;&gt;Chapter 11: Image Segmentation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter12_object-detection.ipynb&quot;&gt;Chapter 12: Object Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter13_timeseries-forecasting.ipynb&quot;&gt;Chapter 13: Timeseries Forecasting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter14_text-classification.ipynb&quot;&gt;Chapter 14: Text Classification&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter15_language-models-and-the-transformer.ipynb&quot;&gt;Chapter 15: Language Models and the Transformer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter16_text-generation.ipynb&quot;&gt;Chapter 16: Text Generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter17_image-generation.ipynb&quot;&gt;Chapter 17: Image Generation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter18_best-practices-for-the-real-world.ipynb&quot;&gt;Chapter 18: Best practices for the real world&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain-academy</title>
      <link>https://github.com/langchain-ai/langchain-academy</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba1020525eea7873f96_LCA-big-green%20(2).svg?sanitize=true&quot; alt=&quot;LangChain Academy&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Welcome to LangChain Academy! This is a growing set of modules focused on foundational concepts within the LangChain ecosystem. Module 0 is basic setup and Modules 1 - 4 focus on LangGraph, progressively adding more advanced themes. In each module folder, you&#39;ll see a set of notebooks. A LangChain Academy accompanies each notebook to guide you through the topic. Each module also has a &lt;code&gt;studio&lt;/code&gt; subdirectory, with a set of relevant graphs that we will explore using the LangGraph API and Studio.&lt;/p&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;h3&gt;Python version&lt;/h3&gt; 
&lt;p&gt;To get the most out of this course, please ensure you&#39;re using Python 3.11 or later. This version is required for optimal compatibility with LangGraph. If you&#39;re on an older version, upgrading will ensure everything runs smoothly.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;python3 --version
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Clone repo&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/langchain-ai/langchain-academy.git
$ cd langchain-academy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Create an environment and install dependencies&lt;/h3&gt; 
&lt;h4&gt;Mac/Linux/WSL&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;$ python3 -m venv lc-academy-env
$ source lc-academy-env/bin/activate
$ pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Windows Powershell&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;PS&amp;gt; python3 -m venv lc-academy-env
PS&amp;gt; Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
PS&amp;gt; lc-academy-env\scripts\activate
PS&amp;gt; pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running notebooks&lt;/h3&gt; 
&lt;p&gt;If you don&#39;t have Jupyter set up, follow installation instructions &lt;a href=&quot;https://jupyter.org/install&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ jupyter notebook
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Setting up env variables&lt;/h3&gt; 
&lt;p&gt;Briefly going over how to set up environment variables. You can also use a &lt;code&gt;.env&lt;/code&gt; file with &lt;code&gt;python-dotenv&lt;/code&gt; library.&lt;/p&gt; 
&lt;h4&gt;Mac/Linux/WSL&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;$ export API_ENV_VAR=&quot;your-api-key-here&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Windows Powershell&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;PS&amp;gt; $env:API_ENV_VAR = &quot;your-api-key-here&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Set OpenAI API key&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you don&#39;t have an OpenAI API key, you can sign up &lt;a href=&quot;https://openai.com/index/openai-api/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; in your environment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sign up and Set LangSmith API&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Sign up for LangSmith &lt;a href=&quot;https://smith.langchain.com/&quot;&gt;here&lt;/a&gt;, find out more about LangSmith&lt;/li&gt; 
 &lt;li&gt;and how to use it within your workflow &lt;a href=&quot;https://www.langchain.com/langsmith&quot;&gt;here&lt;/a&gt;, and relevant library &lt;a href=&quot;https://docs.smith.langchain.com/&quot;&gt;docs&lt;/a&gt;!&lt;/li&gt; 
 &lt;li&gt;Set &lt;code&gt;LANGSMITH_API_KEY&lt;/code&gt;, &lt;code&gt;LANGSMITH_TRACING_V2=true&lt;/code&gt; &lt;code&gt;LANGSMITH_PROJECT=&quot;langchain-academy&quot;&lt;/code&gt;in your environment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set up Tavily API for web search&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Tavily Search API is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You can sign up for an API key &lt;a href=&quot;https://tavily.com/&quot;&gt;here&lt;/a&gt;. It&#39;s easy to sign up and offers a very generous free tier. Some lessons (in Module 4) will use Tavily.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Set &lt;code&gt;TAVILY_API_KEY&lt;/code&gt; in your environment.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Set up LangGraph Studio&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;LangGraph Studio is a custom IDE for viewing and testing agents.&lt;/li&gt; 
 &lt;li&gt;Studio can be run locally and opened in your browser on Mac, Windows, and Linux.&lt;/li&gt; 
 &lt;li&gt;See documentation &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/#local-development-server&quot;&gt;here&lt;/a&gt; on the local Studio development server and &lt;a href=&quot;https://langchain-ai.github.io/langgraph/cloud/how-tos/studio/quick_start/#local-development-server&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Graphs for LangGraph Studio are in the &lt;code&gt;module-x/studio/&lt;/code&gt; folders.&lt;/li&gt; 
 &lt;li&gt;To start the local development server, run the following command in your terminal in the &lt;code&gt;/studio&lt;/code&gt; directory each module:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;langgraph dev
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You should see the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;- üöÄ API: http://127.0.0.1:2024
- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024
- üìö API Docs: http://127.0.0.1:2024/docs
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Open your browser and navigate to the Studio UI: &lt;code&gt;https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&lt;/code&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;To use Studio, you will need to create a .env file with the relevant API keys&lt;/li&gt; 
 &lt;li&gt;Run this from the command line to create these files for module 1 to 5, as an example:&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;for i in {1..5}; do
  cp module-$i/studio/.env.example module-$i/studio/.env
  echo &quot;OPENAI_API_KEY=\&quot;$OPENAI_API_KEY\&quot;&quot; &amp;gt; module-$i/studio/.env
done
echo &quot;TAVILY_API_KEY=\&quot;$TAVILY_API_KEY\&quot;&quot; &amp;gt;&amp;gt; module-4/studio/.env
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>AllenDowney/ThinkPython</title>
      <link>https://github.com/AllenDowney/ThinkPython</link>
      <description>&lt;p&gt;Jupyter notebooks and other resources for Think Python by Allen Downey, published by O&#39;Reilly Media.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Think Python, 3rd edition&lt;/h1&gt; 
&lt;p&gt;Jupyter notebooks and other material for the 3rd edition of &lt;em&gt;Think Python: How to Think Like a Computer Scientist&lt;/em&gt;&lt;/p&gt; 
&lt;p&gt;by Allen B. Downey&lt;/p&gt; 
&lt;p&gt;You can order print and electronic versions of &lt;em&gt;Think Python 3e&lt;/em&gt; from &lt;a href=&quot;https://bookshop.org/a/98697/9781098155438&quot;&gt;Bookshop.org&lt;/a&gt; and &lt;a href=&quot;https://www.amazon.com/_/dp/1098155432?smid=ATVPDKIKX0DER&amp;amp;_encoding=UTF8&amp;amp;tag=oreilly20-20&amp;amp;_encoding=UTF8&amp;amp;tag=greenteapre01-20&amp;amp;linkCode=ur2&amp;amp;linkId=e2a529f94920295d27ec8a06e757dc7c&amp;amp;camp=1789&amp;amp;creative=9325&quot;&gt;Amazon&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The home page for the book is at &lt;a href=&quot;http://thinkpython.com&quot;&gt;Green Tea Press&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>langchain-ai/langchain</title>
      <link>https://github.com/langchain-ai/langchain</link>
      <description>&lt;p&gt;ü¶úüîó Build context-aware reasoning applications&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;docs/static/img/logo-dark.svg&quot; /&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;docs/static/img/logo-light.svg&quot; /&gt; 
 &lt;img alt=&quot;LangChain Logo&quot; src=&quot;https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/static/img/logo-dark.svg?sanitize=true&quot; width=&quot;80%&quot; /&gt; 
&lt;/picture&gt; 
&lt;div&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;p&gt;&lt;a href=&quot;https://opensource.org/licenses/MIT&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/l/langchain-core?style=flat-square&quot; alt=&quot;PyPI - License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/langchain-core&quot;&gt;&lt;img src=&quot;https://img.shields.io/pepy/dt/langchain&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/static/v1?label=Dev%20Containers&amp;amp;message=Open&amp;amp;color=blue&amp;amp;logo=visualstudiocode&amp;amp;style=flat-square&quot; alt=&quot;Open in Dev Containers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codespaces.new/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in Github Codespace&quot; title=&quot;Open in Github Codespace&quot; width=&quot;150&quot; height=&quot;20&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codspeed.io/langchain-ai/langchain&quot;&gt;&lt;img src=&quot;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&quot; alt=&quot;CodSpeed Badge&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/langchainai&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&amp;amp;label=Follow%20%40LangChainAI&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] Looking for the JS/TS library? Check out &lt;a href=&quot;https://github.com/langchain-ai/langchainjs&quot;&gt;LangChain.js&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;LangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development ‚Äî all while future-proofing decisions as the underlying technology evolves.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U langchain
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To learn more about LangChain, check out &lt;a href=&quot;https://python.langchain.com/docs/introduction/&quot;&gt;the docs&lt;/a&gt;. If you‚Äôre looking for more advanced customization or agent orchestration, check out &lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt;, our framework for building controllable agent workflows.&lt;/p&gt; 
&lt;h2&gt;Why use LangChain?&lt;/h2&gt; 
&lt;p&gt;LangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.&lt;/p&gt; 
&lt;p&gt;Use LangChain for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Real-time data augmentation&lt;/strong&gt;. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain‚Äôs vast library of integrations with model providers, tools, vector stores, retrievers, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Model interoperability&lt;/strong&gt;. Swap models in and out as your engineering team experiments to find the best choice for your application‚Äôs needs. As the industry frontier evolves, adapt quickly ‚Äî LangChain‚Äôs abstractions keep you moving without losing momentum.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;LangChain‚Äôs ecosystem&lt;/h2&gt; 
&lt;p&gt;While the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.&lt;/p&gt; 
&lt;p&gt;To improve your LLM application development, pair LangChain with:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.langchain.com/langsmith&quot;&gt;LangSmith&lt;/a&gt; - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://langchain-ai.github.io/langgraph/&quot;&gt;LangGraph&lt;/a&gt; - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows ‚Äî and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.langchain.com/langgraph-platform&quot;&gt;LangGraph Platform&lt;/a&gt; - Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams ‚Äî and iterate quickly with visual prototyping in &lt;a href=&quot;https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/&quot;&gt;LangGraph Studio&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Additional resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/tutorials/&quot;&gt;Tutorials&lt;/a&gt;: Simple walkthroughs with guided examples on getting started with LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/how_to/&quot;&gt;How-to Guides&lt;/a&gt;: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/docs/concepts/&quot;&gt;Conceptual Guides&lt;/a&gt;: Explanations of key concepts behind the LangChain framework.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://forum.langchain.com/&quot;&gt;LangChain Forum&lt;/a&gt;: Connect with the community and share all of your technical questions, ideas, and feedback.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://python.langchain.com/api_reference/&quot;&gt;API Reference&lt;/a&gt;: Detailed reference on navigating base packages and integrations for LangChain.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://chat.langchain.com/&quot;&gt;Chat LangChain&lt;/a&gt;: Ask questions &amp;amp; chat with our documentation.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>facebookresearch/sam2</title>
      <link>https://github.com/facebookresearch/sam2</link>
      <description>&lt;p&gt;The repository provides code for running inference with the Meta Segment Anything Model 2 (SAM 2), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SAM 2: Segment Anything in Images and Videos&lt;/h1&gt; 
&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://ai.meta.com/research/&quot;&gt;AI at Meta, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://nikhilaravi.com/&quot;&gt;Nikhila Ravi&lt;/a&gt;, &lt;a href=&quot;https://gabeur.github.io/&quot;&gt;Valentin Gabeur&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=E8DVVYQAAAAJ&amp;amp;hl=en&quot;&gt;Yuan-Ting Hu&lt;/a&gt;, &lt;a href=&quot;https://ronghanghu.com/&quot;&gt;Ronghang Hu&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=4LWx24UAAAAJ&amp;amp;hl=en&quot;&gt;Chaitanya Ryali&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=VeTSl0wAAAAJ&amp;amp;hl=en&quot;&gt;Tengyu Ma&lt;/a&gt;, &lt;a href=&quot;https://hkhedr.com/&quot;&gt;Haitham Khedr&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.de/citations?user=Tpt57v0AAAAJ&amp;amp;hl=en&quot;&gt;Roman R√§dle&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?hl=fr&amp;amp;user=n-SnMhoAAAAJ&quot;&gt;Chloe Rolland&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=c8IpF9gAAAAJ&amp;amp;hl=en&quot;&gt;Laura Gustafson&lt;/a&gt;, &lt;a href=&quot;https://ericmintun.github.io/&quot;&gt;Eric Mintun&lt;/a&gt;, &lt;a href=&quot;https://junting.github.io/&quot;&gt;Junting Pan&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.co.in/citations?user=m34oaWEAAAAJ&amp;amp;hl=en&quot;&gt;Kalyan Vasudev Alwala&lt;/a&gt;, &lt;a href=&quot;https://www.nicolascarion.com/&quot;&gt;Nicolas Carion&lt;/a&gt;, &lt;a href=&quot;https://chaoyuan.org/&quot;&gt;Chao-Yuan Wu&lt;/a&gt;, &lt;a href=&quot;https://www.rossgirshick.info/&quot;&gt;Ross Girshick&lt;/a&gt;, &lt;a href=&quot;https://pdollar.github.io/&quot;&gt;Piotr Doll√°r&lt;/a&gt;, &lt;a href=&quot;https://feichtenhofer.github.io/&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[&lt;a href=&quot;https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/&quot;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.meta.com/sam2&quot;&gt;&lt;code&gt;Project&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://sam2.metademolab.com/&quot;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.meta.com/datasets/segment-anything-video&quot;&gt;&lt;code&gt;Dataset&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://ai.meta.com/blog/segment-anything-2&quot;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#citing-sam-2&quot;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/assets/model_diagram.png?raw=true&quot; alt=&quot;SAM 2 architecture&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Segment Anything Model 2 (SAM 2)&lt;/strong&gt; is a foundation model towards solving promptable visual segmentation in images and videos. We extend SAM to video by considering images as a video with a single frame. The model design is a simple transformer architecture with streaming memory for real-time video processing. We build a model-in-the-loop data engine, which improves model and data via user interaction, to collect &lt;a href=&quot;https://ai.meta.com/datasets/segment-anything-video&quot;&gt;&lt;strong&gt;our SA-V dataset&lt;/strong&gt;&lt;/a&gt;, the largest video segmentation dataset to date. SAM 2 trained on our data provides strong performance across a wide range of tasks and visual domains.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/assets/sa_v_dataset.jpg?raw=true&quot; alt=&quot;SA-V dataset&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Latest updates&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;12/11/2024 -- full model compilation for a major VOS speedup and a new &lt;code&gt;SAM2VideoPredictor&lt;/code&gt; to better handle multi-object tracking&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We now support &lt;code&gt;torch.compile&lt;/code&gt; of the entire SAM 2 model on videos, which can be turned on by setting &lt;code&gt;vos_optimized=True&lt;/code&gt; in &lt;code&gt;build_sam2_video_predictor&lt;/code&gt;, leading to a major speedup for VOS inference.&lt;/li&gt; 
 &lt;li&gt;We update the implementation of &lt;code&gt;SAM2VideoPredictor&lt;/code&gt; to support independent per-object inference, allowing us to relax the assumption of prompting for multi-object tracking and adding new objects after tracking starts.&lt;/li&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/RELEASE_NOTES.md&quot;&gt;&lt;code&gt;RELEASE_NOTES.md&lt;/code&gt;&lt;/a&gt; for full details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;09/30/2024 -- SAM 2.1 Developer Suite (new checkpoints, training code, web demo) is released&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A new suite of improved model checkpoints (denoted as &lt;strong&gt;SAM 2.1&lt;/strong&gt;) are released. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#model-description&quot;&gt;Model Description&lt;/a&gt; for details. 
  &lt;ul&gt; 
   &lt;li&gt;To use the new SAM 2.1 checkpoints, you need the latest model code from this repo. If you have installed an earlier version of this repo, please first uninstall the previous version via &lt;code&gt;pip uninstall SAM-2&lt;/code&gt;, pull the latest code from this repo (with &lt;code&gt;git pull&lt;/code&gt;), and then reinstall the repo following &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#installation&quot;&gt;Installation&lt;/a&gt; below.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;The training (and fine-tuning) code has been released. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/training/README.md&quot;&gt;&lt;code&gt;training/README.md&lt;/code&gt;&lt;/a&gt; on how to get started.&lt;/li&gt; 
 &lt;li&gt;The frontend + backend code for the SAM 2 web demo has been released. See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/demo/README.md&quot;&gt;&lt;code&gt;demo/README.md&lt;/code&gt;&lt;/a&gt; for details.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;SAM 2 needs to be installed first before use. The code requires &lt;code&gt;python&amp;gt;=3.10&lt;/code&gt;, as well as &lt;code&gt;torch&amp;gt;=2.5.1&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.20.1&lt;/code&gt;. Please follow the instructions &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. You can install SAM 2 on a GPU machine using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/facebookresearch/sam2.git &amp;amp;&amp;amp; cd sam2

pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are installing on Windows, it&#39;s strongly recommended to use &lt;a href=&quot;https://learn.microsoft.com/en-us/windows/wsl/install&quot;&gt;Windows Subsystem for Linux (WSL)&lt;/a&gt; with Ubuntu.&lt;/p&gt; 
&lt;p&gt;To use the SAM 2 predictor and run the example notebooks, &lt;code&gt;jupyter&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; are required and can be installed by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -e &quot;.[notebooks]&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;It&#39;s recommended to create a new Python environment via &lt;a href=&quot;https://www.anaconda.com/&quot;&gt;Anaconda&lt;/a&gt; for this installation and install PyTorch 2.5.1 (or higher) via &lt;code&gt;pip&lt;/code&gt; following &lt;a href=&quot;https://pytorch.org/&quot;&gt;https://pytorch.org/&lt;/a&gt;. If you have a PyTorch version lower than 2.5.1 in your current environment, the installation command above will try to upgrade it to the latest PyTorch version using &lt;code&gt;pip&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The step above requires compiling a custom CUDA kernel with the &lt;code&gt;nvcc&lt;/code&gt; compiler. If it isn&#39;t already available on your machine, please install the &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot;&gt;CUDA toolkits&lt;/a&gt; with a version that matches your PyTorch CUDA version.&lt;/li&gt; 
 &lt;li&gt;If you see a message like &lt;code&gt;Failed to build the SAM 2 CUDA extension&lt;/code&gt; during installation, you can ignore it and still use SAM 2 (some post-processing functionality may be limited, but it doesn&#39;t affect the results in most cases).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/INSTALL.md&quot;&gt;&lt;code&gt;INSTALL.md&lt;/code&gt;&lt;/a&gt; for FAQs on potential issues and solutions.&lt;/p&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Download Checkpoints&lt;/h3&gt; 
&lt;p&gt;First, we need to download a model checkpoint. All the model checkpoints can be downloaded by running:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cd checkpoints &amp;amp;&amp;amp; \
./download_ckpts.sh &amp;amp;&amp;amp; \
cd ..
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or individually from:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt&quot;&gt;sam2.1_hiera_tiny.pt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt&quot;&gt;sam2.1_hiera_small.pt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt&quot;&gt;sam2.1_hiera_base_plus.pt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt&quot;&gt;sam2.1_hiera_large.pt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;(note that these are the improved checkpoints denoted as SAM 2.1; see &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/#model-description&quot;&gt;Model Description&lt;/a&gt; for details.)&lt;/p&gt; 
&lt;p&gt;Then SAM 2 can be used in a few lines as follows for image and video prediction.&lt;/p&gt; 
&lt;h3&gt;Image prediction&lt;/h3&gt; 
&lt;p&gt;SAM 2 has all the capabilities of &lt;a href=&quot;https://github.com/facebookresearch/segment-anything&quot;&gt;SAM&lt;/a&gt; on static images, and we provide image prediction APIs that closely resemble SAM for image use cases. The &lt;code&gt;SAM2ImagePredictor&lt;/code&gt; class has an easy interface for image prompting.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor

checkpoint = &quot;./checkpoints/sam2.1_hiera_large.pt&quot;
model_cfg = &quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot;
predictor = SAM2ImagePredictor(build_sam2(model_cfg, checkpoint))

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    predictor.set_image(&amp;lt;your_image&amp;gt;)
    masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the examples in &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/image_predictor_example.ipynb&quot;&gt;image_predictor_example.ipynb&lt;/a&gt; (also in Colab &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/image_predictor_example.ipynb&quot;&gt;here&lt;/a&gt;) for static image use cases.&lt;/p&gt; 
&lt;p&gt;SAM 2 also supports automatic mask generation on images just like SAM. Please see &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/automatic_mask_generator_example.ipynb&quot;&gt;automatic_mask_generator_example.ipynb&lt;/a&gt; (also in Colab &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/automatic_mask_generator_example.ipynb&quot;&gt;here&lt;/a&gt;) for automatic mask generation in images.&lt;/p&gt; 
&lt;h3&gt;Video prediction&lt;/h3&gt; 
&lt;p&gt;For promptable segmentation and tracking in videos, we provide a video predictor with APIs for example to add prompts and propagate masklets throughout a video. SAM 2 supports video inference on multiple objects and uses an inference state to keep track of the interactions in each video.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.build_sam import build_sam2_video_predictor

checkpoint = &quot;./checkpoints/sam2.1_hiera_large.pt&quot;
model_cfg = &quot;configs/sam2.1/sam2.1_hiera_l.yaml&quot;
predictor = build_sam2_video_predictor(model_cfg, checkpoint)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    state = predictor.init_state(&amp;lt;your_video&amp;gt;)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, &amp;lt;your_prompts&amp;gt;):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please refer to the examples in &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/video_predictor_example.ipynb&quot;&gt;video_predictor_example.ipynb&lt;/a&gt; (also in Colab &lt;a href=&quot;https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb&quot;&gt;here&lt;/a&gt;) for details on how to add click or box prompts, make refinements, and track multiple objects in videos.&lt;/p&gt; 
&lt;h2&gt;Load from ü§ó Hugging Face&lt;/h2&gt; 
&lt;p&gt;Alternatively, models can also be loaded from &lt;a href=&quot;https://huggingface.co/models?search=facebook/sam2&quot;&gt;Hugging Face&lt;/a&gt; (requires &lt;code&gt;pip install huggingface_hub&lt;/code&gt;).&lt;/p&gt; 
&lt;p&gt;For image prediction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.sam2_image_predictor import SAM2ImagePredictor

predictor = SAM2ImagePredictor.from_pretrained(&quot;facebook/sam2-hiera-large&quot;)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    predictor.set_image(&amp;lt;your_image&amp;gt;)
    masks, _, _ = predictor.predict(&amp;lt;input_prompts&amp;gt;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For video prediction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torch
from sam2.sam2_video_predictor import SAM2VideoPredictor

predictor = SAM2VideoPredictor.from_pretrained(&quot;facebook/sam2-hiera-large&quot;)

with torch.inference_mode(), torch.autocast(&quot;cuda&quot;, dtype=torch.bfloat16):
    state = predictor.init_state(&amp;lt;your_video&amp;gt;)

    # add new prompts and instantly get the output on the same frame
    frame_idx, object_ids, masks = predictor.add_new_points_or_box(state, &amp;lt;your_prompts&amp;gt;):

    # propagate the prompts to get masklets throughout the video
    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):
        ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Model Description&lt;/h2&gt; 
&lt;h3&gt;SAM 2.1 checkpoints&lt;/h3&gt; 
&lt;p&gt;The table below shows the improved SAM 2.1 checkpoints released on September 29, 2024.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Size (M)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Speed (FPS)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;SA-V test (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;MOSE val (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;LVOS v2 (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_tiny &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_t.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;38.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;91.2&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;71.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;77.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_small &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_s.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;46&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;84.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;73.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;78.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_base_plus &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_b+.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;80.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;64.1&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;78.2&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;73.7&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;78.2&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2.1_hiera_large &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2.1/sam2.1_hiera_l.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;224.4&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;39.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;79.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;80.6&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;SAM 2 checkpoints&lt;/h3&gt; 
&lt;p&gt;The previous SAM 2 checkpoints released on July 29, 2024 can be found as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Size (M)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;Speed (FPS)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;SA-V test (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;MOSE val (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;&lt;strong&gt;LVOS v2 (J&amp;amp;F)&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_tiny &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_t.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;38.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;91.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;75.0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;70.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;75.3&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_small &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_s.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;46&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;85.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.9&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;71.5&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.4&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_base_plus &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_b+.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;80.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;64.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.7&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;72.8&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;75.8&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;sam2_hiera_large &lt;br /&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sam2/configs/sam2/sam2_hiera_l.yaml&quot;&gt;config&lt;/a&gt;, &lt;a href=&quot;https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt&quot;&gt;checkpoint&lt;/a&gt;)&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;224.4&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;39.7&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;76.0&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;74.6&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;79.8&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Speed measured on an A100 with &lt;code&gt;torch 2.5.1, cuda 12.4&lt;/code&gt;. See &lt;code&gt;benchmark.py&lt;/code&gt; for an example on benchmarking (compiling all the model components). Compiling only the image encoder can be more flexible and also provide (a smaller) speed-up (set &lt;code&gt;compile_image_encoder: True&lt;/code&gt; in the config).&lt;/p&gt; 
&lt;h2&gt;Segment Anything Video Dataset&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/sav_dataset/README.md&quot;&gt;sav_dataset/README.md&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Training SAM 2&lt;/h2&gt; 
&lt;p&gt;You can train or fine-tune SAM 2 on custom datasets of images, videos, or both. Please check the training &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/training/README.md&quot;&gt;README&lt;/a&gt; on how to get started.&lt;/p&gt; 
&lt;h2&gt;Web demo for SAM 2&lt;/h2&gt; 
&lt;p&gt;We have released the frontend + backend code for the SAM 2 web demo (a locally deployable version similar to &lt;a href=&quot;https://sam2.metademolab.com/demo&quot;&gt;https://sam2.metademolab.com/demo&lt;/a&gt;). Please see the web demo &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/demo/README.md&quot;&gt;README&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;The SAM 2 model checkpoints, SAM 2 demo code (front-end and back-end), and SAM 2 training code are licensed under &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/LICENSE&quot;&gt;Apache 2.0&lt;/a&gt;, however the &lt;a href=&quot;https://github.com/rsms/inter?tab=OFL-1.1-1-ov-file&quot;&gt;Inter Font&lt;/a&gt; and &lt;a href=&quot;https://github.com/googlefonts/noto-emoji&quot;&gt;Noto Color Emoji&lt;/a&gt; used in the SAM 2 demo code are made available under the &lt;a href=&quot;https://openfontlicense.org/open-font-license-official-text/&quot;&gt;SIL Open Font License, version 1.1&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; and the &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/CODE_OF_CONDUCT.md&quot;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributors&lt;/h2&gt; 
&lt;p&gt;The SAM 2 project was made possible with the help of many contributors (alphabetical):&lt;/p&gt; 
&lt;p&gt;Karen Bergan, Daniel Bolya, Alex Bosenberg, Kai Brown, Vispi Cassod, Christopher Chedeau, Ida Cheng, Luc Dahlin, Shoubhik Debnath, Rene Martinez Doehner, Grant Gardner, Sahir Gomez, Rishi Godugu, Baishan Guo, Caleb Ho, Andrew Huang, Somya Jain, Bob Kamma, Amanda Kallet, Jake Kinney, Alexander Kirillov, Shiva Koduvayur, Devansh Kukreja, Robert Kuo, Aohan Lin, Parth Malani, Jitendra Malik, Mallika Malhotra, Miguel Martin, Alexander Miller, Sasha Mitts, William Ngan, George Orlin, Joelle Pineau, Kate Saenko, Rodrick Shepard, Azita Shokrpour, David Soofian, Jonathan Torres, Jenny Truong, Sagar Vaze, Meng Wang, Claudette Ward, Pengchuan Zhang.&lt;/p&gt; 
&lt;p&gt;Third-party code: we use a GPU-based connected component algorithm adapted from &lt;a href=&quot;https://github.com/zsef123/Connected_components_PyTorch&quot;&gt;&lt;code&gt;cc_torch&lt;/code&gt;&lt;/a&gt; (with its license in &lt;a href=&quot;https://raw.githubusercontent.com/facebookresearch/sam2/main/LICENSE_cctorch&quot;&gt;&lt;code&gt;LICENSE_cctorch&lt;/code&gt;&lt;/a&gt;) as an optional post-processing step for the mask predictions.&lt;/p&gt; 
&lt;h2&gt;Citing SAM 2&lt;/h2&gt; 
&lt;p&gt;If you use SAM 2 or the SA-V dataset in your research, please use the following BibTeX entry.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\&quot;a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\&#39;a}r, Piotr and Feichtenhofer, Christoph},
  journal={arXiv preprint arXiv:2408.00714},
  url={https://arxiv.org/abs/2408.00714},
  year={2024}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>QwenLM/Qwen2.5-Omni</title>
      <link>https://github.com/QwenLM/Qwen2.5-Omni</link>
      <description>&lt;p&gt;Qwen2.5-Omni is an end-to-end multimodal model by Qwen team at Alibaba Cloud, capable of understanding text, audio, vision, video, and performing real-time speech generation.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen2.5-Omni&lt;/h1&gt; 
&lt;p align=&quot;left&quot;&gt; &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/README_CN.md&quot;&gt;‰∏≠Êñá&lt;/a&gt; &amp;nbsp;ÔΩú &amp;nbsp; English&amp;nbsp;&amp;nbsp; &lt;/p&gt; 
&lt;br /&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/Omni_logo.png&quot; width=&quot;400&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; üíú &lt;a href=&quot;https://chat.qwenlm.ai/&quot;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e&quot;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&quot;https://modelscope.cn/collections/Qwen25-Omni-a2505ce0d5514e&quot;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-omni/&quot;&gt;Blog&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìö &lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks&quot;&gt;Cookbooks&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://arxiv.org/abs/2503.20215&quot;&gt;Paper&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;br /&gt; üñ•Ô∏è &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo &quot;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&quot;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&quot;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&quot;https://discord.gg/CV4E9rpNSD&quot;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üìë &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni&quot;&gt;API&lt;/a&gt; 
 &lt;!-- &amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspüñ•Ô∏è &lt;a href=&quot;https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl&quot;&gt;PAI-DSW&lt;/a&gt; --&gt; &lt;/p&gt; 
&lt;p&gt;We release &lt;strong&gt;Qwen2.5-Omni&lt;/strong&gt;, the new flagship end-to-end multimodal model in the Qwen series. Designed for comprehensive multimodal perception, it seamlessly processes diverse inputs including text, images, audio, and video, while delivering real-time streaming responses through both text generation and natural speech synthesis. Let&#39;s click the video below for more information üòÉ&lt;/p&gt; 
&lt;a href=&quot;https://youtu.be/yKcANdkRuNI&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/video_cover.png&quot; alt=&quot;Open Video&quot; /&gt; &lt;/a&gt; 
&lt;h2&gt;News&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2025.06.12: Qwen2.5-Omni-7B ranked first among open source models in the spoken language understanding and reasoning benchmark &lt;a href=&quot;https://arxiv.org/abs/2506.04779&quot;&gt;MMSU&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;2025.06.09: Congratulations to our open source Qwen2.5-Omni-7B for ranking first in the &lt;a href=&quot;https://sakshi113.github.io/mmau_homepage/#leaderboard&quot;&gt;MMAU&lt;/a&gt; leaderboard, and first in the &lt;a href=&quot;https://github.com/ddlBoJack/MMAR&quot;&gt;MMAR&lt;/a&gt; of open source models in the audio understanding and reasoning evaluation!&lt;/li&gt; 
 &lt;li&gt;2025.05.16: We release 4-bit quantized Qwen2.5-Omni-7B (GPTQ-Int4/AWQ) models that maintain comparable performance to the original version on multimodal evaluations while reducing GPU VRAM consumption by over 50%+. See &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#gptq-int4-and-awq-usage&quot;&gt;GPTQ-Int4 and AWQ Usage&lt;/a&gt; for details, and models can be obtained from Hugging Face (&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ&quot;&gt;AWQ&lt;/a&gt;) and ModelScope (&lt;a href=&quot;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&quot;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-AWQ&quot;&gt;AWQ&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;2025.05.13: &lt;a href=&quot;https://github.com/alibaba/MNN/raw/master/apps/Android/MnnLlmChat/README.md#releases&quot;&gt;MNN Chat App&lt;/a&gt; support Qwen2.5-Omni now, let&#39;s experience Qwen2.5-Omni on the edge devices! Please refer to &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-mnn&quot;&gt;Deployment with MNN&lt;/a&gt; for information about memory consumption and inference speed benchmarks.&lt;/li&gt; 
 &lt;li&gt;2025.04.30: Exciting! We We have released Qwen2.5-Omni-3B to enable more platforms to run Qwen2.5-Omni. The model can be downloaded from &lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Omni-3B&quot;&gt;Hugging Face&lt;/a&gt;. The &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#performance&quot;&gt;performance&lt;/a&gt; of this model is updated, and please refer to &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#minimum-gpu-memory-requirements&quot;&gt;Minimum GPU memory requirements&lt;/a&gt; for information about resource consumption. And for best experience, &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#--transformers-usage&quot;&gt;transformers&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-vllm&quot;&gt;vllm&lt;/a&gt; code have update, you can pull the &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&quot;&gt;official docker&lt;/a&gt; again to get them.&lt;/li&gt; 
 &lt;li&gt;2025.04.11: We release the new vllm version which support audio ouput now! Please experience it from source or our docker image.&lt;/li&gt; 
 &lt;li&gt;2025.04.02: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Qwen2.5-Omni reaches top-1 on Hugging Face Trending!&lt;/li&gt; 
 &lt;li&gt;2025.03.29: ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è Qwen2.5-Omni reaches top-2 on Hugging Face Trending!&lt;/li&gt; 
 &lt;li&gt;2025.03.26: Real-time interaction with Qwen2.5-Omni is available on &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;Qwen Chat&lt;/a&gt;. Let&#39;s start this amazing journey now!&lt;/li&gt; 
 &lt;li&gt;2025.03.26: We have released the &lt;a href=&quot;https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e&quot;&gt;Qwen2.5-Omni&lt;/a&gt;. For more details, please check our &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-omni/&quot;&gt;blog&lt;/a&gt;!&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#overview&quot;&gt;Overview&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#key-features&quot;&gt;Key Features&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#model-architecture&quot;&gt;Model Architecture&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#performance&quot;&gt;Performance&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#quickstart&quot;&gt;Quickstart&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#--transformers-usage&quot;&gt;Transformers Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-modelscope-usage&quot;&gt;ModelScope Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#gptq-int4-and-awq-usage&quot;&gt;GPTQ-Int4 and AWQ Usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#usage-tips&quot;&gt;Usage Tips&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#cookbooks-for-more-usage-cases&quot;&gt;Cookbooks for More Usage Cases&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#api-inference&quot;&gt;API inference&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#customization-settings&quot;&gt;Customization Settings&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#chat-with-qwen25-omni&quot;&gt;Chat with Qwen2.5-Omni&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#online-demo&quot;&gt;Online Demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#launch-local-web-ui-demo&quot;&gt;Launch Local Web UI Demo&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#real-time-interaction&quot;&gt;Real-Time Interaction&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-vllm&quot;&gt;Deployment with vLLM&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#deployment-with-mnn&quot;&gt;Deployment with MNN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&quot;&gt;Docker&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- - [Citation](#citation) --&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;h3&gt;Introduction&lt;/h3&gt; 
&lt;p&gt;Qwen2.5-Omni is an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Key Features&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Omni and Novel Architecture&lt;/strong&gt;: We propose Thinker-Talker architecture, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. We propose a novel position embedding, named TMRoPE (Time-aligned Multimodal RoPE), to synchronize the timestamps of video inputs with audio.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Real-Time Voice and Video Chat&lt;/strong&gt;: Architecture designed for fully real-time interactions, supporting chunked input and immediate output.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Natural and Robust Speech Generation&lt;/strong&gt;: Surpassing many existing streaming and non-streaming alternatives, demonstrating superior robustness and naturalness in speech generation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Strong Performance Across Modalities&lt;/strong&gt;: Exhibiting exceptional performance across all modalities when benchmarked against similarly sized single-modality models. Qwen2.5-Omni outperforms the similarly sized Qwen2-Audio in audio capabilities and achieves comparable performance to Qwen2.5-VL-7B.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Excellent End-to-End Speech Instruction Following&lt;/strong&gt;: Qwen2.5-Omni shows performance in end-to-end speech instruction following that rivals its effectiveness with text inputs, evidenced by benchmarks such as MMLU and GSM8K.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Model Architecture&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3&gt;Performance&lt;/h3&gt; 
&lt;p&gt;We conducted a comprehensive evaluation of Qwen2.5-Omni, which demonstrates strong performance across all modalities when compared to similarly sized single-modality models and closed-source models like Qwen2.5-VL-7B, Qwen2-Audio, and Gemini-1.5-pro. In tasks requiring the integration of multiple modalities, such as OmniBench, Qwen2.5-Omni achieves state-of-the-art performance. Furthermore, in single-modality tasks, it excels in areas including speech recognition (Common Voice), translation (CoVoST2), audio understanding (MMAU), image reasoning (MMMU, MMStar), video understanding (MVBench), and speech generation (Seed-tts-eval and subjective naturalness).&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png&quot; /&gt; &lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;details&gt; 
 &lt;summary&gt;Multimodality -&amp;gt; Text&lt;/summary&gt; 
 &lt;table class=&quot;tg&quot;&gt;
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Datasets&lt;/th&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Model&lt;/th&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Performance&lt;/th&gt; 
   &lt;/tr&gt;
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;10&quot;&gt;OmniBench&lt;br /&gt;Speech | Sound Event | Music | Avg&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Gemini-1.5-Pro&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;42.67%|42.26%|46.23%|42.91%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MIO-Instruct&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;36.96%|33.58%|11.32%|33.80%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;AnyGPT (7B)&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;17.77%|20.75%|13.21%|18.04%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;video-SALMONN&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;34.11%|31.70%|&lt;strong&gt;56.60%&lt;/strong&gt;|35.64%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;UnifiedIO2-xlarge&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;39.56%|36.98%|29.25%|38.00%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;UnifiedIO2-xxlarge&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;34.24%|36.98%|24.53%|33.98%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|-|40.50%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Baichuan-Omni-1.5&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|-|42.90%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;52.14%|52.08%|52.83%|52.19%&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;55.25%&lt;/strong&gt;|&lt;strong&gt;60.00%&lt;/strong&gt;|52.83%|&lt;strong&gt;56.13%&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Audio -&amp;gt; Text&lt;/summary&gt; 
 &lt;table class=&quot;tg&quot;&gt;
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Datasets&lt;/th&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Model&lt;/th&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Performance&lt;/th&gt; 
   &lt;/tr&gt;
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;ASR&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;12&quot;&gt;Librispeech&lt;br /&gt;dev-clean | dev other | test-clean | test-other&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;SALMONN&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|2.1|4.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;SpeechVerse&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|2.1|4.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Whisper-large-v3&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|1.8|3.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Llama-3-8B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|-|3.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Llama-3-70B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|-|3.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-ASR-Multilingual&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|&lt;strong&gt;1.6&lt;/strong&gt;|&lt;strong&gt;2.8&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|1.7|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MinMo&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|1.7|3.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.8|4.0|2.0|4.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;1.3&lt;/strong&gt;|&lt;strong&gt;3.4&lt;/strong&gt;|&lt;strong&gt;1.6&lt;/strong&gt;|3.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;2.0|4.1|2.2|4.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.6|3.5|1.8|3.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;5&quot;&gt;Common Voice 15&lt;br /&gt;en | zh | yue | fr&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Whisper-large-v3&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;9.3|12.8|10.9|10.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MinMo&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;7.9|6.3|6.4|8.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;8.6|6.9|&lt;strong&gt;5.9&lt;/strong&gt;|9.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;9.1|6.0|11.6|9.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;7.6&lt;/strong&gt;|&lt;strong&gt;5.2&lt;/strong&gt;|7.3|&lt;strong&gt;7.5&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;8&quot;&gt;Fleurs&lt;br /&gt;zh | en&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Whisper-large-v3&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;7.7|4.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-ASR-Multilingual&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|&lt;strong&gt;3.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Megrez-3B-Omni&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;10.8|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;4.4|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MinMo&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;3.0|3.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;7.5|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;3.2|5.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;3.0&lt;/strong&gt;|4.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;6&quot;&gt;Wenetspeech&lt;br /&gt;test-net | test-meeting&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-ASR-Chinese&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;4.7|5.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Megrez-3B-Omni&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|16.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;6.9|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MinMo&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;6.8|7.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;6.3|8.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;5.9|7.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;4&quot;&gt;Voxpopuli-V1.0-en&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Llama-3-8B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;6.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Llama-3-70B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;5.7&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;6.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;5.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;S2TT&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;9&quot;&gt;CoVoST2&lt;br /&gt;en-de | de-en | en-zh | zh-en&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;SALMONN&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;18.6|-|33.1|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;SpeechLLaMA&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|27.1|-|12.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;BLSP&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;14.1|-|-|-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|-|&lt;strong&gt;48.2&lt;/strong&gt;|27.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MinMo&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;-|&lt;strong&gt;39.9&lt;/strong&gt;|46.7|26.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;25.1|33.9|41.5|15.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;29.9|35.2|45.2|24.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;28.3|38.1|41.4|26.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;30.2&lt;/strong&gt;|37.7|41.4|&lt;strong&gt;29.4&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;SER&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;6&quot;&gt;Meld&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;WavLM-large&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.542&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.524&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.557&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.553&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.558&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.570&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;VSC&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;6&quot;&gt;VocalSound&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;CLAP&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.495&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Pengi&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.604&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.929&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.939&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.936&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.939&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;Music&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;3&quot;&gt;GiantSteps Tempo&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Llark-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.86&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.88&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.88&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;3&quot;&gt;MusicCaps&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;LP-MusicCaps&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.291|0.149|0.089|&lt;strong&gt;0.061&lt;/strong&gt;|0.129|0.130&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.325|&lt;strong&gt;0.163&lt;/strong&gt;|&lt;strong&gt;0.093&lt;/strong&gt;|0.057|&lt;strong&gt;0.132&lt;/strong&gt;|&lt;strong&gt;0.229&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.328&lt;/strong&gt;|0.162|0.090|0.055|0.127|0.225&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;Audio Reasoning&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;4&quot;&gt;MMAU&lt;br /&gt;Sound | Music | Speech | Avg&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Gemini-Pro-V1.5&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;56.75|49.40|58.55|54.90&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;54.95|50.98|42.04|49.20&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;70.27&lt;/strong&gt;|60.48|59.16|63.30&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;67.87|&lt;strong&gt;69.16|59.76|65.60&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;Voice Chatting&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;9&quot;&gt;VoiceBench&lt;br /&gt;AlpacaEval | CommonEval | SD-QA | MMSU&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Ultravox-v0.4.1-LLaMA-3.1-8B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;4.55&lt;/strong&gt;|3.90|53.35|47.17&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MERaLiON&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;4.50|3.77|55.06|34.95&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Megrez-3B-Omni&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;3.50|2.95|25.95|27.03&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Lyra-Base&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;3.85|3.50|38.25|49.74&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;4.42|&lt;strong&gt;4.15&lt;/strong&gt;|50.72|54.78&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Baichuan-Omni-1.5&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;4.50|4.05|43.40|57.25&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;3.74|3.43|35.71|35.72&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;4.32|4.00|49.37|50.23&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;4.49|3.93|&lt;strong&gt;55.71&lt;/strong&gt;|&lt;strong&gt;61.32&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;9&quot;&gt;VoiceBench&lt;br /&gt;OpenBookQA | IFEval | AdvBench | Avg&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Ultravox-v0.4.1-LLaMA-3.1-8B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;65.27|&lt;strong&gt;66.88&lt;/strong&gt;|98.46|71.45&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MERaLiON&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;27.23|62.93|94.81|62.91&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Megrez-3B-Omni&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;28.35|25.71|87.69|46.25&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Lyra-Base&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;72.75|36.28|59.62|57.66&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MiniCPM-o&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;78.02|49.25|97.69|71.69&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Baichuan-Omni-1.5&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;74.51|54.54|97.31|71.14&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2-Audio&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;49.45|26.33|96.73|55.35&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;74.73|42.10|98.85|68.81&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;81.10&lt;/strong&gt;|52.87|&lt;strong&gt;99.42&lt;/strong&gt;|&lt;strong&gt;74.12&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Image -&amp;gt; Text&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Dataset&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; 
    &lt;th&gt;Other Best&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; 
    &lt;th&gt;GPT-4o-mini&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMMU&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;59.2&lt;/td&gt; 
    &lt;td&gt;53.1&lt;/td&gt; 
    &lt;td&gt;53.9&lt;/td&gt; 
    &lt;td&gt;58.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;60.0&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMMU-Pro&lt;sub&gt;overall&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;36.6&lt;/td&gt; 
    &lt;td&gt;29.7&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;38.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;37.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MathVista&lt;sub&gt;testmini&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;67.9&lt;/td&gt; 
    &lt;td&gt;59.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;68.2&lt;/td&gt; 
    &lt;td&gt;52.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MathVision&lt;sub&gt;full&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;25.0&lt;/td&gt; 
    &lt;td&gt;20.8&lt;/td&gt; 
    &lt;td&gt;23.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;25.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMBench-V1.1-EN&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;81.8&lt;/td&gt; 
    &lt;td&gt;77.8&lt;/td&gt; 
    &lt;td&gt;80.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;82.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;76.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMVet&lt;sub&gt;turbo&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;66.8&lt;/td&gt; 
    &lt;td&gt;62.1&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;67.1&lt;/td&gt; 
    &lt;td&gt;66.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMStar&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;55.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;64.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;54.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MME&lt;sub&gt;sum&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;2340&lt;/td&gt; 
    &lt;td&gt;2117&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;2372&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;2347&lt;/td&gt; 
    &lt;td&gt;2003&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MuirBench&lt;/td&gt; 
    &lt;td&gt;59.2&lt;/td&gt; 
    &lt;td&gt;48.0&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;59.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;CRPE&lt;sub&gt;relation&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;76.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.7&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;76.4&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;RealWorldQA&lt;sub&gt;avg&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;70.3&lt;/td&gt; 
    &lt;td&gt;62.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;68.5&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MME-RealWorld&lt;sub&gt;en&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;61.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;55.6&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;57.4&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MM-MT-Bench&lt;/td&gt; 
    &lt;td&gt;6.0&lt;/td&gt; 
    &lt;td&gt;5.0&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;6.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;AI2D&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;79.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;83.9&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;TextVQA&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;84.4&lt;/td&gt; 
    &lt;td&gt;79.8&lt;/td&gt; 
    &lt;td&gt;83.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;DocVQA&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;95.2&lt;/td&gt; 
    &lt;td&gt;93.3&lt;/td&gt; 
    &lt;td&gt;93.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;95.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ChartQA&lt;sub&gt;test Avg&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;85.3&lt;/td&gt; 
    &lt;td&gt;82.8&lt;/td&gt; 
    &lt;td&gt;84.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;OCRBench_V2&lt;sub&gt;en&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;57.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;51.7&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;56.3&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Dataset&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; 
    &lt;th&gt;Grounding DINO&lt;/th&gt; 
    &lt;th&gt;Gemini 1.5 Pro&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcoco&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;90.5&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;90.0&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;90.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcoco&lt;sub&gt;textA&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;93.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;91.8&lt;/td&gt; 
    &lt;td&gt;92.5&lt;/td&gt; 
    &lt;td&gt;93.2&lt;/td&gt; 
    &lt;td&gt;72.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcoco&lt;sub&gt;textB&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;86.6&lt;/td&gt; 
    &lt;td&gt;84.0&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;74.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcoco+&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;85.4&lt;/td&gt; 
    &lt;td&gt;81.1&lt;/td&gt; 
    &lt;td&gt;84.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;88.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;62.5&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcoco+&lt;sub&gt;textA&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;87.5&lt;/td&gt; 
    &lt;td&gt;89.1&lt;/td&gt; 
    &lt;td&gt;89.0&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcoco+&lt;sub&gt;textB&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;73.2&lt;/td&gt; 
    &lt;td&gt;76.9&lt;/td&gt; 
    &lt;td&gt;75.9&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcocog+&lt;sub&gt;val&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;85.0&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;86.1&lt;/td&gt; 
    &lt;td&gt;75.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Refcocog+&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;87.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;85.1&lt;/td&gt; 
    &lt;td&gt;87.2&lt;/td&gt; 
    &lt;td&gt;87.0&lt;/td&gt; 
    &lt;td&gt;76.2&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;ODinW&lt;/td&gt; 
    &lt;td&gt;42.4&lt;/td&gt; 
    &lt;td&gt;39.2&lt;/td&gt; 
    &lt;td&gt;37.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;36.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;PointGrounding&lt;/td&gt; 
    &lt;td&gt;66.5&lt;/td&gt; 
    &lt;td&gt;46.2&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;67.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video(without audio) -&amp;gt; Text&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Dataset&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; 
    &lt;th&gt;Other Best&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-VL-7B&lt;/th&gt; 
    &lt;th&gt;GPT-4o-mini&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Video-MME&lt;sub&gt;w/o sub&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;64.3&lt;/td&gt; 
    &lt;td&gt;62.0&lt;/td&gt; 
    &lt;td&gt;63.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;65.1&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;64.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;Video-MME&lt;sub&gt;w sub&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;72.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;68.6&lt;/td&gt; 
    &lt;td&gt;67.9&lt;/td&gt; 
    &lt;td&gt;71.6&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MVBench&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;68.7&lt;/td&gt; 
    &lt;td&gt;67.2&lt;/td&gt; 
    &lt;td&gt;69.6&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;EgoSchema&lt;sub&gt;test&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;68.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;61.4&lt;/td&gt; 
    &lt;td&gt;63.2&lt;/td&gt; 
    &lt;td&gt;65.0&lt;/td&gt; 
    &lt;td&gt;-&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Zero-shot Speech Generation&lt;/summary&gt; 
 &lt;table class=&quot;tg&quot;&gt;
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Datasets&lt;/th&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Model&lt;/th&gt; 
    &lt;th class=&quot;tg-0lax&quot;&gt;Performance&lt;/th&gt; 
   &lt;/tr&gt;
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;Content Consistency&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;11&quot;&gt;SEED&lt;br /&gt;test-zh | test-en | test-hard &lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-TTS_ICL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.11 | 2.24 | 7.58&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-TTS_RL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;1.00&lt;/strong&gt; | 1.94 | &lt;strong&gt;6.42&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MaskGCT&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;2.27 | 2.62 | 10.27&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;E2_TTS&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.97 | 2.19 | -&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;F5-TTS&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.56 | &lt;strong&gt;1.83&lt;/strong&gt; | 8.67&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;CosyVoice 2&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.45 | 2.57 | 6.83&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;CosyVoice 2-S&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.45 | 2.38 | 8.08&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B_ICL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.95 | 2.87 | 9.92&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B_RL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.58 | 2.51 | 7.86&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B_ICL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.70 | 2.72 | 7.97&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B_RL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;1.42 | 2.32 | 6.54&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-9j4x&quot; colspan=&quot;3&quot;&gt;Speaker Similarity&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot; rowspan=&quot;11&quot;&gt;SEED&lt;br /&gt;test-zh | test-en | test-hard &lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-TTS_ICL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.796 | 0.762 | 0.776&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Seed-TTS_RL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;&lt;strong&gt;0.801&lt;/strong&gt; | &lt;strong&gt;0.766&lt;/strong&gt; | &lt;strong&gt;0.782&lt;/strong&gt;&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;MaskGCT&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.774 | 0.714 | 0.748&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;E2_TTS&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.730 | 0.710 | -&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;F5-TTS&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.741 | 0.647 | 0.713&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;CosyVoice 2&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.748 | 0.652 | 0.724&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;CosyVoice 2-S&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.753 | 0.654 | 0.732&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B_ICL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.741 | 0.635 | 0.748&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-3B_RL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.744 | 0.635 | 0.746&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B_ICL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.752 | 0.632 | 0.747&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;Qwen2.5-Omni-7B_RL&lt;/td&gt; 
    &lt;td class=&quot;tg-0lax&quot;&gt;0.754 | 0.641 | 0.752&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Text -&amp;gt; Text&lt;/summary&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Dataset&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-Omni-3B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-7B&lt;/th&gt; 
    &lt;th&gt;Qwen2.5-3B&lt;/th&gt; 
    &lt;th&gt;Qwen2-7B&lt;/th&gt; 
    &lt;th&gt;Llama3.1-8B&lt;/th&gt; 
    &lt;th&gt;Gemma2-9B&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMLU-Pro&lt;/td&gt; 
    &lt;td&gt;47.0&lt;/td&gt; 
    &lt;td&gt;40.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;56.3&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;43.7&lt;/td&gt; 
    &lt;td&gt;44.1&lt;/td&gt; 
    &lt;td&gt;48.3&lt;/td&gt; 
    &lt;td&gt;52.1&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MMLU-redux&lt;/td&gt; 
    &lt;td&gt;71.0&lt;/td&gt; 
    &lt;td&gt;60.9&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;64.4&lt;/td&gt; 
    &lt;td&gt;67.3&lt;/td&gt; 
    &lt;td&gt;67.2&lt;/td&gt; 
    &lt;td&gt;72.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LiveBench&lt;sub&gt;0831&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;29.6&lt;/td&gt; 
    &lt;td&gt;22.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;35.9&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;26.8&lt;/td&gt; 
    &lt;td&gt;29.2&lt;/td&gt; 
    &lt;td&gt;26.7&lt;/td&gt; 
    &lt;td&gt;30.6&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GPQA&lt;/td&gt; 
    &lt;td&gt;30.8&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;36.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;30.3&lt;/td&gt; 
    &lt;td&gt;34.3&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
    &lt;td&gt;32.8&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MATH&lt;/td&gt; 
    &lt;td&gt;71.5&lt;/td&gt; 
    &lt;td&gt;63.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;75.5&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;65.9&lt;/td&gt; 
    &lt;td&gt;52.9&lt;/td&gt; 
    &lt;td&gt;51.9&lt;/td&gt; 
    &lt;td&gt;44.3&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;GSM8K&lt;/td&gt; 
    &lt;td&gt;88.7&lt;/td&gt; 
    &lt;td&gt;82.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;86.7&lt;/td&gt; 
    &lt;td&gt;85.7&lt;/td&gt; 
    &lt;td&gt;84.5&lt;/td&gt; 
    &lt;td&gt;76.7&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;HumanEval&lt;/td&gt; 
    &lt;td&gt;78.7&lt;/td&gt; 
    &lt;td&gt;70.7&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;74.4&lt;/td&gt; 
    &lt;td&gt;79.9&lt;/td&gt; 
    &lt;td&gt;72.6&lt;/td&gt; 
    &lt;td&gt;68.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MBPP&lt;/td&gt; 
    &lt;td&gt;73.2&lt;/td&gt; 
    &lt;td&gt;70.4&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;79.2&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;72.7&lt;/td&gt; 
    &lt;td&gt;67.2&lt;/td&gt; 
    &lt;td&gt;69.6&lt;/td&gt; 
    &lt;td&gt;74.9&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;MultiPL-E&lt;/td&gt; 
    &lt;td&gt;65.8&lt;/td&gt; 
    &lt;td&gt;57.6&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;70.4&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;60.2&lt;/td&gt; 
    &lt;td&gt;59.1&lt;/td&gt; 
    &lt;td&gt;50.7&lt;/td&gt; 
    &lt;td&gt;53.4&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;LiveCodeBench&lt;sub&gt;2305-2409&lt;/sub&gt;&lt;/td&gt; 
    &lt;td&gt;24.6&lt;/td&gt; 
    &lt;td&gt;16.5&lt;/td&gt; 
    &lt;td&gt;&lt;strong&gt;28.7&lt;/strong&gt;&lt;/td&gt; 
    &lt;td&gt;19.9&lt;/td&gt; 
    &lt;td&gt;23.9&lt;/td&gt; 
    &lt;td&gt;8.3&lt;/td&gt; 
    &lt;td&gt;18.9&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;p&gt;Below, we provide simple examples to show how to use Qwen2.5-Omni with ü§ñ ModelScope and ü§ó Transformers.&lt;/p&gt; 
&lt;p&gt;The codes of Qwen2.5-Omni has been in the latest Hugging face transformers and we advise you to install with command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install transformers==4.52.3
pip install accelerate
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or you might encounter the following error:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;KeyError: &#39;qwen2_5_omni&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and you can also use our &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&quot;&gt;official docker image&lt;/a&gt; to start without building from source.&lt;/p&gt; 
&lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved audio, images and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# It&#39;s highly recommended to use `[decord]` feature for faster video loading.
pip install qwen-omni-utils[decord] -U
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are not using Linux, you might not be able to install &lt;code&gt;decord&lt;/code&gt; from PyPI. In that case, you can use &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt; which will fall back to using torchvision for video processing. However, you can still &lt;a href=&quot;https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source&quot;&gt;install decord from source&lt;/a&gt; to get decord used when loading video.&lt;/p&gt; 
&lt;p&gt;We are preparing &lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks&quot;&gt;cookbooks&lt;/a&gt; for many capabilities, including audio understanding, voice chatting, screen recording interaction, video information extracting, omni chatting and more. Welcome to learn more!&lt;/p&gt; 
&lt;h3&gt;ü§ó Transformers Usage&lt;/h3&gt; 
&lt;p&gt;Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import soundfile as sf

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

# default: Load the model on the available device(s)
model = Qwen2_5OmniForConditionalGeneration.from_pretrained(&quot;Qwen/Qwen2.5-Omni-7B&quot;, torch_dtype=&quot;auto&quot;, device_map=&quot;auto&quot;)

# We recommend enabling flash_attention_2 for better acceleration and memory saving.
# model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
#     &quot;Qwen/Qwen2.5-Omni-7B&quot;,
#     torch_dtype=&quot;auto&quot;,
#     device_map=&quot;auto&quot;,
#     attn_implementation=&quot;flash_attention_2&quot;,
# )

processor = Qwen2_5OmniProcessor.from_pretrained(&quot;Qwen/Qwen2.5-Omni-7B&quot;)

conversation = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4&quot;},
        ],
    },
]

# set use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&quot;pt&quot;, padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(text)
sf.write(
    &quot;output.wav&quot;,
    audio.reshape(-1).detach().cpu().numpy(),
    samplerate=24000,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Minimum GPU memory requirements&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;15(s) Video&lt;/th&gt; 
   &lt;th&gt;30(s) Video&lt;/th&gt; 
   &lt;th&gt;60(s) Video&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-3B&lt;/td&gt; 
   &lt;td&gt;FP32&lt;/td&gt; 
   &lt;td&gt;89.10 GB&lt;/td&gt; 
   &lt;td&gt;Not Recommend&lt;/td&gt; 
   &lt;td&gt;Not Recommend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-3B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;18.38 GB&lt;/td&gt; 
   &lt;td&gt;22.43 GB&lt;/td&gt; 
   &lt;td&gt;28.22 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; 
   &lt;td&gt;FP32&lt;/td&gt; 
   &lt;td&gt;93.56 GB&lt;/td&gt; 
   &lt;td&gt;Not Recommend&lt;/td&gt; 
   &lt;td&gt;Not Recommend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;31.11 GB&lt;/td&gt; 
   &lt;td&gt;41.85 GB&lt;/td&gt; 
   &lt;td&gt;60.19 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; is test with &lt;code&gt;attn_implementation=&quot;flash_attention_2&quot;&lt;/code&gt;. However, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource &lt;a href=&quot;https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator&quot;&gt;here&lt;/a&gt;. We are currently planning to develop a version that can perform inference with lower resource consumption requirements so that Qwen2.5-Omni can run on most platforms. Stay tuned!&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Video URL resource usage&lt;/summary&gt; 
 &lt;p&gt;Video URL compatibility largely depends on the third-party library version. The details are in the table below. Change the backend by &lt;code&gt;FORCE_QWENVL_VIDEO_READER=torchvision&lt;/code&gt; or &lt;code&gt;FORCE_QWENVL_VIDEO_READER=decord&lt;/code&gt; if you prefer not to use the default one.&lt;/p&gt; 
 &lt;table&gt; 
  &lt;thead&gt; 
   &lt;tr&gt; 
    &lt;th&gt;Backend&lt;/th&gt; 
    &lt;th&gt;HTTP&lt;/th&gt; 
    &lt;th&gt;HTTPS&lt;/th&gt; 
   &lt;/tr&gt; 
  &lt;/thead&gt; 
  &lt;tbody&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;gt;= 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;torchvision &amp;lt; 0.19.0&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td&gt;decord&lt;/td&gt; 
    &lt;td&gt;‚úÖ&lt;/td&gt; 
    &lt;td&gt;‚ùå&lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt; 
 &lt;/table&gt; 
&lt;/details&gt; 
&lt;details&gt; 
 &lt;summary&gt;Batch inference&lt;/summary&gt; 
 &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# Sample messages for batch inference

# Conversation with video only
conversation1 = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;/path/to/video.mp4&quot;},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;/path/to/audio.wav&quot;},
        ]
    }
]

# Conversation with pure text
conversation3 = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: &quot;who are you?&quot;
    }
]


# Conversation with mixed media
conversation4 = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;}
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;image&quot;, &quot;image&quot;: &quot;/path/to/image.jpg&quot;},
            {&quot;type&quot;: &quot;video&quot;, &quot;video&quot;: &quot;/path/to/video.mp4&quot;},
            {&quot;type&quot;: &quot;audio&quot;, &quot;audio&quot;: &quot;/path/to/audio.wav&quot;},
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What are the elements can you see and hear in these medias?&quot;},
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# set use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&quot;pt&quot;, padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch Inference
text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)
text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(text)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;ü§ñ ModelScope Usage&lt;/h3&gt; 
&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope, &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; 
&lt;h3&gt;GPTQ-Int4 and AWQ Usage&lt;/h3&gt; 
&lt;p&gt;To improve the Qwen2.5-Omni-7B&#39;s operability on devices with constrained GPU memory, we implemented 4-bit quantization of the Thinker&#39;s weights using GPTQ and AWQ, effectively reducing GPU VRAM usage. Ohter key optimizations include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enhanced the inference pipeline to load model weights on-demand for each module and offload them to CPU memory once inference is complete, preventing peak VRAM usage from becoming excessive.&lt;/li&gt; 
 &lt;li&gt;Converted the code2wav module to support streaming inference, thereby avoiding the pre-allocation of excessive GPU memory.&lt;/li&gt; 
 &lt;li&gt;Adjusted the ODE solver from a second-order (RK4) to a first-order (Euler) method to further decrease computational overhead.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These improvements aim to ensure efficient performance of Qwen2.5-Omni across a range of hardware configurations, particularly those with lower GPU memory availability (RTX3080, 4080, 5070, etc). Currently, the relevant models and usage methods can be obtained from Hugging Face (&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&quot;https://huggingface.co/Qwen/Qwen2.5-Omni-7B-AWQ&quot;&gt;AWQ&lt;/a&gt;) and ModelScope (&lt;a href=&quot;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-GPTQ-Int4&quot;&gt;GPTQ-Int4&lt;/a&gt;|&lt;a href=&quot;https://modelscope.cn/models/Qwen/Qwen2.5-Omni-7B-AWQ&quot;&gt;AWQ&lt;/a&gt;). As below, we provide simple example to show how to use Qwen2.5-Omni-7B-GPTQ-Int4 with &lt;code&gt;gptqmodel&lt;/code&gt; as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install transformers==4.52.3
pip install accelerate
pip install gptqmodel==2.0.0
pip install numpy==2.0.0

git clone https://github.com/QwenLM/Qwen2.5-Omni.git

cd Qwen2.5-Omni/low-VRAM-mode/

CUDA_VISIBLE_DEVICES=0 python3 low_VRAM_demo_gptq.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To use Qwen2.5-Omni-7B-AWQ with &lt;code&gt;autoawq&lt;/code&gt; please run:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pip install transformers==4.52.3
pip install accelerate
pip install autoawq==0.2.9

git clone https://github.com/QwenLM/Qwen2.5-Omni.git

cd Qwen2.5-Omni/low-VRAM-mode/

CUDA_VISIBLE_DEVICES=0 python3 low_VRAM_demo_awq.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following two tables present a performance comparison and GPU memory consumption between Qwen2.5-Omni-7B-GPTQ-Int4/Qwen2.5-Omni-7B-AWQ and Qwen2.5-Omni-7B on specific evaluation benchmarks. The data demonstrates that the GPTQ-Int4/AWQ model maintains comparable performance while reducing GPU memory requirements by over 50%+, enabling a broader range of devices to run and experience the high-performance Qwen2.5-Omni-7B model. Notably, the GPTQ-Int4/AWQ variant exhibits slightly slower inference speeds compared to the native Qwen2.5-Omni-7B model due to quantization techniques and CPU offload mechanisms.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Evaluation Set&lt;/th&gt; 
   &lt;th&gt;Task&lt;/th&gt; 
   &lt;th&gt;Metrics&lt;/th&gt; 
   &lt;th&gt;Qwen2.5-Omni-7B&lt;/th&gt; 
   &lt;th&gt;Qwen2.5-Omni-7B-GPTQ-Int4&lt;/th&gt; 
   &lt;th&gt;Qwen2.5-Omni-7B-AWQ&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;LibriSpeech test-other&lt;/td&gt; 
   &lt;td&gt;ASR&lt;/td&gt; 
   &lt;td&gt;WER ‚¨áÔ∏è&lt;/td&gt; 
   &lt;td&gt;3.4&lt;/td&gt; 
   &lt;td&gt;3.71&lt;/td&gt; 
   &lt;td&gt;3.91&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;WenetSpeech test-net&lt;/td&gt; 
   &lt;td&gt;ASR&lt;/td&gt; 
   &lt;td&gt;WER ‚¨áÔ∏è&lt;/td&gt; 
   &lt;td&gt;5.9&lt;/td&gt; 
   &lt;td&gt;6.62&lt;/td&gt; 
   &lt;td&gt;6.31&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Seed-TTS test-hard&lt;/td&gt; 
   &lt;td&gt;TTS (Speaker: Chelsie)&lt;/td&gt; 
   &lt;td&gt;WER ‚¨áÔ∏è&lt;/td&gt; 
   &lt;td&gt;8.7&lt;/td&gt; 
   &lt;td&gt;10.3&lt;/td&gt; 
   &lt;td&gt;8.88&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;MMLU-Pro&lt;/td&gt; 
   &lt;td&gt;Text -&amp;gt; Text&lt;/td&gt; 
   &lt;td&gt;Accuracy ‚¨ÜÔ∏è&lt;/td&gt; 
   &lt;td&gt;47.0&lt;/td&gt; 
   &lt;td&gt;43.76&lt;/td&gt; 
   &lt;td&gt;45.66&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OmniBench&lt;/td&gt; 
   &lt;td&gt;Speech -&amp;gt; Text&lt;/td&gt; 
   &lt;td&gt;Accuracy ‚¨ÜÔ∏è&lt;/td&gt; 
   &lt;td&gt;56.13&lt;/td&gt; 
   &lt;td&gt;53.59&lt;/td&gt; 
   &lt;td&gt;54.64&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;VideoMME&lt;/td&gt; 
   &lt;td&gt;Multimodality -&amp;gt; Text&lt;/td&gt; 
   &lt;td&gt;Accuracy ‚¨ÜÔ∏è&lt;/td&gt; 
   &lt;td&gt;72.4&lt;/td&gt; 
   &lt;td&gt;68.0&lt;/td&gt; 
   &lt;td&gt;72.0&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Precision&lt;/th&gt; 
   &lt;th&gt;15(s) Video&lt;/th&gt; 
   &lt;th&gt;30(s) Video&lt;/th&gt; 
   &lt;th&gt;60(s) Video&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; 
   &lt;td&gt;FP32&lt;/td&gt; 
   &lt;td&gt;93.56 GB&lt;/td&gt; 
   &lt;td&gt;Not Recommend&lt;/td&gt; 
   &lt;td&gt;Not Recommend&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; 
   &lt;td&gt;BF16&lt;/td&gt; 
   &lt;td&gt;31.11 GB&lt;/td&gt; 
   &lt;td&gt;41.85 GB&lt;/td&gt; 
   &lt;td&gt;60.19 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; 
   &lt;td&gt;GPTQ-Int4&lt;/td&gt; 
   &lt;td&gt;11.64 GB&lt;/td&gt; 
   &lt;td&gt;17.43 GB&lt;/td&gt; 
   &lt;td&gt;29.51 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Qwen-Omni-7B&lt;/td&gt; 
   &lt;td&gt;AWQ&lt;/td&gt; 
   &lt;td&gt;11.77 GB&lt;/td&gt; 
   &lt;td&gt;17.84 GB&lt;/td&gt; 
   &lt;td&gt;30.31 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Usage Tips&lt;/h3&gt; 
&lt;h4&gt;Prompt for audio output&lt;/h4&gt; 
&lt;p&gt;If users need audio output, the system prompt must be set as &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;, otherwise the audio output may not work as expected.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{
    &quot;role&quot;: &quot;system&quot;,
    &quot;content&quot;: [
          {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;}
    ]
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Use audio in video&lt;/h4&gt; 
&lt;p&gt;In the process of multimodal interaction, the videos provided by users are often accompanied by audio (such as questions about the content in the video, or sounds generated by certain events in the video). This information is conducive to the model providing a better interactive experience. So we provide the following options for users to decide whether to use audio in video.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# first place, in data preprocessing
audios, images, videos = process_mm_info(conversations, use_audio_in_video=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;# second place, in model processor
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors=&quot;pt&quot;, 
                   padding=True, use_audio_in_video=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;#  third place, in model inference
text_ids, audio = model.generate(**inputs, use_audio_in_video=True)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter in these places must be set to the same, otherwise unexpected results will occur.&lt;/p&gt; 
&lt;h4&gt;Use audio output or not&lt;/h4&gt; 
&lt;p&gt;The model supports both text and audio outputs, if users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after init the model. This option will save about &lt;code&gt;2GB&lt;/code&gt; of GPU memory but the &lt;code&gt;return_audio&lt;/code&gt; option for &lt;code&gt;generate&lt;/code&gt; function will only allow to be set at &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen2.5-Omni-7B&quot;,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)
model.disable_talker()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to obtain a flexible experience, we recommend that users can decide whether to return audio when &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs to get text responses faster.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen2.5-Omni-7B&quot;,
    torch_dtype=&quot;auto&quot;,
    device_map=&quot;auto&quot;
)
...
text_ids = model.generate(**inputs, return_audio=False)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Change voice type of output audio&lt;/h4&gt; 
&lt;p&gt;Qwen2.5-Omni supports the ability to change the voice of the output audio. The &lt;code&gt;&quot;Qwen/Qwen2.5-Omni-7B&quot;&lt;/code&gt; checkpoint supports two voice types as follows:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Voice Type&lt;/th&gt; 
   &lt;th&gt;Gender&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chelsie&lt;/td&gt; 
   &lt;td&gt;Female&lt;/td&gt; 
   &lt;td&gt;A honeyed, velvety voice that carries a gentle warmth and luminous clarity.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ethan&lt;/td&gt; 
   &lt;td&gt;Male&lt;/td&gt; 
   &lt;td&gt;A bright, upbeat voice with infectious energy and a warm, approachable vibe.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Users can use the &lt;code&gt;speaker&lt;/code&gt; parameter of &lt;code&gt;generate&lt;/code&gt; function to specify the voice type. By defalut, if &lt;code&gt;speaker&lt;/code&gt; is not specified, the default voice type is &lt;code&gt;Chelsie&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;text_ids, audio = model.generate(**inputs, speaker=&quot;Chelsie&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;text_ids, audio = model.generate(**inputs, speaker=&quot;Ethan&quot;)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Flash-Attention 2 to speed up generation&lt;/h4&gt; 
&lt;p&gt;First, make sure to install the latest version of Flash Attention 2:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -U flash-attn --no-build-isolation
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;flash attention repository&lt;/a&gt;. FlashAttention-2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;To load and run a model using FlashAttention-2, add &lt;code&gt;attn_implementation=&quot;flash_attention_2&quot;&lt;/code&gt; when loading the model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from transformers import Qwen2_5OmniForConditionalGeneration

model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
    &quot;Qwen/Qwen2.5-Omni-7B&quot;,
    device_map=&quot;auto&quot;,
    torch_dtype=torch.bfloat16,
    attn_implementation=&quot;flash_attention_2&quot;,
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Cookbooks for More Usage Cases&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Cookbook&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Open&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/universal_audio_understanding.ipynb&quot;&gt;Universal Audio Understanding&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Speech recongnition, speech-to-text translation and audio analysis.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/voice_chatting.ipynb&quot;&gt;Voice Chatting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Chatting with Qwen2.5-Omni by voice input and output.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/screen_recording_interaction.ipynb&quot;&gt;Screen Recording Interaction&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Get the information and content you want to know by asking questions in real time on the recording screen.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/video_information_extracting.ipynb&quot;&gt;Video Information Extracting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Obtaining information from the video stream.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/omni_chatting_for_music.ipynb&quot;&gt;Omni Chatting for Music&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Chat with Qwen2.5-Omni about music content in a audio and video stream.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/omni_chatting_for_math.ipynb&quot;&gt;Omni Chatting for Math&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Chat with Qwen2.5-Omni about math content in a audio and video stream.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/QwenLM/Qwen2.5-Omni/raw/main/cookbooks/multi_round_omni_chatting.ipynb&quot;&gt;Multi Round Omni Chatting&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Conducted multiple rounds of audio and video dialogues with Qwen2.5-Omni to provide the most comprehensive ability demonstration.&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://colab.research.google.com/github/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb&quot;&gt;&lt;img src=&quot;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&quot; alt=&quot;Colab&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;API Inference&lt;/h3&gt; 
&lt;p&gt;To explore Qwen2.5-Omni, we encourage you to test our cutting-edge API service for a faster and efficient experience.&lt;/p&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install openai
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Examples&lt;/h4&gt; 
&lt;p&gt;You can use the OpenAI API service to interact with Qwen2.5-Omni like below. And for more usage, please refer to the tutorial at &lt;a href=&quot;https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni&quot;&gt;aliyun&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import base64
import numpy as np
import soundfile as sf

from openai import OpenAI

client = OpenAI(
    api_key=&quot;your_api_key&quot;,
    base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,
)

messages = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.&quot;,
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;video_url&quot;, &quot;video_url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4&quot;},
        ],
    },
]

# Qwen-Omni only supports stream mode
completion = client.chat.completions.create(
    model=&quot;qwen-omni-turbo&quot;,
    messages=messages,
    modalities=[&quot;text&quot;, &quot;audio&quot;],
    audio={
        &quot;voice&quot;: &quot;Cherry&quot;, # Cherry, Ethan, Serena, Chelsie is available
        &quot;format&quot;: &quot;wav&quot;
    },
    stream=True,
    stream_options={&quot;include_usage&quot;: True}
)

text = []
audio_string = &quot;&quot;
for chunk in completion:
    if chunk.choices:
        if hasattr(chunk.choices[0].delta, &quot;audio&quot;):
            try:
                audio_string += chunk.choices[0].delta.audio[&quot;data&quot;]
            except Exception as e:
                text.append(chunk.choices[0].delta.audio[&quot;transcript&quot;])
    else:
        print(chunk.usage)

print(&quot;&quot;.join(text))
wav_bytes = base64.b64decode(audio_string)
wav_array = np.frombuffer(wav_bytes, dtype=np.int16)
sf.write(&quot;output.wav&quot;, wav_array, samplerate=24000)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Customization Settings&lt;/h3&gt; 
&lt;p&gt;Since Qwen2.5-Omni does not support prompt settings when using &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#prompt-for-audio-output&quot;&gt;audio output&lt;/a&gt; (including local deployment and API inference), we suggest that if you need to control the output of the model or modify the personality settings of the model, you can try adding similar content to the conversation template as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;conversation = [
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;You are a shopping guide, now responsible for introducing various products.&quot;},
        ],
    },
    {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Sure, I got it.&quot;},
        ],
    },
    {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: [
            {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Who are you?&quot;},
        ],
    },
]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Chat with Qwen2.5-Omni&lt;/h2&gt; 
&lt;h3&gt;Online Demo&lt;/h3&gt; 
&lt;p&gt;Without deployment, you can experience online web demo directly by visiting our &lt;a href=&quot;https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo&quot;&gt;Hugginface Spaces&lt;/a&gt; and &lt;a href=&quot;https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo&quot;&gt;Modelscope Studio&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Launch Local Web UI Demo&lt;/h3&gt; 
&lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started or you can launch the web demo directly from our &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&quot;&gt;official docker image&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Installation&lt;/h4&gt; 
&lt;p&gt;Before you begin, ensure that you have the required dependencies installed on your system. You can install them by running the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements_web_demo.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Running the Demo with FlashAttention-2&lt;/h4&gt; 
&lt;p&gt;Once the required packages are installed, you can launch the web demo using the following command. This command will start a web server and provide you with a link to access the UI in your web browser.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: For enhanced performance and efficiency, especially in multi-image and video processing scenarios, we strongly recommend using &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;FlashAttention-2&lt;/a&gt;. FlashAttention-2 provides significant improvements in memory usage and speed, making it ideal for handling large-scale models and data processing.&lt;/p&gt; 
&lt;p&gt;To enable FlashAttention-2, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# default for Qwen2.5-Omni-7B
python web_demo.py --flash-attn2
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# for Qwen2.5-Omni-3B
python web_demo.py --flash-attn2 -c Qwen/Qwen2.5-Omni-3B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will load the model with FlashAttention-2 enabled.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Default Usage&lt;/strong&gt;: If you prefer to run the demo without FlashAttention-2 or if you do not specify the &lt;code&gt;--flash-attn2&lt;/code&gt; option, the demo will load the model using the standard attention implementation:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# default for Qwen2.5-Omni-7B
python web_demo.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# for Qwen2.5-Omni-3B
python web_demo.py -c Qwen/Qwen2.5-Omni-3B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Running on local: http://127.0.0.1:7860/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Copy this link and paste it into your browser to access the web UI, where you can interact with the model by inputting text, uploading audios/images/videos, changing voice type or using any other provided functionalities.&lt;/p&gt; 
&lt;h3&gt;Real-Time Interaction&lt;/h3&gt; 
&lt;p&gt;The streaming Real-time interaction with Qwen2.5-Omni is available now, please visit &lt;a href=&quot;https://chat.qwen.ai/&quot;&gt;Qwen Chat&lt;/a&gt; and select the voice/video calls in the chat box to experience.&lt;/p&gt; 
&lt;h2&gt;Deployment with vLLM&lt;/h2&gt; 
&lt;p&gt;We recommend using vLLM for fast Qwen2.5-Omni deployment and inference. You need to install from our provided &lt;a href=&quot;https://github.com/fyabc/vllm/tree/qwen2_omni_public&quot;&gt;source&lt;/a&gt; to get vLLM support for Qwen2.5-Omni or use our &lt;a href=&quot;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Omni/main/#-docker&quot;&gt;official docker image&lt;/a&gt;. You can also check &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html&quot;&gt;vLLM official documentation&lt;/a&gt; for more details about online serving and offline inference.&lt;/p&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git
cd vllm
git checkout de8f43fbe9428b14d31ac5ec45d065cd3e5c3ee0
pip install setuptools_scm torchdiffeq resampy x_transformers qwen-omni-utils accelerate
pip install -r requirements/cuda.txt
pip install --upgrade setuptools wheel
pip install .
pip install transformers==4.52.3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Inference Local&lt;/h3&gt; 
&lt;p&gt;You can use vLLM to inference Qwen2.5-Omni locally, we provide example in &lt;a href=&quot;https://github.com/fyabc/vllm/raw/qwen2_omni_public/examples/offline_inference/qwen2_5_omni/end2end.py&quot;&gt;vLLM repo&lt;/a&gt; which can generate audio output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# git clone -b qwen2_omni_public https://github.com/fyabc/vllm.git
# cd vllm
# git checkout de8f43fbe9428b14d31ac5ec45d065cd3e5c3ee0
# cd examples/offline_inference/qwen2_5_omni/

# only text output for single GPU
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only

# only text output for multi GPUs (example in 4 GPUs)
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --thinker-only --thinker-devices [0,1,2,3] --thinker-gpu-memory-utilization 0.9 

# audio output for single GPU
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --output-dir output_wav

# audio output for multi GPUs (example in 4 GPUs)
python end2end.py --model Qwen/Qwen2.5-Omni-7B --prompt audio-in-video-v2 --enforce-eager --do-wave --voice-type Chelsie --warmup-voice-type Chelsie --thinker-devices [0,1] --talker-devices [2] --code2wav-devices [3] --thinker-gpu-memory-utilization 0.9 --talker-gpu-memory-utilization 0.9 --output-dir output_wav
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;vLLM Serve usage&lt;/h3&gt; 
&lt;p&gt;You can also use vLLM serve through &lt;code&gt;pip install vllm&amp;gt;=0.8.5.post1&lt;/code&gt;, and vLLM serve for Qwen2.5-Omni only supports thinker now, meaning only text output is supported. You can start vLLM servev through the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# for single GPU
vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16
# for multi GPUs (example in 4 GPUs)
vllm serve /path/to/Qwen2.5-Omni-7B/ --port 8000 --host 127.0.0.1 --dtype bfloat16 -tp 4
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then you can use the chat API as below (via curl for example):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl http://localhost:8000/v1/chat/completions \
    -H &quot;Content-Type: application/json&quot; \
    -d &#39;{
    &quot;messages&quot;: [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
        {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: &quot;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&quot;}},
        {&quot;type&quot;: &quot;audio_url&quot;, &quot;audio_url&quot;: {&quot;url&quot;: &quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/cough.wav&quot;}},
        {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What is the text in the illustrate ans what it the sound in the audio?&quot;}
    ]}
    ]
    }&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Deployment with MNN&lt;/h2&gt; 
&lt;p&gt;Qwen2.5-Omni is now supported in MNN, enabling deployment on edge devices. The MNN models for Qwen2.5-Omni are available for download through Hugging Face (&lt;a href=&quot;https://huggingface.co/taobao-mnn/Qwen2.5-Omni-7B-MNN&quot;&gt;7B&lt;/a&gt;|&lt;a href=&quot;https://huggingface.co/taobao-mnn/Qwen2.5-Omni-3B-MNN&quot;&gt;3B&lt;/a&gt;) and ModelScope (&lt;a href=&quot;https://modelscope.cn/models/MNN/Qwen2.5-Omni-7B-MNN&quot;&gt;7B&lt;/a&gt;|&lt;a href=&quot;https://modelscope.cn/models/MNN/Qwen2.5-Omni-3B-MNN&quot;&gt;3B&lt;/a&gt;), along with usage instructions. For detailed information, you can visit &lt;a href=&quot;https://github.com/alibaba/MNN&quot;&gt;MNN&lt;/a&gt; to learn about it.&lt;/p&gt; 
&lt;p&gt;The table below shows memory consumption and inference speed benchmarks for the Qwen2.5-Omni MNN implementation across various mobile SoC platforms.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Platform&lt;/th&gt; 
   &lt;th&gt;Snapdragon 8 Gen 1&lt;/th&gt; 
   &lt;th&gt;Snapdragon 8 Elite&lt;/th&gt; 
   &lt;th&gt;Snapdragon 8 Gen 1&lt;/th&gt; 
   &lt;th&gt;Snapdragon 8 Elite&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model Size&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;7B&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
   &lt;td&gt;3B&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Memory Peak&lt;/td&gt; 
   &lt;td&gt;5.8G&lt;/td&gt; 
   &lt;td&gt;5.8G&lt;/td&gt; 
   &lt;td&gt;3.6G&lt;/td&gt; 
   &lt;td&gt;3.6G&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Thinker Prefill Speed&lt;/td&gt; 
   &lt;td&gt;25.58 tok/s&lt;/td&gt; 
   &lt;td&gt;46.32 tok/s&lt;/td&gt; 
   &lt;td&gt;54.31 tok/s&lt;/td&gt; 
   &lt;td&gt;55.16 tok/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Thinker Decode Speed&lt;/td&gt; 
   &lt;td&gt;8.35 tok/s&lt;/td&gt; 
   &lt;td&gt;11.52 tok/s&lt;/td&gt; 
   &lt;td&gt;15.84 tok/s&lt;/td&gt; 
   &lt;td&gt;23.31 tok/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Talker Prefill Speed&lt;/td&gt; 
   &lt;td&gt;17.21 tok/s&lt;/td&gt; 
   &lt;td&gt;97.77 tok/s&lt;/td&gt; 
   &lt;td&gt;34.58 tok/s&lt;/td&gt; 
   &lt;td&gt;217.82 tok/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Talker Decode Speed&lt;/td&gt; 
   &lt;td&gt;18.75 tok/s&lt;/td&gt; 
   &lt;td&gt;38.65 tok/s&lt;/td&gt; 
   &lt;td&gt;51.90 tok/s&lt;/td&gt; 
   &lt;td&gt;62.34 tok/s&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Code2Wav Speed&lt;/td&gt; 
   &lt;td&gt;20.83 tok/s&lt;/td&gt; 
   &lt;td&gt;27.36 tok/s&lt;/td&gt; 
   &lt;td&gt;28.45 tok/s&lt;/td&gt; 
   &lt;td&gt;27.36 tok/s&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;üê≥ Docker&lt;/h2&gt; 
&lt;p&gt;To simplify the deploy process, we provide docker images with pre-build environments: &lt;a href=&quot;https://hub.docker.com/r/qwenllm/qwen-omni&quot;&gt;qwenllm/qwen-omni&lt;/a&gt;. You only need to install the driver and download model files to launch demos.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --gpus all --ipc=host --network=host --rm --name qwen2.5-omni -it qwenllm/qwen-omni:2.5-cu121 bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And you can also launch the web demo by:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To enable FlashAttention-2, use the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B --flash-attn2
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;‚≠ê&lt;/span&gt; and citation &lt;span&gt;üìù&lt;/span&gt; :)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-BibTeX&quot;&gt;
@article{Qwen2.5-Omni,
  title={Qwen2.5-Omni Technical Report},
  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},
  journal={arXiv preprint arXiv:2503.20215},
  year={2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;br /&gt;</description>
    </item>
    
  </channel>
</rss>
