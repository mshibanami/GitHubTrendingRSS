<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Perl Weekly Trending</title>
    <description>Weekly Trending of Perl in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:45:39 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>glpi-project/glpi-agent</title>
      <link>https://github.com/glpi-project/glpi-agent</link>
      <description>&lt;p&gt;GLPI Agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&quot;https://raw.githubusercontent.com/glpi-project/glpi-agent/develop/share/html/logo.png&quot; alt=&quot;GLPI Agent&quot; width=&quot;32&quot; height=&quot;32&quot; /&gt; GLPI Agent&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-ci.yml/badge.svg?sanitize=true&quot; alt=&quot;GLPI Agent CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-packaging.yml&quot;&gt;&lt;img src=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-packaging.yml/badge.svg?sanitize=true&quot; alt=&quot;GLPI Agent Packaging&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/glpi-project/glpi-agent/develop/#download&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/glpi-project/glpi-agent/total.svg?sanitize=true&quot; alt=&quot;Github All Releases&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://twitter.com/GLPI_PROJECT&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/GLPI_PROJECT.svg?style=social&amp;amp;label=Follow&quot; alt=&quot;Twitter Follow&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;The GLPI Agent is a generic management agent. It can perform a certain number of tasks, according to its own execution plan, or on behalf of a GLPI server acting as a control point.&lt;/p&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;This agent is based on a fork of &lt;a href=&quot;https://github.com/fusioninventory/fusioninventory-agent&quot;&gt;FusionInventory agent&lt;/a&gt; and so works mainly like FusionInventory agent. It introduces new features and a new protocol to communicate directly with a GLPI server and its native inventory feature. Anyway it also keeps the compatibility with &lt;a href=&quot;https://github.com/fusioninventory/fusioninventory-for-glpi&quot;&gt;FusionInventory for GLPI plugin&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Download&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Release: See &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/releases&quot;&gt;our github releases&lt;/a&gt; for official win32, MacOSX &amp;amp; linux packages.&lt;/li&gt; 
 &lt;li&gt;Development builds: 
  &lt;ul&gt; 
   &lt;li&gt;nightly builds for last &#39;develop&#39; branch commits: &lt;a href=&quot;http://nightly.glpi-project.org/glpi-agent&quot;&gt;GLPI-Agent nightly builds&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;with a github account, you can also access artifacts for any other branches supporting &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/actions/workflows/glpi-agent-packaging.yml?query=is%3Asuccess+event%3Apush+-branch%3Adevelop&quot;&gt;&quot;GLPI Agent Packaging&quot; workflow&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;The GLPI Agent has its &lt;a href=&quot;https://github.com/glpi-project/doc-agent&quot;&gt;dedicated documentation project&lt;/a&gt; where any contribution will also be appreciated.&lt;/p&gt; 
&lt;p&gt;The documentation itself is &lt;a href=&quot;https://glpi-agent.readthedocs.io/&quot;&gt;readable online&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://glpi-agent.readthedocs.io/en/latest/?badge=latest&quot;&gt;&lt;img src=&quot;https://readthedocs.org/projects/glpi-agent/badge/?version=latest&quot; alt=&quot;Documentation Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;h3&gt;Core&lt;/h3&gt; 
&lt;p&gt;Minimum perl version: 5.8&lt;/p&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;File::Which&lt;/li&gt; 
 &lt;li&gt;LWP::UserAgent&lt;/li&gt; 
 &lt;li&gt;Net::IP&lt;/li&gt; 
 &lt;li&gt;Text::Template&lt;/li&gt; 
 &lt;li&gt;UNIVERSAL::require&lt;/li&gt; 
 &lt;li&gt;XML::LibXML&lt;/li&gt; 
 &lt;li&gt;Cpanel::JSON::XS&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Compress::Zlib, for message compression&lt;/li&gt; 
 &lt;li&gt;HTTP::Daemon, for web interface&lt;/li&gt; 
 &lt;li&gt;IO::Socket::SSL, for HTTPS support&lt;/li&gt; 
 &lt;li&gt;LWP::Protocol::https, for HTTPS support&lt;/li&gt; 
 &lt;li&gt;Proc::Daemon, for daemon mode (Unix only)&lt;/li&gt; 
 &lt;li&gt;Proc::PID::File, for daemon mode (Unix only)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Inventory task&lt;/h3&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::CUPS, for printers detection&lt;/li&gt; 
 &lt;li&gt;Parse::EDID, for EDID data parsing&lt;/li&gt; 
 &lt;li&gt;DateTime, for reliable timezone name extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional programs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;dmidecode, for DMI data retrieval&lt;/li&gt; 
 &lt;li&gt;lspci, for PCI bus scanning&lt;/li&gt; 
 &lt;li&gt;hdparm, for additional disk drive info retrieval&lt;/li&gt; 
 &lt;li&gt;monitor-get-edid-using-vbe, monitor-get-edid or get-edid, for EDID data access&lt;/li&gt; 
 &lt;li&gt;ssh-keyscan, for host SSH public key retrieval&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Network discovery tasks&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thread::Queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::NBName, for NetBios method support&lt;/li&gt; 
 &lt;li&gt;Net::SNMP, for SNMP method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional programs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;arp, for arp table lookup method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Network inventory tasks&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::SNMP&lt;/li&gt; 
 &lt;li&gt;Thread::Queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Crypt::DES, for SNMPv3 support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wake on LAN task&lt;/h3&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::Write::Layer2, for ethernet method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deploy task&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Digest::SHA&lt;/li&gt; 
 &lt;li&gt;File::Copy::Recursive&lt;/li&gt; 
 &lt;li&gt;Cpanel::JSON::XS&lt;/li&gt; 
 &lt;li&gt;URI::Escape&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Mandatory Perl modules for P2P Support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::Ping&lt;/li&gt; 
 &lt;li&gt;Parallel::ForkManager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MSI Packaging&lt;/h3&gt; 
&lt;p&gt;Tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/glpi-project/dmidecode&quot;&gt;dmidecode&lt;/a&gt; modified to be built with mingw32&lt;/li&gt; 
 &lt;li&gt;hdparm&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.7-zip.org/&quot;&gt;7zip&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Perl::Dist::Strawberry&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;MacOSX Packaging&lt;/h3&gt; 
&lt;p&gt;Tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/glpi-project/dmidecode/tree/macosx&quot;&gt;dmidecode&lt;/a&gt; modified to be built on macosx&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/munki/munki-pkg&quot;&gt;munkipkg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Xcode&lt;/li&gt; 
 &lt;li&gt;productbuild&lt;/li&gt; 
 &lt;li&gt;hdiutil&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Public databases&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Pci.ids&lt;/li&gt; 
 &lt;li&gt;Usb.ids&lt;/li&gt; 
 &lt;li&gt;SysObject.ids: &lt;a href=&quot;https://github.com/glpi-project/sysobject.ids&quot;&gt;sysobject.ids&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related contribs&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/glpi-project/glpi-agent/develop/CONTRIB.md&quot;&gt;CONTRIB&lt;/a&gt; to find references to GLPI Agent related scritps/files&lt;/p&gt; 
&lt;h2&gt;Contacts&lt;/h2&gt; 
&lt;p&gt;Project websites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;main site: &lt;a href=&quot;https://glpi-project.org/&quot;&gt;https://glpi-project.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;forum: &lt;a href=&quot;https://forum.glpi-project.org/&quot;&gt;https://forum.glpi-project.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;github: &lt;a href=&quot;http://github.com/glpi-project/glpi-agent&quot;&gt;http://github.com/glpi-project/glpi-agent&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project Telegram channel:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://t.me/glpien&quot;&gt;https://t.me/glpien&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please report any issues on project &lt;a href=&quot;https://github.com/glpi-project/glpi-agent/issues&quot;&gt;github issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Active authors&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Guillaume Bougard &lt;a href=&quot;mailto:gbougard@teclib.com&quot;&gt;gbougard@teclib.com&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Copyright 2006-2010 &lt;a href=&quot;https://www.ocsinventory-ng.org/&quot;&gt;OCS Inventory contributors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2010-2019 &lt;a href=&quot;https://fusioninventory.org&quot;&gt;FusionInventory Team&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2011-2021 &lt;a href=&quot;https://www.teclib-edition.com/&quot;&gt;Teclib Editions&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-GPL%20v2-blue.svg?sanitize=true&quot; alt=&quot;License: GPL v2&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This software is licensed under the terms of GPLv2+, see LICENSE file for details.&lt;/p&gt; 
&lt;h2&gt;Additional pieces of software&lt;/h2&gt; 
&lt;p&gt;The glpi-injector script is based on fusioninventory-injector script:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;author: Pascal Danek&lt;/li&gt; 
 &lt;li&gt;copyright: 2005 Pascal Danek&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;GLPI::Agent::Task::Inventory::Vmsystem contains code from imvirt:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;url: &lt;a href=&quot;http://micky.ibh.net/~liske/imvirt.html&quot;&gt;http://micky.ibh.net/~liske/imvirt.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;author: Thomas Liske &lt;a href=&quot;mailto:liske@ibh.de&quot;&gt;liske@ibh.de&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;copyright: 2008 IBH IT-Service GmbH &lt;a href=&quot;http://www.ibh.de/&quot;&gt;http://www.ibh.de/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;License: GPLv2+&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>adrienverge/openfortivpn</title>
      <link>https://github.com/adrienverge/openfortivpn</link>
      <description>&lt;p&gt;Client for PPP+TLS VPN tunnel services&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openfortivpn&lt;/h1&gt; 
&lt;p&gt;openfortivpn is a client for PPP+TLS VPN tunnel services. It spawns a pppd process and operates the communication between the gateway and this process.&lt;/p&gt; 
&lt;p&gt;It is compatible with Fortinet VPNs.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;man openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Simply connect to a VPN:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username=foo
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect to a VPN using an authentication realm:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username=foo --realm=bar
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Store password securely with a pinentry program:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username=foo --pinentry=pinentry-mac
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect with a user certificate and no password:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --username= --password= --user-cert=cert.pem --user-key=key.pem
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Connect using SAML login:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 --saml-login
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Don&#39;t set IP routes and don&#39;t add VPN nameservers to &lt;code&gt;/etc/resolv.conf&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn vpn-gateway:8443 -u foo --no-routes --no-dns --pppd-no-peerdns
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Using a configuration file:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;openfortivpn -c /etc/openfortivpn/my-config
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With &lt;code&gt;/etc/openfortivpn/my-config&lt;/code&gt; containing:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;host = vpn-gateway
port = 8443
username = foo
set-dns = 0
pppd-use-peerdns = 0
# X509 certificate sha256 sum, trust only this one!
trusted-cert = e46d4aff08ba6914e64daa85bc6112a422fa7ce16631bff0b592a28556f993db
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;For the full list of config options, see the &lt;code&gt;CONFIGURATION&lt;/code&gt; section of&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;man openfortivpn
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Smartcard&lt;/h2&gt; 
&lt;p&gt;Smartcard support needs &lt;code&gt;openssl pkcs engine&lt;/code&gt; and &lt;code&gt;opensc&lt;/code&gt; to be installed. The pkcs11-engine from libp11 needs to be compiled with p11-kit-devel installed. Check &lt;a href=&quot;https://github.com/adrienverge/openfortivpn/issues/464&quot;&gt;#464&lt;/a&gt; for a discussion of known issues in this area.&lt;/p&gt; 
&lt;p&gt;Building on Fedora since &lt;a href=&quot;https://src.fedoraproject.org/rpms/openssl/c/13b583a535e62d12521cfeb5088a68e5811eb6e6?branch=rawhide&quot;&gt;this update&lt;/a&gt; will NOT include engine support unless &lt;code&gt;openssl-devel-engine&lt;/code&gt; is installed. Try first to use &lt;code&gt;pkcs11-provider&lt;/code&gt; on OpenSSL &amp;gt;= 3.0.&lt;/p&gt; 
&lt;p&gt;To make use of your smartcard put at least &lt;code&gt;pkcs11:&lt;/code&gt; to the user-cert config or commandline option. It takes the full or a partial PKCS#11 token URI.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-ini&quot;&gt;user-cert = pkcs11:
user-cert = pkcs11:token=someuser
user-cert = pkcs11:model=PKCS%2315%20emulated;manufacturer=piv_II;serial=012345678;token=someuser
username =
password =
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In most cases &lt;code&gt;user-cert = pkcs11:&lt;/code&gt; will do it, but if needed you can get the token-URI with &lt;code&gt;p11tool --list-token-urls&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Multiple readers are currently not supported.&lt;/p&gt; 
&lt;p&gt;Smartcard support has been tested with Yubikey under Linux, but other PIV enabled smartcards may work too. On Mac OS X Mojave it is known that the pkcs engine-by-id is not found.&lt;/p&gt; 
&lt;h2&gt;Installing&lt;/h2&gt; 
&lt;h3&gt;Installing existing packages&lt;/h3&gt; 
&lt;p&gt;Some Linux distributions provide &lt;code&gt;openfortivpn&lt;/code&gt; packages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.fedoraproject.org/pkgs/openfortivpn&quot;&gt;Fedora / CentOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://software.opensuse.org/package/openfortivpn&quot;&gt;openSUSE / SLE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.gentoo.org/packages/net-vpn/openfortivpn&quot;&gt;Gentoo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/NixOS/nixpkgs/tree/master/pkgs/by-name/op/openfortivpn&quot;&gt;NixOS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://archlinux.org/packages/extra/x86_64/openfortivpn&quot;&gt;Arch Linux&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.debian.org/stable/openfortivpn&quot;&gt;Debian&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://packages.ubuntu.com/search?keywords=openfortivpn&quot;&gt;Ubuntu&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/getsolus/packages/tree/main/packages/o/openfortivpn&quot;&gt;Solus&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pkgs.alpinelinux.org/package/edge/testing/x86_64/openfortivpn&quot;&gt;Alpine Linux&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;On macOS both &lt;a href=&quot;https://formulae.brew.sh/formula/openfortivpn&quot;&gt;Homebrew&lt;/a&gt; and &lt;a href=&quot;https://ports.macports.org/port/openfortivpn&quot;&gt;MacPorts&lt;/a&gt; provide an &lt;code&gt;openfortivpn&lt;/code&gt; package. Either &lt;a href=&quot;https://brew.sh/&quot;&gt;install Homebrew&lt;/a&gt; then install openfortivpn:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Install &#39;Homebrew&#39;
/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;

# Install &#39;openfortivpn&#39;
brew install openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or &lt;a href=&quot;https://www.macports.org/install.php&quot;&gt;install MacPorts&lt;/a&gt; then install openfortivpn:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Install &#39;openfortivpn&#39;
sudo port install openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;A more complete overview can be obtained from &lt;a href=&quot;https://repology.org/project/openfortivpn/versions&quot;&gt;repology&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Building and installing from source&lt;/h3&gt; 
&lt;p&gt;For other distros, you&#39;ll need to build and install from source:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install build dependencies.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;RHEL/CentOS/Fedora: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;openssl-devel&lt;/code&gt; &lt;code&gt;make&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Debian/Ubuntu: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;libssl-dev&lt;/code&gt; &lt;code&gt;make&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Arch Linux: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;openssl&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Gentoo Linux: &lt;code&gt;net-dialup/ppp&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;openSUSE: &lt;code&gt;gcc&lt;/code&gt; &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;libopenssl-devel&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;macOS (Homebrew): &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;openssl@1.1&lt;/code&gt; &lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;FreeBSD: &lt;code&gt;automake&lt;/code&gt; &lt;code&gt;autoconf&lt;/code&gt; &lt;code&gt;libressl&lt;/code&gt; &lt;code&gt;pkgconf&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;On Linux, if you manage your kernel yourself, ensure to compile those modules:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;CONFIG_PPP=m
CONFIG_PPP_ASYNC=m
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On macOS, install &#39;Homebrew&#39; to install the build dependencies:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# Install &#39;Homebrew&#39;
/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;

# Install Dependencies
brew install automake autoconf openssl@1.1 pkg-config

# You may need to make this openssl available to compilers and pkg-config
export LDFLAGS=&quot;-L/usr/local/opt/openssl/lib $LDFLAGS&quot;
export CPPFLAGS=&quot;-I/usr/local/opt/openssl/include $CPPFLAGS&quot;
export PKG_CONFIG_PATH=&quot;/usr/local/opt/openssl/lib/pkgconfig:$PKG_CONFIG_PATH&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Build and install.&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./autogen.sh
./configure --prefix=/usr/local --sysconfdir=/etc
make
sudo make install
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If targeting platforms with pppd &amp;lt; 2.5.0 such as current version of macOS, we suggest you configure with option --enable-legacy-pppd:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./autogen.sh
./configure --prefix=/usr/local --sysconfdir=/etc --enable-legacy-pppd
make
sudo make install
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you need to specify the openssl location you can set the &lt;code&gt;$PKG_CONFIG_PATH&lt;/code&gt; environment variable. For fine-tuning check the available configure arguments with &lt;code&gt;./configure --help&lt;/code&gt; especially when you are cross compiling.&lt;/p&gt; &lt;p&gt;Finally, install runtime dependency &lt;code&gt;ppp&lt;/code&gt; or &lt;code&gt;pppd&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Running as root?&lt;/h2&gt; 
&lt;p&gt;openfortivpn needs elevated privileges at three steps during tunnel set up:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;when spawning a &lt;code&gt;/usr/sbin/pppd&lt;/code&gt; process;&lt;/li&gt; 
 &lt;li&gt;when setting IP routes through VPN (when the tunnel is up);&lt;/li&gt; 
 &lt;li&gt;when adding nameservers to &lt;code&gt;/etc/resolv.conf&lt;/code&gt; (when the tunnel is up).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For these reasons, you need to use &lt;code&gt;sudo openfortivpn&lt;/code&gt;. If you need it to be usable by non-sudoer users, you might consider adding an entry in &lt;code&gt;/etc/sudoers&lt;/code&gt; or a file under &lt;code&gt;/etc/sudoers.d&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;visudo -f /etc/sudoers.d/openfortivpn
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-text&quot;&gt;Cmnd_Alias  OPENFORTIVPN = /usr/bin/openfortivpn

%adm       ALL = (ALL) OPENFORTIVPN
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Adapt the above example by changing the &lt;code&gt;openfortivpn&lt;/code&gt; path or choosing a group different from &lt;code&gt;adm&lt;/code&gt; - such as a dedicated &lt;code&gt;openfortivpn&lt;/code&gt; group.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt;: Make sure only trusted users can run openfortivpn as root! As described in &lt;a href=&quot;https://github.com/adrienverge/openfortivpn/issues/54&quot;&gt;#54&lt;/a&gt;, a malicious user could use &lt;code&gt;--pppd-plugin&lt;/code&gt; and &lt;code&gt;--pppd-log&lt;/code&gt; options to divert the program&#39;s behaviour.&lt;/p&gt; 
&lt;h2&gt;SSO/SAML/2FA&lt;/h2&gt; 
&lt;p&gt;In some cases, the server may require the VPN client to load and interact with a web page containing JavaScript. Depending on the complexity of the web page, interpreting the web page might be beyond the reach of a command line program such as openfortivpn.&lt;/p&gt; 
&lt;p&gt;In such cases, you may use an external program spawning a full-fledged web browser such as &lt;a href=&quot;https://github.com/gm-vm/openfortivpn-webview&quot;&gt;openfortivpn-webview&lt;/a&gt; to authenticate and retrieve a session cookie. This cookie can be fed to openfortivpn using option &lt;code&gt;--cookie-on-stdin&lt;/code&gt;. Obviously, such a solution requires a graphic session.&lt;/p&gt; 
&lt;p&gt;When started using &lt;code&gt;--saml-login&lt;/code&gt; the program creates a web server that accepts SAML login requests. To login using SAML you just have to open &lt;code&gt;&amp;lt;your-vpn-domain&amp;gt;/remote/saml/start?redirect=1&lt;/code&gt; and follow the login steps. At the end of the login process the page will be redirected to &lt;code&gt;http://127.0.0.1:8020/?id=&amp;lt;session-id&amp;gt;&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Feel free to make pull requests!&lt;/p&gt; 
&lt;p&gt;C coding style should follow the &lt;a href=&quot;https://www.kernel.org/doc/html/latest/process/coding-style.html&quot;&gt;Linux kernel coding style&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ThePrimeagen/.dotfiles</title>
      <link>https://github.com/ThePrimeagen/.dotfiles</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;.dotfiles&lt;/h1&gt; 
&lt;h3&gt;Kinesis Advantage 360&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Will there be a travel case?&lt;/li&gt; 
 &lt;li&gt;Will there be blank key caps?&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>trinityrnaseq/trinityrnaseq</title>
      <link>https://github.com/trinityrnaseq/trinityrnaseq</link>
      <description>&lt;p&gt;Trinity RNA-Seq de novo transcriptome assembly&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;trinityrnaseq&lt;/h1&gt; 
&lt;p&gt;Trinity RNA-Seq de novo transcriptome assembly see the Trinity &lt;a href=&quot;https://github.com/trinityrnaseq/trinityrnaseq/wiki&quot;&gt;wiki&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We encourage you to contribute to Trinity! Please check out the &lt;a href=&quot;https://github.com/trinityrnaseq/trinityrnaseq/wiki/Contributing&quot;&gt;Contributing&lt;/a&gt; for the guidelines.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>brendangregg/FlameGraph</title>
      <link>https://github.com/brendangregg/FlameGraph</link>
      <description>&lt;p&gt;Stack trace visualizer&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Flame Graphs visualize profiled code&lt;/h1&gt; 
&lt;p&gt;Main Website: &lt;a href=&quot;http://www.brendangregg.com/flamegraphs.html&quot;&gt;http://www.brendangregg.com/flamegraphs.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Example (click to zoom):&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg&quot;&gt;&lt;img src=&quot;http://www.brendangregg.com/FlameGraphs/cpu-bash-flamegraph.svg?sanitize=true&quot; alt=&quot;Example&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Click a box to zoom the Flame Graph to this stack frame only. To search and highlight all stack frames matching a regular expression, click the &lt;em&gt;search&lt;/em&gt; button in the upper right corner or press Ctrl-F. By default, search is case sensitive, but this can be toggled by pressing Ctrl-I or by clicking the &lt;em&gt;ic&lt;/em&gt; button in the upper right corner.&lt;/p&gt; 
&lt;p&gt;Other sites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The Flame Graph article in ACMQ and CACM: &lt;a href=&quot;http://queue.acm.org/detail.cfm?id=2927301&quot;&gt;http://queue.acm.org/detail.cfm?id=2927301&lt;/a&gt; &lt;a href=&quot;http://cacm.acm.org/magazines/2016/6/202665-the-flame-graph/abstract&quot;&gt;http://cacm.acm.org/magazines/2016/6/202665-the-flame-graph/abstract&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU profiling using Linux perf_events, DTrace, SystemTap, or ktap: &lt;a href=&quot;http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html&quot;&gt;http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU profiling using XCode Instruments: &lt;a href=&quot;http://schani.wordpress.com/2012/11/16/flame-graphs-for-instruments/&quot;&gt;http://schani.wordpress.com/2012/11/16/flame-graphs-for-instruments/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CPU profiling using Xperf.exe: &lt;a href=&quot;http://randomascii.wordpress.com/2013/03/26/summarizing-xperf-cpu-usage-with-flame-graphs/&quot;&gt;http://randomascii.wordpress.com/2013/03/26/summarizing-xperf-cpu-usage-with-flame-graphs/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Memory profiling: &lt;a href=&quot;http://www.brendangregg.com/FlameGraphs/memoryflamegraphs.html&quot;&gt;http://www.brendangregg.com/FlameGraphs/memoryflamegraphs.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Other examples, updates, and news: &lt;a href=&quot;http://www.brendangregg.com/flamegraphs.html#Updates&quot;&gt;http://www.brendangregg.com/flamegraphs.html#Updates&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Flame graphs can be created in three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Capture stacks&lt;/li&gt; 
 &lt;li&gt;Fold stacks&lt;/li&gt; 
 &lt;li&gt;flamegraph.pl&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;1. Capture stacks&lt;/h1&gt; 
&lt;p&gt;Stack samples can be captured using Linux perf_events, FreeBSD pmcstat (hwpmc), DTrace, SystemTap, and many other profilers. See the stackcollapse-* converters.&lt;/p&gt; 
&lt;h3&gt;Linux perf_events&lt;/h3&gt; 
&lt;p&gt;Using Linux perf_events (aka &quot;perf&quot;) to capture 60 seconds of 99 Hertz stack samples, both user- and kernel-level stacks, all processes:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# perf record -F 99 -a -g -- sleep 60
# perf script &amp;gt; out.perf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now only capturing PID 181:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# perf record -F 99 -p 181 -g -- sleep 60
# perf script &amp;gt; out.perf
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;DTrace&lt;/h3&gt; 
&lt;p&gt;Using DTrace to capture 60 seconds of kernel stacks at 997 Hertz:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# dtrace -x stackframes=100 -n &#39;profile-997 /arg0/ { @[stack()] = count(); } tick-60s { exit(0); }&#39; -o out.kern_stacks
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Using DTrace to capture 60 seconds of user-level stacks for PID 12345 at 97 Hertz:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# dtrace -x ustackframes=100 -n &#39;profile-97 /pid == 12345 &amp;amp;&amp;amp; arg1/ { @[ustack()] = count(); } tick-60s { exit(0); }&#39; -o out.user_stacks
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;60 seconds of user-level stacks, including time spent in-kernel, for PID 12345 at 97 Hertz:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# dtrace -x ustackframes=100 -n &#39;profile-97 /pid == 12345/ { @[ustack()] = count(); } tick-60s { exit(0); }&#39; -o out.user_stacks
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Switch &lt;code&gt;ustack()&lt;/code&gt; for &lt;code&gt;jstack()&lt;/code&gt; if the application has a ustack helper to include translated frames (eg, node.js frames; see: &lt;a href=&quot;http://dtrace.org/blogs/dap/2012/01/05/where-does-your-node-program-spend-its-time/&quot;&gt;http://dtrace.org/blogs/dap/2012/01/05/where-does-your-node-program-spend-its-time/&lt;/a&gt;). The rate for user-level stack collection is deliberately slower than kernel, which is especially important when using &lt;code&gt;jstack()&lt;/code&gt; as it performs additional work to translate frames.&lt;/p&gt; 
&lt;h1&gt;2. Fold stacks&lt;/h1&gt; 
&lt;p&gt;Use the stackcollapse programs to fold stack samples into single lines. The programs provided are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse.pl&lt;/code&gt;: for DTrace stacks&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-perf.pl&lt;/code&gt;: for Linux perf_events &quot;perf script&quot; output&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-pmc.pl&lt;/code&gt;: for FreeBSD pmcstat -G stacks&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-stap.pl&lt;/code&gt;: for SystemTap stacks&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-instruments.pl&lt;/code&gt;: for XCode Instruments&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-vtune.pl&lt;/code&gt;: for Intel VTune profiles&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-ljp.awk&lt;/code&gt;: for Lightweight Java Profiler&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-jstack.pl&lt;/code&gt;: for Java jstack(1) output&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-gdb.pl&lt;/code&gt;: for gdb(1) stacks&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-go.pl&lt;/code&gt;: for Golang pprof stacks&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-vsprof.pl&lt;/code&gt;: for Microsoft Visual Studio profiles&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;stackcollapse-wcp.pl&lt;/code&gt;: for wallClockProfiler output&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Usage example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;For perf_events:
$ ./stackcollapse-perf.pl out.perf &amp;gt; out.folded

For DTrace:
$ ./stackcollapse.pl out.kern_stacks &amp;gt; out.kern_folded
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output looks like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;unix`_sys_sysenter_post_swapgs 1401
unix`_sys_sysenter_post_swapgs;genunix`close 5
unix`_sys_sysenter_post_swapgs;genunix`close;genunix`closeandsetf 85
unix`_sys_sysenter_post_swapgs;genunix`close;genunix`closeandsetf;c2audit`audit_closef 26
unix`_sys_sysenter_post_swapgs;genunix`close;genunix`closeandsetf;c2audit`audit_setf 5
unix`_sys_sysenter_post_swapgs;genunix`close;genunix`closeandsetf;genunix`audit_getstate 6
unix`_sys_sysenter_post_swapgs;genunix`close;genunix`closeandsetf;genunix`audit_unfalloc 2
unix`_sys_sysenter_post_swapgs;genunix`close;genunix`closeandsetf;genunix`closef 48
[...]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;3. flamegraph.pl&lt;/h1&gt; 
&lt;p&gt;Use flamegraph.pl to render a SVG.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ ./flamegraph.pl out.kern_folded &amp;gt; kernel.svg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An advantage of having the folded input file (and why this is separate to flamegraph.pl) is that you can use grep for functions of interest. Eg:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ grep cpuid out.kern_folded | ./flamegraph.pl &amp;gt; cpuid.svg
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Provided Examples&lt;/h1&gt; 
&lt;h3&gt;Linux perf_events&lt;/h3&gt; 
&lt;p&gt;An example output from Linux &quot;perf script&quot; is included, gzip&#39;d, as example-perf-stacks.txt.gz. The resulting flame graph is example-perf.svg:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.brendangregg.com/FlameGraphs/example-perf.svg&quot;&gt;&lt;img src=&quot;http://www.brendangregg.com/FlameGraphs/example-perf.svg?sanitize=true&quot; alt=&quot;Example&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can create this using:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ gunzip -c example-perf-stacks.txt.gz | ./stackcollapse-perf.pl --all | ./flamegraph.pl --color=java --hash &amp;gt; example-perf.svg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This shows my typical workflow: I&#39;ll gzip profiles on the target, then copy them to my laptop for analysis. Since I have hundreds of profiles, I leave them gzip&#39;d!&lt;/p&gt; 
&lt;p&gt;Since this profile included Java, I used the flamegraph.pl --color=java palette. I&#39;ve also used stackcollapse-perf.pl --all, which includes all annotations that help flamegraph.pl use separate colors for kernel and user level code. The resulting flame graph uses: green == Java, yellow == C++, red == user-mode native, orange == kernel.&lt;/p&gt; 
&lt;p&gt;This profile was from an analysis of vert.x performance. The benchmark client, wrk, is also visible in the flame graph.&lt;/p&gt; 
&lt;h3&gt;DTrace&lt;/h3&gt; 
&lt;p&gt;An example output from DTrace is also included, example-dtrace-stacks.txt, and the resulting flame graph, example-dtrace.svg:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;http://www.brendangregg.com/FlameGraphs/example-dtrace.svg&quot;&gt;&lt;img src=&quot;http://www.brendangregg.com/FlameGraphs/example-dtrace.svg?sanitize=true&quot; alt=&quot;Example&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can generate this using:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ ./stackcollapse.pl example-stacks.txt | ./flamegraph.pl &amp;gt; example.svg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This was from a particular performance investigation: the Flame Graph identified that CPU time was spent in the lofs module, and quantified that time.&lt;/p&gt; 
&lt;h1&gt;Options&lt;/h1&gt; 
&lt;p&gt;See the USAGE message (--help) for options:&lt;/p&gt; 
&lt;p&gt;USAGE: ./flamegraph.pl [options] infile &amp;gt; outfile.svg&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--title TEXT     # change title text
--subtitle TEXT  # second level title (optional)
--width NUM      # width of image (default 1200)
--height NUM     # height of each frame (default 16)
--minwidth NUM   # omit smaller functions. In pixels or use &quot;%&quot; for 
                 # percentage of time (default 0.1 pixels)
--fonttype FONT  # font type (default &quot;Verdana&quot;)
--fontsize NUM   # font size (default 12)
--countname TEXT # count type label (default &quot;samples&quot;)
--nametype TEXT  # name type label (default &quot;Function:&quot;)
--colors PALETTE # set color palette. choices are: hot (default), mem,
                 # io, wakeup, chain, java, js, perl, red, green, blue,
                 # aqua, yellow, purple, orange
--bgcolors COLOR # set background colors. gradient choices are yellow
                 # (default), blue, green, grey; flat colors use &quot;#rrggbb&quot;
--hash           # colors are keyed by function name hash
--cp             # use consistent palette (palette.map)
--reverse        # generate stack-reversed flame graph
--inverted       # icicle graph
--flamechart     # produce a flame chart (sort by time, do not merge stacks)
--negate         # switch differential hues (blue&amp;lt;-&amp;gt;red)
--notes TEXT     # add notes comment in SVG (for debugging)
--help           # this message

eg,
./flamegraph.pl --title=&quot;Flame Graph: malloc()&quot; trace.txt &amp;gt; graph.svg
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As suggested in the example, flame graphs can process traces of any event, such as malloc()s, provided stack traces are gathered.&lt;/p&gt; 
&lt;h1&gt;Consistent Palette&lt;/h1&gt; 
&lt;p&gt;If you use the &lt;code&gt;--cp&lt;/code&gt; option, it will use the $colors selection and randomly generate the palette like normal. Any future flamegraphs created using the &lt;code&gt;--cp&lt;/code&gt; option will use the same palette map. Any new symbols from future flamegraphs will have their colors randomly generated using the $colors selection.&lt;/p&gt; 
&lt;p&gt;If you don&#39;t like the palette, just delete the palette.map file.&lt;/p&gt; 
&lt;p&gt;This allows your to change your colorscheme between flamegraphs to make the differences REALLY stand out.&lt;/p&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;p&gt;Say we have 2 captures, one with a problem, and one when it was working (whatever &quot;it&quot; is):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;cat working.folded | ./flamegraph.pl --cp &amp;gt; working.svg
# this generates a palette.map, as per the normal random generated look.

cat broken.folded | ./flamegraph.pl --cp --colors mem &amp;gt; broken.svg
# this svg will use the same palette.map for the same events, but a very
# different colorscheme for any new events.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Take a look at the demo directory for an example:&lt;/p&gt; 
&lt;p&gt;palette-example-working.svg&lt;br /&gt; palette-example-broken.svg&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>so-fancy/diff-so-fancy</title>
      <link>https://github.com/so-fancy/diff-so-fancy</link>
      <description>&lt;p&gt;Good-lookin&#39; diffs. Actually… nah… The best-lookin&#39; diffs. 🎉&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🎩 diff-so-fancy &lt;a href=&quot;https://circleci.com/gh/so-fancy/diff-so-fancy&quot;&gt;&lt;img src=&quot;https://circleci.com/gh/so-fancy/diff-so-fancy.svg?style=shield&quot; alt=&quot;Circle CI build&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://ci.appveyor.com/project/stevemao/diff-so-fancy/branch/master&quot;&gt;&lt;img src=&quot;https://ci.appveyor.com/api/projects/status/github/so-fancy/diff-so-fancy?branch=master&amp;amp;svg=true&quot; alt=&quot;AppVeyor build&quot; /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;diff-so-fancy&lt;/code&gt; strives to make your diffs &lt;strong&gt;human&lt;/strong&gt; readable instead of machine readable. This helps improve code quality and helps you spot defects faster.&lt;/p&gt; 
&lt;h2&gt;🖼️ Screenshot&lt;/h2&gt; 
&lt;p&gt;Vanilla &lt;code&gt;git diff&lt;/code&gt; vs &lt;code&gt;git&lt;/code&gt; and &lt;code&gt;diff-so-fancy&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/so-fancy/diff-so-fancy/next/diff-so-fancy.png&quot; alt=&quot;diff-highlight vs diff-so-fancy&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;📦 Install&lt;/h2&gt; 
&lt;p&gt;Simply copy the &lt;code&gt;diff-so-fancy&lt;/code&gt; script from the latest release into your &lt;code&gt;$PATH&lt;/code&gt; and you&#39;re done. Alternately to test development features you can clone this repo and then put the &lt;code&gt;diff-so-fancy&lt;/code&gt; script (symlink will work) into your &lt;code&gt;$PATH&lt;/code&gt;. The &lt;code&gt;lib/&lt;/code&gt; directory will need to be kept relative to the core script.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;diff-so-fancy&lt;/code&gt; is also available from the &lt;a href=&quot;https://www.npmjs.com/package/diff-so-fancy&quot;&gt;NPM registry&lt;/a&gt;, &lt;a href=&quot;https://formulae.brew.sh/formula/diff-so-fancy&quot;&gt;brew&lt;/a&gt;, &lt;a href=&quot;https://packages.fedoraproject.org/pkgs/diff-so-fancy/diff-so-fancy/&quot;&gt;Fedora&lt;/a&gt;, in the &lt;a href=&quot;https://archlinux.org/packages/extra/any/diff-so-fancy/&quot;&gt;Arch extra repo&lt;/a&gt;, and as &lt;a href=&quot;https://github.com/aos/dsf-debian&quot;&gt;ppa:aos for Debian/Ubuntu Linux&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Issues relating to packaging (&quot;installation does not work&quot;, &quot;version is out of date&quot;, etc.) should be directed to those packages&#39; repositories/issue trackers where applicable.&lt;/p&gt; 
&lt;h2&gt;✨ Usage&lt;/h2&gt; 
&lt;h3&gt;With git&lt;/h3&gt; 
&lt;p&gt;Configure git to use &lt;code&gt;diff-so-fancy&lt;/code&gt; for all diff output:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --global core.pager &quot;diff-so-fancy | less --tabs=4 -RF&quot;
git config --global interactive.diffFilter &quot;diff-so-fancy --patch&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Manually with diff&lt;/h3&gt; 
&lt;p&gt;Use &lt;code&gt;-u&lt;/code&gt; with &lt;code&gt;diff&lt;/code&gt; for unified output, and pipe the output to &lt;code&gt;diff-so-fancy&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;diff -u file_a file_b | diff-so-fancy
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It also supports the recursive mode of diff with &lt;code&gt;-r&lt;/code&gt; or &lt;code&gt;--recursive&lt;/code&gt; as &lt;strong&gt;first argument&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;diff -r -u folder_a folder_b | diff-so-fancy
&lt;/code&gt;&lt;/pre&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;diff --recursive -u folder_a folder_b | diff-so-fancy
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;⚒️ Options&lt;/h2&gt; 
&lt;h3&gt;markEmptyLines&lt;/h3&gt; 
&lt;p&gt;Should the first block of an empty line be colored. (Default: true)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --bool --global diff-so-fancy.markEmptyLines false
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;changeHunkIndicators&lt;/h3&gt; 
&lt;p&gt;Simplify git header chunks to a more human readable format. (Default: true)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --bool --global diff-so-fancy.changeHunkIndicators false
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;stripLeadingSymbols&lt;/h3&gt; 
&lt;p&gt;Should the pesky &lt;code&gt;+&lt;/code&gt; or &lt;code&gt;-&lt;/code&gt; at line-start be removed. (Default: true)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --bool --global diff-so-fancy.stripLeadingSymbols false
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;useUnicodeRuler&lt;/h3&gt; 
&lt;p&gt;By default, the separator for the file header uses Unicode line-drawing characters. If this is causing output errors on your terminal, set this to &lt;code&gt;false&lt;/code&gt; to use ASCII characters instead. (Default: true)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --bool --global diff-so-fancy.useUnicodeRuler false
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;rulerWidth&lt;/h3&gt; 
&lt;p&gt;By default, the separator for the file header spans the full width of the terminal. Use this setting to set the width of the file header manually.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git config --global diff-so-fancy.rulerWidth 80
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;👨 The diff-so-fancy team&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Person&lt;/th&gt; 
   &lt;th&gt;Role&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;@scottchiefbaker&lt;/td&gt; 
   &lt;td&gt;Project lead&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;@OJFord&lt;/td&gt; 
   &lt;td&gt;Bug triage&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;@GenieTim&lt;/td&gt; 
   &lt;td&gt;Travis OSX fixes&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;@AOS&lt;/td&gt; 
   &lt;td&gt;Debian packager&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;@Stevemao/@Paul Irish&lt;/td&gt; 
   &lt;td&gt;NPM release team&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;🧬 Contributing&lt;/h2&gt; 
&lt;p&gt;Pull requests are quite welcome, and should target the &lt;a href=&quot;https://github.com/so-fancy/diff-so-fancy/tree/next&quot;&gt;&lt;code&gt;next&lt;/code&gt; branch&lt;/a&gt;. We are also looking for any feedback or ideas on how to make &lt;code&gt;diff-so-fancy&lt;/code&gt; even &lt;em&gt;fancier&lt;/em&gt;.&lt;/p&gt; 
&lt;h3&gt;Other documentation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/so-fancy/diff-so-fancy/next/pro-tips.md&quot;&gt;Pro-tips on advanced usage&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/so-fancy/diff-so-fancy/next/reporting-bugs.md&quot;&gt;Reporting Bugs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/so-fancy/diff-so-fancy/next/hacking-and-testing.md&quot;&gt;Hacking and Testing&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/so-fancy/diff-so-fancy/next/history.md&quot;&gt;History&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔃 Alternatives&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dandavison/delta&quot;&gt;Delta&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jesseduffield/lazygit&quot;&gt;Lazygit&lt;/a&gt; with diff-so-fancy &lt;a href=&quot;https://github.com/jesseduffield/lazygit/raw/master/docs/Custom_Pagers.md#diff-so-fancy&quot;&gt;integration&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🏛️ License&lt;/h2&gt; 
&lt;p&gt;MIT&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>FelixKrueger/TrimGalore</title>
      <link>https://github.com/FelixKrueger/TrimGalore</link>
      <description>&lt;p&gt;A wrapper around Cutadapt and FastQC to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Trim Galore&lt;/h1&gt; 
&lt;p&gt;&lt;em&gt;Trim Galore&lt;/em&gt; is a wrapper around &lt;a href=&quot;https://github.com/marcelm/cutadapt&quot;&gt;Cutadapt&lt;/a&gt; and &lt;a href=&quot;http://www.bioinformatics.babraham.ac.uk/projects/fastqc/&quot;&gt;FastQC&lt;/a&gt; to consistently apply adapter and quality trimming to FastQ files, with extra functionality for RRBS data.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://zenodo.org/badge/latestdoi/62039322&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/62039322.svg?sanitize=true&quot; alt=&quot;DOI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://travis-ci.org/FelixKrueger/TrimGalore&quot;&gt;&lt;img src=&quot;https://travis-ci.org/FelixKrueger/TrimGalore.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://bioconda.github.io/recipes/trim-galore/README.html&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?sanitize=true&quot; alt=&quot;install with bioconda&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://quay.io/repository/biocontainers/trim-galore&quot;&gt;&lt;img src=&quot;https://quay.io/repository/biocontainers/trim-galore/status&quot; alt=&quot;container ready&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Trim Galore&lt;/em&gt; is a a Perl wrapper around two tools: &lt;a href=&quot;https://github.com/marcelm/cutadapt&quot;&gt;Cutadapt&lt;/a&gt; and &lt;a href=&quot;http://www.bioinformatics.babraham.ac.uk/projects/fastqc/&quot;&gt;FastQC&lt;/a&gt;. To use, ensure that these two pieces of software are available and copy the &lt;code&gt;trim_galore&lt;/code&gt; script to a location available on the &lt;code&gt;PATH&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Check that cutadapt is installed
cutadapt --version
# Check that FastQC is installed
fastqc -v
# Install Trim Galore
curl -fsSL https://github.com/FelixKrueger/TrimGalore/archive/0.6.10.tar.gz -o trim_galore.tar.gz
tar xvzf trim_galore.tar.gz
# Run Trim Galore
~/TrimGalore-0.6.10/trim_galore
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you are using Bioconda:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda install trim-galore
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;For instructions on how to use &lt;em&gt;Trim Galore&lt;/em&gt;, please see the &lt;a href=&quot;https://raw.githubusercontent.com/FelixKrueger/TrimGalore/master/Docs/Trim_Galore_User_Guide.md&quot;&gt;User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;Trim Galore&lt;/em&gt; was developed at The Babraham Institute by &lt;a href=&quot;https://github.com/FelixKrueger/&quot;&gt;@FelixKrueger&lt;/a&gt;, now part of &lt;a href=&quot;https://altoslabs.com/&quot;&gt;Altos Labs&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ddclient/ddclient</title>
      <link>https://github.com/ddclient/ddclient</link>
      <description>&lt;p&gt;ddclient updates dynamic DNS entries for accounts on a wide range of dynamic DNS services.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DDCLIENT&lt;/h1&gt; 
&lt;p&gt;&lt;code&gt;ddclient&lt;/code&gt; is a Perl client used to update dynamic DNS entries for accounts on many dynamic DNS services. It uses &lt;code&gt;curl&lt;/code&gt; for internet access.&lt;/p&gt; 
&lt;h2&gt;Alternatives&lt;/h2&gt; 
&lt;p&gt;You might also want to consider using one of the following, if they support your dynamic DNS provider(s): &lt;a href=&quot;https://github.com/troglobit/inadyn&quot;&gt;https://github.com/troglobit/inadyn&lt;/a&gt; or &lt;a href=&quot;https://github.com/lopsided98/dnsupdate&quot;&gt;https://github.com/lopsided98/dnsupdate&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported services&lt;/h2&gt; 
&lt;p&gt;Dynamic DNS services currently supported include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.1984.is/product/freedns&quot;&gt;1984.is&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.changeip.com&quot;&gt;ChangeIP&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.cloudflare.com&quot;&gt;CloudFlare&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.cloudns.net&quot;&gt;ClouDNS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.ddns.fm/&quot;&gt;DDNS.fm&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.digitalocean.com/&quot;&gt;DigitalOcean&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dinahosting.com&quot;&gt;dinahosting&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://directnic.com&quot;&gt;Directnic&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.dondominio.com&quot;&gt;DonDominio&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dnsmadeeasy.com&quot;&gt;DNS Made Easy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dnsexit.com/dns/dns-api&quot;&gt;DNSExit&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.dnshome.de&quot;&gt;dnsHome.de&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://api.domeneshop.no/docs/#tag/ddns/paths/~1dyndns~1update/get&quot;&gt;Domeneshop&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.dslreports.com&quot;&gt;DslReports&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://duckdns.org&quot;&gt;Duck DNS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://account.dyn.com&quot;&gt;DynDNS.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.easydns.com&quot;&gt;EasyDNS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.enom.com&quot;&gt;Enom&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://freedns.afraid.org&quot;&gt;Freedns&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://freemyip.com&quot;&gt;Freemyip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gandi.net&quot;&gt;Gandi&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.godaddy.com&quot;&gt;GoDaddy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dns.he.net&quot;&gt;Hurricane Electric&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://faq.infomaniak.com/2376&quot;&gt;Infomaniak&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.inwx.com/&quot;&gt;INWX&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.loopia.se&quot;&gt;Loopia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.mythic-beasts.com/support/api/dnsv2/dynamic-dns&quot;&gt;Mythic Beasts&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.namecheap.com&quot;&gt;NameCheap&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.nearlyfreespeech.net/services/dns&quot;&gt;NearlyFreeSpeech.net&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://njal.la/docs/ddns&quot;&gt;Njalla&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.noip.com&quot;&gt;Noip&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;nsupdate - see nsupdate(1) and ddns-confgen(8)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.ovhcloud.com&quot;&gt;OVH&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://porkbun.com&quot;&gt;Porkbun&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.regfish.de/domains/dyndns&quot;&gt;regfish.de&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.sitelutions.com&quot;&gt;Sitelutions&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://dns.yandex.com&quot;&gt;Yandex&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.zoneedit.com&quot;&gt;Zoneedit&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;code&gt;ddclient&lt;/code&gt; supports finding your IP address from many cable and DSL broadband routers.&lt;/p&gt; 
&lt;p&gt;Comments, suggestions and requests: please file an issue at &lt;a href=&quot;https://github.com/ddclient/ddclient/issues/new&quot;&gt;https://github.com/ddclient/ddclient/issues/new&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The code was originally written by Paul Burry and is now hosted and maintained through github.com. Please check out &lt;a href=&quot;https://ddclient.net&quot;&gt;https://ddclient.net&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;REQUIREMENTS&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;An account from a supported dynamic DNS service provider&lt;/li&gt; 
 &lt;li&gt;Perl v5.10.1 or later 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;JSON::PP&lt;/code&gt; perl library for JSON support&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Linux, macOS, or any other Unix-ish system&lt;/li&gt; 
 &lt;li&gt;An implementation of &lt;code&gt;make&lt;/code&gt; (such as &lt;a href=&quot;https://www.gnu.org/software/make/&quot;&gt;GNU Make&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;If you are installing from a clone of the Git repository, you will also need &lt;a href=&quot;https://www.gnu.org/software/autoconf/&quot;&gt;GNU Autoconf&lt;/a&gt; and &lt;a href=&quot;https://www.gnu.org/software/automake/&quot;&gt;GNU Automake&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;DOWNLOAD&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://github.com/ddclient/ddclient/releases&quot;&gt;https://github.com/ddclient/ddclient/releases&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;INSTALLATION&lt;/h2&gt; 
&lt;h3&gt;Distribution Package&lt;/h3&gt; 
&lt;a href=&quot;https://repology.org/project/ddclient/versions&quot;&gt; &lt;img src=&quot;https://repology.org/badge/vertical-allrepos/ddclient.svg?sanitize=true&quot; alt=&quot;Packaging status&quot; align=&quot;right&quot; /&gt; &lt;/a&gt; The easiest way to install ddclient is to install a package offered by your operating system. See the image to the right for a list of distributions with a ddclient package. 
&lt;h3&gt;Manual Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Extract the distribution tarball (&lt;code&gt;.tar.gz&lt;/code&gt; file) and &lt;code&gt;cd&lt;/code&gt; into the directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;tar xvfa ddclient-3.XX.X.tar.gz
cd ddclient-3.XX.X
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(If you are installing from a clone of the Git repository, you must run &lt;code&gt;./autogen&lt;/code&gt; before continuing to the next step.)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the following commands to build and install:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;./configure \
    --prefix=/usr \
    --sysconfdir=/etc \
    --localstatedir=/var
make
make VERBOSE=1 check
sudo make install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Edit &lt;code&gt;/etc/ddclient/ddclient.conf&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;systemd&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;cp sample-etc_systemd.service /etc/systemd/system/ddclient.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;enable automatic startup when booting&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;systemctl enable ddclient.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;start the first time by hand&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;systemctl start ddclient.service
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Known issues&lt;/h2&gt; 
&lt;p&gt;This is a list for quick referencing of known issues. For further details check out the linked issues and the changelog.&lt;/p&gt; 
&lt;p&gt;Note that any issues prior to version v3.9.1 will not be listed here. If a fix is committed but not yet part of any tagged release, the notes here will reference the not-yet-released version number.&lt;/p&gt; 
&lt;h3&gt;v3.11.2 - v3.9.1: SSL parameter breaks HTTP-only IP acquisition&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;ssl&lt;/code&gt; parameter forces all connections to use HTTPS. While technically working as expected, this behavior keeps coming up as a pain point when using HTTP-only IP querying sites such as &lt;a href=&quot;http://checkip.dyndns.org&quot;&gt;http://checkip.dyndns.org&lt;/a&gt;. Starting with v4.0.0, the behavior is changed to respect &lt;code&gt;http://&lt;/code&gt; in a URL. A separate parameter to disallow all HTTP connections or warn about them may be added later.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: v4.0.0 uses HTTP to connect to URLs starting with &lt;code&gt;http://&lt;/code&gt;. See &lt;a href=&quot;https://github.com/ddclient/ddclient/pull/608&quot;&gt;here&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Workaround&lt;/strong&gt;: Disable the SSL parameter&lt;/p&gt; 
&lt;h3&gt;v3.10.0: Chunked encoding not corretly supported in IO::Socket HTTP code&lt;/h3&gt; 
&lt;p&gt;Using the IO::Socket HTTP code will break in various ways whenever the server responds using HTTP 1.1 chunked encoding. Refer to &lt;a href=&quot;https://github.com/ddclient/ddclient/issues/548&quot;&gt;this issue&lt;/a&gt; for more info.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: v3.11.0 - IO::Socket has been deprecated there and curl has been made the standard.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Workaround&lt;/strong&gt;: Use curl for transfers by either setting &lt;code&gt;-curl&lt;/code&gt; in the command line or by adding &lt;code&gt;curl=yes&lt;/code&gt; in the config&lt;/p&gt; 
&lt;h3&gt;v3.10.0: Spammed updates to some providers&lt;/h3&gt; 
&lt;p&gt;This issue arises when using the &lt;code&gt;use&lt;/code&gt; parameter in the config and using one of these providers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Cloudflare&lt;/li&gt; 
 &lt;li&gt;Hetzner&lt;/li&gt; 
 &lt;li&gt;Digitalocean&lt;/li&gt; 
 &lt;li&gt;Infomaniak&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Fix&lt;/strong&gt;: v3.11.2&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Workaround&lt;/strong&gt;: Use the &lt;code&gt;usev4&lt;/code&gt;/&lt;code&gt;usev6&lt;/code&gt; parameters instead of &lt;code&gt;use&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;TROUBLESHOOTING&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Enable debugging and verbose messages: &lt;code&gt;ddclient --daemon=0 --debug --verbose&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Do you need to specify a proxy? If so, just add a &lt;code&gt;proxy=your.isp.proxy&lt;/code&gt; to the &lt;code&gt;ddclient.conf&lt;/code&gt; file.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Define the IP address of your router with &lt;code&gt;fwv4=xxx.xxx.xxx.xxx&lt;/code&gt; in &lt;code&gt;/etc/ddclient/ddclient.conf&lt;/code&gt; and then try &lt;code&gt;$ ddclient --daemon=0 --query&lt;/code&gt; to see if the router status web page can be understood.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Need support for another router/firewall? Define the router yourself with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;usev4=fwv4
fwv4=url-to-your-router-status-page
fwv4-skip=&quot;regular expression matching any string preceding your IP address, if necessary&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ddclient does something like this to provide builtin support for common routers. For example, the Linksys routers could have been added with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;usev4=fwv4
fwv4=192.168.1.1/Status.htm
fwv4-skip=WAN.*?IP Address
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;OR &lt;a href=&quot;https://github.com/ddclient/ddclient/issues/new&quot;&gt;create a new issue&lt;/a&gt; containing the output from:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl --include --location http://url.of.your.firewall/ip-status-page
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;so that we can add a new firewall definition to a future release of ddclient.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Some broadband routers require the use of a password when ddclient accesses its status page to determine the router&#39;s WAN IP address. If this is the case for your router, add&lt;/p&gt; &lt;pre&gt;&lt;code&gt;fw-login=your-router-login
fw-password=your-router-password
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to the beginning of your ddclient.conf file. Note that some routers use either &#39;root&#39; or &#39;admin&#39; as their login while some others accept anything.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;USING DDCLIENT WITH &lt;code&gt;ppp&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you are using a ppp connection, you can easily update your DynDNS entry with each connection, with:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;## configure pppd to update DynDNS with each connection
cp sample-etc_ppp_ip-up.local /etc/ppp/ip-up.local
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you may just configure ddclient to operate as a daemon and monitor your ppp interface.&lt;/p&gt; 
&lt;h2&gt;USING DDCLIENT WITH &lt;code&gt;cron&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you have not configured ddclient to use daemon-mode, you&#39;ll need to configure cron to force an update once a month so that the dns entry will not become stale.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;## configure cron to force an update twice a month
cp sample-etc_cron.d_ddclient /etc/cron.d/ddclient
vi /etc/cron.d/ddclient
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;USING DDCLIENT WITH &lt;code&gt;dhcpcd&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you are using dhcpcd-1.3.17 or thereabouts, you can easily update your DynDNS entry automatically every time your lease is obtained or renewed by creating an executable file named: &lt;code&gt;/etc/dhcpc/dhcpcd-{your-interface}.exe&lt;/code&gt; ie.: &lt;code&gt;cp sample-etc_dhcpc_dhcpcd-eth0.exe /etc/dhcpc/dhcpcd-{your-interface}.exe&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;In my case, it is named dhcpcd-eth0.exe and contains the lines:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;#!/bin/sh
PATH=/usr/bin:/root/bin:${PATH}
logger -t dhcpcd IP address changed to $1
ddclient --proxy fasthttp.sympatico.ca --wildcard --ip $1 | logger -t ddclient
exit 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Other DHCP clients may have another method of calling out to programs for updating DNS entries.&lt;/p&gt; 
&lt;p&gt;Alternatively, you may just configure ddclient to operate as a daemon and monitor your ethernet interface.&lt;/p&gt; 
&lt;h2&gt;USING DDCLIENT WITH &lt;code&gt;dhclient&lt;/code&gt;&lt;/h2&gt; 
&lt;p&gt;If you are using the ISC DHCP client (dhclient), you can update your DynDNS entry automatically every time your lease is obtained or renewed by creating an executable file named: &lt;code&gt;/etc/dhclient-exit-hooks&lt;/code&gt; ie.: &lt;code&gt;cp sample-etc_dhclient-exit-hooks /etc/dhclient-exit-hooks&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;Edit &lt;code&gt;/etc/dhclient-exit-hooks&lt;/code&gt; to change any options required.&lt;/p&gt; 
&lt;p&gt;Alternatively, you may just configure ddclient to operate as a daemon and monitor your ethernet interface.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>netdisco/netdisco</title>
      <link>https://github.com/netdisco/netdisco</link>
      <description>&lt;p&gt;A web-based network management tool.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://metacpan.org/pod/App::Netdisco&quot;&gt;&lt;img src=&quot;https://badge.fury.io/pl/App-Netdisco.svg?sanitize=true&quot; alt=&quot;CPAN version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://metacpan.org/pod/App::Netdisco&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release-date/netdisco/netdisco.svg?label=released&quot; alt=&quot;Release date&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/netdisco/netdisco/actions/workflows/test_and_publish.yml&quot;&gt;&lt;img src=&quot;https://github.com/netdisco/netdisco/actions/workflows/test_and_publish.yml/badge.svg?event=push&quot; alt=&quot;Tests Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://store.docker.com/community/images/netdisco/netdisco&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/docker%20images-ready-blue.svg?sanitize=true&quot; alt=&quot;Docker Image&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Netdisco&lt;/strong&gt; is a web-based network management tool suitable for small to very large networks. IP and MAC address data is collected into a PostgreSQL database using SNMP, CLI, or device APIs. Some of the things you can do with Netdisco:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Locate a machine on the network by MAC or IP and show the switch port it lives at&lt;/li&gt; 
 &lt;li&gt;Turn off a switch port, or change the VLAN or PoE status of a port&lt;/li&gt; 
 &lt;li&gt;Inventory your network hardware by model, vendor, software and operating system&lt;/li&gt; 
 &lt;li&gt;Pretty pictures of your network&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the demo at: &lt;a href=&quot;https://netdisco2-demo.herokuapp.com/&quot;&gt;https://netdisco2-demo.herokuapp.com/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Netdisco is written in Perl and Python and is self-contained apart from the PostgreSQL database, so is very easy to install and runs well on any linux or unix system. We also have &lt;a href=&quot;https://store.docker.com/community/images/netdisco/netdisco&quot;&gt;docker images&lt;/a&gt; if you prefer.&lt;/p&gt; 
&lt;p&gt;It includes a lightweight web server for the interface, a backend daemon to gather data from your network, and a command line interface for troubleshooting. There is a simple configuration file in YAML format.&lt;/p&gt; 
&lt;p&gt;Please check out the &lt;a href=&quot;https://metacpan.org/pod/App::Netdisco&quot;&gt;installation instructions&lt;/a&gt; on CPAN. When upgrading, make sure to check the latest &lt;a href=&quot;https://github.com/netdisco/netdisco/wiki/Release-Notes&quot;&gt;Release Notes&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also speak to someone in the &lt;a href=&quot;https://kiwiirc.com/nextclient/irc.libera.chat/netdisco&quot;&gt;&lt;code&gt;#netdisco@libera&lt;/code&gt;&lt;/a&gt; IRC channel, or on the &lt;a href=&quot;https://lists.sourceforge.net/lists/listinfo/netdisco-users&quot;&gt;community email list&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>holzschu/a-shell</title>
      <link>https://github.com/holzschu/a-shell</link>
      <description>&lt;p&gt;A terminal for iOS, with multiple windows&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;a-shell: A terminal for iOS, with multiple windows&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Platform-iOS%2014.0+-lightgrey.svg?sanitize=true&quot; alt=&quot;Platform: iOS&quot; /&gt; &lt;a href=&quot;https://twitter.com/a_Shell_iOS&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Twitter-@a__Shell__iOS-blue.svg?style=flat&quot; alt=&quot;Twitter&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/cvYnZm69Gy&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/935519150305050644?color=5865f2&amp;amp;label=Discord&amp;amp;style=flat&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;The goal in this project is to provide a simple Unix-like terminal on iOS. It uses &lt;a href=&quot;https://github.com/holzschu/ios_system/&quot;&gt;ios_system&lt;/a&gt; for command interpretation, and includes all commands from the &lt;a href=&quot;https://github.com/holzschu/ios_system/&quot;&gt;ios_system&lt;/a&gt; ecosystem (nslookup, whois, python3, lua, pdflatex, lualatex...)&lt;/p&gt; 
&lt;p&gt;The project uses iPadOS 13 ability to create and manage multiple windows. Each window has its own context, appearance, command history and current directory. &lt;code&gt;newWindow&lt;/code&gt; opens a new window, &lt;code&gt;exit&lt;/code&gt; closes the current window.&lt;/p&gt; 
&lt;p&gt;For help, type &lt;code&gt;help&lt;/code&gt; in the command line. &lt;code&gt;help -l&lt;/code&gt; lists all the available commands. &lt;code&gt;help -l | grep command&lt;/code&gt; will tell you if your favorite command is already installed.&lt;/p&gt; 
&lt;p&gt;You can change the appearance of a-Shell using &lt;code&gt;config&lt;/code&gt;. It lets you change the font, the font size, the background color, the text color and the cursor color and shape. Each window can have its own appearance. &lt;code&gt;config -p&lt;/code&gt; will make the settings for the current window permanent, that is used for all future windows. With &lt;code&gt;config -t&lt;/code&gt; you can also configure the toolbar.&lt;/p&gt; 
&lt;p&gt;When opening a new window, a-Shell executes the file &lt;code&gt;.profile&lt;/code&gt; if it exists. You can use this mechanism to customize further, e.g. have custom environment variables or cleanup temporary files.&lt;/p&gt; 
&lt;p&gt;For more tips on how to use a-Shell, see &lt;a href=&quot;https://bianshen00009.gitbook.io/a-guide-to-a-shell/&quot;&gt;the document&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;AppStore&lt;/h2&gt; 
&lt;p&gt;a-Shell is now &lt;a href=&quot;https://holzschu.github.io/a-Shell_iOS/&quot;&gt;available on the AppStore&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;How to compile it?&lt;/h2&gt; 
&lt;p&gt;If you want to compile the project yourself, you will need the following steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;download the entire project and its sub-modules: &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;download all the xcFrameworks: &lt;code&gt;downloadFrameworks.sh&lt;/code&gt; 
  &lt;ul&gt; 
   &lt;li&gt;this will download the standard Apple frameworks (in &lt;code&gt;xcfs/.build/artefacts/xcfs&lt;/code&gt;, with checksum control).&lt;/li&gt; 
   &lt;li&gt;There are too many Python frameworks (more than 2000) for automatic download. You can either remove them from the &quot;Embed&quot; step in the project, or compile them: 
    &lt;ul&gt; 
     &lt;li&gt;You&#39;ll need the Xcode command line tools, if you don&#39;t already have them: &lt;code&gt;sudo xcode-select --install&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;You also need the OpenSSL libraries (libssl and libcrypto), XQuartz (freetype), and Node.js (npm) for macOS (we provide the versions for iOS and simulator).&lt;/li&gt; 
     &lt;li&gt;change directory to &lt;code&gt;cpython&lt;/code&gt;: &lt;code&gt;cd cpython&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;build Python 3.11 and all the associated libraries / frameworks: &lt;code&gt;sh ./downloadAndCompile.sh&lt;/code&gt; (this step takes several hours on a 2GHz i5 MBP, YMMV).&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;a-Shell now runs on the devices. a-Shell mini can run on the devices and the simulator.&lt;/p&gt; 
&lt;p&gt;Because Python 3.x uses functions that are only available on the iOS 14 SDK, I&#39;ve set the minimum iOS version to 14.0. It also reduces the size of the binaries, so &lt;code&gt;ios_system&lt;/code&gt; and the other frameworks have the same settings. If you need to run it on an iOS 13 device, you&#39;ll have to recompile most frameworks.&lt;/p&gt; 
&lt;h2&gt;Home directory&lt;/h2&gt; 
&lt;p&gt;In iOS, you cannot write in the &lt;code&gt;~&lt;/code&gt; directory, only in &lt;code&gt;~/Documents/&lt;/code&gt;, &lt;code&gt;~/Library/&lt;/code&gt; and &lt;code&gt;~/tmp&lt;/code&gt;. Most Unix programs assume the configuration files are in &lt;code&gt;$HOME&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;So a-Shell changes several environment variables so that they point to &lt;code&gt;~/Documents&lt;/code&gt;. Type &lt;code&gt;env&lt;/code&gt; to see them.&lt;/p&gt; 
&lt;p&gt;Most configuration files (Python packages, TeX files, Clang SDK...) are in &lt;code&gt;~/Library&lt;/code&gt;.&lt;/p&gt; 
&lt;h2&gt;Sandbox and Bookmarks&lt;/h2&gt; 
&lt;p&gt;a-Shell uses iOS 13 ability to access directories in other Apps sandbox. Type &lt;code&gt;pickFolder&lt;/code&gt; to access a directory inside another App. Once you have selected a directory, you can do pretty much anything you want here, so be careful.&lt;/p&gt; 
&lt;p&gt;All the directories you access with &lt;code&gt;pickFolder&lt;/code&gt; are bookmarked, so you can return to them later without &lt;code&gt;pickFolder&lt;/code&gt;. You can also bookmark the current directory with &lt;code&gt;bookmark&lt;/code&gt;. &lt;code&gt;showmarks&lt;/code&gt; will list all the existing bookmarks, &lt;code&gt;jump mark&lt;/code&gt; and &lt;code&gt;cd ~mark&lt;/code&gt; will change the current directory to this specific bookmark, &lt;code&gt;renamemark&lt;/code&gt; will let you change the name of a specific bookmark and &lt;code&gt;deletemark&lt;/code&gt; will delete a bookmark.&lt;/p&gt; 
&lt;p&gt;A user-configurable option in Settings lets you use the commands &lt;code&gt;s&lt;/code&gt;, &lt;code&gt;g&lt;/code&gt;, &lt;code&gt;l&lt;/code&gt;, &lt;code&gt;r&lt;/code&gt; and &lt;code&gt;d&lt;/code&gt; instead or as well.&lt;/p&gt; 
&lt;p&gt;If you are lost, &lt;code&gt;cd&lt;/code&gt; will always bring you back to &lt;code&gt;~/Documents/&lt;/code&gt;. &lt;code&gt;cd -&lt;/code&gt; will change to the previous directory.&lt;/p&gt; 
&lt;h2&gt;Shortcuts&lt;/h2&gt; 
&lt;p&gt;a-Shell is compatible with Apple Shortcuts, giving users full control of the Shell. You can write complex Shortcuts to download, process and release files using a-Shell commands. There are three shortcuts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;Execute Command&lt;/code&gt;, which takes a list of commands and executes them in order. The input can also be a file or a text node, in which case the commands inside the node are executed.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;Put File&lt;/code&gt; and &lt;code&gt;Get File&lt;/code&gt; are used to transfer files to and from a-Shell.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Shortcuts can be executed either &quot;In Extension&quot; or &quot;In App&quot;. &quot;In Extension&quot; means the shortcut runs in a lightweight version of the App, without no graphical user interface. It is good for light commands that do not require configuration files or system libraries (mkdir, nslookup, whois, touch, cat, echo...). &quot;In App&quot; opens the main application to execute the shortcut. It has access to all the commands, but will take longer. Once a shortcut has opened the App, you can return to the Shortcuts app by calling the command &lt;code&gt;open shortcuts://&lt;/code&gt;. The default behaviour is to try to run the commands &quot;in Extension&quot; as much as possible, based on the content of the commands. You can force a specific shortcut to run &quot;in App&quot; or &quot;in Extension&quot;, with the warning that it won&#39;t always work.&lt;/p&gt; 
&lt;p&gt;Both kind of shortcuts run by default in the same specific directory, &lt;code&gt;$SHORTCUTS&lt;/code&gt; or &lt;code&gt;~shortcuts&lt;/code&gt;. Of course, since you can run the commands &lt;code&gt;cd&lt;/code&gt; and &lt;code&gt;jump&lt;/code&gt; in a shortcut, you can pretty much go anywhere.&lt;/p&gt; 
&lt;h2&gt;Programming / add more commands:&lt;/h2&gt; 
&lt;p&gt;a-Shell has several programming languages installed: Python, Lua, JS, C, C++ and TeX.&lt;/p&gt; 
&lt;p&gt;For C and C++, you compile your programs with &lt;code&gt;clang program.c&lt;/code&gt; and it produces a webAssembly file. You can then execute it with either &lt;code&gt;wasm a.out&lt;/code&gt; or &lt;code&gt;a.out&lt;/code&gt;. You can also link multiple object files together, make a static library with &lt;code&gt;ar&lt;/code&gt;, etc. Once you are satisfied with your program, if you move it to a directory in the &lt;code&gt;$PATH&lt;/code&gt; (e.g. &lt;code&gt;~/Documents/bin&lt;/code&gt;), it will be executed if you type &lt;code&gt;program&lt;/code&gt; on the command line.&lt;/p&gt; 
&lt;p&gt;You can also cross-compile programs on your main computer using our specific &lt;a href=&quot;https://github.com/holzschu/wasi-sdk&quot;&gt;WASI-sdk&lt;/a&gt;, and transfer the WebAssembly file to your iPad or iPhone.&lt;/p&gt; 
&lt;p&gt;Precompiled WebAssembly commands specific for a-Shell are available here: &lt;a href=&quot;https://github.com/holzschu/a-Shell-commands&quot;&gt;https://github.com/holzschu/a-Shell-commands&lt;/a&gt; These include &lt;code&gt;zip&lt;/code&gt;, &lt;code&gt;unzip&lt;/code&gt;, &lt;code&gt;xz&lt;/code&gt;, &lt;code&gt;ffmpeg&lt;/code&gt;... You install them on your iPad using the &lt;code&gt;pkg&lt;/code&gt; command: &lt;code&gt;pkg install zip&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;We have the limitations of WebAssembly: no sockets, no forks (we do have interactive user input, now). Piping input from other commands with &lt;code&gt;command | wasm program.wasm&lt;/code&gt; works fine.&lt;/p&gt; 
&lt;p&gt;For Python, you can install more packages with &lt;code&gt;pip install packagename&lt;/code&gt;, but only if they are pure Python. The C compiler is not yet able to produce dynamic libraries that could be used by Python.&lt;/p&gt; 
&lt;p&gt;TeX files are not installed by default. Type any TeX command and the system will prompt you to download them. Same with LuaTeX files.&lt;/p&gt; 
&lt;h2&gt;VoiceOver&lt;/h2&gt; 
&lt;p&gt;If you enable VoiceOver in Settings, a-Shell will work with VoiceOver: reading commands as you type them, reading the result, letting you read the screen with your finger...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>yaml/yaml-test-suite</title>
      <link>https://github.com/yaml/yaml-test-suite</link>
      <description>&lt;p&gt;Comprehensive, language independent Test Suite for YAML&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;YAML Test Suite&lt;/h1&gt; 
&lt;p&gt;Comprehensive Test Suite for YAML&lt;/p&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;This repository contains data for testing the correctness of YAML processors.&lt;/p&gt; 
&lt;p&gt;The types of data include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Metadata about the test 
  &lt;ul&gt; 
   &lt;li&gt;Name (short phrase)&lt;/li&gt; 
   &lt;li&gt;Tags&lt;/li&gt; 
   &lt;li&gt;Description&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Input YAML&lt;/li&gt; 
 &lt;li&gt;Canonical output YAML&lt;/li&gt; 
 &lt;li&gt;Matching JSON&lt;/li&gt; 
 &lt;li&gt;Token stream notation&lt;/li&gt; 
 &lt;li&gt;Event stream notation&lt;/li&gt; 
 &lt;li&gt;Error data&lt;/li&gt; 
 &lt;li&gt;etc&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To get a quick overview of the tests you can have a look at the &lt;a href=&quot;http://matrix.yaml.info/&quot;&gt;YAML Test Matrix&lt;/a&gt;, made from &lt;a href=&quot;https://github.com/perlpunk/yaml-test-matrix&quot;&gt;https://github.com/perlpunk/yaml-test-matrix&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also view the latest test results from 15 different parsers in &lt;a href=&quot;https://tinyurl.com/2p97ah8a&quot;&gt;this Google sheet&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;The tests are available in 2 forms. Files in the &lt;code&gt;src&lt;/code&gt; directory encode all the data for YAML using YAML. The data from these tests is also available in a form where each test has its own directory.&lt;/p&gt; 
&lt;p&gt;For that, use the latest data release under &lt;a href=&quot;https://github.com/yaml/yaml-test-suite/releases&quot;&gt;https://github.com/yaml/yaml-test-suite/releases&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/yaml/yaml-test-suite -b data-YYYY-MM-DD
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are tests which have multiple similar subtests. Those subtests are in their own numeric directories under the parent id, e.g.:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;VJP3/
VJP3/00
VJP3/00/===
VJP3/00/error
VJP3/00/in.yaml
VJP3/00/test.event
VJP3/01
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The releases are made from the &lt;code&gt;data&lt;/code&gt; branch, which is made from the data in the YAML in the &lt;code&gt;main&lt;/code&gt; branch. You shouldn&#39;t use the data branch directly as the branch contains unreleased commits which might be wrong, and it is squashed and force pushed from time to time.&lt;/p&gt; 
&lt;h3&gt;Special Characters&lt;/h3&gt; 
&lt;p&gt;The YAML files use a number of non-ascii unicode characters to indicate the presence of certain characters that would be otherwise hard to read.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;␣&lt;/code&gt; is used for trailing space characters&lt;/li&gt; 
 &lt;li&gt;Hard tabs are reresented by one of: (expanding to 4 spaces) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;———»&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;——»&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;—»&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;»&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;↵&lt;/code&gt; us used to show trailing newline characters&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;∎&lt;/code&gt; is used at the end when there is no final newline character&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;←&lt;/code&gt; indicates a carriage return character&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;⇔&lt;/code&gt; indicates a byte order mark (BOM) character&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also these are used in test event output:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;SPC&amp;gt;&lt;/code&gt; for a space character&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;&amp;lt;TAB&amp;gt;&lt;/code&gt; for a tab character&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;The &lt;code&gt;data&lt;/code&gt; branch files&lt;/h2&gt; 
&lt;p&gt;The YAML test files in the &lt;code&gt;src/&lt;/code&gt; dir are turned into data files in the &lt;code&gt;data&lt;/code&gt; branch. The &lt;code&gt;make data-update&lt;/code&gt; command generates the &lt;code&gt;data&lt;/code&gt; branch files under the &lt;code&gt;./data/&lt;/code&gt; directory. For instance, a file &lt;code&gt;src/AB3D.yaml&lt;/code&gt; will generate a &lt;code&gt;data/AB3D/&lt;/code&gt; directory.&lt;/p&gt; 
&lt;p&gt;A YAML test file can have 1 or more tests. Originally each file had one test, and all the data files were under &lt;code&gt;data/AB3D/&lt;/code&gt;. If a YAML test file has more than one test, subdirectories are created: &lt;code&gt;data/AB3D/00/&lt;/code&gt;, &lt;code&gt;data/AB3D/01/&lt;/code&gt;, &lt;code&gt;data/AB3D/02/&lt;/code&gt;, etc.&lt;/p&gt; 
&lt;p&gt;The test files are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;===&lt;/code&gt; -- The name/label of the test&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;in.yaml&lt;/code&gt; -- The YAML input to be parsed or loaded&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;test.event&lt;/code&gt; -- The event DSL produced by the parser test program&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;in.json&lt;/code&gt; -- The JSON value that shoiuld load the same as &lt;code&gt;in.yaml&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;out.yaml&lt;/code&gt; -- The most normal output a dumper would produce&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;error&lt;/code&gt; -- This file indicates the YAML should fail to parse&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;emit.yaml&lt;/code&gt; -- Output an emitter would produce&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Makefile Targets&lt;/h2&gt; 
&lt;p&gt;The Makefile has a number of targets for automating the process of adding new tests and also preprocessing them into the &lt;code&gt;data&lt;/code&gt; branch.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make data&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Create a &lt;code&gt;data&lt;/code&gt; worktree subdirectory with all the tests as data files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make data-update&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Update the &lt;code&gt;data&lt;/code&gt; branch directory with the latest info in the &lt;code&gt;src&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make export&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Creates an &lt;code&gt;export.tsv&lt;/code&gt; file with all the data from the &lt;code&gt;src&lt;/code&gt; test files. This tsv data can be copied into a google spreadsheet. The &lt;a href=&quot;https://play.yaml.io/main/parser&quot;&gt;YAML parser playground&lt;/a&gt; has a button to copy a test to the same tsv form.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make import&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Make a directory called &lt;code&gt;new&lt;/code&gt; from a file named &lt;code&gt;import.tsv&lt;/code&gt;. The &lt;code&gt;import.tsv&lt;/code&gt; file should have data copied from a google spreadsheet.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make add-new&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Copy the new tests under &lt;code&gt;new/&lt;/code&gt; into &lt;code&gt;src/&lt;/code&gt; to make a PR for new tests.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make testml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Generate &lt;code&gt;.tml&lt;/code&gt; files under a &lt;code&gt;testml/&lt;/code&gt; directory for all the suite tests.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;make clean&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Remove generated files and directories.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Libaries using this test suite&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;C 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/yaml/libyaml&quot;&gt;libyaml&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/pantoniou/libfyaml&quot;&gt;libfyaml&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;C++ 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/biojppm/rapidyaml&quot;&gt;rapidyaml&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;C# 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/aaubry/YamlDotNet&quot;&gt;YamlDotNet&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;D 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/dlang-community/D-YAML&quot;&gt;dyaml&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Delphi 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/neslib/Neslib.Yaml&quot;&gt;Neslib.Yaml&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Haskell 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/haskell-hvr/HsYAML&quot;&gt;HsYAML&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Java 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://bitbucket.org/asomov/snakeyaml-engine&quot;&gt;SnakeYAML Engine&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Javascript 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/eemeli/yaml&quot;&gt;yaml&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Nim 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/flyx/NimYAML&quot;&gt;NimYAML&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Perl 5 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/perlpunk/YAML-PP-p5&quot;&gt;YAML::PP&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Scala 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/VirtusLab/scala-yaml&quot;&gt;Scala-Yaml&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If your library is using the test suite, drop us a line and we can add it here. It would also be nice if you could add a link back to this test suite.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>inverse-inc/packetfence</title>
      <link>https://github.com/inverse-inc/packetfence</link>
      <description>&lt;p&gt;PacketFence is a fully supported, trusted, Free and Open Source network access control (NAC) solution. Boasting an impressive feature set including a captive-portal for registration and remediation, centralized wired and wireless management, powerful BYOD management options, 802.1X support, layer-2 isolation of problematic devices; PacketFence c…&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PacketFence&lt;/h1&gt; 
&lt;h2&gt;What is PacketFence?&lt;/h2&gt; 
&lt;p&gt;PacketFence is a fully supported, trusted, Free and Open Source network access control (NAC) system. Boasting an impressive feature set including a captive-portal for registration and remediation, centralized wired and wireless management, 802.1X support, layer-2 isolation of problematic devices, integration with IDS solutions and vulnerability scanners; PacketFence can be used to effectively secure networks - from small to very large heterogeneous networks.&lt;/p&gt; 
&lt;p&gt;You want to know who is on your network? You want to give different access to your network based on who is connecting? PacketFence is for you!&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;Follow the instructions provided in the &lt;a href=&quot;https://packetfence.org/support/index.html#/documentation&quot;&gt;Administration Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;More Information&lt;/h2&gt; 
&lt;p&gt;Noteworthy changes since the last release see the &lt;a href=&quot;https://github.com/inverse-inc/packetfence/raw/devel/NEWS.asciidoc&quot;&gt;NEWS file&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Upgrading? See the &lt;a href=&quot;https://github.com/inverse-inc/packetfence/raw/devel/docs/PacketFence_Upgrade_Guide.asciidoc&quot;&gt;Upgrade Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For more details and developer visible changes see the &lt;a href=&quot;https://github.com/inverse-inc/packetfence&quot;&gt;project&#39;s page on Github&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;Join the &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot;&gt;community&lt;/a&gt; or request &lt;a href=&quot;https://packetfence.org/support/index.html#/commercial&quot;&gt;commercial support&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contribute&lt;/h2&gt; 
&lt;p&gt;PacketFence is a collaborative effort in order to create the best Open Source NAC solution. There are multiple ways you can contribute to the project.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;You are a network vendor&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Provide Inverse with switches, access points, wireless controllers, etc. so we can support even more equipment.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;You are a security software vendor&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Provide Inverse with licenses of your software so we can integrate your IDS, Netflow analyzer, IPS, Web filter, etc. directly into PacketFence and its captive portal technology.&lt;/p&gt; 
&lt;h3&gt;&lt;em&gt;You are a PacketFence user&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;You can provide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation reviews, enhancements and translations&lt;/li&gt; 
 &lt;li&gt;Share your ideas and participate to the discussion in &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot; title=&quot;Community Mailing Lists&quot;&gt;mailing lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Provide Inverse with switches, access points, wireless controllers, etc. so we can support even more equipment&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;em&gt;You are a developer&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;You can provide:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Documentation reviews, enhancements and translations&lt;/li&gt; 
 &lt;li&gt;Share your ideas and participate to the discussion in &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot; title=&quot;Community Mailing Lists&quot;&gt;mailing lists&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Patches for bugs or enhancements&lt;/li&gt; 
 &lt;li&gt;Write tests&lt;/li&gt; 
 &lt;li&gt;Handle tasks in our Roadmap&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;em&gt;You are a security researcher&lt;/em&gt;&lt;/h3&gt; 
&lt;p&gt;Push PacketFence into new areas by leveraging the extensibility built into PacketFence. A lot of the low-level plumbing is done for you so you can focus on demoing your ideas.&lt;/p&gt; 
&lt;p&gt;Get in touch with us on the developer &lt;a href=&quot;https://packetfence.org/support/index.html#/community&quot; title=&quot;Community Mailing Lists&quot;&gt;mailing list&lt;/a&gt; with your ideas!&lt;/p&gt; 
&lt;h2&gt;Source&lt;/h2&gt; 
&lt;p&gt;Feel free to fork our &lt;a href=&quot;https://github.com/inverse-inc/packetfence&quot;&gt;github repository&lt;/a&gt; if you are willing to contribute.&lt;/p&gt; 
&lt;p&gt;Most of the development happens in branches. Once ready for integration into &lt;a href=&quot;https://github.com/inverse-inc/packetfence/tree/devel&quot;&gt;devel&lt;/a&gt;, a pull request is opened and a code review takes place. See the list of &lt;a href=&quot;https://github.com/inverse-inc/packetfence/branches&quot;&gt;all branches in the works&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;PacketFence is available in various languages. The following list describes the official translations alongside their maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;English - Inverse inc.&lt;/li&gt; 
 &lt;li&gt;Brazilian Portuguese - Diego de Souza Lopes&lt;/li&gt; 
 &lt;li&gt;French - Inverse inc.&lt;/li&gt; 
 &lt;li&gt;Norwegian&lt;/li&gt; 
 &lt;li&gt;Polish - Maciej Uhlig&lt;/li&gt; 
 &lt;li&gt;Spanish (Spain) - Dominique Couot&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you would like to translate the software in an other language, please consult the &lt;a href=&quot;https://packetfence.org/support/faq/article/how-to-translate-packetfence-in-another-language.html&quot;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Licensed under the GNU General Public License v2.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href=&quot;https://inverse.ca/&quot;&gt;Inverse inc.&lt;/a&gt; leads the development of the solution. Over the years, numerous people and organizations have contributed to the project and we would like to thank them all !&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>darold/ora2pg</title>
      <link>https://github.com/darold/ora2pg</link>
      <description>&lt;p&gt;Ora2Pg is a free tool used to migrate an Oracle database to a PostgreSQL compatible schema. It connects your Oracle database, scan it automatically and extracts its structure or data, it then generates SQL scripts that you can load into PostgreSQL.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;NAME Ora2Pg - Oracle to PostgreSQL database schema converter&lt;/p&gt; 
&lt;p&gt;DESCRIPTION Ora2Pg is a free tool used to migrate an Oracle database to a PostgreSQL compatible schema. It connects to your Oracle database, scans it automatically and extracts its structure or data, then generates SQL scripts that you can load into your PostgreSQL database.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ora2Pg can be used for anything from reverse engineering an Oracle
database to huge enterprise database migration or simply replicating
some Oracle data into a PostgreSQL database. It is really easy to use
and doesn&#39;t require any Oracle database knowledge other than providing
the parameters needed to connect to the Oracle database.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;FEATURES Ora2Pg consists of a Perl script (ora2pg) and a Perl module (Ora2Pg.pm). The only thing you have to modify is the configuration file ora2pg.conf by setting the DSN to the Oracle database and optionally the name of a schema. Once that&#39;s done, you just have to set the type of export you want: TABLE with constraints, VIEW, MVIEW, TABLESPACE, SEQUENCE, INDEXES, TRIGGER, GRANT, FUNCTION, PROCEDURE, PACKAGE, PARTITION, TYPE, INSERT or COPY, FDW, QUERY, KETTLE, SYNONYM.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;By default, Ora2Pg exports to a file that you can load into PostgreSQL
with the psql client, but you can also import directly into a PostgreSQL
database by setting its DSN into the configuration file. With all
configuration options of ora2pg.conf, you have full control of what
should be exported and how.

Features included:

        - Export full database schema (tables, views, sequences, indexes), with
          unique, primary, foreign key and check constraints.
        - Export grants/privileges for users and groups.
        - Export range/list partitions and sub partitions.
        - Export a table selection (by specifying the table names).
        - Export Oracle schema to a PostgreSQL 8.4+ schema.
        - Export predefined functions, triggers, procedures, packages and
          package bodies.
        - Export full data or following a WHERE clause.
        - Full support of Oracle BLOB object as PG BYTEA.
        - Export Oracle views as PG tables.
        - Export Oracle user defined types.
        - Provide some basic automatic conversion of PLSQL code to PLPGSQL.
        - Works on any platform.
        - Export Oracle tables as foreign data wrapper tables.
        - Export materialized view.
        - Show a  report of an Oracle database content.
        - Migration cost assessment of an Oracle database.
        - Migration difficulty level assessment of an Oracle database.
        - Migration cost assessment of PL/SQL code from a file.
        - Migration cost assessment of Oracle SQL queries stored in a file.
        - Generate XML ktr files to be used with Pentaho Data Integrator (Kettle)
        - Export Oracle locator and spatial geometries into PostGIS.
        - Export DBLINK as Oracle FDW.
        - Export SYNONYMS as views.
        - Export DIRECTORY as external table or directory for external_file extension.
        - Dispatch a list of SQL orders over multiple PostgreSQL connections
        - Perform a diff between Oracle and PostgreSQL database for test purposes.
        - MySQL/MariaDB and Microsoft SQL Server migration.

Ora2Pg does its best to automatically convert your Oracle database to
PostgreSQL but there&#39;s still manual work to do. The Oracle specific
PL/SQL code generated for functions, procedures, packages and triggers
has to be reviewed to match the PostgreSQL syntax. You will find some
useful recommendations on porting Oracle PL/SQL code to PostgreSQL
PL/PGSQL at &quot;Converting from other Databases to PostgreSQL&quot;, section:
Oracle (http://wiki.postgresql.org/wiki/Main_Page).

See http://ora2pg.darold.net/report.html for an HTML sample of an Oracle
database migration report.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;INSTALLATION All Perl modules can always be found at CPAN (&lt;a href=&quot;http://search.cpan.org/&quot;&gt;http://search.cpan.org/&lt;/a&gt;). Just type the full name of the module (ex: DBD::Oracle) into the search input box, it will bring you to the page for download.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Releases of Ora2Pg are published at SF.net
(https://sourceforge.net/projects/ora2pg/).

On Windows(TM) you should install Strawberry Perl
(http://strawberryperl.com/) and the OSes corresponding Oracle clients.
Since version 5.32, the Perl distribution includes pre-compiled driver
for DBD::Oracle and DBD::Pg.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Required The Oracle Instant Client or a full Oracle installation must be installed on the system. You can download the RPM from Oracle download center:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    rpm -ivh oracle-instantclient12.2-basic-12.2.0.1.0-1.x86_64.rpm
    rpm -ivh oracle-instantclient12.2-devel-12.2.0.1.0-1.x86_64.rpm
    rpm -ivh oracle-instantclient12.2-jdbc-12.2.0.1.0-1.x86_64.rpm
    rpm -ivh oracle-instantclient12.2-sqlplus-12.2.0.1.0-1.x86_64.rpm

or simply download the corresponding ZIP archives from Oracle download
center and install them where you want, for example:
/opt/oracle/instantclient_12_2/

You also need a modern Perl distribution (Perl 5.10 or later). To
connect to a database and proceed with its migration, you need the DBI
Perl module &amp;gt; 1.614. To migrate an Oracle database, you need the
DBD::Oracle Perl module to be installed.

To install DBD::Oracle and have it working, you need to have the Oracle
client libraries installed and the ORACLE_HOME environment variable must
be defined.

If you plan to export a MySQL database, you need to install the Perl
module DBD::MySQL which requires that the MySQL client libraries are
installed.

If you plan to export a SQL Server database, you need to install the
Perl module DBD::ODBC which requires that the unixODBC package is
installed.

On some Perl distributions, you may need to install the Time::HiRes Perl
module.

If your distribution doesn&#39;t include these Perl modules, you can install
them using CPAN:

        perl -MCPAN -e &#39;install DBD::Oracle&#39;
        perl -MCPAN -e &#39;install DBD::MySQL&#39;
        perl -MCPAN -e &#39;install DBD::ODBC&#39;
        perl -MCPAN -e &#39;install Time::HiRes&#39;

otherwise, use the packages provided by your distribution.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Optional By default, Ora2Pg dumps exports to flat files. To load them into your PostgreSQL database, you need the PostgreSQL client (psql). If you don&#39;t have it on the host running Ora2Pg, you can always transfer these files to a host with the psql client installed. If you prefer to load exports &#39;on the fly&#39;, the Perl module DBD::Pg is required.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ora2Pg allows you to dump all output in a compressed gzip file. To do
this, you need the Compress::Zlib Perl module or, if you prefer using
bzip2 compression, the program bzip2 must be available in your PATH.

If your distribution doesn&#39;t include these Perl modules, you can install
them using CPAN:

        perl -MCPAN -e &#39;install DBD::Pg&#39;
        perl -MCPAN -e &#39;install Compress::Zlib&#39;

otherwise, use the packages provided by your distribution.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Instruction for SQL Server For SQL Server, you need to install the unixodbc package and the Perl DBD::ODBC driver:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        sudo apt install unixodbc
        sudo apt install libdbd-odbc-perl

or

        sudo yum install unixodbc
        sudo yum install perl-DBD-ODBC
        sudo yum install perl-DBD-Pg

Then install the Microsoft ODBC Driver for SQL Server. Follow the
instructions for to your operating system from here:

        https://docs.microsoft.com/fr-fr/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver16

Once done, set the following in the /etc/odbcinst.ini file by adjusting
the SQL Server ODBC driver version:

        [msodbcsql18]
        Description=Microsoft ODBC Driver 18 for SQL Server
        Driver=/opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.0.so.1.1
        UsageCount=1

See ORACLE_DSN to learn how to use the driver to connect to your MSSQL
database.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Installing Ora2Pg Like any other Perl Module, Ora2Pg can be installed with the following commands:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        tar xjf ora2pg-x.x.tar.bz2
        cd ora2pg-x.x/
        perl Makefile.PL
        make &amp;amp;&amp;amp; make install

This will install Ora2Pg.pm into your site Perl repository, ora2pg into
/usr/local/bin/ and ora2pg.conf into /etc/ora2pg/.

On Windows(TM), you may use instead:

        perl Makefile.PL
        gmake &amp;amp;&amp;amp; gmake install

This will install scripts and libraries into your Perl site installation
directory and the ora2pg.conf file as well as all documentation files
into C:\ora2pg\

To install ora2pg in a different directory than the default one, simply
use this command:

        perl Makefile.PL PREFIX=&amp;lt;your_install_dir&amp;gt;
        make &amp;amp;&amp;amp; make install

then set PERL5LIB to the path to your installation directory before
using Ora2Pg.

        export PERL5LIB=&amp;lt;your_install_dir&amp;gt;
        ora2pg -c config/ora2pg.conf -t TABLE -b outdir/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Packaging If you want to build binary packages for your preferred Linux distribution, take a look at the packaging/ directory of the source tarball. It contains everything needed to build RPM, Slackware and Debian packages. See the README file in that directory.&lt;/p&gt; 
&lt;p&gt;Installing DBD::Oracle Ora2Pg needs the Perl module DBD::Oracle for connectivity to an Oracle database from Perl DBI. You can get DBD::Oracle from CPAN, a Perl module repository.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;After setting ORACLE_HOME and LD_LIBRARY_PATH environment variables as
root user, install DBD::Oracle. Proceed as follows:

        export LD_LIBRARY_PATH=/usr/lib/oracle/12.2/client64/lib
        export ORACLE_HOME=/usr/lib/oracle/12.2/client64
        perl -MCPAN -e &#39;install DBD::Oracle&#39;

If you are running for the first time, it will ask many questions; you
can keep defaults by pressing ENTER key, but you need to provide one
appropriate mirror site for CPAN to download the modules. Install
through CPAN manually if the above doesn&#39;t work:

        #perl -MCPAN -e shell
        cpan&amp;gt; get DBD::Oracle
        cpan&amp;gt; quit
        cd ~/.cpan/build/DBD-Oracle*
        export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
        export ORACLE_HOME=/usr/lib/oracle/11.2/client64
        perl Makefile.PL
        make
        make install

Installing DBD::Oracle requires that the three Oracle packages:
instant-client, SDK and SQLplus are installed as well as the libaio1
library.

If you are using Instant Client from ZIP archives, the LD_LIBRARY_PATH
and ORACLE_HOME will be the same and must be set to the directory where
you have installed the files. For example:
/opt/oracle/instantclient_12_2/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;CONFIGURATION Configuring Ora2Pg can be as simple as choosing the Oracle database to export and choosing the export type. This can be done in a minute.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;By reading this documentation you will also be able to:

        - Select only certain tables and/or columns for export.
        - Rename some tables and/or columns during export.
        - Select data to export following a WHERE clause per table.
        - Delay database constraints during data loading.
        - Compress exported data to save disk space.
        - and much more.

The Oracle database migration is fully controlled through a single
configuration file named ora2pg.conf. The format of this file consists
of a directive name in upper case followed by a tab character and a
value. Comments are lines beginning with a #.

There&#39;s no specific order to place the configuration directives, they
are set at the time they are read in the configuration file.

For configuration directives that just take a single value, you can use
them multiple times in the configuration file but only the last
occurrence found in the file will be used. For configuration directives
that allow a list of values, you can use them multiple times, the values
will be appended to the list. If you use the IMPORT directive to load a
custom configuration file, directives defined in this file will be
stored from the place the IMPORT directive is found, so it is better to
put it at the end of the configuration file.

Values set in command line options will override values from the
configuration file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Ora2Pg usage First of all be sure that libraries and binaries paths include the Oracle Instant Client installation:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        export LD_LIBRARY_PATH=/usr/lib/oracle/11.2/client64/lib
        export PATH=&quot;/usr/lib/oracle/11.2/client64/bin:$PATH&quot;

By default, Ora2Pg will look for /etc/ora2pg/ora2pg.conf configuration
file. If the file exists, you can simply execute:

        /usr/local/bin/ora2pg

or under Windows(TM) run ora2pg.bat file, located in your Perl bin
directory. Windows(TM) users may also find a template configuration file
in C:\ora2pg

If you want to call another configuration file, just give the path as a
command line argument:

        /usr/local/bin/ora2pg -c /etc/ora2pg/new_ora2pg.conf

Here are all command line parameters available when using ora2pg:

Usage: ora2pg [-dhpqv --estimate_cost --dump_as_html] [--option value]

    -a | --allow str  : Comma separated list of objects to allow from export.
                        Can be used with SHOW_COLUMN too.
    -b | --basedir dir: Set the default output directory, where files
                        resulting from exports will be stored.
    -c | --conf file  : Set an alternate configuration file other than the
                        default /etc/ora2pg/ora2pg.conf.
    -C | --cdc_file file: File used to store/read SCN per table during export.
                        default: TABLES_SCN.log in the current directory. This
                        is the file written by the --cdc_ready option.
    -d | --debug      : Enable verbose output.
    -D | --data_type str : Allow custom type replacement at command line.
    -e | --exclude str: Comma separated list of objects to exclude from export.
                        Can be used with SHOW_COLUMN too.
    -h | --help       : Print this short help.
    -g | --grant_object type : Extract privilege from the given object type.
                        See possible values with GRANT_OBJECT configuration.
    -i | --input file : File containing Oracle PL/SQL code to convert with
                        no Oracle database connection initiated.
    -j | --jobs num   : Number of parallel process to send data to PostgreSQL.
    -J | --copies num : Number of parallel connections to extract data from Oracle.
    -l | --log file   : Set a log file. Default is stdout.
    -L | --limit num  : Number of tuples extracted from Oracle and stored in
                        memory before writing, default: 10000.
    -m | --mysql      : Export a MySQL database instead of an Oracle schema.
    -M | --mssql      : Export a Microsoft SQL Server database.
    -n | --namespace schema : Set the Oracle schema to extract from.
    -N | --pg_schema schema : Set PostgreSQL&#39;s search_path.
    -o | --out file   : Set the path to the output file where SQL will
                        be written. Default: output.sql in running directory.
    -O | --options    : Used to override any configuration parameter, it can
                        be used multiple time. Syntax: -O &quot;PARAM_NAME=value&quot;
    -p | --plsql      : Enable PLSQL to PLPGSQL code conversion.
    -P | --parallel num: Number of parallel tables to extract at the same time.
    -q | --quiet      : Disable progress bar.
    -r | --relative   : use \ir instead of \i in the psql scripts generated.
    -s | --source DSN : Allow to set the Oracle DBI datasource.
    -S | --scn    SCN : Allow to set the Oracle System Change Number (SCN) to
                        use to export data. It will be used in the WHERE clause
                        to get the data. It is used with action COPY or INSERT.
    -t | --type export: Set the export type. It will override the one
                        given in the configuration file (TYPE).
    -T | --temp_dir dir: Set a distinct temporary directory when two
                        or more ora2pg are run in parallel.
    -u | --user name  : Set the Oracle database connection user.
                        ORA2PG_USER environment variable can be used instead.
    -v | --version    : Show Ora2Pg Version and exit.
    -w | --password pwd : Set the password of the Oracle database user.
                        ORA2PG_PASSWD environment variable can be used instead.
    -W | --where clause : Set the WHERE clause to apply to the Oracle query to
                        retrieve data. Can be used multiple time.
    --forceowner      : Force ora2pg to set tables and sequences owner like in
                  Oracle database. If the value is set to a username this one
                  will be used as the objects owner. By default it&#39;s the user
                  used to connect to the Pg database that will be the owner.
    --nls_lang code: Set the Oracle NLS_LANG client encoding.
    --client_encoding code: Set the PostgreSQL client encoding.
    --view_as_table str: Comma separated list of views to export as table.
    --estimate_cost   : Activate the migration cost evaluation with SHOW_REPORT
    --cost_unit_value minutes: Number of minutes for a cost evaluation unit.
                  default: 5 minutes, corresponds to a migration conducted by a
                  PostgreSQL expert. Set it to 10 if this is your first migration.
   --dump_as_html     : Force ora2pg to dump report in HTML, used only with
                        SHOW_REPORT. Default is to dump report as simple text.
   --dump_as_csv      : As above but force ora2pg to dump report in CSV.
   --dump_as_json     : As above but force ora2pg to dump report in JSON.
   --dump_as_sheet    : Report migration assessment with one CSV line per database.
   --init_project name: Initialise a typical ora2pg project tree. Top directory
                           dump_as_* selected switches, suffixes
                           will be .html, .csv, .json.
   --init_project name: Initialise a typical ora2pg project tree. Top directory
                        will be created under project base dir.
   --project_base dir : Define the base dir for ora2pg project trees. Default
                        is current directory.
   --print_header     : Used with --dump_as_sheet to print the CSV header
                        especially for the first run of ora2pg.
   --human_days_limit num : Set the number of person-days limit where the migration
                        assessment level switch from B to C. Default is set to
                        5 person-days.
   --audit_user list  : Comma separated list of usernames to filter queries in
                        the DBA_AUDIT_TRAIL table. Used only with SHOW_REPORT
                        and QUERY export type.
   --pg_dsn DSN       : Set the datasource to PostgreSQL for direct import.
   --pg_user name     : Set the PostgreSQL user to use.
   --pg_pwd password  : Set the PostgreSQL password to use.
   --count_rows       : Force ora2pg to perform a real row count in TEST,
                        TEST_COUNT and SHOW_TABLE actions.
   --no_header        : Do not append Ora2Pg header to output file
   --oracle_speed     : Use to know at which speed Oracle is able to send
                        data. No data will be processed or written.
   --ora2pg_speed     : Use to know at which speed Ora2Pg is able to send
                        transformed data. Nothing will be written.
   --blob_to_lo       : export BLOB as large objects, can only be used with
                        action SHOW_COLUMN, TABLE and INSERT.
   --cdc_ready        : use current SCN per table to export data and register
                        them into a file named TABLES_SCN.log per default. It
                        can be changed using -C | --cdc_file.
   --lo_import        : use psql \lo_import command to import BLOB as large
                        object. Can be use to import data with COPY and import
                        large object manually in a second pass. It is recquired
                        for BLOB &amp;gt; 1GB. See documentation for more explanation.
   --mview_as_table str: Comma separated list of materialized views to export
                        as regular table.
   --drop_if_exists   : Drop the object before creation if it exists.
   --delete clause    : Set the DELETE clause to apply to the Oracle query to
                        be applied before importing data. Can be used multiple
                        time.
   --oracle_fdw_prefetch: Set the oracle_fdw prefetch value. Larger values
                        generally result in faster data transfer at the cost
                        of greater memory utilisation at the destination.

See full documentation at https://ora2pg.darold.net/ for more help or
see manpage with &#39;man ora2pg&#39;.

ora2pg will return 0 on success, 1 on error. It will return 2 when a
child process has been interrupted and you&#39;ve gotten the warning
message: &quot;WARNING: an error occured during data export. Please check
what&#39;s happened.&quot; Most of the time this is an OOM issue, so first try
reducing DATA_LIMIT value.

For developers, it is possible to add your own custom option(s) in the
Perl script ora2pg as any configuration directive from ora2pg.conf can
be passed in lower case to the new Ora2Pg object instance. See ora2pg
code on how to add your own option.

Note that performance might be improved by updating stats on Oracle:

        BEGIN
        DBMS_STATS.GATHER_SCHEMA_STATS
        DBMS_STATS.GATHER_DATABASE_STATS 
        DBMS_STATS.GATHER_DICTIONARY_STATS
        END;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate a migration template The two options --project_base and --init_project indicate to ora2pg that it should create a project template with a work tree, a configuration file and a script to export all objects from the Oracle database. Here is a sample of the command usage:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg --project_base /app/migration/ --init_project test_project
        Creating project test_project.
        /app/migration/test_project/
                schema/
                        dblinks/
                        directories/
                        functions/
                        grants/
                        mviews/
                        packages/
                        partitions/
                        procedures/
                        sequences/
                        synonyms/
                        tables/
                        tablespaces/
                        triggers/
                        types/
                        views/
                sources/
                        functions/
                        mviews/
                        packages/
                        partitions/
                        procedures/
                        triggers/
                        types/
                        views/
                data/
                config/
                reports/

        Generating generic configuration file
        Creating script export_schema.sh to automate all exports.
        Creating script import_all.sh to automate all imports.

It creates a generic config file where you just have to define the
Oracle database connection and a shell script called export_schema.sh.
The sources/ directory will contain the Oracle code, the schema/
directory will contain the code ported to PostgreSQL. The reports/
directory will contain the HTML and JSON reports with the migration cost
assessment.

If you want to use your own default config file, use the -c option to
give the path to that file. Rename it with .dist suffix if you want
ora2pg to apply the generic configuration values; otherwise, the
configuration file will be copied untouched.

Once you have set the connection to the Oracle Database you can execute
the script export_schema.sh that will export all object types from your
Oracle database and output DDL files into the schema&#39;s subdirectories.
At end of the export it will give you the command to export data later
when the import of the schema is done and verified.

You can choose to load the DDL files generated manually or use the
second script import_all.sh to import those files interactively. If this
kind of migration is not something common for you, it&#39;s recommended that
you use those scripts.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Oracle database connection There are 5 configuration directives to control the access to the Oracle database.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ORACLE_HOME
    Used to set the ORACLE_HOME environment variable for the Oracle
    libraries required by the DBD::Oracle Perl module.

ORACLE_DSN
    This directive is used to set the data source name in the standard
    DBI DSN form. For example:

            dbi:Oracle:host=oradb_host.myhost.com;sid=DB_SID;port=1521

    or

            dbi:Oracle:DB_SID

    On 18c this could be for example:

            dbi:Oracle:host=192.168.1.29;service_name=pdb1;port=1521

    For the second notation, the SID should be declared in the
    well-known file $ORACLE_HOME/network/admin/tnsnames.ora or in the
    path given to the TNS_ADMIN environment variable.

    For MySQL the DSN will look like this:

            dbi:mysql:host=192.168.1.10;database=sakila;port=3306

    The &#39;sid&#39; part is replaced by &#39;database&#39;.

    For MS SQL Server it will look like this:

            dbi:ODBC:driver=msodbcsql18;server=mydb.database.windows.net;database=testdb;TrustServerCertificate=yes

ORACLE_USER and ORACLE_PWD
    These two directives are used to define the user and password for
    the Oracle database connection. Note that if possible, it is better
    to login as Oracle super admin to avoid grant problems during the
    database scan and ensure nothing is missing.

    If you do not supply credentials with ORACLE_PWD and you have
    installed the Term::ReadKey Perl module, Ora2Pg will ask for the
    password interactively. If ORACLE_USER is not set it will also be
    asked interactively.

    To connect to a local ORACLE instance with connections &quot;as sysdba&quot;
    you have to set ORACLE_USER to &quot;/&quot; and an empty password.

    To make a connection using an Oracle Secure External Password Store
    (SEPS), first configure the Oracle Wallet and then set both the
    ORACLE_USER and ORACLE_PWD directives to the special value of
    &quot;__SEPS__&quot; (without the quotes but with the double underscore).

USER_GRANTS
    Set this directive to 1 if you connect to Oracle database as a
    simple user and do not have enough grants to extract things from the
    DBA_... tables. It will use ALL_... tables instead.

    Warning: if you use export type GRANT, you must set this
    configuration option to 0 or it will not work.

TRANSACTION
    This directive may be used if you want to change the default
    isolation level of the data export transaction. Default is now to
    set the level to a serializable transaction to ensure data
    consistency. The allowed values for this directive are:

            readonly: &#39;SET TRANSACTION READ ONLY&#39;,
            readwrite: &#39;SET TRANSACTION READ WRITE&#39;,
            serializable: &#39;SET TRANSACTION ISOLATION LEVEL SERIALIZABLE&#39;
            committed: &#39;SET TRANSACTION ISOLATION LEVEL READ COMMITTED&#39;,

    Releases before 6.2 used to set the isolation level to READ ONLY
    transaction but in some cases this was breaking data consistency so
    now default is set to SERIALIZABLE.

INPUT_FILE
    This directive does not control the Oracle database connection but
    rather it purely disables the use of any Oracle database by
    accepting a file as argument. Set this directive to a file
    containing PL/SQL Oracle Code like function, procedure or full
    package body to prevent Ora2Pg from connecting to an Oracle database
    and just apply its conversion tool to the content of the file. This
    can be used with most export types: TABLE, TRIGGER, PROCEDURE, VIEW,
    FUNCTION or PACKAGE, etc.

ORA_INITIAL_COMMAND
    This directive can be used to send an initial command to Oracle,
    just after the connection. For example to unlock a policy before
    reading objects or to set some session parameters. This directive
    can be used multiple times.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data encryption with Oracle server If your Oracle Client config file already includes the encryption method, then DBD::Oracle uses those settings to encrypt the connection while extracting data. For example, if you have configured the Oracle Client config file (sqlnet.ora or .sqlnet) with the following information:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        # Configure encryption of connections to Oracle
        SQLNET.ENCRYPTION_CLIENT = required
        SQLNET.ENCRYPTION_TYPES_CLIENT = (AES256, RC4_256)
        SQLNET.CRYPTO_SEED = &#39;should be 10-70 random characters&#39;

Any tool that uses the Oracle client to communicate with the database
will have encrypted connections if you setup session encryption as shown
above.

For example, Perl&#39;s DBI uses DBD::Oracle, which uses the Oracle client
for actual database communication. If the Oracle client installation
used by Perl is setup to request encrypted connections, then your Perl
connection to an Oracle database will also be encrypted.

Full details at
https://kb.berkeley.edu/jivekb/entry.jspa?externalID=1005
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Testing connection Once you have set the Oracle database DSN, you can execute ora2pg to see if it works:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg -t SHOW_VERSION -c config/ora2pg.conf

will show the Oracle database server version. Take some time here to
test your installation as most problems occur here. The other
configuration steps are more technical.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Troubleshooting If the output.sql file hasn&#39;t exported anything other than the PostgreSQL transaction header and footer, there are two possible reasons: 1) The perl script ora2pg dumps an ORA-XXX error, which means that your DSN or login information is wrong - check the error and your settings and try again. 2) The perl script says nothing and the output file is empty: the user lacks permissions to extract something from the database. Try to connect to Oracle as super user or review the USER_GRANTS directive above and the next section, especially the SCHEMA directive.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;LOGFILE
    By default, all messages are sent to the standard output. If you
    provide a file path to this directive, all output will be appended
    to this file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Oracle schema to export The Oracle database export can be limited to a specific Schema or Namespace; this may be mandatory depending on the database connection user.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SCHEMA
    This directive is used to set the schema name to use during export.
    For example:

            SCHEMA  APPS

    will extract objects associated with the APPS schema.

    When no schema name is provided and EXPORT_SCHEMA is enabled, Ora2Pg
    will export all objects from all schemas of the Oracle instance with
    their names prefixed with the schema name.

EXPORT_SCHEMA
    By default, the Oracle schema is not exported into the PostgreSQL
    database and all objects are created under the default Pg namespace.
    If you want to also export this schema and create all objects under
    this namespace, set the EXPORT_SCHEMA directive to 1. This will set
    the schema search_path at the top of the export SQL file to the
    schema name set in the SCHEMA directive with the default pg_catalog
    schema. If you want to change this path, use the directive
    PG_SCHEMA.

CREATE_SCHEMA
    Enable/disable the CREATE SCHEMA SQL order at the start of the
    output file. It is enabled by default and concerns the TABLE export
    type.

COMPILE_SCHEMA
    By default, Ora2Pg will only export valid PL/SQL code. You can force
    Oracle to compile again the invalidated code to get a chance to have
    it obtain the valid status and then be able to export it.

    Enable this directive to force Oracle to compile schema before
    exporting code. When this directive is enabled and SCHEMA is set to
    a specific schema name, only invalid objects in this schema will be
    recompiled. If SCHEMA is not set then all schema will be recompiled.
    To force recompilation of invalid object in a specific schema, set
    COMPILE_SCHEMA to the schema name you want to recompile.

    This will ask Oracle to validate the PL/SQL that could have been
    invalidated after an export/import for example. The &#39;VALID&#39; or
    &#39;INVALID&#39; status applies to functions, procedures, packages and user
    defined types. It also concerns disabled triggers.

EXPORT_INVALID
    If the above configuration directive is not enough to validate your
    PL/SQL code, enable this configuration directive to allow export of
    all PL/SQL code even if it is marked as invalid. The &#39;VALID&#39; or
    &#39;INVALID&#39; status applies to functions, procedures, packages,
    triggers and user defined types.

PG_SCHEMA
    Allows you to define/force the PostgreSQL schema to use. By default,
    if you set EXPORT_SCHEMA to 1, the PostgreSQL search_path will be
    set to the schema name exported set as value of the SCHEMA
    directive.

    The value can be a comma-delimited list of schema names but not when
    using TABLE export type because in this case it will generate the
    CREATE SCHEMA statement and it doesn&#39;t support multiple schema
    names. For example, if you set PG_SCHEMA to something like
    &quot;user_schema, public&quot;, the search path will be set like this:

            SET search_path = user_schema, public;

    forcing the use of an other schema (here user_schema) than the one
    from Oracle schema set in the SCHEMA directive.

    You can also set the default search_path for the PostgreSQL user you
    are using to connect to the destination database by using:

            ALTER ROLE username SET search_path TO user_schema, public;

    in this case you don&#39;t have to set PG_SCHEMA.

SYSUSERS
    Without explicit schema, Ora2Pg will export all objects that do not
    belong to system schemas or roles:

            SYSTEM,CTXSYS,DBSNMP,EXFSYS,LBACSYS,MDSYS,MGMT_VIEW,
            OLAPSYS,ORDDATA,OWBSYS,ORDPLUGINS,ORDSYS,OUTLN,
            SI_INFORMTN_SCHEMA,SYS,SYSMAN,WK_TEST,WKSYS,WKPROXY,
            WMSYS,XDB,APEX_PUBLIC_USER,DIP,FLOWS_020100,FLOWS_030000,
            FLOWS_040100,FLOWS_010600,FLOWS_FILES,MDDATA,ORACLE_OCM,
            SPATIAL_CSW_ADMIN_USR,SPATIAL_WFS_ADMIN_USR,XS$NULL,PERFSTAT,
            SQLTXPLAIN,DMSYS,TSMSYS,WKSYS,APEX_040000,APEX_040200,
            DVSYS,OJVMSYS,GSMADMIN_INTERNAL,APPQOSSYS,DVSYS,DVF,
            AUDSYS,APEX_030200,MGMT_VIEW,ODM,ODM_MTR,TRACESRV,MTMSYS,
            OWBSYS_AUDIT,WEBSYS,WK_PROXY,OSE$HTTP$ADMIN,
            AURORA$JIS$UTILITY$,AURORA$ORB$UNAUTHENTICATED,
            DBMS_PRIVILEGE_CAPTURE,CSMIG,MGDSYS,SDE,DBSFWUSER

    Depending on your Oracle installation, you may have several other
    system roles defined. To append these users to the schema exclusion
    list, just set the SYSUSERS configuration directive to a
    comma-separated list of system users to exclude. For example:

            SYSUSERS        INTERNAL,SYSDBA,BI,HR,IX,OE,PM,SH

    will add users INTERNAL and SYSDBA to the schema exclusion list.

FORCE_OWNER
    By default, the owner of the database objects is the one you&#39;re
    using to connect to PostgreSQL using the psql command. If you use an
    other user (postgres for example), you can force Ora2Pg to set the
    object owner to be the one used in the Oracle database by setting
    the directive to 1, or to a completely different username by setting
    the directive value to that username.

FORCE_SECURITY_INVOKER
    Ora2Pg uses the function&#39;s security privileges set in Oracle and it
    is often defined as SECURITY DEFINER. If you want to override those
    security privileges for all functions and use SECURITY DEFINER
    instead, enable this directive.

USE_TABLESPACE
    When enabled this directive forces ora2pg to export all tables and
    indexes using the tablespace name defined in Oracle database. This
    works only with tablespaces that are not TEMP, USERS or SYSTEM.

WITH_OID
    Activating this directive will force Ora2Pg to add WITH (OIDS) when
    creating tables or views as tables. Default is same as PostgreSQL,
    disabled.

LOOK_FORWARD_FUNCTION
    List of schemas to get functions/procedures meta information that
    are used in the current schema export. When replacing calls to
    functions with OUT parameters, if a function is declared in an other
    package, then the function call rewriting can not be done because
    Ora2Pg only knows about functions declared in the current schema. By
    setting a comma-separated list of schemas as value of this
    directive, Ora2Pg will look forward in these packages for all
    functions/procedures/packages declarations before proceeding to
    current schema export.

NO_FUNCTION_METADATA
    Forces Ora2Pg to not look for function declarations. Note that this
    will prevent Ora2Pg from rewriting function replacement calls if
    needed. Do not enable it unless looking forward at functions breaks
    other exports.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Export type The export action is performed following a single configuration directive &#39;TYPE&#39;, some others add more control over what should be exported.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;TYPE
    Here are the different values of the TYPE directive, default is
    TABLE:

            - TABLE: Extract all tables with indexes, primary keys, unique keys,
              foreign keys and check constraints.
            - VIEW: Extract only views.
            - GRANT: Extract roles converted to Pg groups, users and grants on all
              objects.
            - SEQUENCE: Extract all sequences and their last positions.
            - TABLESPACE: Extract storage spaces for tables and indexes (Pg &amp;gt;= v8).
            - TRIGGER: Extract triggers defined on actions.
            - FUNCTION: Extract functions.
            - PROCEDURE: Extract procedures.
            - PACKAGE: Extract packages and package bodies.
            - INSERT: Extract data as INSERT statements.
            - COPY: Extract data as COPY statements.
            - PARTITION: Extract range and list Oracle partitions with subpartitions.
            - TYPE: Extract user defined Oracle types.
            - FDW: Export Oracle tables as foreign tables for Oracle, MySQL and SQL Server FDW.
            - MVIEW: Export materialized views.
            - QUERY: Try to automatically convert Oracle SQL queries.
            - KETTLE: Generate XML ktr template files for use by Kettle.
            - DBLINK: Generate Oracle foreign data wrapper server to use as dblink.
            - SYNONYM: Export Oracle&#39;s synonyms as views on other schema&#39;s objects.
            - DIRECTORY: Export Oracle&#39;s directories as external_file extension objects.
            - LOAD: Dispatch a list of queries over multiple PostgreSQL connections.
            - TEST: Perform a diff between Oracle and PostgreSQL databases.
            - TEST_COUNT: Perform a row count diff between Oracle and PostgreSQL tables.
            - TEST_VIEW: Perform a row count diff between Oracle and PostgreSQL views.
            - TEST_DATA: Perform data validation check on rows on both sides.
            - SEQUENCE_VALUES: Export DDL to set the last values of sequences

    Only one type of export can be performed at a time so the TYPE
    directive must be unique. If you have more than one only the last
    found in the file will be registered.

    Some export types cannot or should not be loaded directly into the
    PostgreSQL database and still require little manual editing. This is
    the case for GRANT, TABLESPACE, TRIGGER, FUNCTION, PROCEDURE, TYPE,
    QUERY and PACKAGE export types especially if you have PL/SQL code or
    Oracle specific SQL in them.

    For TABLESPACE you must ensure that file paths exist on the system
    and for SYNONYM you may need to ensure that the object&#39;s owners and
    schemas correspond to the new PostgreSQL database design.

    Note that you can chain multiple exports by giving to the TYPE
    directive a comma-separated list of export types, but in this case
    you must not use COPY or INSERT with other export types.

    Ora2Pg will convert Oracle partitions using table inheritance,
    triggers and functions. See documentation at PostgreSQL:
    http://www.postgresql.org/docs/current/interactive/ddl-partitioning.
    html

    The TYPE export allows export of user defined Oracle types. If you
    don&#39;t use the --plsql command line parameter it simply dumps Oracle
    user type as-is else Ora2Pg will try to convert it to PostgreSQL
    syntax.

    The KETTLE export type requires that the Oracle and PostgreSQL DNS
    are defined.

    Since Ora2Pg v8.1 there are three new export types:

            SHOW_VERSION : display Oracle version
            SHOW_SCHEMA  : display the list of schemas available in the database.
            SHOW_TABLE   : display the list of tables available.
            SHOW_COLUMN  : display the list of tables columns available and the
                    Ora2PG conversion type from Oracle to PostgreSQL that will be
                    applied. It will also warn you if there are PostgreSQL reserved
                    words in Oracle object names.

    Here is an example of the SHOW_COLUMN output:

            [2] TABLE CURRENT_SCHEMA (1 rows) (Warning: &#39;CURRENT_SCHEMA&#39; is a reserved word in PostgreSQL)
                    CONSTRAINT : NUMBER(22) =&amp;gt; bigint (Warning: &#39;CONSTRAINT&#39; is a reserved word in PostgreSQL)
                    FREEZE : VARCHAR2(25) =&amp;gt; varchar(25) (Warning: &#39;FREEZE&#39; is a reserved word in PostgreSQL)
            ...
            [6] TABLE LOCATIONS (23 rows)
                    LOCATION_ID : NUMBER(4) =&amp;gt; smallint
                    STREET_ADDRESS : VARCHAR2(40) =&amp;gt; varchar(40)
                    POSTAL_CODE : VARCHAR2(12) =&amp;gt; varchar(12)
                    CITY : VARCHAR2(30) =&amp;gt; varchar(30)
                    STATE_PROVINCE : VARCHAR2(25) =&amp;gt; varchar(25)
                    COUNTRY_ID : CHAR(2) =&amp;gt; char(2)

    These extraction keywords are used to only display the requested
    information and exit. This allows you to quickly explore on what you
    are going to work with.

    The SHOW_COLUMN allows another ora2pg command line option: &#39;--allow
    relname&#39; or &#39;-a relname&#39; to limit the displayed information to the
    given table.

    The SHOW_ENCODING export type will display the NLS_LANG and
    CLIENT_ENCODING values that Ora2Pg will use and the real encoding of
    the Oracle database with the corresponding client encoding that
    could be used with PostgreSQL.

    Ora2Pg allows you to export your Oracle, MySQL or MSSQL table
    definitions to be used with the oracle_fdw, mysql_fdw or tds_fdw
    foreign data wrapper. By using type FDW your tables will be exported
    as follows:

            CREATE FOREIGN TABLE oratab (
                    id        integer           NOT NULL,
                    text      character varying(30),
                    floating  double precision  NOT NULL
            ) SERVER oradb OPTIONS (table &#39;ORATAB&#39;);

    Now you can use the table like a regular PostgreSQL table.

    Release 10 adds a new export type designed to evaluate the content
    of the database to migrate, in terms of objects and cost to complete
    the migration:

            SHOW_REPORT  : show a detailed report of the Oracle database content.

    Here is a sample report: http://ora2pg.darold.net/report.html

    There is also a more advanced report with migration cost. See the
    dedicated chapter about Migration Cost Evaluation.

ESTIMATE_COST
    Activate the migration cost evaluation. Must only be used with
    SHOW_REPORT, FUNCTION, PROCEDURE, PACKAGE and QUERY export types.
    Default is disabled. You may want to use the --estimate_cost command
    line option instead to activate this functionality. Note that
    enabling this directive will force PLSQL_PGSQL activation.

COST_UNIT_VALUE
    Sets the value in minutes of the migration cost evaluation unit.
    Default is five minutes per unit. See --cost_unit_value to change
    the unit value at command line.

DUMP_AS_HTML
    By default when using SHOW_REPORT the migration report is generated
    as simple text. Enabling this directive will force ora2pg to create
    a report in HTML format.

    See http://ora2pg.darold.net/report.html for a sample report.

DUMP_AS_JSON
    By default when using SHOW_REPORT the migration report is generated
    as simple text. Enabling this directive will force ora2pg to create
    a report in JSON format.

    See http://ora2pg.darold.net/report.html for a sample report.

DUMP_AS_CSV
    By default when using SHOW_REPORT the migration report is generated
    as simple text, enabling this directive will force ora2pg to create
    a report in CSV format.

    See http://ora2pg.darold.net/report.html for a sample report.

DUMP_AS_FILE_PREFIX
    By default when using SHOW_REPORT the migration report is generated
    to stout. Enabling this directive in conjunction with DUMP_AS_*
    directives will force ora2pg to create a report files with the given
    extensions and formats. This option allows you to combine multiple
    DUMP_AS_* formats.

    See http://ora2pg.darold.net/report.html for a sample report.

HUMAN_DAYS_LIMIT
    Use this directive to redefine the number of person-days limit where
    the migration assessment level must switch from B to C. Default is
    set to 10 person-days.

JOBS
    This configuration directive adds multiprocess support to COPY,
    FUNCTION and PROCEDURE export types. The value is the number of
    processes to use. Default is to disable multiprocessing.

    This directive is used to set the number of cores to use to
    parallelize data import into PostgreSQL. During FUNCTION or
    PROCEDURE export type each function will be translated to plpgsql
    using a new process. The performance gain can be very important when
    you have tons of functions to convert.

    There&#39;s no limitation in parallel processing other than the number
    of cores and the PostgreSQL I/O performance capabilities.

    Doesn&#39;t work under Windows Operating System, it is simply disabled.

ORACLE_COPIES
    This configuration directive adds multiprocess support to extract
    data from Oracle. The value is the number of processes to use to
    parallelize the select query. Default is parallel query disabled.

    The parallelism is built on splitting the query following the number
    of cores given as value to ORACLE_COPIES as follows:

            SELECT * FROM MYTABLE WHERE ABS(MOD(COLUMN, ORACLE_COPIES)) = CUR_PROC

    where COLUMN is a technical key like a primary or unique key where
    split will be based and the current core used by the query
    (CUR_PROC). You can also force the column name to use using the
    DEFINED_PK configuration directive.

    Doesn&#39;t work under Windows Operating System, it is simply disabled.

DEFINED_PK
    This directive is used to define the technical key to use to split
    the query between number of cores set with the ORACLE_COPIES
    variable. For example:

            DEFINED_PK      EMPLOYEES:employee_id

    The parallel query that will be used supposing that -J or
    ORACLE_COPIES is set to 8:

            SELECT * FROM EMPLOYEES WHERE ABS(MOD(employee_id, 8)) = N

    where N is the current process forked starting from 0.

PARALLEL_TABLES
    This directive is used to define the number of tables that will be
    processed in parallel for data extraction. The limit is the number
    of cores on your machine. Ora2Pg will open one database connection
    for each parallel table extraction. This directive, when higher than
    1, will invalidate ORACLE_COPIES but not JOBS, so the real number of
    processes that will be used is PARALLEL_TABLES * JOBS.

    Note that this directive when set higher than 1 will also
    automatically enable the FILE_PER_TABLE directive if you are
    exporting to files. This is used to export tables and views in
    separate files.

    Use PARALLEL_TABLES to use parallelism with COPY, INSERT and
    TEST_DATA actions. It is also useful with TEST, TEST_COUNT, and
    SHOW_TABLE if --count_rows is used for real row count.

DEFAULT_PARALLELISM_DEGREE
    You can force Ora2Pg to use /*+ PARALLEL(tbname, degree) */ hint in
    each query used to export data from Oracle by setting a value higher
    than 1 to this directive. A value of 0 or 1 disables the use of
    parallel hint. Default is disabled.

FDW_SERVER
    This directive is used to set the name of the foreign data server
    that is used in the &quot;CREATE SERVER name FOREIGN DATA WRAPPER
    &amp;lt;fdw_extension&amp;gt; ...&quot; command. This name will then be used in the
    &quot;CREATE FOREIGN TABLE ...&quot; SQL commands and to import data using
    oracle_fdw. By default, no foreign server is defined. This only
    concerns export types FDW, COPY and INSERT. For export type FDW, the
    default value is orcl.

FDW_IMPORT_SCHEMA
    Schema where foreign tables for data migration will be created. If
    you use several instances of ora2pg for data migration through the
    foreign data wrapper, you might need to change the name of the
    schema for each instance. Default: ora2pg_fdw_import

ORACLE_FDW_PREFETCH
    The default behavior of Ora2Pg is to NOT set the &quot;prefetch&quot; option
    for oracle_fdw when used for COPY and INSERT. This directive allows
    the prefetch to be set. See oracle_fdw documentation for the current
    default.

ORACLE_FDW_COPY_MODE
    When using Ora2Pg COPY with oracle_fdw, it is possible to use two
    different modes: 1) &quot;local&quot;, which uses psql on the host running
    Ora2Pg for the &quot;TO&quot; binary stream; 2) &quot;server&quot;, which uses
    PostgreSQL server-side COPY for the &quot;TO&quot; binary stream. Both modes
    use psql for the &quot;FROM STDIN BINARY&quot;. However, &quot;local&quot; runs the psql
    &quot;FROM STDIN BINARY&quot; on the host Ora2Pg is run from, whereas &quot;server&quot;
    runs the psql &quot;FROM STDIN BINARY&quot; on the PostgreSQL server. &quot;local&quot;
    mode should work on any PostgreSQL-based system, including managed
    offerings, which are not expected to support use of &quot;server&quot; mode
    due to permissions. The default is &quot;local&quot; as this is compatible
    with more configurations.

ORACLE_FDW_COPY_FORMAT
    When using Ora2Pg COPY with oracle_fdw, it is possible to use either
    BINARY or CSV data format. BINARY provides better performance,
    however, requires exact data type matching between the FDW and
    destination table. CSV provides greater flexibility with respect to
    data type matching: if the FDW and destination data types are
    functionally-compatible, the columns can be copied. The default is
    &quot;binary&quot;.

DROP_FOREIGN_SCHEMA
    By default, Ora2Pg drops the temporary schema ora2pg_fdw_import used
    to import the Oracle foreign schema before each new import. If you
    want to preserve the existing schema because of modifications or the
    use of a third-party server, disable this directive.

EXTERNAL_TO_FDW
    This directive, enabled by default, allows exporting Oracle&#39;s
    External Tables as file_fdw foreign tables. To not export these
    tables at all, set the directive to 0.

INTERNAL_DATE_MAX
    Internal timestamps retrieved from custom types are extracted in the
    following format: 01-JAN-77 12.00.00.000000 AM. It is impossible to
    know the exact century that must be used, so by default any year
    below 49 will be added to 2000 and others to 1900. You can use this
    directive to change the default value 49. This is only relevant if
    you have a user-defined type with a timestamp column.

AUDIT_USER
    Set the comma-separated list of usernames that must be used to
    filter queries from the DBA_AUDIT_TRAIL table. Default is to not
    scan this table and to never look for queries. This parameter is
    used only with SHOW_REPORT and QUERY export type with no input file
    for queries. Note that queries will be normalized before output
    unlike when a file is given at input using the -i option or INPUT
    directive.

FUNCTION_CHECK
    Disable this directive if you want to disable check_function_bodies.

            SET check_function_bodies = false;

    It disables validation of the function body string during CREATE
    FUNCTION. Default is to use the postgresql.conf setting, which
    enables it by default.

ENABLE_BLOB_EXPORT
    Exporting BLOBs takes time; in some circumstances you may want to
    export all data except the BLOB columns. In this case, disable this
    directive and the BLOB columns will not be included into data
    export. Take care that the target bytea column does not have a NOT
    NULL constraint.

ENABLE_CLOB_EXPORT
    Same behavior as ENABLE_BLOB_EXPORT but for CLOB.

DATA_EXPORT_ORDER
    By default, data export order will be done by sorting on table name.
    If you have huge tables at the end of alphabetic order and you are
    using multiprocess, it can be better to set the sort order on size
    so that multiple small tables can be processed before the largest
    tables finish. In this case set this directive to size. Possible
    values are name and size. Note that export types SHOW_TABLE and
    SHOW_COLUMN will use this sort order too, not only COPY or INSERT
    export type. If you want to give your custom export order, just give
    a filename as value that contains the ordered list tables to export.
    Must be a list of one table per line, in uppercase for Oracle.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Limiting objects to export You may want to export only a part of an Oracle database. Here is a set of configuration directives that will allow you to control which parts of the database should be exported.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;ALLOW
    This directive allows you to set a list of objects to which the
    export must be limited, excluding all other objects of the same type
    of export. The value is a space or comma-separated list of object
    names to export. You can include valid regex into the list. For
    example:

            ALLOW           EMPLOYEES SALE_.* COUNTRIES .*_GEOM_SEQ

    will export objects with names EMPLOYEES, COUNTRIES, all objects
    beginning with &#39;SALE_&#39; and all objects with a name ending in
    &#39;_GEOM_SEQ&#39;. The object depends of the export type. Note that regex
    will not work with 8i database, you must use the % placeholder
    instead, Ora2Pg will use the LIKE operator.

    This is the way to declare global filters that will be used with the
    current export type. You can also use extended filters that will be
    applied to specific objects or only on their related export type.
    For example:

            ora2pg -p -c ora2pg.conf -t TRIGGER -a &#39;TABLE[employees]&#39;

    will limit export of triggers to those defined on table employees.
    If you want to extract all triggers but not some INSTEAD OF
    triggers:

            ora2pg -c ora2pg.conf -t TRIGGER -e &#39;VIEW[trg_view_.*]&#39;

    Or a more complex form:

            ora2pg -p -c ora2pg.conf -t TABLE -a &#39;TABLE[EMPLOYEES]&#39; \
                    -e &#39;INDEX[emp_.*];CKEY[emp_salary_min]&#39;

    This command will export the definition of the employee table but
    will exclude all indexes beginning with &#39;emp_&#39; and the CHECK
    constraint called &#39;emp_salary_min&#39;.

    When exporting partitions you can exclude some partition tables by
    using

            ora2pg -p -c ora2pg.conf -t PARTITION -e &#39;PARTITION[PART_199.* PART_198.*]&#39;

    This will exclude partitioned tables for years 1980 to 1999 from the
    export but not the main partition table. The trigger will also be
    adapted to exclude those tables.

    With GRANT export you can use this extended form to exclude some
    users from the export or limit the export to some others:

            ora2pg -p -c ora2pg.conf -t GRANT -a &#39;USER1 USER2&#39;

    or

            ora2pg -p -c ora2pg.conf -t GRANT -a &#39;GRANT[USER1 USER2]&#39;

    will limit export grants to users USER1 and USER2. But if you don&#39;t
    want to export grants on some functions for these users, for
    example:

            ora2pg -p -c ora2pg.conf -t GRANT -a &#39;USER1 USER2&#39; -e &#39;FUNCTION[adm_.*];PROCEDURE[adm_.*]&#39;

    Advanced filters may need some learning.

    Oracle doesn&#39;t allow the use of lookahead expressions so you may
    want to exclude some objects that match the ALLOW regexp you have
    defined. For example if you want to export all tables starting with
    E but not those starting with EXP it is not possible to do that in a
    single expression. This is why you can start a regular expression
    with the ! character to exclude objects matching the regexp given
    just after. Our previous example can be written as follows:

            ALLOW   E.* !EXP.*

    it will be translated into:

             REGEXP_LIKE(..., &#39;^E.*$&#39;) AND NOT REGEXP_LIKE(..., &#39;^EXP.*$&#39;)

    in the object search expression.

EXCLUDE
    This directive is the opposite of the previous. It allows you to
    define a space or comma-separated list of object names to exclude
    from the export. You can include valid regex in the list. For
    example:

            EXCLUDE         EMPLOYEES TMP_.* COUNTRIES

    will exclude objects with names EMPLOYEES, COUNTRIES and all tables
    beginning with &#39;tmp_&#39;.

    For example, you can ban some unwanted functions from export with
    this directive:

            EXCLUDE         write_to_.* send_mail_.*

    This example will exclude all functions, procedures or functions in
    a package with names beginning with those regex. Note that regex
    will not work with 8i database, you must use the % placeholder
    instead, Ora2Pg will use the NOT LIKE operator.

    See above (directive &#39;ALLOW&#39;) for the extended syntax.

NO_EXCLUDED_TABLE
    By default, Ora2Pg excludes from export some Oracle &quot;garbage&quot; tables
    from export that should never be part of an export. This behavior
    generates a lot of REGEXP_LIKE expressions which slow down the
    export when looking at tables. To disable this behavior enable this
    directive, you will have to exclude or clean up later by yourself
    the unwanted tables. The regexps used to exclude the tables are
    defined in the array @EXCLUDED_TABLES in lib/Ora2Pg.pm. Note this
    behavior is independent of the EXCLUDE configuration directive.

VIEW_AS_TABLE
    Set which views to export as tables. By default none. Value must be
    a list of view names or regexps separated by space or comma. If the
    object name is a view and the export type is TABLE, the view will be
    exported as a create table statement. If export type is COPY or
    INSERT, the corresponding data will be exported.

    See chapter &quot;Exporting views as PostgreSQL table&quot; for more details.

MVIEW_AS_TABLE
    Set which materialized views to export as tables. By default none.
    Value must be a list of materialized view names or regexps separated
    by space or comma. If the object name is a materialized view and the
    export type is TABLE, the view will be exported as a create table
    statement. If export type is COPY or INSERT, the corresponding data
    will be exported.

NO_VIEW_ORDERING
    By default, Ora2Pg tries to order views to avoid errors at import
    time with nested views. With a huge number of views this can take a
    very long time, you can bypass this ordering by enabling this
    directive.

GRANT_OBJECT
    When exporting GRANTs you can specify a comma separated list of
    objects for which privileges will be exported. Default is export for
    all objects. Here are the possible values: TABLE, VIEW, MATERIALIZED
    VIEW, SEQUENCE, PROCEDURE, FUNCTION, PACKAGE BODY, TYPE, SYNONYM,
    DIRECTORY. Only one object type is allowed at a time. For example
    set it to TABLE if you just want to export privileges on tables. You
    can use the -g option to overwrite it.

    When used this directive prevents the export of users unless it is
    set to USER. In this case only user definitions are exported.

WHERE
    This directive allows you to specify a WHERE clause filter when
    dumping the contents of tables. The value is constructed as follows:
    TABLE_NAME[WHERE_CLAUSE], or if you have only one where clause for
    all tables just put the where clause as the value. Both are possible
    too. Here are some examples:

            # Global where clause applying to all tables included in the export
            WHERE  1=1

            # Apply the where clause only on table TABLE_NAME
            WHERE  TABLE_NAME[ID1=&#39;001&#39;]

            # Applies two different clauses on tables TABLE_NAME and OTHER_TABLE
            # and a generic where clause on DATE_CREATE to all other tables
            WHERE  TABLE_NAME[ID1=&#39;001&#39; OR ID1=&#39;002] DATE_CREATE &amp;gt; &#39;2001-01-01&#39; OTHER_TABLE[NAME=&#39;test&#39;]

    Any WHERE clause not included in a table name bracket clause will be
    applied to all exported tables including the tables defined in the
    WHERE clause. These WHERE clauses are very useful if you want to
    archive some data or only export some recent data.

    To be able to quickly test data import it is useful to limit data
    export to the first thousand tuples of each table. For Oracle define
    the following clause:

            WHERE   ROWNUM &amp;lt; 1000

    and for MySQL, use the following:

            WHERE   1=1 LIMIT 1,1000

    This can also be restricted to some tables&#39; data export.

    Command line option -W or --where will override this directive for
    the global part and per table if the table names are the same.

TOP_MAX
    This directive is used to limit the number of items shown in the top
    N lists like the top list of tables per number of rows and the top
    list of largest tables in megabytes. By default it is set to 10
    items.

LOG_ON_ERROR
    Enable this directive if you want to continue direct data import on
    error. When Ora2Pg receives an error in the COPY or INSERT statement
    from PostgreSQL it will log the statement to a file called
    TABLENAME_error.log in the output directory and continue to next
    bulk of data. Like this you can try to fix the statement and
    manually reload the error log file. Default is disabled: abort
    import on error.

REPLACE_QUERY
    Sometimes you may want to extract data from an Oracle table but you
    need a custom query for that. Not just a &quot;SELECT * FROM table&quot; like
    Ora2Pg does but a more complex query. This directive allows you to
    overwrite the query used by Ora2Pg to extract data. The format is
    TABLENAME[SQL_QUERY]. If you have multiple tables to extract by
    replacing the Ora2Pg query, you can define multiple REPLACE_QUERY
    lines.

            REPLACE_QUERY   EMPLOYEES[SELECT e.id,e.firstname,lastname FROM EMPLOYEES e JOIN EMP_UPDT u ON (e.id=u.id AND u.cdate&amp;gt;&#39;2014-08-01 00:00:00&#39;)]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Control of Full Text Search export Several directives can be used to control how Ora2Pg exports the Oracle&#39;s Text search indexes. By default, CONTEXT indexes will be exported to PostgreSQL FTS indexes, while CTXCAT indexes will be exported as indexes using the pg_trgm extension.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;CONTEXT_AS_TRGM
    Forces Ora2Pg to translate Oracle Text indexes into PostgreSQL
    indexes using the pg_trgm extension. By default, CONTEXT indexes are
    translated into FTS indexes and CTXCAT indexes use pg_trgm. Most of
    the time using pg_trgm is sufficient, which is why this directive
    exists. You need to create the pg_trgm extension in the destination
    database before importing the objects:

            CREATE EXTENSION pg_trgm;

FTS_INDEX_ONLY
    By default, Ora2Pg creates a function-based index to translate
    Oracle Text indexes:

            CREATE INDEX ON t_document
                    USING gin(to_tsvector(&#39;pg_catalog.french&#39;, title));

    You will have to rewrite the CONTAINS() clause using to_tsvector(),
    for example:

            SELECT id,title FROM t_document
                    WHERE to_tsvector(title) @@ to_tsquery(&#39;search_word&#39;);

    To force Ora2Pg to create an extra tsvector column with dedicated
    triggers for FTS indexes, disable this directive. In this case,
    Ora2Pg will add the column as follows: ALTER TABLE t_document ADD
    COLUMN tsv_title tsvector; Then update the column to compute FTS
    vectors if data have been loaded before: UPDATE t_document SET
    tsv_title = to_tsvector(&#39;pg_catalog.french&#39;, coalesce(title,&#39;&#39;)); To
    automatically update the column when a modification in the title
    column occurs, Ora2Pg adds the following trigger:

            CREATE FUNCTION tsv_t_document_title() RETURNS trigger AS $$
            BEGIN
                   IF TG_OP = &#39;INSERT&#39; OR new.title != old.title THEN
                           new.tsv_title :=
                           to_tsvector(&#39;pg_catalog.french&#39;, coalesce(new.title,&#39;&#39;));
                   END IF;
                   return new;
            END
            $$ LANGUAGE plpgsql;
            CREATE TRIGGER trig_tsv_t_document_title BEFORE INSERT OR UPDATE
             ON t_document
             FOR EACH ROW EXECUTE PROCEDURE tsv_t_document_title();

    When the Oracle text index is defined over multiple columns, Ora2Pg
    will use setweight() to set weights in the order of the column
    declarations.

FTS_CONFIG
    Use this directive to force which text search configuration to use.
    When it is not set, Ora2Pg will autodetect the stemmer used by
    Oracle for each index and use pg_catalog.english if the information
    is not found.

USE_UNACCENT
    If you want to perform text searches in an accent-insensitive way,
    enable this directive. Ora2Pg will create a helper function using
    unaccent() and create the pg_trgm indexes using this function. With
    FTS, Ora2Pg will redefine your text search configuration, for
    example:

          CREATE TEXT SEARCH CONFIGURATION fr (COPY = french); 
          ALTER TEXT SEARCH CONFIGURATION fr
                  ALTER MAPPING FOR hword, hword_part, word WITH unaccent, french_stem;

    then set the FTS_CONFIG ora2pg.conf directive to fr instead of
    pg_catalog.english.

    When enabled, Ora2pg will create the wrapper function:

          CREATE OR REPLACE FUNCTION unaccent_immutable(text)
          RETURNS text AS
          $$
              SELECT public.unaccent(&#39;public.unaccent&#39;, $1);
          $$ LANGUAGE sql IMMUTABLE
             COST 1;

    The indexes are exported as follows:

          CREATE INDEX t_document_title_unaccent_trgm_idx ON t_document 
              USING gin (unaccent_immutable(title) gin_trgm_ops);

    In your queries, you will need to use the same function in the
    search to be able to use the function-based index. Example:

            SELECT * FROM t_document
                    WHERE unaccent_immutable(title) LIKE &#39;%donnees%&#39;;

USE_LOWER_UNACCENT
    Same as above but calls lower() in the unaccent_immutable()
    function:

          CREATE OR REPLACE FUNCTION unaccent_immutable(text)
          RETURNS text AS
          $$
              SELECT lower(public.unaccent(&#39;public.unaccent&#39;, $1));
          $$ LANGUAGE sql IMMUTABLE;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Modifying object structure One of the great uses of Ora2Pg is its flexibility to replicate an Oracle database into a PostgreSQL database with a different structure or schema. There are three configuration directives that allow you to map these differences.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;REORDERING_COLUMNS
    Enable this directive to reorder columns and minimize the footprint
    on disk, so that more rows fit on a data page, which is the most
    important factor for speed. Default is disabled, meaning the same
    order as in Oracle tables definition, which should be enough for
    most uses. This directive is only used with TABLE export.

MODIFY_STRUCT
    This directive allows you to limit the columns to extract for a
    given table. The value consists of a space-separated list of table
    names with a set of columns between parentheses as follows:

            MODIFY_STRUCT   TABLE_NAME(colname1,colname2,...) ...

    for example:

            MODIFY_STRUCT   T_TEST1(id,dossier) T_TEST2(id,fichier)

    This will only extract columns &#39;id&#39; and &#39;dossier&#39; from table T_TEST1
    and columns &#39;id&#39; and &#39;fichier&#39; from the T_TEST2 table. This
    directive can only be used with TABLE, COPY or INSERT export. With
    TABLE export create table DDL will respect the new list of columns
    and all indexes or foreign keys pointing to or from a removed column
    will not be exported.

EXCLUDE_COLUMNS
    Instead of redefining the table structure with MODIFY_STRUCT you may
    want to exclude some columns from the table export. The value
    consists of a space-separated list of table names with a set of
    column between parentheses as follows:

            EXCLUDE_COLUMNS TABLE_NAME(colname1,colname2,...) ...

    for example:

            EXCLUDE_COLUMNS T_TEST1(id,dossier) T_TEST2(id,fichier)

    This will exclude columns &#39;id&#39; and &#39;dossier&#39; from table T_TEST1 and
    columns &#39;id&#39; and &#39;fichier&#39; from the T_TEST2 table from the export.
    This directive can only be used with TABLE, COPY or INSERT export.
    With TABLE export create table DDL will respect the new list of
    columns and all indexes or foreign keys pointing to or from a
    removed column will not be exported.

REPLACE_TABLES
    This directive allows you to remap a list of Oracle table names to a
    PostgreSQL table names during export. The value is a list of
    space-separated values with the following structure:

            REPLACE_TABLES  ORIG_TBNAME1:DEST_TBNAME1 ORIG_TBNAME2:DEST_TBNAME2

    Oracle tables ORIG_TBNAME1 and ORIG_TBNAME2 will be respectively
    renamed to DEST_TBNAME1 and DEST_TBNAME2

REPLACE_COLS
    Like table names, column names can be remapped to different names
    using the following syntax:

            REPLACE_COLS    ORIG_TBNAME(ORIG_COLNAME1:NEW_COLNAME1,ORIG_COLNAME2:NEW_COLNAME2)

    For example:

            REPLACE_COLS    T_TEST(dico:dictionary,dossier:folder)

    will rename Oracle columns &#39;dico&#39; and &#39;dossier&#39; from table T_TEST to
    new names &#39;dictionary&#39; and &#39;folder&#39;.

REPLACE_AS_BOOLEAN
    If you want to change the type of some Oracle columns to PostgreSQL
    boolean during the export you can define here a list of tables and
    columns separated by spaces as follows.

            REPLACE_AS_BOOLEAN     TB_NAME1:COL_NAME1 TB_NAME1:COL_NAME2 TB_NAME2:COL_NAME2

    The values set in the boolean columns list will be replaced with &#39;t&#39;
    and &#39;f&#39; following the default replacement values and those
    additionally set in directive BOOLEAN_VALUES.

    Note that if you have modified the table name with REPLACE_TABLES
    and/or the column&#39;s name, you need to use the name of the original
    table and/or column.

            REPLACE_COLS            TB_NAME1(OLD_COL_NAME1:NEW_COL_NAME1)
            REPLACE_AS_BOOLEAN      TB_NAME1:OLD_COL_NAME1

    You can also give a type and precision to automatically convert all
    fields of that type to boolean. For example:

            REPLACE_AS_BOOLEAN      NUMBER:1 CHAR:1 TB_NAME1:COL_NAME1 TB_NAME1:COL_NAME2

    will also replace any field of type number(1) or char(1) as a
    boolean in all exported tables.

BOOLEAN_VALUES
    Use this to add additional definitions of the possible boolean
    values used in Oracle fields. You must set a space-separated list of
    TRUE:FALSE values. By default, here are the values recognized by
    Ora2Pg:

            BOOLEAN_VALUES          yes:no y:n 1:0 true:false enabled:disabled

    Any values defined here will be added to the default list.

REPLACE_ZERO_DATE
    When Ora2Pg finds a &quot;zero&quot; date: 0000-00-00 00:00:00 it is replaced
    by a NULL. This could be a problem if your column is defined with a
    NOT NULL constraint. If you can not remove the constraint, use this
    directive to set an arbitrary date that will be used instead. You
    can also use -INFINITY if you don&#39;t want to use a fake date.

INDEXES_SUFFIX
    Add the given value as suffix to index names. Useful if you have
    indexes with the same name as tables. For example:

            INDEXES_SUFFIX          _idx

    will add _idx at the end of all index names. Not very common but
    helpful.

INDEXES_RENAMING
    Enable this directive to rename all indexes using
    tablename_columns_names. Could be very useful for databases that
    have multiple instances of the same index name or that use the same
    name as a table, which is not allowed by PostgreSQL. Disabled by
    default.

USE_INDEX_OPCLASS
    Operator classes text_pattern_ops, varchar_pattern_ops, and
    bpchar_pattern_ops support B-tree indexes on the corresponding
    types. The difference from the default operator classes is that the
    values are compared strictly character by character rather than
    according to locale-specific collation rules. This makes these
    operator classes suitable for use by queries involving pattern
    matching expressions (LIKE or POSIX regular expressions) when the
    database does not use the standard &quot;C&quot; locale. If enabled with value
    1, this will force Ora2Pg to export all indexes defined on
    varchar2() and char() columns using those operators. If you set it
    to a value greater than 1, it will only change indexes on columns
    where the character limit is greater than or equal to this value.
    For example, set it to 128 to create these kinds of indexes on
    columns of type varchar2(N) where N &amp;gt;= 128.

RENAME_PARTITION
    Enable this directive if you want your partition tables to be
    renamed. Disabled by default. If you have multiple partitioned
    tables, when exported to PostgreSQL some partitions could have the
    same name but different parent tables. This is not allowed - table
    names must be unique. In this case, enable this directive. A
    partition will be renamed following the rule: &quot;tablename&quot;_part&quot;pos&quot;
    where &quot;pos&quot; is the partition number. For subpartition this is:
    &quot;tablename&quot;_part&quot;pos&quot;_subpart&quot;pos&quot; If this is partition/subpartition
    default: &quot;tablename&quot;_part_default
    &quot;tablename&quot;_part&quot;pos&quot;_subpart_default

DISABLE_PARTITION
    If you don&#39;t want to reproduce the partitioning like in Oracle and
    want to export all partitioned Oracle data into the main single
    table in PostgreSQL, enable this directive. Ora2Pg will export all
    data into the main table name. Default is to use partitioning -
    Ora2Pg will export data from each partition and import them into the
    PostgreSQL dedicated partition table.

PARTITION_BY_REFERENCE
    How to export partition by reference. Possible values are none,
    duplicate or the number of hash partitions to create. Default is
    none to not export the partitions by reference.

    Value &#39;none&#39; means no translation and export of partition by
    reference like before. Value &#39;duplicate&#39; will duplicate the
    referenced column in the partitioned table and apply the same
    partitioning from the referenced table to the partitioned table. If
    the value is a number, the table will be partitioned with the HASH
    method using the value as the modulo. For example, if you set it to
    4 it will create 4 HASH partitions.

DISABLE_UNLOGGED
    By default, Ora2Pg exports Oracle tables with the NOLOGGING
    attribute as UNLOGGED tables. You may want to fully disable this
    feature because you will lose all data from unlogged tables in case
    of a PostgreSQL crash. Set it to 1 to export all tables as normal
    tables.

DOUBLE_MAX_VARCHAR
    Increase varchar max character constraints to support PostgreSQL two
    bytes character encoding when the source database applies the length
    constraint on characters not bytes. Default disabled.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Oracle Spatial to PostGIS Ora2Pg fully exports Spatial objects from Oracle database. There are some configuration directives that can be used to control the export.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;AUTODETECT_SPATIAL_TYPE
    By default, Ora2Pg looks at indexes to see the spatial constraint
    type and dimensions defined under Oracle. Those constraints are
    passed at index creation using for example:

            CREATE INDEX ... INDEXTYPE IS MDSYS.SPATIAL_INDEX
            PARAMETERS(&#39;sdo_indx_dims=2, layer_gtype=point&#39;);

    If those Oracle constraint parameters are not set, the default is to
    export those columns as generic type GEOMETRY to be able to receive
    any spatial type.

    The AUTODETECT_SPATIAL_TYPE directive allows Ora2Pg to autodetect
    the real spatial type and dimension used in a spatial column;
    otherwise a non- constrained &quot;geometry&quot; type is used. Enabling this
    feature will force Ora2Pg to scan a sample of 50,000 columns to look
    at the GTYPE used. You can increase or reduce the sample size by
    setting the value of AUTODETECT_SPATIAL_TYPE to the desired number
    of lines to scan. The directive is enabled by default.

    For example, in the case of a column named shape and defined with
    Oracle type SDO_GEOMETRY, with AUTODETECT_SPATIAL_TYPE disabled it
    will be converted as:

        shape geometry(GEOMETRY) or shape geometry(GEOMETRYZ, 4326)

    and if the directive is enabled and the column just contains a
    single geometry type that uses a single dimension:

        shape geometry(POLYGON, 4326) or shape geometry(POLYGONZ, 4326)

    with a two or three dimensional polygon.

CONVERT_SRID
    This directive allows you to control the automatic conversion of
    Oracle SRID to standard EPSG. If enabled, Ora2Pg will use the Oracle
    function sdo_cs.map_oracle_srid_to_epsg() to convert all SRIDs.
    Enabled by default.

    If the SDO_SRID returned by Oracle is NULL, it will be replaced by
    the default value 8307 converted to its EPSG value: 4326 (see
    DEFAULT_SRID).

    If the value is higher than 1, all SRIDs will be forced to this
    value. In this case, DEFAULT_SRID will not be used when Oracle
    returns a null value, and the value will be forced to CONVERT_SRID.

    Note that it is also possible to set the EPSG value on the Oracle
    side when sdo_cs.map_oracle_srid_to_epsg() returns NULL if you want
    to force the value:

      system@db&amp;gt; UPDATE sdo_coord_ref_sys SET legacy_code=41014 WHERE srid = 27572;

DEFAULT_SRID
    Use this directive to override the default EPSG SRID to use: 4326.
    Can be overwritten by CONVERT_SRID, see above.

GEOMETRY_EXTRACT_TYPE
    This directive can take three values: WKT (default), WKB and
    INTERNAL. When it is set to WKT, Ora2Pg will use
    SDO_UTIL.TO_WKTGEOMETRY() to extract the geometry data. When it is
    set to WKB, Ora2Pg will use the binary output using
    SDO_UTIL.TO_WKBGEOMETRY(). If these two extract types are called at
    the Oracle side, they are slow and you can easily reach Out Of
    Memory when you have lots of rows. Also, WKB is not able to export
    3D geometry and some geometries like CURVEPOLYGON. In this case, you
    may use the INTERNAL extraction type. It will use a Pure Perl
    library to convert the SDO_GEOMETRY data into a WKT representation,
    the translation is done on Ora2Pg side. This is a work in progress,
    please validate your exported data geometries before use. Default
    spatial object extraction type is INTERNAL.

POSTGIS_SCHEMA
    Use this directive to add a specific schema to the search path to
    look for PostGIS functions.

ST_SRID_FUNCTION
    Oracle function to use to extract the SRID from ST_Geometry meta
    information. Default: ST_SRID, for example it should be set to
    sde.st_srid for ArcSDE.

ST_DIMENSION_FUNCTION
    Oracle function to use to extract the dimension from ST_Geometry
    meta information. Default: ST_DIMENSION, for example it should be
    set to sde.st_dimension for ArcSDE.

ST_GEOMETRYTYPE_FUNCTION
    Oracle function to use to extract the geometry type from an
    ST_Geometry column. Default: ST_GEOMETRYTYPE, for example it should
    be set to sde.st_geometrytype for ArcSDE.

ST_ASBINARY_FUNCTION
    Oracle function used to convert an ST_Geometry value into WKB
    format. Default: ST_ASBINARY, for example it should be set to
    sde.st_asbinary for ArcSDE.

ST_ASTEXT_FUNCTION
    Oracle function used to convert an ST_Geometry value into WKT
    format. Default: ST_ASTEXT, for example it should be set to
    sde.st_astext for ArcSDE.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PostgreSQL Import By default, conversion to PostgreSQL format is written to a file named &#39;output.sql&#39;. The command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        psql mydb &amp;lt; output.sql

will import the contents of file output.sql into the PostgreSQL mydb
database.

DATA_LIMIT
    When performing INSERT/COPY export, Ora2Pg processes data in chunks
    of DATA_LIMIT tuples for speed improvement. Tuples are stored in
    memory before being written to disk, so if you want speed and have
    enough system resources you can increase this limit to a higher
    value, for example: 100000 or 1000000. Before release 7.0, a value
    of 0 meant no limit so that all tuples were stored in memory before
    being flushed to disk. In the 7.x branch this has been removed and
    chunks will be set to the default: 10000

BLOB_LIMIT
    When Ora2Pg detects a table with BLOB data, it will automatically
    reduce the value of this directive by dividing it by 10 until its
    value is below 1000. You can control this value by setting
    BLOB_LIMIT. Exporting BLOBs uses lot of resources; setting it to a
    too high value can produce OOM errors.

CLOB_AS_BLOB
    Applies same behavior on CLOBs than BLOBs with BLOB_LIMIT settings.
    This is especially useful if you have large CLOB data. Default:
    enabled

OUTPUT
    The Ora2Pg output filename can be changed with this directive.
    Default value is output.sql. If you set the file name with extension
    .gz or .bz2 the output will be automatically compressed. This
    requires that the Compress::Zlib Perl module is installed if the
    filename extension is .gz and that the bzip2 system command is
    installed for the .bz2 extension.

OUTPUT_DIR
    Since release 7.0, you can define a base directory where the files
    will be written. The directory must exist.

BZIP2
    This directive allows you to specify the full path to the bzip2
    program if it can not be found in the PATH environment variable.

FILE_PER_CONSTRAINT
    Allows object constraints to be saved in a separate file during
    schema export. The file will be named CONSTRAINTS_OUTPUT, where
    OUTPUT is the value of the corresponding configuration directive.
    You can use .gz or .bz2 extension to enable compression. Default is
    to save all data in the OUTPUT file. This directive is usable only
    with TABLE export type.

    The constraints can be imported quickly into PostgreSQL using the
    LOAD export type to parallelize their creation over multiple (-j or
    JOBS) connections.

FILE_PER_INDEX
    Allows indexes to be saved in a separate file during schema export.
    The file will be named INDEXES_OUTPUT, where OUTPUT is the value of
    the corresponding configuration directive. You can use .gz or .bz2
    file extension to enable compression. Default is to save all data in
    the OUTPUT file. This directive is usable only with TABLE AND
    TABLESPACE export type. With the TABLESPACE export, it is used to
    write &quot;ALTER INDEX ... TABLESPACE ...&quot; into a separate file named
    TBSP_INDEXES_OUTPUT that can be loaded at the end of the migration
    after the indexes creation to move the indexes.

    The indexes can be imported quickly into PostgreSQL using the LOAD
    export type to parallelize their creation over multiple (-j or JOBS)
    connections.

FILE_PER_FKEYS
    Allows foreign key declarations to be saved in a separate file
    during schema export. By default foreign keys are exported into the
    main output file or in the CONSTRAINT_output.sql file. When enabled,
    foreign keys will be exported into a file named FKEYS_output.sql

FILE_PER_TABLE
    Allows data export to be saved in one file per table/view. The files
    will be named as tablename_OUTPUT, where OUTPUT is the value of the
    corresponding configuration directive. You can still use .gz or .bz2
    extension in the OUTPUT directive to enable compression. Default 0
    will save all data in one file, set it to 1 to enable this feature.
    This is usable only during INSERT or COPY export type.

FILE_PER_FUNCTION
    Allows functions, procedures and triggers to be saved in one file
    per object. The files will be named as objectname_OUTPUT, where
    OUTPUT is the value of the corresponding configuration directive.
    You can still use .gz or .bz2 extension in the OUTPUT directive to
    enable compression. Default 0 will save all in one single file, set
    it to 1 to enable this feature. This is usable only during the
    corresponding export type; the package body export has a special
    behavior.

    When export type is PACKAGE and you&#39;ve enabled this directive,
    Ora2Pg will create a directory per package, named with the lower
    case name of the package, and will create one file per
    function/procedure in that directory. If the configuration directive
    is not enabled, it will create one file per package as
    packagename_OUTPUT, where OUTPUT is the value of the corresponding
    directive.

TRUNCATE_TABLE
    If this directive is set to 1, a TRUNCATE TABLE instruction will be
    added before loading data. This is usable only during INSERT or COPY
    export types.

    When activated, the instruction will be added only if there&#39;s no
    global DELETE clause or no specific one for to the current table
    (see below).

DELETE
    Supports including a DELETE FROM ... WHERE clause filter before
    importing data to perform a delete of some lines instead of
    truncating tables. Value is constructed as follows:
    TABLE_NAME[DELETE_WHERE_CLAUSE], or if you have only one where
    clause for all tables just put the delete clause as a single value.
    Both are possible too. Here are some examples:

            DELETE  1=1    # Apply to all tables and delete all tuples
            DELETE  TABLE_TEST[ID1=&#39;001&#39;]   # Apply only on table TABLE_TEST
            DELETE  TABLE_TEST[ID1=&#39;001&#39; OR ID1=&#39;002] DATE_CREATE &amp;gt; &#39;2001-01-01&#39; TABLE_INFO[NAME=&#39;test&#39;]

    The last example applies two different delete where clauses on
    tables TABLE_TEST and TABLE_INFO and a generic delete where clause
    on DATE_CREATE to all other tables. If TRUNCATE_TABLE is enabled it
    will be applied to all tables not covered by the DELETE definition.

    These DELETE clauses might be useful with regular &quot;updates&quot;.

STOP_ON_ERROR
    Set this parameter to 0 to not include the call to \set
    ON_ERROR_STOP ON in all SQL scripts generated by Ora2Pg. By default
    this order is always present so that the script will immediately
    abort when an error is encountered.

COPY_FREEZE
    Enable this directive to use COPY FREEZE instead of a simple COPY to
    export data with rows already frozen. This is intended as a
    performance option for initial data loading. Rows will be frozen
    only if the table being loaded has been created or truncated in the
    current sub-transaction. This will only work with export to file and
    when -J or ORACLE_COPIES is not set or defaults to 1. It can be used
    with direct import into PostgreSQL under the same condition but -j
    or JOBS must also be unset or default to 1.

CREATE_OR_REPLACE
    By default Ora2Pg uses CREATE OR REPLACE in functions and views DDL.
    If you need not to override existing functions or views, disable
    this configuration directive - DDL will not include OR REPLACE.

DROP_IF_EXISTS
    To add a DROP &amp;lt;OBJECT&amp;gt; IF EXISTS before creating the object, enable
    this directive. Can be useful in iterative work. Default is
    disabled.

EXPORT_GTT
    PostgreSQL does not support Global Temporary Tables natively but you
    can use the pgtt extension to emulate this behavior. Enable this
    directive to export global temporary tables.

PGTT_NOSUPERUSER
    By default the pgtt extension is loaded using superuser privileges.
    Enable it if you run the SQL scripts generated using a non superuser
    user. It will use:

        LOAD &#39;$libdir/plugins/pgtt&#39;;

    instead of default:

        LOAD &#39;pgtt&#39;;

NO_HEADER
    Enabling this directive will prevent Ora2Pg from printing its header
    into output files. Only the translated code will be written.

PSQL_RELATIVE_PATH
    By default, Ora2Pg uses \i psql command to execute generated SQL
    files. If you want to use a relative path following the script
    execution file, enabling this option will use \ir. See psql help for
    more information.

DATA_VALIDATION_ROWS
    Number of rows that must be retrieved on both sides for data
    validation. Default is to compare the first 10000 rows. A value of 0
    means compare all rows.

DATA_VALIDATION_ORDERING
    Order of rows between both sides is different once the data has been
    modified. In this case data must be ordered using a primary key or a
    unique index, meaning that a table without such object cannot be
    compared. If the validation is done just after the data migration
    without any data modification, the validation can be done on all
    tables without any ordering.

DATA_VALIDATION_ERROR
    Stop validating data from a table after a certain amount of row
    mismatches. Default is to stop after 10 rows validation errors.

TRANSFORM_VALUE
    Use this directive to specify which transformation should be applied
    to a column when exporting data. Value must be a semicolon-separated
    list of

       TABLE[COLUMN_NAME, &amp;lt;replace code in SELECT target list&amp;gt;]

    For example, to replace the string &#39;Oracle&#39; with &#39;PostgreSQL&#39; in a
    varchar2 column, use the following.

       TRANSFORM_VALUE   ERROR_LOG_SAMPLE[DBMS_TYPE:regexp_replace(&quot;DBMS_TYPE&quot;,&#39;Oracle&#39;,&#39;PostgreSQL&#39;)]

    or to replace all Oracle char(0) in a string with a space character:

        TRANSFORM_VALUE   CLOB_TABLE[CHARDATA:translate(&quot;CHARDATA&quot;, chr(0), &#39; &#39;)]

    The expression will be applied in the SQL statement used to extract
    data from the source database.

NO_START_SCN
    Enable this directive if you don&#39;t want to export all data based on
    current SCN. By default Ora2Pg get first the current SCN and then
    retrieve all table data using this SCN to be consistant in case of
    data modification.

When using Ora2Pg export type INSERT or COPY to dump data to a file and
FILE_PER_TABLE is enabled, you will be warned that Ora2Pg will not
export data again if the file already exists. This is to prevent
downloading table data twice when dealing with huge amount of data. To
force the download of data from these tables you have to remove the
existing output file first.

If you want to import data on the fly to the PostgreSQL database, you
have three configuration directives to set the PostgreSQL database
connection. This is only possible with COPY or INSERT export type as for
database schema there&#39;s no real benefit to do that.

PG_DSN
    Use this directive to set the PostgreSQL data source namespace using
    DBD::Pg Perl module as follows:

            dbi:Pg:dbname=pgdb;host=localhost;port=5432

    will connect to database &#39;pgdb&#39; on localhost at tcp port 5432.

    Note that this directive is only used for data export, other exports
    need to be imported manually through the use of psql or any other
    PostgreSQL client.

    To use SSL encrypted connection you must add sslmode=require to the
    connection string as follows:

            dbi:Pg:dbname=pgdb;host=localhost;port=5432;sslmode=require

PG_USER and PG_PWD
    These two directives are used to set the login user and password.

    If you do not supply credentials with PG_PWD and you have installed
    the Term::ReadKey Perl module, Ora2Pg will ask for the password
    interactively. If PG_USER is not set it will be asked interactively
    too.

SYNCHRONOUS_COMMIT
    Specifies whether transaction commit will wait for WAL records to be
    written to disk before the command returns a &quot;success&quot; indication to
    the client. This is equivalent to setting the synchronous_commit
    directive in the postgresql.conf file. This is only used when you
    load data directly to PostgreSQL; the default is off to disable
    synchronous commit to gain speed at writing data. Some modified
    versions of PostgreSQL, like Greenplum, do not have this setting, so
    in this case set this directive to 1, and ora2pg will not try to
    change the setting.

PG_INITIAL_COMMAND
    This directive can be used to send an initial command to PostgreSQL,
    just after the connection. For example to set some session
    parameters. This directive can be used multiple times.

INSERT_ON_CONFLICT
    When enabled this instructs Ora2Pg to add an ON CONFLICT DO NOTHING
    clause to all INSERT statements generated for this type of data
    export.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Column type control PG_NUMERIC_TYPE If set to 1, replace portable numeric type with PostgreSQL internal type. Oracle data type NUMBER(p,s) is approximately converted to real and float PostgreSQL data types. If you have monetary fields or don&#39;t want rounding issues with the extra decimals, you should preserve the same numeric(p,s) PostgreSQL data type. Do this only if you need exactness because using numeric(p,s) is slower than using real or double.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PG_INTEGER_TYPE
    If set to 1, replace portable numeric type with PostgreSQL internal
    type. Oracle data types NUMBER(p) or NUMBER are converted to
    smallint, integer or bigint PostgreSQL data types following the
    value of the precision. If NUMBER without precision is set to
    DEFAULT_NUMERIC (see below).

DEFAULT_NUMERIC
    NUMBER without precision is converted by default to bigint only if
    PG_INTEGER_TYPE is true. You can override this value to any PG type,
    like integer or float.

DATA_TYPE
    If you&#39;re experiencing any problems in data type schema conversion,
    with this directive you can take full control of the correspondence
    between Oracle and PostgreSQL types to redefine data type
    translation used in Ora2pg. The syntax is a comma-separated list of
    &quot;Oracle datatype:PostgreSQL datatype&quot;. Here is the default list
    used:

            DATA_TYPE       VARCHAR2:varchar,NVARCHAR2:varchar,NVARCHAR:varchar,NCHAR:char,DATE:timestamp(0),LONG:text,LONG RAW:bytea,CLOB:text,NCLOB:text,BLOB:bytea,BFILE:bytea,RAW(16):uuid,RAW(32):uuid,RAW:bytea,UROWID:oid,ROWID:oid,FLOAT:double precision,DEC:decimal,DECIMAL:decimal,DOUBLE PRECISION:double precision,INT:integer,INTEGER:integer,REAL:real,SMALLINT:smallint,BINARY_FLOAT:double precision,BINARY_DOUBLE:double precision,TIMESTAMP:timestamp,XMLTYPE:xml,BINARY_INTEGER:integer,PLS_INTEGER:integer,TIMESTAMP WITH TIME ZONE:timestamp with time zone,TIMESTAMP WITH LOCAL TIME ZONE:timestamp with time zone

    The directive and the list definition must be a single line.

    Note that when RAW(16) or RAW(32) columns are found or when the RAW
    column has &quot;SYS_GUID()&quot; as default value, Ora2Pg will automatically
    translate the type of the column into uuid which might be the right
    translation in most cases. In this case data will be automatically
    migrated as PostgreSQL uuid data type provided by the &quot;uuid-ossp&quot;
    extension.

    If you want to replace a type with a precision and scale you need to
    escape the comma with a backslash. For example, if you want to
    replace all NUMBER(*,0) with bigint instead of numeric(38) add the
    following:

           DATA_TYPE       NUMBER(*\,0):bigint

    You don&#39;t have to repeat all default type conversions. Instead just
    specify the ones you want to rewrite.

    There&#39;s a special case with BFILE when they are converted to type
    TEXT - they will just contain the full path to the external file. If
    you set the destination type to BYTEA (the default), Ora2Pg will
    export the content of the BFILE as bytea. The third case is when you
    set the destination type to EFILE - in this case, Ora2Pg will export
    it as an EFILE record: (DIRECTORY, FILENAME). Use the DIRECTORY
    export type to export the existing directories as well as privileges
    on those directories.

    There&#39;s no SQL function available to retrieve the path to the BFILE.
    Ora2Pg has to create one using the DBMS_LOB package.

            CREATE OR REPLACE FUNCTION ora2pg_get_bfilename( p_bfile IN BFILE )
            RETURN VARCHAR2
            AS
                l_dir   VARCHAR2(4000);
                l_fname VARCHAR2(4000);
                l_path  VARCHAR2(4000);
            BEGIN
                dbms_lob.FILEGETNAME( p_bfile, l_dir, l_fname );
                SELECT directory_path INTO l_path FROM all_directories
                    WHERE directory_name = l_dir;
                l_dir := rtrim(l_path,&#39;/&#39;);
                RETURN l_dir || &#39;/&#39; || l_fname;
            END;

    This function is only created if Ora2Pg found a table with a BFILE
    column and that the destination type is TEXT. The function is
    dropped at the end of the export. This concern both, COPY and INSERT
    export type.

    There&#39;s no SQL function available to retrieve BFILE as an EFILE
    record, therefore Ora2Pg needs to create one using the DBMS_LOB
    package.

            CREATE OR REPLACE FUNCTION ora2pg_get_efile( p_bfile IN BFILE )
            RETURN VARCHAR2
            AS
                l_dir   VARCHAR2(4000);
                l_fname VARCHAR2(4000);
            BEGIN
                dbms_lob.FILEGETNAME( p_bfile, l_dir, l_fname );
                RETURN &#39;(&#39; || l_dir || &#39;,&#39; || l_fnamei || &#39;)&#39;;
            END;

    This function is only created if Ora2Pg finds a table with a BFILE
    column and that the destination type is EFILE. The function is
    dropped at the end of the export. This concerns both COPY and INSERT
    export types.

    To set the destination type, use the DATA_TYPE configuration
    directive:

            DATA_TYPE       BFILE:EFILE

    for example.

    The EFILE type is a user defined type created by the PostgreSQL
    extension external_file that can be found here:
    https://github.com/darold/external_file This is a port of the BFILE
    Oracle type to PostgreSQL.

    There&#39;s no SQL function available to retrieve the content of a
    BFILE. Ora2Pg needs to create one using the DBMS_LOB package.

            CREATE OR REPLACE FUNCTION ora2pg_get_bfile( p_bfile IN BFILE ) RETURN
            BLOB
              AS
                    filecontent BLOB := NULL;
                    src_file BFILE := NULL;
                    l_step PLS_INTEGER := 12000;
                    l_dir   VARCHAR2(4000);
                    l_fname VARCHAR2(4000);
                    offset NUMBER := 1;
              BEGIN
                IF p_bfile IS NULL THEN
                  RETURN NULL;
                END IF;

                DBMS_LOB.FILEGETNAME( p_bfile, l_dir, l_fname );
                src_file := BFILENAME( l_dir, l_fname );
                IF src_file IS NULL THEN
                    RETURN NULL;
                END IF;

                DBMS_LOB.FILEOPEN(src_file, DBMS_LOB.FILE_READONLY);
                DBMS_LOB.CREATETEMPORARY(filecontent, true);
                DBMS_LOB.LOADBLOBFROMFILE (filecontent, src_file, DBMS_LOB.LOBMAXSIZE, offset, offset);
                DBMS_LOB.FILECLOSE(src_file);
                RETURN filecontent;
            END;

    This function is only created if Ora2Pg finds a table with a BFILE
    column and that the destination type is bytea (the default). The
    function is dropped at the end of the export. This applies to both
    COPY and INSERT export types.

    Regarding ROWID and UROWID, they are converted into OID by &quot;logical&quot;
    default, but this will throw an error during data import. There is
    no equivalent data type so you might want to use the DATA_TYPE
    directive to change the corresponding type in PostgreSQL. You should
    consider replacing this data type with a bigserial (autoincremented
    sequence), text, or uuid data type.

MODIFY_TYPE
    Sometimes you need to force the destination type. For example, a
    column exported as timestamp by Ora2Pg can be forced into type date.
    Value is a comma-separated list of TABLE:COLUMN:TYPE structures. If
    you need to use commas or spaces inside type definitions, you will
    have to escape them with backslashes.

            MODIFY_TYPE     TABLE1:COL3:varchar,TABLE1:COL4:decimal(9\,6)

    The type of table1.col3 will be replaced by varchar and table1.col4
    by decimal with precision and scale.

    If the column&#39;s type is a user-defined type, Ora2Pg will autodetect
    the composite type and will export its data using ROW(). Some Oracle
    user-defined types are just arrays of a native types. In this case,
    you may want to transform this column into a simple array of a
    PostgreSQL native type. To do so, just redefine the destination type
    as wanted, and Ora2Pg will also transform the data as an array. For
    example, with the following definition in Oracle:

            CREATE OR REPLACE TYPE mem_type IS VARRAY(10) of VARCHAR2(15);
            CREATE TABLE club (Name VARCHAR2(10),
                    Address VARCHAR2(20),
                    City VARCHAR2(20),
                    Phone VARCHAR2(8),
                    Members mem_type
            );

    custom type &quot;mem_type&quot; is just a string array and can be translated
    into the following in PostgreSQL:

            CREATE TABLE club (
                    name varchar(10),
                    address varchar(20),
                    city varchar(20),
                    phone varchar(8),
                    members text[]
            ) ;

    To do so, just use the directive as follows:

            MODIFY_TYPE     CLUB:MEMBERS:text[]

    Ora2Pg will take care to transform all data of this column into the
    correct format. Only arrays of characters and numeric types are
    supported.

TO_NUMBER_CONVERSION
    By default, Oracle&#39;s call to function TO_NUMBER will be translated
    as a cast into numeric. For example, TO_NUMBER(&#39;10.1234&#39;) is
    converted into PostgreSQL call to_number(&#39;10.1234&#39;)::numeric. If you
    want, you can cast the call to integer or bigint by changing the
    value of the configuration directive. If you need better control of
    the format, just set it as value, for example: TO_NUMBER_CONVERSION
    99999999999999999999.9999999999 will convert the code above as:
    TO_NUMBER(&#39;10.1234&#39;, &#39;99999999999999999999.9999999999&#39;) Any value of
    the directive that is not numeric, integer or bigint will be taken
    as a mask format. If set to none, no conversion will be done.

VARCHAR_TO_TEXT
    By default varchar2 without size constraint are translated into
    text. If you want to keep the varchar name, disable this directive.

FORCE_IDENTITY_BIGINT
    Usually identity columns must be bigint to correspond to an auto
    increment sequence so Ora2Pg always forces it to be a bigint. If,
    for any reason you want Ora2Pg to respect the DATA_TYPE you have set
    for identity columns then disable this directive.

TO_CHAR_NOTIMEZONE
    If you want Ora2Pg to remove any timezone information from the
    format part of the TO_CHAR() function, enable this directive.
    Disabled by default.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Taking export under control The following other configuration directives interact directly with the export process and give you fine granularity in database export control.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;SKIP
    For TABLE export you may not want to export all schema constraints,
    the SKIP configuration directive allows you to specify a
    space-separated list of constraints that should not be exported.
    Possible values are:

            - fkeys: turn off foreign key constraints
            - pkeys: turn off primary keys
            - ukeys: turn off unique column constraints
            - indexes: turn off all other index types
            - checks: turn off check constraints

    For example:

            SKIP    indexes,checks

    will remove indexes and check constraints from export.

PKEY_IN_CREATE
    Enable this directive if you want to add primary key definition
    inside the create table statement. If disabled (the default) primary
    key definition will be added with an alter table statement. Enable
    it if you are exporting to GreenPlum PostgreSQL database.

KEEP_PKEY_NAMES
    By default names of the primary and unique keys in the source Oracle
    database are ignored and key names are autogenerated in the target
    PostgreSQL database with the PostgreSQL internal default naming
    rules. If you want to preserve Oracle primary and unique key names
    set this option to 1.

FKEY_ADD_UPDATE
    This directive allows you to add an ON UPDATE CASCADE option to a
    foreign key when an ON DELETE CASCADE is defined or always. Oracle
    does not support this feature, you have to use triggers to operate
    the ON UPDATE CASCADE. As PostgreSQL has this feature, you can
    choose how to add the foreign key option. There are three values to
    this directive: never, the default that means that foreign keys will
    be declared exactly like in Oracle. The second value is delete, that
    mean that the ON UPDATE CASCADE option will be added only if the ON
    DELETE CASCADE is already defined on the foreign Keys. The last
    value, always, will force all foreign keys to be defined using the
    update option.

FKEY_DEFERRABLE
    When exporting tables, Ora2Pg normally exports constraints as they
    are, if they are non-deferrable they are exported as non-deferrable.
    However, non-deferrable constraints will probably cause problems
    when attempting to import data to Pg. The FKEY_DEFERRABLE option set
    to 1 will cause all foreign key constraints to be exported as
    deferrable.

DEFER_FKEY
    In addition to exporting data when the DEFER_FKEY option is set to
    1, it will add a command to defer all foreign key constraints during
    data export and the import will be done in a single transaction.
    This will work only if foreign keys have been exported as deferrable
    and you are not using direct import to PostgreSQL (PG_DSN is not
    defined). Constraints will then be checked at the end of the
    transaction.

    This directive can also be enabled if you want to force all foreign
    keys to be created as deferrable and initially deferred during
    schema export (TABLE export type).

DROP_FKEY
    If deferring foreign keys is not possible due to the amount of data
    in a single transaction, you&#39;ve not exported foreign keys as
    deferrable or you are using direct import to PostgreSQL, you can use
    the DROP_FKEY directive.

    It will drop all foreign keys before all data import and recreate
    them at the end of the import.

DROP_INDEXES
    This directive allows you to gain a lot of speed during data import
    by removing all indexes that are not an automatic index (indexes of
    primary keys) and recreate them at the end of data import. Of course
    it is far better to not import indexes and constraints before having
    imported all data.

DISABLE_TRIGGERS
    This directive is used to disable triggers on all tables in COPY or
    INSERT export modes. Available values are USER (disable user-defined
    triggers only) and ALL (includes RI system triggers). Default is 0:
    do not add SQL statements to disable triggers before data import.

    If you want to disable triggers during data migration, set the value
    to USER if you are connected as a non-superuser and ALL if you are
    connected as a PostgreSQL superuser. A value of 1 is equal to USER.

DISABLE_SEQUENCE
    If set to 1, it disables alter of sequences on all tables during
    COPY or INSERT export mode. This is used to prevent the update of
    sequences during data migration. Default is 0, alter sequences.

NOESCAPE
    By default, all data that are not of type date or time are escaped.
    If you experience any problems with that, you can set it to 1 to
    disable character escaping during data export. This directive is
    only used during a COPY export. See STANDARD_CONFORMING_STRINGS for
    enabling/disabling escape with INSERT statements.

STANDARD_CONFORMING_STRINGS
    This controls whether ordinary string literals (&#39;...&#39;) treat
    backslashes literally, as specified in the SQL standard. This was
    the default before Ora2Pg v8.5 so that all strings were escaped
    first; now this is currently on, causing Ora2Pg to use the escape
    string syntax (E&#39;...&#39;) if this parameter is not set to 0. This is
    the exact behavior of the same option in PostgreSQL. This directive
    is only used during data export to build INSERT statements. See
    NOESCAPE for enabling/disabling escape in COPY statements.

TRIM_TYPE
    If you want to convert CHAR(n) from Oracle into varchar(n) or text
    in PostgreSQL using directive DATA_TYPE, you might want to do some
    trimming on the data. By default, Ora2Pg will auto-detect this
    conversion and remove any whitespace at both leading and trailing
    positions. If you just want to remove the leading characters, set
    the value to LEADING. If you just want to remove the trailing
    characters, set the value to TRAILING. Default value is BOTH.

TRIM_CHAR
    The default trimming character is space; use this directive if you
    need to change the character that will be removed. For example, set
    it to - if you have leading - in the char(n) field. To use space as
    trimming character, comment this directive, this is the default
    value.

PRESERVE_CASE
    If you want to preserve the case of Oracle object names, set this
    directive to 1. By default, Ora2Pg will convert all Oracle object
    names to lower case. I do not recommend enabling this unless you
    will always have to double-quote object names in all your SQL
    scripts.

ORA_RESERVED_WORDS
    Allow escaping of column names using Oracle reserved words. Value is
    a list of comma-separated reserved words. Default:
    audit,comment,references.

USE_RESERVED_WORDS
    Enable this directive if you have table or column names that are
    reserved words for PostgreSQL. Ora2Pg will double quote the name of
    the object.

GEN_USER_PWD
    Set this directive to 1 to replace default passwords with random
    passwords for all extracted users during a GRANT export.

PG_SUPPORTS_MVIEW
    Since PostgreSQL 9.3, materialized views are supported with the SQL
    syntax &#39;CREATE MATERIALIZED VIEW&#39;. To force Ora2Pg to use the native
    PostgreSQL support, you must enable this configuration - enabled by
    default. If you want to use the old style with table and a set of
    functions, you should disable it.

PG_SUPPORTS_IFEXISTS
    PostgreSQL versions below 9.x do not support IF EXISTS in DDL
    statements. Disabling the directive with value 0 will prevent Ora2Pg
    to from adding those keywords in all generated statements. Default
    value is 1, enabled.

PG_VERSION
    Set the PostgreSQL major version number of the target database. Ex:
    9.6 or 13. Default is current major version at time of a new
    release. This replaces the old and deprecated PG_SUPPORTS_*
    configuration directives described below.

PG_SUPPORTS_ROLE (Deprecated)
    This option is deprecated since Ora2Pg release v7.3.

    By default Oracle roles are translated into PostgreSQL groups. If
    you have PostgreSQL 8.1 or higher, consider the use of ROLES and set
    this directive to 1 to export roles.

PG_SUPPORTS_INOUT (Deprecated)
    This option is deprecated since Ora2Pg release v7.3.

    If set to 0, all IN, OUT or INOUT parameters will not be used in the
    generated PostgreSQL function declarations (disable it for
    PostgreSQL database versions lower than 8.1). This is now enabled by
    default.

PG_SUPPORTS_DEFAULT
    This directive enables or disables the use of default parameter
    values in function exports. Prior to PostgreSQL 8.4, such default
    values were not supported. This feature is now enabled by default.

PG_SUPPORTS_WHEN (Deprecated)
    Adds support for WHEN clauses on triggers as PostgreSQL v9.0 now
    supports them. This directive is enabled by default; set it to 0 to
    disable this feature.

PG_SUPPORTS_INSTEADOF (Deprecated)
    Adds support for INSTEAD OF usage on triggers (used with PG &amp;gt;= 9.1).
    If this directive is disabled, the INSTEAD OF triggers will be
    rewritten as Pg rules.

PG_SUPPORTS_CHECKOPTION
    When enabled, exports views with CHECK OPTION. Disable it if you
    have a PostgreSQL version prior to 9.4. Default: 1, enabled.

PG_SUPPORTS_IFEXISTS
    If disabled, do not export objects with IF EXISTS statements.
    Enabled by default.

PG_SUPPORTS_PARTITION
    PostgreSQL versions prior to 10.0 do not have native partitioning.
    Enable this directive if you want to use declarative partitioning.
    Enabled by default.

PG_SUPPORTS_SUBSTR
    Some versions of PostgreSQL like Redshift don&#39;t support substr() and
    need to be replaced by a call to substring(). In this case, disable
    it.

PG_SUPPORTS_NAMED_OPERATOR
    Disable this directive if you are using PG &amp;lt; 9.5. PL/SQL operators
    used in named parameters =&amp;gt; will be replaced by PostgreSQL&#39;s
    proprietary operator := Enabled by default.

PG_SUPPORTS_IDENTITY
    Enable this directive if you have PostgreSQL &amp;gt;= 10 to use IDENTITY
    columns instead of serial or bigserial data types. If
    PG_SUPPORTS_IDENTITY is disabled and there is an IDENTITY column in
    the Oracle table, they are exported as serial or bigserial columns.
    When it is enabled they are exported as IDENTITY columns like:

          CREATE TABLE identity_test_tab (
                  id bigint GENERATED ALWAYS AS IDENTITY,
                  description varchar(30)
          ) ;

    If there are non-default sequence options set in Oracle, they will
    be appended after the IDENTITY keyword. Additionally in both cases,
    Ora2Pg will create a file AUTOINCREMENT_output.sql with an embedded
    function to update the associated sequences with the restart value
    set to &quot;SELECT max(colname)+1 FROM tablename&quot;. Of course this file
    must be imported after data import otherwise sequence will be kept
    at start value. Enabled by default.

PG_SUPPORTS_PROCEDURE
    PostgreSQL v11 adds support for PROCEDURE, enable it if you use such
    version.

BITMAP_AS_GIN
    Use btree_gin extension to create bitmap-like index with pg &amp;gt;= 9.4.
    You will need to create the extension yourself: create extension
    btree_gin; Default is to create GIN index, when disabled, a btree
    index will be created.

PG_BACKGROUND
    Use pg_background extension to create an autonomous transaction
    instead of using a dblink wrapper. With pg &amp;gt;= 9.5 only. Default is
    to use dblink. See https://github.com/vibhorkum/pg_background about
    this extension.

DBLINK_CONN
    By default if you have an autonomous transaction translated using
    dblink extension instead of pg_background, the connection is defined
    using the values set with PG_DSN, PG_USER and PG_PWD. If you want to
    fully override the connection string, use this directive to set the
    connection in the autonomous transaction wrapper function. For
    example:

            DBLINK_CONN    port=5432 dbname=pgdb host=localhost user=pguser password=pgpass

LONGREADLEN
    Use this directive to set the database handle&#39;s &#39;LongReadLen&#39;
    attribute to a value that will be larger than the expected size of
    the LOBs. The default is 1MB which may not be enough to extract
    BLOBs or CLOBs. If the size of the LOB exceeds the &#39;LongReadLen&#39;
    DBD::Oracle will return an &#39;ORA-24345: A Truncation&#39; error. Default:
    1023*1024 bytes.

    Take a look at this page to learn more:
    http://search.cpan.org/~pythian/DBD-Oracle-1.22/Oracle.pm#Data_Inter
    face_for_Persistent_LOBs

    Important note: If you increase the value of this directive take
    care that DATA_LIMIT will probably need to be reduced. Even if you
    only have a 1MB blob, trying to read 10000 of them (the default
    DATA_LIMIT) all at once will require 10GB of memory. You may extract
    data from those tables separately and set a DATA_LIMIT to 500 or
    lower, otherwise you may experience some out of memory issues.

LONGTRUNKOK
    If you want to bypass the &#39;ORA-24345: A Truncation&#39; error, set this
    directive to 1. It will truncate the data extracted to the
    LongReadLen value. Disabled by default so that you will be warned if
    your LongReadLen value is not high enough.

USE_LOB_LOCATOR
    Disable this if you want to load the full content of BLOB and CLOB
    and not use LOB locators. In this case, you will have to set
    LONGREADLEN to the right value. Note that this will not improve the
    speed of BLOB export as most of the time is always consumed by the
    bytea escaping and in this case, export is done line by line and not
    by chunk of DATA_LIMIT rows. For more information on how it works,
    see
    http://search.cpan.org/~pythian/DBD-Oracle-1.74/lib/DBD/Oracle.pm#Da
    ta_Interface_for_LOB_Locators

    Default is enabled; it uses LOB locators.

LOB_CHUNK_SIZE
    Oracle recommends reading from and writing to a LOB in batches using
    a multiple of the LOB chunk size. This chunk size defaults to 8k
    (8192). Recent tests have shown that the best performance can be
    reached with higher values like 512K or 4Mb.

    A quick benchmark with 30120 rows with different size of BLOB
    (200x5Mb, 19800x212k, 10000x942K, 100x17Mb, 20x156Mb), with
    DATA_LIMIT=100, LONGREADLEN=170Mb and a total table size of 20GB
    gives:

           no lob locator  : 22m46,218s (1365 sec., avg: 22 recs/sec)
           chunk size 8k   : 15m50,886s (951 sec., avg: 31 recs/sec)
           chunk size 512k : 1m28,161s (88 sec., avg: 342 recs/sec)
           chunk size 4Mb  : 1m23,717s (83 sec., avg: 362 recs/sec)

    In conclusion, it can be more than 10 times faster with
    LOB_CHUNK_SIZE set to 4Mb. Depending on the size of most BLOBs, you
    may want to adjust the value here. For example, if you have a
    majority of small lobs below 8K, using 8192 is better to not waste
    space. Default value for LOB_CHUNK_SIZE is 512000.

XML_PRETTY
    Forces the use of getStringVal() instead of getClobVal() for XML
    data export. Default is 1, enabled for backward compatibility. Set
    it to 0 to use extract method like CLOB. Note that XML values
    extracted with getStringVal() must not exceed VARCHAR2 size limit
    (4000); otherwise, it will return an error.

ENABLE_MICROSECOND
    Set it to 0 if you want to disable export of milliseconds from
    Oracle timestamp columns. By default, milliseconds are exported by
    using the following format:

            &#39;YYYY-MM-DD HH24:MI:SS.FF&#39;

    Disabling will force the use of the following Oracle format:

            to_char(..., &#39;YYYY-MM-DD HH24:MI:SS&#39;)

    By default, milliseconds are exported.

DISABLE_COMMENT
    Set this to 1 if you don&#39;t want to export comments associated with
    tables and columns definition. Default is enabled.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Control MySQL export behavior MYSQL_PIPES_AS_CONCAT Enable this if double pipe and double ampersand (|| and &amp;amp;&amp;amp;) should not be taken as equivalent to OR and AND. It depends on the variable @sql_mode. Use it only if Ora2Pg fails on auto-detecting this behavior.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;MYSQL_INTERNAL_EXTRACT_FORMAT
    Enable this directive if you want EXTRACT() replacement to use the
    internal format returned as an integer, for example DD HH24:MM:SS
    will be replaced with format; DDHH24MMSS::bigint, this depends on
    your apps usage.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Control SQL Server export behavior DROP_ROWVERSION PostgreSQL has no equivalent to rowversion datatype and feature. If you want to remove these useless columns, enable this directive. Columns of datatype &#39;rowversion&#39; or &#39;timestamp&#39; will not be exported.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;CASE_INSENSITIVE_SEARCH
    Emulate the same behavior of MSSQL with case-insensitive search. If
    the value is citext, it will use the citext data type instead of
    char/varchar/text in tables DDL (Ora2Pg will add a CHECK constraint
    for columns with a precision). Instead of citext, you can also set a
    collation name that will be used in the column definitions. To
    disable case-insensitive search set it to: none.

SELECT_TOP
    Appends a TOP N clause to the SELECT command used to extract the
    data from SQL Server. This is equivalent to a WHERE ROWNUM &amp;lt; 1000
    clause for Oracle.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Special options to handle character encoding NLS_LANG and NLS_NCHAR By default, Ora2Pg will set NLS_LANG to AMERICAN_AMERICA.AL32UTF8 and NLS_NCHAR to AL32UTF8. It is not recommended to change these settings, but in some cases it could be useful. Using your own settings with these configuration directives will change the client encoding on the Oracle side by setting the environment variables $ENV{NLS_LANG} and $ENV{NLS_NCHAR}.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;BINMODE
    By default, Ora2Pg will force Perl to use UTF8 encoding. This is
    done through a call to the Perl pragma:

            use open &#39;:utf8&#39;;

    You can override this encoding by using the BINMODE directive. For
    example, you can set it to :locale to use your locale or iso-8859-7.
    It will respectively use:

            use open &#39;:locale&#39;;
            use open &#39;:encoding(iso-8859-7)&#39;;

    If you have changed the NLS_LANG to non-UTF8 encoding, you might
    want to set this directive. See
    http://perldoc.perl.org/5.14.2/open.html for more information. Most
    of the time, leave this directive commented.

CLIENT_ENCODING
    By default, PostgreSQL client encoding is automatically set to UTF8
    to avoid encoding issues. If you have changed the value of NLS_LANG,
    you might have to change the encoding of the PostgreSQL client.

    You can take a look at the PostgreSQL supported character sets here:
    http://www.postgresql.org/docs/9.0/static/multibyte.html

FORCE_PLSQL_ENCODING
    Enable this directive to force UTF8 encoding of the PL/SQL code
    exported. Could be helpful in some rare conditions.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;PLSQL to PLPGSQL conversion Automatic code conversion from Oracle PL/SQL to PostgreSQL PL/PGSQL is a work in progress in Ora2Pg and you will likely have manual work. The Perl code used for automatic conversion is stored in a specific Perl Module named Ora2Pg/PLSQL.pm. Feel free to modify/add your own code and send me patches. The main work is on function, procedure, package and package body headers and parameter rewrites.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PLSQL_PGSQL
    Enable/disable PLSQL to PLPGSQL conversion. Enabled by default.

NULL_EQUAL_EMPTY
    Ora2Pg can replace all conditions with a test on NULL by calling the
    coalesce() function to mimic the Oracle behavior where empty strings
    are considered equal to NULL.

            (field1 IS NULL) is replaced by (coalesce(field1::text, &#39;&#39;) = &#39;&#39;)
            (field2 IS NOT NULL) is replaced by (field2 IS NOT NULL AND field2::text &amp;lt;&amp;gt; &#39;&#39;)

    You might want this replacement to ensure your application will have
    the same behavior, but if you have control over your application, a
    better way is to transform empty strings into NULL because
    PostgreSQL differentiates between them.

EMPTY_LOB_NULL
    Force empty_clob() and empty_blob() to be exported as NULL instead
    of an empty string for the first one and &#39;\x&#39; for the second. If
    NULL is allowed in your column, this might improve data export speed
    if you have lots of empty lobs. Default is to preserve the exact
    data from Oracle.

PACKAGE_AS_SCHEMA
    If you don&#39;t want to export packages as schemas but as simple
    functions, you might also want to replace all calls to
    package_name.function_name. If you disable the PACKAGE_AS_SCHEMA
    directive then Ora2Pg will replace all calls to
    package_name.function_name() with package_name_function_name().
    Default is to use a schema to emulate packages.

    The replacement will be done in all kinds of DDL or code that is
    parsed by the PLSQL to PLPGSQL converter. PLSQL_PGSQL must be
    enabled or -p used in command line.

REWRITE_OUTER_JOIN
    Enable this directive if the rewrite of Oracle native syntax (+) of
    OUTER JOIN is broken. This will force Ora2Pg to not rewrite such
    code. Default is to try to rewrite simple forms of right outer joins
    for now.

UUID_FUNCTION
    By default, Ora2Pg will convert calls to SYS_GUID() Oracle function
    with a call to uuid_generate_v4 from the uuid-ossp extension. You
    can redefine it to use the gen_random_uuid function from the
    pgcrypto extension by changing the function name. Default is
    uuid_generate_v4.

    Note that when RAW(16) or RAW(32) columns are found or when the RAW
    column has &quot;SYS_GUID()&quot; as default value, Ora2Pg will automatically
    translate the type of the column into uuid which might be the right
    translation in most cases. In this case data will be automatically
    migrated as PostgreSQL uuid data type provided by the &quot;uuid-ossp&quot;
    extension.

FUNCTION_STABLE
    By default, Oracle functions are marked as STABLE as they can not
    modify data unless when used in PL/SQL with variable assignment or
    as conditional expressions. You can force Ora2Pg to create these
    functions as VOLATILE by disabling this configuration directive.

COMMENT_COMMIT_ROLLBACK
    By default, calls to COMMIT/ROLLBACK are kept untouched by Ora2Pg to
    force the user to review the logic of the function. Once it is fixed
    in Oracle source code or you want to comment these calls, enable the
    following directive.

COMMENT_SAVEPOINT
    It is common to see SAVEPOINT calls inside PL/SQL procedures
    together with a ROLLBACK TO savepoint_name. When
    COMMENT_COMMIT_ROLLBACK is enabled, you may want to also comment
    SAVEPOINT calls; in this case enable it.

STRING_CONSTANT_REGEXP
    Ora2Pg replaces all string constants during the PL/SQL to PL/PGSQL
    translation. String constants are all text included between single
    quotes. If you have some string placeholders used in dynamic calls
    to queries, you can set a list of regexps to be temporarily replaced
    to not break the parser. For example:

            STRING_CONSTANT_REGEXP         &amp;lt;placeholder value=&quot;.*&quot;&amp;gt;

    The list of regexps must use the semi colon as separator.

ALTERNATIVE_QUOTING_REGEXP
    To support the Alternative Quoting Mechanism (&#39;Q&#39; or &#39;q&#39;) for String
    Literals, set the regexp with the text capture to use to extract the
    text part. For example, with a variable declared as:

            c_sample VARCHAR2(100 CHAR) := q&#39;{This doesn&#39;t work.}&#39;;

    the regexp to use must be:

            ALTERNATIVE_QUOTING_REGEXP     q&#39;{(.*)}&#39;

    Ora2pg will use the $$ delimiter; for the example the result will
    be:

            c_sample varchar(100) := $$This doesn&#39;t work.$$;

    The value of this configuration directive can be a list of regexps
    separated by a semicolon. The capture part (between parentheses) is
    mandatory in each regexp if you want to restore the string constant.

USE_ORAFCE
    If you want to use functions defined in the Orafce library and
    prevent Ora2Pg from translating calls to these functions, enable
    this directive. The Orafce library can be found here:
    https://github.com/orafce/orafce

    By default, Ora2pg rewrites add_month(), add_year(), date_trunc()
    and to_char() functions, but you may prefer to use the orafce
    version of these functions that do not need any code transformation.

AUTONOMOUS_TRANSACTION
    Enable translation of autonomous transactions into wrapper functions
    using dblink or pg_background extension. If you don&#39;t want to use
    this translation and just want the function to be exported as a
    normal one without the pragma call, disable this directive.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Materialized View Materialized views are exported as snapshot &quot;Snapshot Materialized Views&quot; as PostgreSQL only supports full refresh.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;If you want to import materialized views in PostgreSQL prior to 9.3, you
have to set configuration directive PG_SUPPORTS_MVIEW to 0. In this case
Ora2Pg will export all materialized views as explained in this document:

        http://tech.jonathangardner.net/wiki/PostgreSQL/Materialized_Views.

When exporting materialized views, Ora2Pg will first add the SQL code to
create the &quot;materialized_views&quot; table:

        CREATE TABLE materialized_views (
                mview_name text NOT NULL PRIMARY KEY,
                view_name text NOT NULL,
                iname text,
                last_refresh TIMESTAMP WITH TIME ZONE
        );

All materialized views will have an entry in this table. It then adds
the plpgsql code to create three functions:

        create_materialized_view(text, text, text) used to create a materialized view
        drop_materialized_view(text) used to delete a materialized view
        refresh_full_materialized_view(text) used to refresh a view

Then it adds the SQL code to create the view and the materialized view:

        CREATE VIEW mviewname_mview AS
        SELECT ... FROM ...;

        SELECT create_materialized_view(&#39;mviewname&#39;,&#39;mviewname_mview&#39;, change with the name of the column to be used for the index);

The first argument is the name of the materialized view, the second is
the name of the view on which the materialized view is based, and the
third is the column name on which the index should be built (typically
the primary key). This column is not automatically deduced so you need
to replace its name.

As mentioned above, Ora2Pg only supports snapshot materialized views so
the table will be entirely refreshed by first truncating the table and
then loading all data again from the view:

         refresh_full_materialized_view(&#39;mviewname&#39;);

To drop the materialized view, you just have to call the
drop_materialized_view() function with the name of the materialized view
as a parameter.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Other configuration directives DEBUG Set it to 1 to enable verbose output.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;IMPORT
    You can define common Ora2Pg configuration directives in a single
    file that can be imported into other configuration files with the
    IMPORT configuration directive as follows:

            IMPORT  commonfile.conf

    This will import all configuration directives defined in
    commonfile.conf into the current configuration file.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Exporting views as PostgreSQL tables You can export any Oracle view as a PostgreSQL table simply by setting the TYPE configuration option to TABLE to get the corresponding create table statement. Or use type COPY or INSERT to export the corresponding data. To allow this, you have to specify your views in the VIEW_AS_TABLE configuration option.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Then if Ora2Pg finds the view, it will extract its schema (if
TYPE=TABLE) into a PG create table form, then it will extract the data
(if TYPE=COPY or INSERT) following the view schema.

For example, with the following view:

        CREATE OR REPLACE VIEW product_prices (category_id, product_count, low_price, high_price) AS
        SELECT  category_id, COUNT(*) as product_count,
            MIN(list_price) as low_price,
            MAX(list_price) as high_price
         FROM   product_information
        GROUP BY category_id;

Setting VIEW_AS_TABLE to product_prices and using export type TABLE,
will force Ora2Pg to detect columns&#39; returned types and to generate a
create table statement:

        CREATE TABLE product_prices (
                category_id bigint,
                product_count integer,
                low_price numeric,
                high_price numeric
        );

Data will be loaded following the COPY or INSERT export type and the
view declaration.

You can use the ALLOW and EXCLUDE directives in addition to filter other
objects to export.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Export as Kettle transformation XML files The KETTLE export type is useful if you want to use Pentaho Data Integrator (Kettle) to import data to PostgreSQL. With this type of export, Ora2Pg will generate one XML Kettle transformation file (.ktr) per table and add a line to manually execute the transformation in the output.sql file. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg -c ora2pg.conf -t KETTLE -j 12 -a MYTABLE -o load_mydata.sh

will generate one file called &#39;HR.MYTABLE.ktr&#39; and add a line to the
output file (load_mydata.sh):

        #!/bin/sh

        KETTLE_TEMPLATE_PATH=&#39;.&#39;

        JAVAMAXMEM=4096 ./pan.sh -file $KETTLE_TEMPLATE_PATH/HR.MYTABLE.ktr -level Detailed

The -j 12 option will create a template with 12 processes to insert data
into PostgreSQL. It is also possible to specify the number of parallel
queries used to extract data from Oracle with the -J command line option
as follows:

        ora2pg -c ora2pg.conf -t KETTLE -J 4 -j 12 -a EMPLOYEES -o load_mydata.sh

This is only possible if there is a unique key defined on a numeric
column or if you have defined the technical key to be used to split the
query between cores in the DEFINED_PKEY configuration directive. For
example:

        DEFINED_PK      EMPLOYEES:employee_id

This will force the number of Oracle connection copies to 4 and define
the SQL query as follows in the Kettle XML transformation file:

        &amp;lt;sql&amp;gt;SELECT * FROM HR.EMPLOYEES WHERE ABS(MOD(employee_id,${Internal.Step.Unique.Count}))=${Internal.Step.Unique.Number}&amp;lt;/sql&amp;gt;

The KETTLE export type requires that the Oracle and PostgreSQL DSN are
defined. You can also activate the TRUNCATE_TABLE directive to force a
truncation of the table before data import.

The KETTLE export type is an original work by Marc Cousin.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Migration Cost Assessment Estimating the cost of migrating from Oracle to PostgreSQL is not easy. To obtain a good assessment of this migration cost, Ora2Pg will inspect all database objects, all functions and stored procedures to detect if there are any objects and PL/SQL code that can not be automatically converted by Ora2Pg.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Ora2Pg has a content analysis mode that inspects the Oracle database to
generate a text report on what the Oracle database contains and what
cannot be exported.

To activate the &quot;analysis and report&quot; mode, you have to use the export
type SHOW_REPORT with in the following command:

        ora2pg -t SHOW_REPORT

Here is a sample report obtained with this command:

        --------------------------------------
        Ora2Pg: Oracle Database Content Report
        --------------------------------------
        Version Oracle Database 10g Enterprise Edition Release 10.2.0.1.0
        Schema  HR
        Size  880.00 MB
     
        --------------------------------------
        Object  Number  Invalid Comments
        --------------------------------------
        CLUSTER   2 0 Clusters are not supported and will not be exported.
        FUNCTION  40  0 Total size of function code: 81992.
        INDEX     435 0 232 index(es) are concerned by the export, others are automatically generated and will
                                        do so on PostgreSQL. 1 bitmap index(es). 230 b-tree index(es). 1 reversed b-tree index(es)
                                        Note that bitmap index(es) will be exported as b-tree index(es) if any. Cluster, domain,
                                        bitmap join and IOT indexes will not be exported at all. Reverse indexes are not exported
                                        too, you may use a trigram-based index (see pg_trgm) or a reverse() function based index
                                        and search. You may also use &#39;varchar_pattern_ops&#39;, &#39;text_pattern_ops&#39; or &#39;bpchar_pattern_ops&#39;
                                        operators in your indexes to improve search with the LIKE operator respectively into
                                        varchar, text or char columns.
        MATERIALIZED VIEW 1 0 All materialized view will be exported as snapshot materialized views, they
                                        are only updated when fully refreshed.
        PACKAGE BODY  2 1 Total size of package code: 20700.
        PROCEDURE 7 0 Total size of procedure code: 19198.
        SEQUENCE  160 0 Sequences are fully supported, but all call to sequence_name.NEXTVAL or sequence_name.CURRVAL
                                        will be transformed into NEXTVAL(&#39;sequence_name&#39;) or CURRVAL(&#39;sequence_name&#39;).
        TABLE     265 0 1 external table(s) will be exported as standard table. See EXTERNAL_TO_FDW configuration
                                        directive to export as file_fdw foreign tables or use COPY in your code if you just
                                        want to load data from external files. 2 binary columns. 4 unknown types.
        TABLE PARTITION 8 0 Partitions are exported using table inheritance and check constraint. 1 HASH partitions.
                                        2 LIST partitions. 6 RANGE partitions. Note that Hash partitions are not supported.
        TRIGGER   30  0 Total size of trigger code: 21677.
        TYPE      7 1 5 type(s) are concerned by the export, others are not supported. 2 Nested Tables.
                                        2 Object type. 1 Subtype. 1 Type Boby. 1 Type inherited. 1 Varrays. Note that Type
                                        inherited and Subtype are converted as table, type inheritance is not supported.
        TYPE BODY 0 3 Export of type with member method are not supported, they will not be exported.
        VIEW      7 0 Views are fully supported, but if you have updatable views you will need to use
                                        INSTEAD OF triggers.
        DATABASE LINK 1 0 Database links will not be exported. You may try the dblink perl contrib module or use
                                        the SQL/MED PostgreSQL features with the different Foreign Data Wrapper (FDW) extensions.
                                    
        Note: Invalid code will not be exported unless the EXPORT_INVALID configuration directive is activated.

Once the database has been analysed, Ora2Pg, through its ability to
convert SQL and PL/SQL code from Oracle syntax to PostgreSQL, can go
further by estimating the code complexity and time necessary to perform
a full database migration.

To estimate the migration cost in person-days, Ora2Pg allows you to use
a configuration directive called ESTIMATE_COST that you can also enable
at command line:

        --estimate_cost

This feature can only be used with the SHOW_REPORT, FUNCTION, PROCEDURE,
PACKAGE and QUERY export types.

        ora2pg -t SHOW_REPORT  --estimate_cost

The generated report is the same as above but with a new &#39;Estimated
cost&#39; column as follows:

        --------------------------------------
        Ora2Pg: Oracle Database Content Report
        --------------------------------------
        Version Oracle Database 10g Express Edition Release 10.2.0.1.0
        Schema  HR
        Size  890.00 MB
     
        --------------------------------------
        Object  Number  Invalid Estimated cost  Comments
        --------------------------------------
        DATABASE LINK  3 0 9 Database links will be exported as SQL/MED PostgreSQL&#39;s Foreign Data Wrapper (FDW) extensions
                                        using oracle_fdw.
        FUNCTION  2 0 7 Total size of function code: 369 bytes. HIGH_SALARY: 2, VALIDATE_SSN: 3.
        INDEX 21  0 11  11 index(es) are concerned by the export, others are automatically generated and will do so
                                        on PostgreSQL. 11 b-tree index(es). Note that bitmap index(es) will be exported as b-tree
                                        index(es) if any. Cluster, domain, bitmap join and IOT indexes will not be exported at all.
                                        Reverse indexes are not exported too, you may use a trigram-based index (see pg_trgm) or a
                                        reverse() function based index and search. You may also use &#39;varchar_pattern_ops&#39;, &#39;text_pattern_ops&#39;
                                        or &#39;bpchar_pattern_ops&#39; operators in your indexes to improve search with the LIKE operator
                                        respectively into varchar, text or char columns.
        JOB 0 0 0 Job are not exported. You may set external cron job with them.
        MATERIALIZED VIEW 1 0 3 All materialized view will be exported as snapshot materialized views, they
                                                are only updated when fully refreshed.
        PACKAGE BODY  0 2 54  Total size of package code: 2487 bytes. Number of procedures and functions found
                                                inside those packages: 7. two_proc.get_table: 10, emp_mgmt.create_dept: 4,
                                                emp_mgmt.hire: 13, emp_mgmt.increase_comm: 4, emp_mgmt.increase_sal: 4,
                                                emp_mgmt.remove_dept: 3, emp_mgmt.remove_emp: 2.
        PROCEDURE 4 0 39  Total size of procedure code: 2436 bytes. TEST_COMMENTAIRE: 2, SECURE_DML: 3,
                                                PHD_GET_TABLE: 24, ADD_JOB_HISTORY: 6.
        SEQUENCE  3 0 0 Sequences are fully supported, but all call to sequence_name.NEXTVAL or sequence_name.CURRVAL
                                                will be transformed into NEXTVAL(&#39;sequence_name&#39;) or CURRVAL(&#39;sequence_name&#39;).
        SYNONYM   3 0 4 SYNONYMs will be exported as views. SYNONYMs do not exists with PostgreSQL but a common workaround
                                                is to use views or set the PostgreSQL search_path in your session to access
                                                object outside the current schema.
                                                user1.emp_details_view_v is an alias to hr.emp_details_view.
                                                user1.emp_table is an alias to hr.employees@other_server.
                                                user1.offices is an alias to hr.locations.
        TABLE 17  0 8.5 1 external table(s) will be exported as standard table. See EXTERNAL_TO_FDW configuration
                                        directive to export as file_fdw foreign tables or use COPY in your code if you just want to
                                        load data from external files. 2 binary columns. 4 unknown types.
        TRIGGER 1 1 4 Total size of trigger code: 123 bytes. UPDATE_JOB_HISTORY: 2.
        TYPE  7 1 5 5 type(s) are concerned by the export, others are not supported. 2 Nested Tables. 2 Object type.
                                        1 Subtype. 1 Type Boby. 1 Type inherited. 1 Varrays. Note that Type inherited and Subtype are
                                        converted as table, type inheritance is not supported.
        TYPE BODY 0 3 30  Export of type with member method are not supported, they will not be exported.
        VIEW  1 1 1 Views are fully supported, but if you have updatable views you will need to use INSTEAD OF triggers.
        --------------------------------------
        Total 65  8 162.5 162.5 cost migration units means approximatively 2 man day(s).

The last line shows the total estimated migration cost in person-days
following the number of migration units estimated for each object. Each
migration unit represents approximately five minutes for a PostgreSQL
expert. If this is your first migration, you can increase it with the
configuration directive COST_UNIT_VALUE or the --cost_unit_value command
line option:

        ora2pg -t SHOW_REPORT  --estimate_cost --cost_unit_value 10

Ora2Pg is also able to give you a migration difficulty level assessment.
Here&#39;s a sample:

Migration level: B-5

    Migration levels:
        A - Migration that might be run automatically
        B - Migration with code rewrite and a person-days cost up to 5 days
        C - Migration with code rewrite and a person-days cost above 5 days
    Technical levels:
        1 = trivial: no stored functions and no triggers
        2 = easy: no stored functions but with triggers, no manual rewriting
        3 = simple: stored functions and/or triggers, no manual rewriting
        4 = manual: no stored functions but with triggers or views with code rewriting
        5 = difficult: stored functions and/or triggers with code rewriting

This assessment consists of a letter A or B to specify whether the
migration needs manual rewriting or not, and a number from 1 up to 5 to
indicate a technical difficulty level. You have an additional option
--human_days_limit to specify the number of person-days limit where the
migration level should be set to C to indicate that it needs a huge
amount of work and full project management with migration support.
Default is 10 person-days. You can use the configuration directive
HUMAN_DAYS_LIMIT to change this default value permanently.

This feature has been developed to help you or your boss to decide which
database to migrate first and the team that must be mobilized to conduct
the migration.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Global Oracle and MySQL migration assessment Ora2Pg comes with a script ora2pg_scanner that can be used when you have a huge number of instances and schemas to scan for migration assessment.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Usage: ora2pg_scanner -l CSVFILE [-o OUTDIR]

   -b | --binpath DIR: full path to directory where the ora2pg binary resides.
                Might be useful only on Windows OS.
   -c | --config FILE: set custom configuration file to use, otherwise ora2pg
                will use the default: /etc/ora2pg/ora2pg.conf.
   -l | --list FILE : CSV file containing a list of databases to scan with
                all required information. The first line of the file
                can contain the following header that describes the
                format that must be used:

                &quot;type&quot;,&quot;schema/database&quot;,&quot;dsn&quot;,&quot;user&quot;,&quot;password&quot;

   -o | --outdir DIR : (optional) by default all reports will be dumped to a
                directory named &#39;output&#39;, it will be created automatically.
                If you want to change the name of this directory, set the name
                at second argument.

   -t | --test : just try all connections by retrieving the required schema
                 or database name. Useful to validate your CSV list file.
   -u | --unit MIN : redefine globally the migration cost unit value in minutes.
                 Default is taken from the ora2pg.conf (default 5 minutes).

   Here is a full example of a CSV databases list file:

        &quot;type&quot;,&quot;schema/database&quot;,&quot;dsn&quot;,&quot;user&quot;,&quot;password&quot;
        &quot;MYSQL&quot;,&quot;sakila&quot;,&quot;dbi:mysql:host=192.168.1.10;database=sakila;port=3306&quot;,&quot;root&quot;,&quot;secret&quot;
        &quot;ORACLE&quot;,&quot;HR&quot;,&quot;dbi:Oracle:host=192.168.1.10;sid=XE;port=1521&quot;,&quot;system&quot;,&quot;manager&quot;
        &quot;MSSQL&quot;,&quot;HR&quot;,&quot;dbi:ODBC:driver=msodbcsql18;server=srv.database.windows.net;database=testdb&quot;,&quot;system&quot;,&quot;manager&quot;

   The CSV field separator must be a comma.

   Note that if you want to scan all schemas from an Oracle instance, you just
   have to leave the schema field empty. Ora2Pg will automatically detect all
   available schemas and generate a report for each one. Of course, you need to
   use a connection user with enough privileges to be able to scan all schemas.
   For example:

        &quot;ORACLE&quot;,&quot;&quot;,&quot;dbi:Oracle:host=192.168.1.10;sid=XE;port=1521&quot;,&quot;system&quot;,&quot;manager&quot;
        &quot;MSSQL&quot;,&quot;&quot;,&quot;dbi:ODBC:driver=msodbcsql18;server=srv.database.windows.net;database=testdb&quot;,&quot;usrname&quot;,&quot;passwd&quot;

   will generate a report for all schemas in the XE instance. Note that in this
   case the SCHEMA directive in ora2pg.conf must not be set.

It will generate a CSV file with the assessment result, one line per
schema or database and a detailed HTML report for each database scanned.

Hint: Use the -t | --test option beforehand to test all your connections
in your CSV file.

For Windows users, you must use the -b command line option to set the
directory where ora2pg_scanner resides, otherwise the ora2pg command
calls will fail.

In the migration assessment details about functions, Ora2Pg always
includes by default 2 migration units for TEST and 1 unit for SIZE per
1000 characters in the code. This means that by default it will add 15
minutes to the migration assessment per function. Obviously, if you have
unit tests or very simple functions this will not represent the actual
migration time.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Migration assessment method Migration unit scores given to each type of Oracle database object are defined in the Perl library lib/Ora2Pg/PLSQL.pm in the %OBJECT_SCORE variable definition.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;The number of PL/SQL lines associated with a migration unit is also
defined in this file in the $SIZE_SCORE variable value.

The number of migration units associated with each PL/SQL code
difficulty can be found in the same Perl library lib/Ora2Pg/PLSQL.pm in
the hash %UNCOVERED_SCORE initialization.

This assessment method is a work in progress, so I&#39;m expecting feedback
on migration experiences to refine the scores/units attributed in these
variables.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Improving indexes and constraints creation speed Using the LOAD export type and a file containing SQL orders to perform, it is possible to dispatch those orders over multiple PostgreSQL connections. To be able to use this feature, the PG_DSN, PG_USER and PG_PWD must be set. Then:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        ora2pg -t LOAD -c config/ora2pg.conf -i schema/tables/INDEXES_table.sql -j 4

will dispatch index creation over 4 simultaneous PostgreSQL connections.

This will considerably accelerate this part of the migration process
with huge data sizes.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Exporting LONG RAW If you still have columns defined as LONG RAW, Ora2Pg will not be able to export these kinds of data. The OCI library fails to export them and always returns the same first record. To be able to export the data you need to transform the field as BLOB by creating a temporary table before migrating data. For example, the Oracle table:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        SQL&amp;gt; DESC TEST_LONGRAW
         Name                 NULL ?   Type
         -------------------- -------- ----------------------------
         ID                            NUMBER
         C1                            LONG RAW

needs to be &quot;translated&quot; into a table using BLOB as follows:

        CREATE TABLE test_blob (id NUMBER, c1 BLOB);

And then copy the data with the following INSERT query:

        INSERT INTO test_blob SELECT id, to_lob(c1) FROM test_longraw;

Then you just have to exclude the original table from the export (see
EXCLUDE directive) and to rename the new temporary table on the fly
using the REPLACE_TABLES configuration directive.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Global variables Oracle allows the use of global variables defined in packages. Ora2Pg will export these variables for PostgreSQL as user-defined custom variables available in a session. Oracle variable assignments are exported as calls to:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    PERFORM set_config(&#39;pkgname.varname&#39;, value, false);

Use of these variables in the code is replaced by:

    current_setting(&#39;pkgname.varname&#39;)::global_variables_type;

where global_variables_type is the type of the variable extracted from
the package definition.

If the variable is a constant or has a default value assigned at
declaration, Ora2Pg will create a file global_variables.conf with the
definition to include in the postgresql.conf file so that their values
will already be set at database connection. Note that the value can
always be modified by the user so you can not have exactly a constant.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Hints Converting your queries with Oracle style outer join (+) syntax to ANSI standard SQL at the Oracle side can save you a lot of time for the migration. You can use TOAD Query Builder to re-write these using the proper ANSI syntax, see: &lt;a href=&quot;http://www.toadworld.com/products/toad-for-oracle/f/10/t/9518.aspx&quot;&gt;http://www.toadworld.com/products/toad-for-oracle/f/10/t/9518.aspx&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;There&#39;s also an alternative with SQL Developer Data Modeler, see
http://www.thatjeffsmith.com/archive/2012/01/sql-developer-data-modeler-
quick-tip-use-oracle-join-syntax-or-ansi/

Toad is also able to rewrite the native Oracle DECODE() syntax into ANSI
standard SQL CASE statement. You can find some slides about this in a
presentation given at PgConf.RU:
http://ora2pg.darold.net/slides/ora2pg_the_hard_way.pdf
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Test the migration The type of action called s you to check that all objects from Oracle database have been created under PostgreSQL. Of course PG_DSN must be set to be able to check PostgreSQL side.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Note that this feature respects the schema name limitation if
EXPORT_SCHEMA and SCHEMA or PG_SCHEMA are defined. If only EXPORT_SCHEMA
is set all schemas from Oracle and PostgreSQL are scanned. You can
filter to a single schema using SCHEMA and/or PG_SCHEMA but you can not
filter on a list of schemas. To test a list of schemas you will have to
repeat the calls to Ora2Pg by specifying a single schema each time.

For example command:

        ora2pg -t TEST -c config/ora2pg.conf &amp;gt; migration_diff.txt

Will create a file containing the report of all objects and row count on
both sides, Oracle and PostgreSQL, with an error section giving you the
details of the differences for each kind of object. Here is a sample
result:

        [TEST INDEXES COUNT]
        ORACLEDB:DEPARTMENTS:2
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:6
        POSTGRES:employees:6
        [ERRORS INDEXES COUNT]
        Table departments doesn&#39;t have the same number of indexes in Oracle (2) and in PostgreSQL (1).

        [TEST UNIQUE CONSTRAINTS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS UNIQUE CONSTRAINTS COUNT]
        OK, Oracle and PostgreSQL have the same number of unique constraints.

        [TEST PRIMARY KEYS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS PRIMARY KEYS COUNT]
        OK, Oracle and PostgreSQL have the same number of primary keys.

        [TEST CHECK CONSTRAINTS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS CHECK CONSTRAINTS COUNT]
        OK, Oracle and PostgreSQL have the same number of check constraints.

        [TEST NOT NULL CONSTRAINTS COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS NOT NULL CONSTRAINTS COUNT]
        OK, Oracle and PostgreSQL have the same number of not null constraints.

        [TEST COLUMN DEFAULT VALUE COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS COLUMN DEFAULT VALUE COUNT]
        OK, Oracle and PostgreSQL have the same number of column default value.

        [TEST IDENTITY COLUMN COUNT]
        ORACLEDB:DEPARTMENTS:1
        POSTGRES:departments:1
        ORACLEDB:EMPLOYEES:0
        POSTGRES:employees:0
        [ERRORS IDENTITY COLUMN COUNT]
        OK, Oracle and PostgreSQL have the same number of identity column.

        [TEST FOREIGN KEYS COUNT]
        ORACLEDB:DEPARTMENTS:0
        POSTGRES:departments:0
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS FOREIGN KEYS COUNT]
        OK, Oracle and PostgreSQL have the same number of foreign keys.

        [TEST TABLE COUNT]
        ORACLEDB:TABLE:2
        POSTGRES:TABLE:2
        [ERRORS TABLE COUNT]
        OK, Oracle and PostgreSQL have the same number of TABLE.

        [TEST TABLE TRIGGERS COUNT]
        ORACLEDB:DEPARTMENTS:0
        POSTGRES:departments:0
        ORACLEDB:EMPLOYEES:1
        POSTGRES:employees:1
        [ERRORS TABLE TRIGGERS COUNT]
        OK, Oracle and PostgreSQL have the same number of table triggers.

        [TEST TRIGGER COUNT]
        ORACLEDB:TRIGGER:2
        POSTGRES:TRIGGER:2
        [ERRORS TRIGGER COUNT]
        OK, Oracle and PostgreSQL have the same number of TRIGGER.

        [TEST VIEW COUNT]
        ORACLEDB:VIEW:1
        POSTGRES:VIEW:1
        [ERRORS VIEW COUNT]
        OK, Oracle and PostgreSQL have the same number of VIEW.

        [TEST MVIEW COUNT]
        ORACLEDB:MVIEW:0
        POSTGRES:MVIEW:0
        [ERRORS MVIEW COUNT]
        OK, Oracle and PostgreSQL have the same number of MVIEW.

        [TEST SEQUENCE COUNT]
        ORACLEDB:SEQUENCE:1
        POSTGRES:SEQUENCE:0
        [ERRORS SEQUENCE COUNT]
        SEQUENCE does not have the same count in Oracle (1) and in PostgreSQL (0).

        [TEST TYPE COUNT]
        ORACLEDB:TYPE:1
        POSTGRES:TYPE:0
        [ERRORS TYPE COUNT]
        TYPE does not have the same count in Oracle (1) and in PostgreSQL (0).

        [TEST FDW COUNT]
        ORACLEDB:FDW:0
        POSTGRES:FDW:0
        [ERRORS FDW COUNT]
        OK, Oracle and PostgreSQL have the same number of FDW.

        [TEST FUNCTION COUNT]
        ORACLEDB:FUNCTION:3
        POSTGRES:FUNCTION:3
        [ERRORS FUNCTION COUNT]
        OK, Oracle and PostgreSQL have the same number of functions.

        [TEST SEQUENCE VALUES]
        ORACLEDB:EMPLOYEES_NUM_SEQ:1285
        POSTGRES:employees_num_seq:1285
        [ERRORS SEQUENCE VALUES COUNT]
        OK, Oracle and PostgreSQL have the same values for sequences

        [TEST ROWS COUNT]
        ORACLEDB:DEPARTMENTS:27
        POSTGRES:departments:27
        ORACLEDB:EMPLOYEES:854
        POSTGRES:employees:854
        [ERRORS ROWS COUNT]
        OK, Oracle and PostgreSQL have the same number of rows.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Data validation Data validation consists of comparing data retrieved from a foreign table pointing to the source Oracle table and a local PostgreSQL table resulting from the data export.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;To run data validation you can use a direct connection like any other
Ora2Pg action but you can also use the oracle_fdw, mysql_fdw or tds_fdw
extension provided that FDW_SERVER and PG_DSN configuration directives
are set.

By default, Ora2Pg will extract the first 10000 rows from both sides,
you can change this value using directive DATA_VALIDATION_ROWS. When it
is set to zero all rows of the tables will be compared.

Data validation requires that the table has a primary key or unique
index and that the key column is not a LOB. Rows will be sorted using
this unique key. Due to differences in sort behavior between Oracle and
PostgreSQL, if the collation of unique key columns in PostgreSQL is not
&#39;C&#39;, the sort order can be different compared to Oracle. In this case
the data validation will fail.

Data validation must be done before any data is modified.

Ora2Pg will stop comparing two tables after DATA_VALIDATION_ROWS is
reached or after 10 errors have been encountered, results are dumped in
a file named &quot;data_validation.log&quot; written in the current directory by
default. The number of errors before stopping the diff between rows can
be controlled using the configuration directive DATA_VALIDATION_ERROR.
All rows with errors are printed to the output file for your analysis.

It is possible to parallelize data validation by using -P option or the
corresponding configuration directive PARALLEL_TABLES in ora2pg.conf.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use of System Change Number (SCN) Ora2Pg is able to export data as of a specific SCN. You can set it at command line using the -S or --scn option. You can give a specific SCN or if you want to use the current SCN at first connection time set the value to &#39;current&#39;. In this last case if connection user has the &quot;SELECT ANY DICTIONARY&quot; or the &quot;SELECT_CATALOG_ROLE&quot; role, the current SCN is looked up in the v$database view.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Example of use:

    ora2pg -c ora2pg.conf -t COPY --scn 16605281

This adds the following clause to the query used to retrieve data for
example:

    AS OF SCN 16605281

You can also use the --scn option to use the Oracle flashback capability
by specifying a timestamp expression instead of a SCN. For example:

    ora2pg -c ora2pg.conf -t COPY --scn &quot;TO_TIMESTAMP(&#39;2021-12-01 00:00:00&#39;, &#39;YYYY-MM-DD HH:MI:SS&#39;)&quot;

This will add the following clause to the query used to retrieve data:

    AS OF TIMESTAMP TO_TIMESTAMP(&#39;2021-12-01 00:00:00&#39;, &#39;YYYY-MM-DD HH:MI:SS&#39;)

or for example to only retrieve yesterday&#39;s data:

    ora2pg -c ora2pg.conf -t COPY --scn &quot;SYSDATE - 1&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Change Data Capture (CDC) Ora2Pg does not have a feature which allows importing data and only applying changes after the first import. But you can use the --cdc_ready option to export data with registration of the SCN at the time of the table export. All SCNs per table are written to a file named TABLES_SCN.log by default, it can be changed using -C | --cdc_file option.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;These SCNs registered per table during COPY or INSERT export can be used
with a CDC tool. The format of the file is tablename:SCN per line.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Importing BLOB as large objects By default Ora2Pg imports Oracle BLOB as bytea, the destination column is created using the bytea data type. If you want to use large objects instead of bytea, just add the --blob_to_lo option to the ora2pg command. It will create the destination column as data type Oid and will save the BLOB as a large object using the lo_from_bytea() function. The Oid returned by the call to lo_from_bytea() is inserted in the destination column instead of a bytea. Because of the use of the function this option can only be used with actions SHOW_COLUMN, TABLE and INSERT. Action COPY is not allowed.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;If you want to use COPY or have huge size BLOBs ( &amp;gt; 1GB) than cannot be
imported using lo_from_bytea() you can add option --lo_import to the
ora2pg command. This will allow importing data in two passes.

1) Export data using COPY or INSERT will set the Oid destination column
for BLOB to value 0 and save the BLOB value into a dedicated file. It
will also create a Shell script to import the BLOB files into the
database using psql command \lo_import and to update the table Oid
column to the returned large object Oid. The script is named
lo_import-TABLENAME.sh

2) Execute all scripts lo_import-TABLENAME.sh after setting the
environment variables PGDATABASE and optionally PGHOST, PGPORT, PGUSER,
etc. if they do not correspond to the default values for libpq.

You might also execute manually a VACUUM FULL on the table to remove the
bloat created by the table update.

Limitation: the table must have a primary key, it is used to set the
WHERE clause to update the Oid column after the large object import.
Importing BLOB using this second method (--lo_import) is very slow so it
should be reserved for rows where the BLOB &amp;gt; 1GB. For all other rows use
the option --blob_to_lo. To filter the rows you can use the WHERE
configuration directive in ora2pg.conf.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;SUPPORT Author / Maintainer Gilles Darold 
 &lt;gilles at darold dot net&gt;&lt;/gilles&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Please report any bugs, patches, help, etc. to &amp;lt;gilles AT darold DOT
net&amp;gt;.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Feature Requests If you need new features, please let me know at 
 &lt;gilles at darold dot net&gt;
  . This helps a lot in developing a better/more useful tool.
 &lt;/gilles&gt;&lt;/p&gt; 
&lt;p&gt;How To Contribute Any contribution to build a better tool is welcome. Just send me your ideas, features requests or patches and they will be applied.&lt;/p&gt; 
&lt;p&gt;LICENSE Copyright (c) 2000-2025 Gilles Darold - All rights reserved.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;        This program is free software: you can redistribute it and/or modify
        it under the terms of the GNU General Public License as published by
        the Free Software Foundation, either version 3 of the License, or
        any later version.

        This program is distributed in the hope that it will be useful,
        but WITHOUT ANY WARRANTY; without even the implied warranty of
        MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
        GNU General Public License for more details.

        You should have received a copy of the GNU General Public License
        along with this program.  If not, see &amp;lt; http://www.gnu.org/licenses/ &amp;gt;.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;ACKNOWLEDGEMENTS Many thanks to all the great contributors. See changelog for all acknowledgments.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lm-sensors/lm-sensors</title>
      <link>https://github.com/lm-sensors/lm-sensors</link>
      <description>&lt;p&gt;lm-sensors repository&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OVERVIEW OF THE LM-SENSORS PACKAGE&lt;/h1&gt; 
&lt;p&gt;The lm-sensors package, version 3, provides user-space support for the hardware monitoring drivers in Linux 2.6.5 and later. For older kernel versions, you have to use lm-sensors version 2.&lt;/p&gt; 
&lt;p&gt;The directories within this package:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;doc Documentation.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;etc A sample configuration file for libsensors, and a script to convert lm-sensors version 2 configuration files to work with version 3.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;lib The user-space sensors support library code (libsensors).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;prog Several supporting programs. The most important ones are:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;sensors-detect: A stand-alone program for detecting installed hardware and recommending specific modules to load.&lt;/li&gt; 
   &lt;li&gt;sensors: A console tool to report sensor readings and set new sensor limits.&lt;/li&gt; 
   &lt;li&gt;sensord: A daemon to watch sensor values and log problems. It includes RRD support.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;configs This directory contains sample configurations of various boards, contributed by users of lm-sensors.&lt;/p&gt; &lt;p&gt;All configuration files should now be imported from the old &lt;a href=&quot;http://www.lm-sensors.org&quot;&gt;www.lm-sensors.org&lt;/a&gt; website, however additional information about the origin of the configuration files can be found on the old site: &lt;a href=&quot;http://web.archive.org/web/20150901092438/http://www.lm-sensors.org:80/wiki/Configurations&quot;&gt;http://web.archive.org/web/20150901092438/http://www.lm-sensors.org:80/wiki/Configurations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please contribute back a configuration of your board so other users with the same hardware won&#39;t need to recreate it again and again.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;INSTALLING LM-SENSORS&lt;/h2&gt; 
&lt;p&gt;See the INSTALL file.&lt;/p&gt; 
&lt;h2&gt;HARDWARE SUPPORT&lt;/h2&gt; 
&lt;p&gt;To find out what hardware you have, just run &#39;sensors-detect&#39; as root.&lt;/p&gt; 
&lt;p&gt;Most modern mainboards incorporate some form of hardware monitoring chips. These chips read things like chip temperatures, fan rotation speeds and voltage levels. There are quite a few different chips which can be used by mainboard builders for approximately the same results.&lt;/p&gt; 
&lt;p&gt;Laptops, on the other hand, rarely expose any hardware monitoring chip. They often have some BIOS and/or ACPI magic to get the CPU temperature value, but that&#39;s about it. For such laptops, the lm-sensors package is of no use (sensors-detect will not find anything), and you have to use acpi instead.&lt;/p&gt; 
&lt;p&gt;This package doesn&#39;t contain chip-specific knowledge. It will support all the hardware monitoring chips your kernel has drivers for. In other words, if you find out that you have unsupported hardware (e.g. sensors-detect told you so) then it means that you need a more recent kernel, or you even need to wait for a new kernel driver to be written. Updating the lm-sensors package itself will not help.&lt;/p&gt; 
&lt;h2&gt;LIBSENSORS&lt;/h2&gt; 
&lt;p&gt;The kernel drivers communicate their information through the /sys interface. Because every motherboard is different, the drivers always advert the measurements at their pins. This means that the values they report are not always immediately relevant to you. They have to be labelled properly, and sometimes they must be scaled to correspond to real-world values.&lt;/p&gt; 
&lt;p&gt;libsensors is a (shared or static) library of access functions. It offers a simple-to-use interface for applications to access the sensor chip readings and configure them as you like. It has a configuration file where you can put all the motherboard-specific labels and conversion rules. That way, all applications do not need to duplicate the effort and can simply link with libsensors and work out of the box.&lt;/p&gt; 
&lt;h2&gt;APPLICATIONS&lt;/h2&gt; 
&lt;p&gt;This package contains an example console program that reports all current sensors values. This program is called &#39;sensors&#39;. You can use it as a reference implementation for more intricate programs. It also contains a daemon watching for sensor values, logging alarms and feeding an RRD database with the sensor measurements.&lt;/p&gt; 
&lt;p&gt;This package does not contain a nice graphical monitor. See &lt;a href=&quot;http://sensors-applet.sourceforge.net/&quot;&gt;http://sensors-applet.sourceforge.net/&lt;/a&gt; &lt;a href=&quot;https://01.org/powertop/&quot;&gt;https://01.org/powertop/&lt;/a&gt; &lt;a href=&quot;https://wpitchoune.net/psensor/&quot;&gt;https://wpitchoune.net/psensor/&lt;/a&gt; &lt;a href=&quot;https://amanusk.github.io/s-tui/&quot;&gt;https://amanusk.github.io/s-tui/&lt;/a&gt; for such programs.&lt;/p&gt; 
&lt;h2&gt;TROUBLESHOOTING&lt;/h2&gt; 
&lt;p&gt;lm-sensors is blamed for all kinds of problems on a regular basis. While it is always possible that lm-sensors does in fact have a problem, almost everytime the problem will be elsewhere. The most likely culprit for a problem is one of the kernel drivers prividing data to lm-sensors or, rather, the sensors command (and libsensors). If you encounter problems, the following tests are recommended.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check enabled sensors with the &#39;sensors&#39; command. Observe and, if there are errors, write down your test steps and report.&lt;/li&gt; 
 &lt;li&gt;If any problems are observed while running the &#39;sensors&#39; command, disable (unload) kernel drivers one after another. After unloading a driver, run the &#39;sensors&#39; command again and observe if the error is gone. If it is, you found the problematic driver. Record your tests and report to the driver maintainer.&lt;/li&gt; 
 &lt;li&gt;If problems are observed while using libsensors from another application, the same procedure applies (though it is recommended to perform the test with the sensors command first). Again, after identifying the offending driver, record and report your results to the driver maintainer.&lt;/li&gt; 
 &lt;li&gt;If this does not help, try to run ptrace on the sensors command and/or the application using libsensors. If you find any anormalies, record your results and report.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;OTHER INFORMATION&lt;/h2&gt; 
&lt;p&gt;The lm_sensors website can be found at &lt;a href=&quot;https://hwmon.wiki.kernel.org/lm_sensors&quot;&gt;https://hwmon.wiki.kernel.org/lm_sensors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;The developers of this package can be reached using&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;a mailing list lm-sensors 
  &lt;at&gt;
    vger 
   &lt;dot&gt;
     kernel 
    &lt;dot&gt;
      org (you can get information on how to subscribe, unsubscribe, and the list of archives at 
     &lt;a href=&quot;http://vger.kernel.org/vger-lists.html#lm-sensors&quot;&gt;http://vger.kernel.org/vger-lists.html#lm-sensors&lt;/a&gt;; you do not need to be subscribed to post to the list)
    &lt;/dot&gt;
   &lt;/dot&gt;
  &lt;/at&gt;&lt;/li&gt; 
 &lt;li&gt;GitHub (see &lt;a href=&quot;https://github.com/lm-sensors/lm-sensors&quot;&gt;https://github.com/lm-sensors/lm-sensors&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Do not hesitate to contact us if you have questions, suggestions, problems, want to contribute, or just want to report it works for you. But please try to read the documentation before you ask any questions! It&#39;s all under doc/.&lt;/p&gt; 
&lt;p&gt;The latest version of this package can always be found at: &lt;a href=&quot;https://github.com/lm-sensors/lm-sensors&quot;&gt;https://github.com/lm-sensors/lm-sensors&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;The library (libsensors) is released under the GNU Lesser General Public License (LGPL), as included in the file COPYING.LGPL. The rest of this package may be distributed according to the GNU General Public License (GPL), as included in the file COPYING.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>centreon/centreon-plugins</title>
      <link>https://github.com/centreon/centreon-plugins</link>
      <description>&lt;p&gt;Collection of standard plugins to discover and gather cloud-to-edge metrics and status across your whole IT infrastructure.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;centreon-plugins&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/raw/master/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-APACHE2-brightgreen.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;!-- SHIELDS --&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/centreon/centreon-plugins?color=%2384BD00&amp;amp;label=CONTRIBUTORS&amp;amp;style=for-the-badge&quot; alt=&quot;Contributors&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/centreon/centreon-plugins/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/centreon/centreon-plugins?color=%23433b02a&amp;amp;label=STARS&amp;amp;style=for-the-badge&quot; alt=&quot;Stars&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/centreon/centreon-plugins/network/members&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/forks/centreon/centreon-plugins?color=%23009fdf&amp;amp;label=FORKS&amp;amp;style=for-the-badge&quot; alt=&quot;Forks&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/centreon/centreon-plugins/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/centreon/centreon-plugins?color=%230072ce&amp;amp;label=ISSUES&amp;amp;style=for-the-badge&quot; alt=&quot;Issues&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What are Centreon Plugins&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/&quot;&gt;Centreon plugins&lt;/a&gt; is a free and open source project to monitor systems. The project can be used with Centreon and all monitoring softwares compatible with Nagios plugins.&lt;/p&gt; 
&lt;h3&gt;Principles&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/&quot;&gt;Centreon plugins&lt;/a&gt; should comply with &lt;a href=&quot;https://www.monitoring-plugins.org/doc/guidelines.html&quot;&gt;Monitoring Plugins Development Guidelines&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;In short, they return:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An error code: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;0&lt;/code&gt; for &lt;code&gt;OK&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;1&lt;/code&gt; for &lt;code&gt;WARNING&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;2&lt;/code&gt; for &lt;code&gt;CRITICAL&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;3&lt;/code&gt; for &lt;code&gt;UNKNOWN&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;A human understandable output message (example: &lt;code&gt;OK: CPU(s) average usage is 2.66 % - CPU &#39;0&#39; usage : 2.66 %&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;A set of metrics provided as &lt;em&gt;perfdata&lt;/em&gt; after a &lt;code&gt;|&lt;/code&gt; character (example: &lt;code&gt;&#39;cpu.utilization.percentage&#39;=2.66%;;;0;100 &#39;0#core.cpu.utilization.percentage&#39;=2.66%;;;0;100&lt;/code&gt;).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;What can Centreon Plugins monitor?&lt;/h3&gt; 
&lt;p&gt;You can monitor many systems:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Application&lt;/strong&gt;: Apache, Asterisk, Elasticsearch, Github, Jenkins, Kafka, Nginx, Pfsense, Redis, Tomcat, Varnish, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Cloud&lt;/strong&gt;: AWS, Azure, Docker, Office365, Nutanix, Prometheus, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Databases&lt;/strong&gt;: Firebird, Informix, MS SQL, MySQL, Oracle, Postgres, Cassandra.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hardware&lt;/strong&gt;: printers (RFC3805), UPS (Powerware, Mge, Standard), Sun Hardware, Cisco UCS, SensorIP, HP Proliant, HP Bladechassis, Dell Openmanage, Dell CMC, Raritan, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Network&lt;/strong&gt;: Aruba, Brocade, Bluecoat, Brocade, Checkpoint, Cisco AP/IronPort/ASA/Standard, Extreme, Fortigate, H3C, Hirschmann, HP Procurve, F5 BIG-IP, Juniper, PaloAlto, Redback, Riverbed, Ruggedcom, Stonesoft, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Operating systems&lt;/strong&gt;: Linux (SNMP, NRPE), Freebsd (SNMP), AIX (SNMP), Solaris (SNMP), etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: EMC Clariion, Netapp, Nimble, HP MSA p2000, Dell EqualLogic, Qnap, Panzura, Synology, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;To get a complete list, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;perl src/centreon_plugins.pl --list-plugin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Basic Usage&lt;/h3&gt; 
&lt;p&gt;We&#39;ll use a basic example to show you how to monitor a system. I have finished the install section and I want to monitor a Linux in SNMP. First, I need to find the plugin to use in the list:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;perl centreon_plugins.pl --list-plugin | grep -i linux | grep &#39;PLUGIN&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It will return:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PLUGIN: os::linux::local::plugin
PLUGIN: os::linux::snmp::plugin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It seems that &#39;os::linux::snmp::plugin&#39; is the good one. So I verify with the option &lt;code&gt;--help&lt;/code&gt; to be sure:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --help
...
Plugin Description:
  Check Linux operating systems in SNMP.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It&#39;s exactly what I need. Now I&#39;ll add the option &lt;code&gt;--list-mode&lt;/code&gt; to know what can I do with it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --list-mode
...
Modes Available:
 processcount
 time
 list-storages
 disk-usage
 diskio
 uptime
 swap
 cpu-detailed
 load
 traffic
 cpu
 inodes
 list-diskspath
 list-interfaces
 packet-errors
 memory
 tcpcon
 storage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I would like to test the &#39;load&#39; mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load
UNKNOWN: Missing parameter --hostname.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;It&#39;s not working because some options are missing. I can have a description of the mode and options with the option &lt;code&gt;--help&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Eventually, I have to configure some SNMP options:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load --hostname=127.0.0.1 --snmp-version=2c --snmp-community=public
OK: Load average: 0.00, 0.00, 0.00 | &#39;load1&#39;=0.00;;;0; &#39;load5&#39;=0.00;;;0; &#39;load15&#39;=0.00;;;0;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;I can set threshold with options &lt;code&gt;--warning&lt;/code&gt; and &lt;code&gt;--critical&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ perl centreon_plugins.pl --plugin=os::linux::snmp::plugin --mode=load --hostname=127.0.0.1 --snmp-version=2c --snmp-community=public --warning=1,2,3 --critical=2,3,4
OK: Load average: 0.00, 0.00, 0.00 | &#39;load1&#39;=0.00;0:1;0:2;0; &#39;load5&#39;=0.00;0:2;0:3;0; &#39;load15&#39;=0.00;0:3;0:4;0;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For more information or help, please read &lt;a href=&quot;https://raw.githubusercontent.com/centreon/centreon-plugins/develop/doc/en/user/guide.rst&quot;&gt;&#39;doc/en/user/guide.rst&#39;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributions&lt;/h2&gt; 
&lt;h3&gt;Code contributions/pull requests&lt;/h3&gt; 
&lt;p&gt;If you want to contribute by submitting new functionalities, enhancements or bug fixes, first thank you for participating :-) Then have a look, if not already done, to our &lt;strong&gt;&lt;a href=&quot;https://github.com/centreon/centreon-plugins/raw/develop/doc/en/developer/guide.md&quot;&gt;development guide&lt;/a&gt;&lt;/strong&gt;. Then create a &lt;a href=&quot;https://github.com/centreon/centreon-plugins/fork&quot;&gt;fork&lt;/a&gt; and a development branch, and once it&#39;s done, you may submit a &lt;a href=&quot;https://github.com/centreon/centreon-plugins/pulls&quot;&gt;pull request&lt;/a&gt; that the corporate development team will examine.&lt;/p&gt; 
&lt;h3&gt;Issues/bug reports&lt;/h3&gt; 
&lt;p&gt;If you encounter a behaviour that is clearly a bug or a regression, you are welcome to submit an &lt;a href=&quot;https://github.com/centreon/centreon-plugins/issues&quot;&gt;issue&lt;/a&gt;. Please be aware that this is an open source project and that there is no guaranteed response time.&lt;/p&gt; 
&lt;h3&gt;Questions/search for help&lt;/h3&gt; 
&lt;p&gt;If you have trouble using our plugins, but are not sure whether it&#39;s due to a bug or a misuse, please take the time to ask for help on &lt;a href=&quot;https://thewatch.centreon.com/data-collection-6&quot;&gt;The Watch, Data Collection section&lt;/a&gt; and become certain that it is a bug before submitting it here.&lt;/p&gt; 
&lt;h3&gt;Feature/enhancement request&lt;/h3&gt; 
&lt;p&gt;There is high demand for new plugins and new functionalities on existing plugins, so we have to rely on our community to help us prioritize them. How? Post your suggestion on &lt;a href=&quot;https://thewatch.centreon.com/ideas&quot;&gt;The Watch Ideas&lt;/a&gt; with as much detail as possible and we will pick the most voted topics to add them to our product roadmap.&lt;/p&gt; 
&lt;p&gt;To develop a plugin/mode, we need the following information, depending on the protocol:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SNMP&lt;/strong&gt;: MIB files and full snmpwalk of enterprise branch (&lt;code&gt;snmpwalk -ObentU -v 2c -c public address .1.3.6.1.4.1 &amp;gt; equipment.snmpwalk&lt;/code&gt;) or &lt;a href=&quot;https://thewatch.centreon.com/product-how-to-21/snmp-collection-tutorial-132&quot;&gt;SNMP collections&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP API (SOAP, Rest/Json, XML-RPC)&lt;/strong&gt;: the documentation and some curl examples or HTTP &lt;a href=&quot;https://thewatch.centreon.com/data-collection-6/centreon-plugins-discover-collection-modes-131&quot;&gt;collections&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CLI&lt;/strong&gt;: command line examples (command + result).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SQL&lt;/strong&gt;: queries + results + column types or &lt;a href=&quot;https://thewatch.centreon.com/product-how-to-21/sql-collection-tutorial-134&quot;&gt;SQL collections&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JMX&lt;/strong&gt;: mbean names and attributes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If some information is confidential, such as logins or IP addresses, obfuscate them in what is sent publicly and we&#39;ll get in touch with you by private message if this information is needed.&lt;/p&gt; 
&lt;p&gt;Please note that all the developments are open source, we will not commit to a release date. If it is an emergency for you, please contact &lt;a href=&quot;https://www.centreon.com/contact/&quot;&gt;Centreon&#39;s sales team&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Continuous integration&lt;/h3&gt; 
&lt;p&gt;Please follow documentation &lt;a href=&quot;https://raw.githubusercontent.com/centreon/centreon-plugins/develop/doc/CI.md&quot;&gt;here&lt;/a&gt;&lt;/p&gt; 
&lt;!-- URL AND IMAGES FOR SHIELDS --&gt;</description>
    </item>
    
    <item>
      <title>darold/pgbadger</title>
      <link>https://github.com/darold/pgbadger</link>
      <description>&lt;p&gt;A fast PostgreSQL Log Analyzer&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;TABLE OF CONTENTS&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#NAME&quot;&gt;NAME&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#SYNOPSIS&quot;&gt;SYNOPSIS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#DESCRIPTION&quot;&gt;DESCRIPTION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#FEATURE&quot;&gt;FEATURE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#REQUIREMENT&quot;&gt;REQUIREMENT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#INSTALLATION&quot;&gt;INSTALLATION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#POSTGRESQL-CONFIGURATION&quot;&gt;POSTGRESQL-CONFIGURATION&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#LOG-STATEMENTS&quot;&gt;LOG-STATEMENTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#PARALLEL-PROCESSING&quot;&gt;PARALLEL-PROCESSING&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#INCREMENTAL-REPORTS&quot;&gt;INCREMENTAL-REPORTS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#BINARY-FORMAT&quot;&gt;BINARY-FORMAT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#JSON-FORMAT&quot;&gt;JSON-FORMAT&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#AUTHORS&quot;&gt;AUTHORS&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/darold/pgbadger/master/#LICENSE&quot;&gt;LICENSE&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;NAME&lt;/h3&gt; 
&lt;p&gt;pgBadger - a fast PostgreSQL log analysis report&lt;/p&gt; 
&lt;h3&gt;SYNOPSIS&lt;/h3&gt; 
&lt;p&gt;Usage: pgbadger [options] logfile [...]&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PostgreSQL log analyzer with fully detailed reports and graphs.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Arguments:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;logfile can be a single log file, a list of files, or a shell command
returning a list of files. If you want to pass log content from stdin
use - as filename. Note that input from stdin will not work with csvlog.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Options:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt; -a | --average minutes : number of minutes to build the average graphs of
                          queries and connections. Default 5 minutes.
 -A | --histo-average min: number of minutes to build the histogram graphs
                          of queries. Default 60 minutes.
 -b | --begin datetime  : start date/time for the data to be parsed in log
                          (either a timestamp or a time)
 -c | --dbclient host   : only report on entries for the given client host.
 -C | --nocomment       : remove comments like /* ... */ from queries.
 -d | --dbname database : only report on entries for the given database.
 -D | --dns-resolv      : client ip addresses are replaced by their DNS name.
                          Be warned that this can really slow down pgBadger.
 -e | --end datetime    : end date/time for the data to be parsed in log
                          (either a timestamp or a time)
 -E | --explode         : explode the main report by generating one report
                          per database. Global information not related to a
                          database is added to the postgres database report.
 -f | --format logtype  : possible values: syslog, syslog2, stderr, jsonlog,
                          csv, pgbouncer, logplex, rds and redshift. Use this
                          option when pgBadger is not able to detect the log
                          format.
 -G | --nograph         : disable graphs on HTML output. Enabled by default.
 -h | --help            : show this message and exit.
 -H | --html-outdir path: path to directory where HTML report must be written
                          in incremental mode, binary files stay on directory
                          defined with -O, --outdir option.
 -i | --ident name      : programname used as syslog ident. Default: postgres
 -I | --incremental     : use incremental mode, reports will be generated by
                          days in a separate directory, --outdir must be set.
 -j | --jobs number     : number of jobs to run at same time for a single log
                          file. Run as single by default or when working with
                          csvlog format.
 -J | --Jobs number     : number of log files to parse in parallel. Process
                          one file at a time by default.
 -l | --last-parsed file: allow incremental log parsing by registering the
                          last datetime and line parsed. Useful if you want
                          to watch errors since last run or if you want one
                          report per day with a log rotated each week.
 -L | --logfile-list file:file containing a list of log files to parse.
 -m | --maxlength size  : maximum length of a query, it will be restricted to
                          the given size. Default truncate size is 100000.
 -M | --no-multiline    : do not collect multiline statements to avoid garbage
                          especially on errors that generate a huge report.
 -N | --appname name    : only report on entries for given application name
 -o | --outfile filename: define the filename for the output. Default depends
                          on the output format: out.html, out.txt, out.bin,
                          or out.json. This option can be used multiple times
                          to output several formats. To use json output, the
                          Perl module JSON::XS must be installed, to dump
                          output to stdout, use - as filename.
 -O | --outdir path     : directory where out files must be saved.
 -p | --prefix string   : the value of your custom log_line_prefix as
                          defined in your postgresql.conf. Only use it if you
                          aren&#39;t using one of the standard prefixes specified
                          in the pgBadger documentation, such as if your
                          prefix includes additional variables like client IP
                          or application name. MUST contain escape sequences
                          for time (%t, %m or %n) and processes (%p or %c).
                          See examples below.
 -P | --no-prettify     : disable SQL queries prettify formatter.
 -q | --quiet           : don&#39;t print anything to stdout, not even a progress
                          bar.
 -Q | --query-numbering : add numbering of queries to the output when using
                          options --dump-all-queries or --normalized-only.
 -r | --remote-host ip  : set the host where to execute the cat command on
                          remote log file to parse the file locally.
 -R | --retention N     : number of weeks to keep in incremental mode. Defaults
                          to 0, disabled. Used to set the number of weeks to
                          keep in output directory. Older weeks and days
                          directories are automatically removed.
 -s | --sample number   : number of query samples to store. Default: 3.
 -S | --select-only     : only report SELECT queries.
 -t | --top number      : number of queries to store/display. Default: 20.
 -T | --title string    : change title of the HTML page report.
 -u | --dbuser username : only report on entries for the given user.
 -U | --exclude-user username : exclude entries for the specified user from
                          report. Can be used multiple time.
 -v | --verbose         : enable verbose or debug mode. Disabled by default.
 -V | --version         : show pgBadger version and exit.
 -w | --watch-mode      : only report errors just like logwatch could do.
 -W | --wide-char       : encode html output of queries into UTF8 to avoid
                          Perl message &quot;Wide character in print&quot;.
 -x | --extension       : output format. Values: text, html, bin or json.
                          Default: html
 -X | --extra-files     : in incremental mode allow pgBadger to write CSS and
                          JS files in the output directory as separate files.
 -z | --zcat exec_path  : set the full path to the zcat program. Use it if
                          zcat, bzcat or unzip is not in your path.
 -Z | --timezone +/-XX  : Set the number of hours from GMT of the timezone.
                          Use this to adjust date/time in JavaScript graphs.
                          The value can be an integer, ex.: 2, or a float,
                          ex.: 2.5.
 --pie-limit num        : pie data lower than num% will show a sum instead.
 --exclude-query regex  : any query matching the given regex will be excluded
                          from the report. For example: &quot;^(VACUUM|COMMIT)&quot;
                          You can use this option multiple times.
 --exclude-file filename: path of the file that contains each regex to use
                          to exclude queries from the report. One regex per
                          line.
 --include-query regex  : any query that does not match the given regex will
                          be excluded from the report. You can use this
                          option multiple times. For example: &quot;(tbl1|tbl2)&quot;.
 --include-file filename: path of the file that contains each regex to the
                          queries to include from the report. One regex per
                          line.
 --disable-error        : do not generate error report.
 --disable-hourly       : do not generate hourly report.
 --disable-type         : do not generate report of queries by type, database
                          or user.
 --disable-query        : do not generate query reports (slowest, most
                          frequent, queries by users, by database, ...).
 --disable-session      : do not generate session report.
 --disable-connection   : do not generate connection report.
 --disable-lock         : do not generate lock report.
 --disable-temporary    : do not generate temporary report.
 --disable-checkpoint   : do not generate checkpoint/restartpoint report.
 --disable-autovacuum   : do not generate autovacuum report.
 --charset              : used to set the HTML charset to be used.
                          Default: utf-8.
 --csv-separator        : used to set the CSV field separator, default: ,
 --exclude-time  regex  : any timestamp matching the given regex will be
                          excluded from the report. Example: &quot;2013-04-12 .*&quot;
                          You can use this option multiple times.
 --include-time  regex  : only timestamps matching the given regex will be
                          included in the report. Example: &quot;2013-04-12 .*&quot;
                          You can use this option multiple times.
 --exclude-db name      : exclude entries for the specified database from
                          report. Example: &quot;pg_dump&quot;. Can be used multiple
                          times.
 --exclude-appname name : exclude entries for the specified application name
                          from report.  Example: &quot;pg_dump&quot;.  Can be used
                          multiple times.
 --exclude-line regex   : exclude any log entry that will match the given
                          regex. Can be used multiple times.
 --exclude-client name  : exclude log entries for the specified client ip.
                          Can be used multiple times.
 --anonymize            : obscure all literals in queries, useful to hide
                          confidential data.
 --noreport             : no reports will be created in incremental mode.
 --log-duration         : force pgBadger to associate log entries generated
                          by both log_duration = on and log_statement = &#39;all&#39;
 --enable-checksum      : used to add an md5 sum under each query report.
 --journalctl command   : command to use to replace PostgreSQL logfile by
                          a call to journalctl. Basically it might be:
                             journalctl -u postgresql-9.5
 --pid-dir path         : set the path where the pid file must be stored.
                          Default /tmp
 --pid-file file        : set the name of the pid file to manage concurrent
                          execution of pgBadger. Default: pgbadger.pid
 --rebuild              : used to rebuild all html reports in incremental
                          output directories where there&#39;s binary data files.
 --pgbouncer-only       : only show PgBouncer-related menus in the header.
 --start-monday         : in incremental mode, calendar weeks start on
                          Sunday. Use this option to start on a Monday.
 --iso-week-number      : in incremental mode, calendar weeks start on
                          Monday and respect the ISO 8601 week number, range
                          01 to 53, where week 1 is the first week that has
                          at least 4 days in the new year.
 --normalized-only      : only dump all normalized queries to out.txt
 --log-timezone +/-XX   : Set the number of hours from GMT of the timezone
                          that must be used to adjust date/time read from
                          log file before beeing parsed. Using this option
                          makes log search with a date/time more difficult.
                          The value can be an integer, ex.: 2, or a float,
                          ex.: 2.5.
 --prettify-json        : use it if you want json output to be prettified.
 --month-report YYYY-MM : create a cumulative HTML report over the specified
                          month. Requires incremental output directories and
                          the presence of all necessary binary data files
 --day-report YYYY-MM-DD: create an HTML report over the specified day.
                          Requires incremental output directories and the
                          presence of all necessary binary data files
 --noexplain            : do not process lines generated by auto_explain.
 --command CMD          : command to execute to retrieve log entries on
                          stdin. pgBadger will open a pipe to the command
                          and parse log entries generated by the command.
 --no-week              : inform pgbadger to not build weekly reports in
                          incremental mode. Useful if it takes too much time.
 --explain-url URL      : use it to override the url of the graphical explain
                          tool. Default: https://explain.depesz.com/
 --tempdir DIR          : set directory where temporary files will be written
                          Default: File::Spec-&amp;gt;tmpdir() || &#39;/tmp&#39;
 --no-process-info      : disable changing process title to help identify
                          pgbadger process, some system do not support it.
 --dump-all-queries     : dump all queries found in the log file replacing
                          bind parameters included in the queries at their
                          respective placeholders positions.
 --keep-comments        : do not remove comments from normalized queries. It
                          can be useful if you want to distinguish between
                          same normalized queries.
 --no-progressbar       : disable progressbar.
 --dump-raw-csv         : parse the log and dump the information into CSV
                          format. No further processing is done, no report.
 --include-pid PID      : only report events related to the session pid (%p).
                          Can be used multiple time.
 --include-session ID   : only report events related to the session id (%c).
                          Can be used multiple time.
--histogram-query VAL   : use custom inbound for query times histogram.
                          Default inbound in milliseconds:
                      0,1,5,10,25,50,100,500,1000,10000
--histogram-session VAL : use custom inbound for session times histogram.
                          Default inbound in milliseconds:
                      0,500,1000,30000,60000,600000,1800000,3600000,28800000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;pgBadger is able to parse a remote log file using a passwordless ssh connection. Use -r or --remote-host to set the host IP address or hostname. There are also some additional options to fully control the ssh connection.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;--ssh-program ssh        path to the ssh program to use. Default: ssh.
--ssh-port port          ssh port to use for the connection. Default: 22.
--ssh-user username      connection login name. Defaults to running user.
--ssh-identity file      path to the identity file to use.
--ssh-timeout second     timeout to ssh connection failure. Default: 10 sec.
--ssh-option  options    list of -o options to use for the ssh connection.
                         Options always used:
                             -o ConnectTimeout=$ssh_timeout
                             -o PreferredAuthentications=hostbased,publickey
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Log file to parse can also be specified using an URI, supported protocols are http[s] and [s]ftp. The curl command will be used to download the file, and the file will be parsed during download. The ssh protocol is also supported and will use the ssh command like with the remote host use. See examples bellow.&lt;/p&gt; 
&lt;p&gt;Return codes:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0: on success
1: die on error
2: if it has been interrupted using ctr+c for example
3: the pid file already exists or can not be created
4: no log file was given at command line
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Examples:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger /var/log/postgresql.log
pgbadger /var/log/postgres.log.2.gz /var/log/postgres.log.1.gz /var/log/postgres.log
pgbadger /var/log/postgresql/postgresql-2012-05-*
pgbadger --exclude-query=&quot;^(COPY|COMMIT)&quot; /var/log/postgresql.log
pgbadger -b &quot;2012-06-25 10:56:11&quot; -e &quot;2012-06-25 10:59:11&quot; /var/log/postgresql.log
cat /var/log/postgres.log | pgbadger -
# Log line prefix with stderr log output
pgbadger --prefix &#39;%t [%p]: user=%u,db=%d,client=%h&#39; /pglog/postgresql-2012-08-21*
pgbadger --prefix &#39;%m %u@%d %p %r %a : &#39; /pglog/postgresql.log
# Log line prefix with syslog log output
pgbadger --prefix &#39;user=%u,db=%d,client=%h,appname=%a&#39; /pglog/postgresql-2012-08-21*
# Use my 8 CPUs to parse my 10GB file faster, much faster
pgbadger -j 8 /pglog/postgresql-10.1-main.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use URI notation for remote log file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger http://172.12.110.1//var/log/postgresql/postgresql-10.1-main.log
pgbadger ftp://username@172.12.110.14/postgresql-10.1-main.log
pgbadger ssh://username@172.12.110.14:2222//var/log/postgresql/postgresql-10.1-main.log*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can use together a local PostgreSQL log and a remote pgbouncer log file to parse:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger /var/log/postgresql/postgresql-10.1-main.log ssh://username@172.12.110.14/pgbouncer.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Reporting errors every week by cron job:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;30 23 * * 1 /usr/bin/pgbadger -q -w /var/log/postgresql.log -o /var/reports/pg_errors.html
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generate report every week using incremental behavior:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0 4 * * 1 /usr/bin/pgbadger -q `find /var/log/ -mtime -7 -name &quot;postgresql.log*&quot;` -o /var/reports/pg_errors-`date +\%F`.html -l /var/reports/pgbadger_incremental_file.dat
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This supposes that your log file and HTML report are also rotated every week.&lt;/p&gt; 
&lt;p&gt;Or better, use the auto-generated incremental reports:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0 4 * * * /usr/bin/pgbadger -I -q /var/log/postgresql/postgresql.log.1 -O /var/www/pg_reports/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will generate a report per day and per week.&lt;/p&gt; 
&lt;p&gt;In incremental mode, you can also specify the number of weeks to keep in the reports:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;/usr/bin/pgbadger --retention 2 -I -q /var/log/postgresql/postgresql.log.1 -O /var/www/pg_reports/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you have a pg_dump at 23:00 and 13:00 each day during half an hour, you can use pgBadger as follow to exclude these periods from the report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger --exclude-time &quot;2013-09-.* (23|13):.*&quot; postgresql.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will help avoid having COPY statements, as generated by pg_dump, on top of the list of slowest queries. You can also use --exclude-appname &quot;pg_dump&quot; to solve this problem in a simpler way.&lt;/p&gt; 
&lt;p&gt;You can also parse journalctl output just as if it was a log file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger --journalctl &#39;journalctl -u postgresql-9.5&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or worst, call it from a remote host:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger -r 192.168.1.159 --journalctl &#39;journalctl -u postgresql-9.5&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;you don&#39;t need to specify any log file at command line, but if you have other PostgreSQL log files to parse, you can add them as usual.&lt;/p&gt; 
&lt;p&gt;To rebuild all incremental html reports after, proceed as follow:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;rm /path/to/reports/*.js
rm /path/to/reports/*.css
pgbadger -X -I -O /path/to/reports/ --rebuild
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;it will also update all resource files (JS and CSS). Use -E or --explode if the reports were built using this option.&lt;/p&gt; 
&lt;p&gt;pgBadger also supports Heroku PostgreSQL logs using logplex format:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;heroku logs -p postgres | pgbadger -f logplex -o heroku.html -
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;this will stream Heroku PostgreSQL log to pgbadger through stdin.&lt;/p&gt; 
&lt;p&gt;pgBadger can auto detect RDS and cloudwatch PostgreSQL logs using rds format:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger -f rds -o rds_out.html rds.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Each CloudSQL Postgresql log is a fairly normal PostgreSQL log, but encapsulated in JSON format. It is autodetected by pgBadger but in case you need to force the log format use `jsonlog`:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger -f jsonlog -o cloudsql_out.html cloudsql.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is the same as with the jsonlog extension, the json format is different but pgBadger can parse both formats.&lt;/p&gt; 
&lt;p&gt;pgBadger also supports logs produced by CloudNativePG Postgres operator for Kubernetes:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger -f jsonlog -o cnpg_out.html cnpg.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To create a cumulative report over a month use command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger --month-report 2919-05 /path/to/incremental/reports/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;this will add a link to the month name into the calendar view in incremental reports to look at report for month 2019 May. Use -E or --explode if the reports were built using this option.&lt;/p&gt; 
&lt;h3&gt;DESCRIPTION&lt;/h3&gt; 
&lt;p&gt;pgBadger is a PostgreSQL log analyzer built for speed providing fully detailed reports based on your PostgreSQL log files. It&#39;s a small standalone Perl script that outperforms any other PostgreSQL log analyzer.&lt;/p&gt; 
&lt;p&gt;It is written in pure Perl and uses a JavaScript library (flotr2) to draw graphs so that you don&#39;t need to install any additional Perl modules or other packages. Furthermore, this library gives us more features such as zooming. pgBadger also uses the Bootstrap JavaScript library and the FontAwesome webfont for better design. Everything is embedded.&lt;/p&gt; 
&lt;p&gt;pgBadger is able to autodetect your log file format (syslog, stderr, csvlog or jsonlog) if the file is long enough. It is designed to parse huge log files as well as compressed files. Supported compressed formats are gzip, bzip2, lz4, xz, zip and zstd. For the xz format you must have an xz version higher than 5.05 that supports the --robot option. lz4 files must be compressed with the --content-size option for pgbadger to determine the uncompressed file size. For the complete list of features, see below.&lt;/p&gt; 
&lt;p&gt;All charts are zoomable and can be saved as PNG images.&lt;/p&gt; 
&lt;p&gt;You can also limit pgBadger to only report errors or remove any part of the report using command-line options.&lt;/p&gt; 
&lt;p&gt;pgBadger supports any custom format set in the log_line_prefix directive of your postgresql.conf file as long as it at least specifies the %t and %p patterns.&lt;/p&gt; 
&lt;p&gt;pgBadger allows parallel processing of a single log file or multiple files through the use of the -j option specifying the number of CPUs.&lt;/p&gt; 
&lt;p&gt;If you want to save system performance you can also use log_duration instead of log_min_duration_statement to have reports on duration and number of queries only.&lt;/p&gt; 
&lt;h3&gt;FEATURE&lt;/h3&gt; 
&lt;p&gt;pgBadger reports everything about your SQL queries:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    Overall statistics.
    The most frequent waiting queries.
    Queries that waited the most.
    Queries generating the most temporary files.
    Queries generating the largest temporary files.
    The slowest queries.
    Queries that took up the most time.
    The most frequent queries.
    The most frequent errors.
    Histogram of query times.
    Histogram of sessions times.
    Users involved in top queries.
    Applications involved in top queries.
    Queries generating the most cancellation.
    Queries most cancelled.
    The most time consuming prepare/bind queries
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The following reports are also available with hourly charts divided into periods of five minutes:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    SQL queries statistics.
    Temporary file statistics.
    Checkpoints statistics.
    Autovacuum and autoanalyze statistics.
    Cancelled queries.
    Error events (panic, fatal, error and warning).
    Error class distribution.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are also some pie charts about distribution of:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    Locks statistics.
    Queries by type (select/insert/update/delete).
    Distribution of queries type per database/application
    Sessions per database/user/client/application.
    Connections per database/user/client/application.
    Autovacuum and autoanalyze per table.
    Queries per user and total duration per user.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;All charts are zoomable and can be saved as PNG images. SQL queries reported are highlighted and beautified automatically.&lt;/p&gt; 
&lt;p&gt;pgBadger is also able to parse PgBouncer log files and to create the following reports:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    Request Throughput
    Bytes I/O Throughput
    Average Query Duration
    Simultaneous sessions
    Histogram of sessions times
    Sessions per database
    Sessions per user
    Sessions per host
    Established connections
    Connections per database
    Connections per user
    Connections per host
    Most used reserved pools
    Most Frequent Errors/Events
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also have incremental reports with one report per day and a cumulative report per week. Two multiprocess modes are available to speed up log parsing, one using one core per log file, and the second using multiple cores to parse a single file. These modes can be combined.&lt;/p&gt; 
&lt;p&gt;Histogram granularity can be adjusted using the -A command-line option. By default, they will report the mean of each top queries/errors occurring per hour, but you can specify the granularity down to the minute.&lt;/p&gt; 
&lt;p&gt;pgBadger can also be used in a central place to parse remote log files using a passwordless SSH connection. This mode can be used with compressed files and in the multiprocess per file mode (-J), but cannot be used with the CSV log format.&lt;/p&gt; 
&lt;p&gt;Examples of reports can be found here: &lt;a href=&quot;https://pgbadger.darold.net/#reports&quot;&gt;https://pgbadger.darold.net/#reports&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;REQUIREMENT&lt;/h3&gt; 
&lt;p&gt;pgBadger comes as a single Perl script - you do not need anything other than a modern Perl distribution. Charts are rendered using a JavaScript library, so you don&#39;t need anything other than a web browser. Your browser will do all the work.&lt;/p&gt; 
&lt;p&gt;If you plan to parse PostgreSQL CSV log files, you might need some Perl Modules:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    Text::CSV_XS - to parse PostgreSQL CSV log files.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This module is optional, if you don&#39;t have PostgreSQL log in the CSV format, you don&#39;t need to install it.&lt;/p&gt; 
&lt;p&gt;If you want to export statistics as JSON file, you need an additional Perl module:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    JSON::XS - JSON serialising/deserialising, done correctly and fast
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This module is optional, if you don&#39;t select the json output format, you don&#39;t need to install it. You can install it on a Debian-like system using:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    sudo apt-get install libjson-xs-perl
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and on RPM-like system using:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    sudo yum install perl-JSON-XS
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Compressed log file format is autodetected from the file extension. If pgBadger finds a gz extension, it will use the zcat utility; with a bz2 extension, it will use bzcat; with lz4, it will use lz4cat; with zst, it will use zstdcat; if the file extension is zip or xz, then the unzip or xz utility will be used.&lt;/p&gt; 
&lt;p&gt;If those utilities are not found in the PATH environment variable, then use the --zcat command-line option to change this path. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    --zcat=&quot;/usr/local/bin/gunzip -c&quot; or --zcat=&quot;/usr/local/bin/bzip2 -dc&quot;
    --zcat=&quot;C:\tools\unzip -p&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, pgBadger will use the zcat, bzcat, lz4cat, zstdcat and unzip utilities following the file extension. If you use the default autodetection of compression format, you can mix gz, bz2, lz4, xz, zip or zstd files. Specifying a custom value of --zcat option will remove the possibility of mixed compression format.&lt;/p&gt; 
&lt;p&gt;Note that multiprocessing cannot be used with compressed files or CSV files as well as under Windows platform.&lt;/p&gt; 
&lt;h3&gt;INSTALLATION&lt;/h3&gt; 
&lt;p&gt;Download the tarball from GitHub and unpack the archive as follow:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    tar xzf pgbadger-11.x.tar.gz
    cd pgbadger-11.x/
    perl Makefile.PL
    make &amp;amp;&amp;amp; sudo make install
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will copy the Perl script pgbadger to /usr/local/bin/pgbadger by default and the man page into /usr/local/share/man/man1/pgbadger.1. Those are the default installation directories for &#39;site&#39; install.&lt;/p&gt; 
&lt;p&gt;If you want to install all under /usr/ location, use INSTALLDIRS=&#39;perl&#39; as an argument of Makefile.PL. The script will be installed into /usr/bin/pgbadger and the manpage into /usr/share/man/man1/pgbadger.1.&lt;/p&gt; 
&lt;p&gt;For example, to install everything just like Debian does, proceed as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    perl Makefile.PL INSTALLDIRS=vendor
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;By default, INSTALLDIRS is set to site.&lt;/p&gt; 
&lt;h3&gt;POSTGRESQL CONFIGURATION&lt;/h3&gt; 
&lt;p&gt;You must enable and set some configuration directives in your postgresql.conf before starting.&lt;/p&gt; 
&lt;p&gt;You must first enable SQL query logging to have something to parse:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_min_duration_statement = 0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here every statement will be logged, on a busy server you may want to increase this value to only log queries with a longer duration. Note that if you have log_statement set to &#39;all&#39;, nothing will be logged through the log_min_duration_statement directive. See the next chapter for more information.&lt;/p&gt; 
&lt;p&gt;pgBadger supports any custom format set in the log_line_prefix directive of your postgresql.conf file as long as it at least specifies a time escape sequence (%t, %m or %n) and a process-related escape sequence (%p or %c).&lt;/p&gt; 
&lt;p&gt;For example, with &#39;stderr&#39; log format, log_line_prefix must be at least:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_line_prefix = &#39;%t [%p]: &#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Log line prefix could add user, database name, application name and client ip address as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_line_prefix = &#39;%t [%p]: user=%u,db=%d,app=%a,client=%h &#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or for syslog log file format:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_line_prefix = &#39;user=%u,db=%d,app=%a,client=%h &#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Log line prefix for stderr output could also be:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_line_prefix = &#39;%t [%p]: db=%d,user=%u,app=%a,client=%h &#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;or for syslog output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_line_prefix = &#39;db=%d,user=%u,app=%a,client=%h &#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You need to enable other parameters in postgresql.conf to get more information from your log files:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    log_checkpoints = on
    log_connections = on
    log_disconnections = on
    log_lock_waits = on
    log_temp_files = 0
    log_autovacuum_min_duration = 0
    log_error_verbosity = default
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Do not enable log_statement as its log format will not be parsed by pgBadger.&lt;/p&gt; 
&lt;p&gt;Of course your log messages should be in English with or without locale support:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    lc_messages=&#39;en_US.UTF-8&#39;
    lc_messages=&#39;C&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;pgBadger parser does not support other locales, like &#39;fr_FR.UTF-8&#39; for example.&lt;/p&gt; 
&lt;h3&gt;LOG STATEMENTS&lt;/h3&gt; 
&lt;p&gt;Considerations about log_min_duration_statement, log_duration and log_statement configuration directives.&lt;/p&gt; 
&lt;p&gt;If you want the query statistics to include the actual query strings, you must set log_min_duration_statement to 0 or more milliseconds.&lt;/p&gt; 
&lt;p&gt;If you just want to report duration and number of queries and don&#39;t want all details about queries, set log_min_duration_statement to -1 to disable it and enable log_duration in your postgresql.conf file. If you want to add the most common query report, you can either choose to set log_min_duration_statement to a higher value or to enable log_statement.&lt;/p&gt; 
&lt;p&gt;Enabling log_min_duration_statement will add reports about slowest queries and queries that took up the most time. Take care that if you have log_statement set to &#39;all&#39;, nothing will be logged with log_min_duration_statement.&lt;/p&gt; 
&lt;p&gt;Warning: Do not enable both log_min_duration_statement, log_duration and log_statement all together, this will result in wrong counter values. Note that this will also increase drastically the size of your log. log_min_duration_statement should always be preferred.&lt;/p&gt; 
&lt;h3&gt;PARALLEL PROCESSING&lt;/h3&gt; 
&lt;p&gt;To enable parallel processing you just have to use the -j N option where N is the number of cores you want to use.&lt;/p&gt; 
&lt;p&gt;pgBadger will then proceed as follow:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    for each log file
        chunk size = int(file size / N)
        look at start/end offsets of these chunks
        fork N processes and seek to the start offset of each chunk
            each process will terminate when the parser reach the end offset
            of its chunk
            each process write stats into a binary temporary file
        wait for all children processes to terminate
    All binary temporary files generated will then be read and loaded into
    memory to build the html output.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With that method, at start/end of chunks pgBadger may truncate or omit a maximum of N queries per log file, which is an insignificant gap if you have millions of queries in your log file. The chance that the query that you were looking for is lost is near 0, this is why I think this gap is livable. Most of the time the query is counted twice but truncated.&lt;/p&gt; 
&lt;p&gt;When you have many small log files and many CPUs, it is speedier to dedicate one core to one log file at a time. To enable this behavior, you have to use option -J N instead. With 200 log files of 10MB each, the use of the -J option starts being really interesting with 8 cores. Using this method you will be sure not to lose any queries in the reports.&lt;/p&gt; 
&lt;p&gt;Here is a benchmark done on a server with 8 CPUs and a single file of 9.5GB.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;     Option |  1 CPU  | 2 CPU | 4 CPU | 8 CPU
    --------+---------+-------+-------+------
       -j   | 1h41m18 | 50m25 | 25m39 | 15m58
       -J   | 1h41m18 | 54m28 | 41m16 | 34m45
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;With 200 log files of 10MB each, so 2GB in total, the results are slightly different:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;     Option | 1 CPU | 2 CPU | 4 CPU | 8 CPU
    --------+-------+-------+-------+------
       -j   | 20m15 |  9m56 |  5m20 | 4m20
       -J   | 20m15 |  9m49 |  5m00 | 2m40
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;So it is recommended to use -j unless you have hundreds of small log files and can use at least 8 CPUs.&lt;/p&gt; 
&lt;p&gt;IMPORTANT: when you are using parallel parsing, pgBadger will generate a lot of temporary files in the /tmp directory and will remove them at the end, so do not remove those files unless pgBadger is not running. They are all named with the following template tmp_pgbadgerXXXX.bin so they can be easily identified.&lt;/p&gt; 
&lt;h3&gt;INCREMENTAL REPORTS&lt;/h3&gt; 
&lt;p&gt;pgBadger includes an automatic incremental report mode using option -I or --incremental. When running in this mode, pgBadger will generate one report per day and a cumulative report per week. Output is first done in binary format into the mandatory output directory (see option -O or --outdir), then in HTML format for daily and weekly reports with a main index file.&lt;/p&gt; 
&lt;p&gt;The main index file will show a dropdown menu per week with a link to each week report and links to daily reports of each week.&lt;/p&gt; 
&lt;p&gt;For example, if you run pgBadger as follows based on a daily rotated file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;0 4 * * * /usr/bin/pgbadger -I -q /var/log/postgresql/postgresql.log.1 -O /var/www/pg_reports/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;you will have all daily and weekly reports for the full running period.&lt;/p&gt; 
&lt;p&gt;In this mode, pgBadger will create an automatic incremental file in the output directory, so you don&#39;t have to use the -l option unless you want to change the path of that file. This means that you can run pgBadger in this mode each day on a log file rotated each week, and it will not count the log entries twice.&lt;/p&gt; 
&lt;p&gt;To save disk space, you may want to use the -X or --extra-files command-line option to force pgBadger to write JavaScript and CSS to separate files in the output directory. The resources will then be loaded using script and link tags.&lt;/p&gt; 
&lt;h4&gt;Rebuilding reports&lt;/h4&gt; 
&lt;p&gt;Incremental reports can be rebuilt after a pgbadger report fix or a new feature to update all HTML reports. To rebuild all reports where a binary file is still present, proceed as follow:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;rm /path/to/reports/*.js
rm /path/to/reports/*.css
pgbadger -X -I -O /path/to/reports/ --rebuild
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;it will also update all resource files (JS and CSS). Use -E or --explode if the reports were built using this option.&lt;/p&gt; 
&lt;h4&gt;Monthly reports&lt;/h4&gt; 
&lt;p&gt;By default, pgBadger in incremental mode only computes daily and weekly reports. If you want monthly cumulative reports, you will have to use a separate command to specify the report to build. For example, to build a report for August 2019:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger -X --month-report 2019-08 /var/www/pg_reports/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;this will add a link to the month name into the calendar view of incremental reports to look at monthly report. The report for a current month can be run every day, it is entirely rebuilt each time. The monthly report is not built by default because it could take a lot of time following the amount of data.&lt;/p&gt; 
&lt;p&gt;If reports were built with the per-database option ( -E | --explode ), it must be used too when calling pgbadger to build monthly report:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger -E -X --month-report 2019-08 /var/www/pg_reports/
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This is the same when using the rebuild option ( -R | --rebuild ).&lt;/p&gt; 
&lt;h3&gt;BINARY FORMAT&lt;/h3&gt; 
&lt;p&gt;Using the binary format it is possible to create custom incremental and cumulative reports. For example, if you want to refresh a pgBadger report each hour from a daily PostgreSQL log file, you can proceed by running the following commands each hour:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger --last-parsed .pgbadger_last_state_file -o sunday/hourX.bin /var/log/pgsql/postgresql-Sun.log
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;to generate the incremental data files in binary format. And to generate the fresh HTML report from that binary file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;pgbadger sunday/*.bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or as another example, if you generate one log file per hour and you want reports to be rebuilt each time the log file is rotated, proceed as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    pgbadger -o day1/hour01.bin /var/log/pgsql/pglog/postgresql-2012-03-23_10.log
    pgbadger -o day1/hour02.bin /var/log/pgsql/pglog/postgresql-2012-03-23_11.log
    pgbadger -o day1/hour03.bin /var/log/pgsql/pglog/postgresql-2012-03-23_12.log
    ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;When you want to refresh the HTML report, for example, each time after a new binary file is generated, just do the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;    pgbadger -o day1_report.html day1/*.bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Adjust the commands to suit your particular needs.&lt;/p&gt; 
&lt;h3&gt;JSON FORMAT&lt;/h3&gt; 
&lt;p&gt;JSON format is good for sharing data with other languages, which makes it easy to integrate pgBadger result into other monitoring tools, like Cacti or Graphite.&lt;/p&gt; 
&lt;h3&gt;AUTHORS&lt;/h3&gt; 
&lt;p&gt;pgBadger is an original work from Gilles Darold.&lt;/p&gt; 
&lt;p&gt;The pgBadger logo is an original creation of Damien Cazeils.&lt;/p&gt; 
&lt;p&gt;The pgBadger v4.x design comes from the &quot;Art is code&quot; company.&lt;/p&gt; 
&lt;p&gt;This web site is a work of Gilles Darold.&lt;/p&gt; 
&lt;p&gt;pgBadger is maintained by Gilles Darold and everyone who wants to contribute.&lt;/p&gt; 
&lt;p&gt;Many people have contributed to pgBadger, they are all quoted in the Changelog file.&lt;/p&gt; 
&lt;h3&gt;LICENSE&lt;/h3&gt; 
&lt;p&gt;pgBadger is free software distributed under the PostgreSQL Licence.&lt;/p&gt; 
&lt;p&gt;Copyright (c) 2012-2025, Gilles Darold&lt;/p&gt; 
&lt;p&gt;A modified version of the SQL::Beautify Perl Module is embedded in pgBadger with copyright (C) 2009 by Jonas Kramer and is published under the terms of the Artistic License 2.0.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>fusioninventory/fusioninventory-agent</title>
      <link>https://github.com/fusioninventory/fusioninventory-agent</link>
      <description>&lt;p&gt;FusionInventory Agent&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FusionInventory Agent&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://travis-ci.org/fusioninventory/fusioninventory-agent&quot;&gt;&lt;img src=&quot;https://travis-ci.org/fusioninventory/fusioninventory-agent.svg?branch=develop&quot; alt=&quot;Travis Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://ci.appveyor.com/project/fusioninventory/fusioninventory-agent&quot;&gt;&lt;img src=&quot;https://ci.appveyor.com/api/projects/status/f2oh6p3qnr2bck1b?svg=true&quot; alt=&quot;Appveyor Build status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://circleci.com/gh/fusioninventory/fusioninventory-agent&quot;&gt;&lt;img src=&quot;https://circleci.com/gh/fusioninventory/fusioninventory-agent.svg?style=svg&quot; alt=&quot;CircleCI Build status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Summary&lt;/h2&gt; 
&lt;p&gt;The FusionInventory agent is a generic management agent. It can perform a certain number of tasks, according to its own execution plan, or on behalf of a GLPI server with fusioninventory plugin, acting as a control point.&lt;/p&gt; 
&lt;h2&gt;Description&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;http://fusioninventory.org/overview/&quot;&gt;FusionInventory solution overview&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;h3&gt;Core&lt;/h3&gt; 
&lt;p&gt;Minimum perl version: 5.8&lt;/p&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;File::Which&lt;/li&gt; 
 &lt;li&gt;LWP::UserAgent&lt;/li&gt; 
 &lt;li&gt;Net::IP&lt;/li&gt; 
 &lt;li&gt;Text::Template&lt;/li&gt; 
 &lt;li&gt;UNIVERSAL::require&lt;/li&gt; 
 &lt;li&gt;XML::TreePP&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Compress::Zlib, for message compression&lt;/li&gt; 
 &lt;li&gt;HTTP::Daemon, for web interface&lt;/li&gt; 
 &lt;li&gt;IO::Socket::SSL, for HTTPS support&lt;/li&gt; 
 &lt;li&gt;LWP::Protocol::https, for HTTPS support&lt;/li&gt; 
 &lt;li&gt;Proc::Daemon, for daemon mode (Unix only)&lt;/li&gt; 
 &lt;li&gt;Proc::PID::File, for daemon mode (Unix only)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Inventory task&lt;/h3&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::CUPS, for printers detection&lt;/li&gt; 
 &lt;li&gt;Parse::EDID, for EDID data parsing&lt;/li&gt; 
 &lt;li&gt;DateTime, for reliable timezone name extraction&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional programs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;dmidecode, for DMI data retrieval&lt;/li&gt; 
 &lt;li&gt;lspci, for PCI bus scanning&lt;/li&gt; 
 &lt;li&gt;hdparm, for additional disk drive info retrieval&lt;/li&gt; 
 &lt;li&gt;monitor-get-edid-using-vbe, monitor-get-edid or get-edid, for EDID data access&lt;/li&gt; 
 &lt;li&gt;ssh-keyscan, for host SSH public key retrieval&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Network discovery tasks&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Thread::Queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::NBName, for NetBios method support&lt;/li&gt; 
 &lt;li&gt;Net::SNMP, for SNMP method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional programs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;arp, for arp table lookup method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Network inventory tasks&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::SNMP&lt;/li&gt; 
 &lt;li&gt;Thread::Queue&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Crypt::DES, for SNMPv3 support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Wake on LAN task&lt;/h3&gt; 
&lt;p&gt;Optional Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::Write::Layer2, for ethernet method support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Deploy task&lt;/h3&gt; 
&lt;p&gt;Mandatory Perl modules:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Archive::Extract&lt;/li&gt; 
 &lt;li&gt;Digest::SHA&lt;/li&gt; 
 &lt;li&gt;File::Copy::Recursive&lt;/li&gt; 
 &lt;li&gt;JSON::PP&lt;/li&gt; 
 &lt;li&gt;URI::Escape&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Mandatory Perl modules for P2P Support:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Net::Ping&lt;/li&gt; 
 &lt;li&gt;Parallel::ForkManager&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Related contribs&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/fusioninventory/fusioninventory-agent/develop/CONTRIB.md&quot;&gt;CONTRIB&lt;/a&gt; to find references to FusionInventory Agent related scritps/files&lt;/p&gt; 
&lt;h2&gt;Contacts&lt;/h2&gt; 
&lt;p&gt;Project websites:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;main site: &lt;a href=&quot;http://www.fusioninventory.org&quot;&gt;http://www.fusioninventory.org&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Github org: &lt;a href=&quot;http://github.com/fusioninventory/&quot;&gt;http://github.com/fusioninventory/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project mailing lists:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://lists.alioth.debian.org/mailman/listinfo/fusioninventory-user&quot;&gt;http://lists.alioth.debian.org/mailman/listinfo/fusioninventory-user&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://lists.alioth.debian.org/mailman/listinfo/fusioninventory-devel&quot;&gt;http://lists.alioth.debian.org/mailman/listinfo/fusioninventory-devel&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Project IRC channel:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;#FusionInventory on FreeNode IRC Network&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please report any issues on Github.&lt;/p&gt; 
&lt;h2&gt;Copyrights&lt;/h2&gt; 
&lt;p&gt;Copyright 2006-2010 &lt;a href=&quot;https://www.ocsinventory-ng.org/&quot;&gt;OCS Inventory contributors&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2010-2021 &lt;a href=&quot;https://fusioninventory.org&quot;&gt;FusionInventory Team&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyright 2011-2021 &lt;a href=&quot;https://www.teclib-edition.com/&quot;&gt;Teclib Editions&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This software is licensed under the terms of GPLv2+, see LICENSE file for details.&lt;/p&gt; 
&lt;h2&gt;Additional pieces of software&lt;/h2&gt; 
&lt;p&gt;The fusioninventory-injector script:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;author: Pascal Danek&lt;/li&gt; 
 &lt;li&gt;copyright: 2005 Pascal Danek&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;FusionInventory::Agent::Task::Inventory::Input::Virtualization::Vmsystem contains code from imvirt:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;url: &lt;a href=&quot;http://micky.ibh.net/~liske/imvirt.html&quot;&gt;http://micky.ibh.net/~liske/imvirt.html&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;author: Thomas Liske &lt;a href=&quot;mailto:liske@ibh.de&quot;&gt;liske@ibh.de&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;copyright: 2008 IBH IT-Service GmbH &lt;a href=&quot;http://www.ibh.de/&quot;&gt;http://www.ibh.de/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;License: GPLv2+&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>znuny/Znuny</title>
      <link>https://github.com/znuny/Znuny</link>
      <description>&lt;p&gt;Znuny is a free, open-source, and versatile web-based ticketing system for Customer Service, Help Desk, IT Service Management, and more. It is built for transparency and long-term sustainability and is highly tailorable to your organization&#39;s needs.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://www.znuny.org&quot;&gt;&lt;img align=&quot;center&quot; src=&quot;https://raw.githubusercontent.com/znuny/Znuny/dev/var/httpd/htdocs/skins/Agent/default/img/logo.png&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://download.znuny.org/releases/znuny-latest-7.2.tar.gz&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/tag/znuny/Znuny?filter=rel-7_2_*&amp;amp;label=latest%20release&amp;amp;color=ff9b00&quot; /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/actions&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/actions/workflow/status/znuny/Znuny/ci.yaml?label=CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/actions&quot;&gt;&lt;img src=&quot;https://badge.proxy.znuny.com/Znuny/rel-7_2&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://translations.znuny.org/engage/znuny/&quot;&gt;&lt;img src=&quot;https://translations.znuny.org/widgets/znuny/-/znuny/svg-badge.svg?sanitize=true&quot; alt=&quot;Translation status&quot; /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-raw/znuny/Znuny?&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/issues?q=is%3Aissue+is%3Aclosed&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-closed-raw/znuny/Znuny?color=#44CC44&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/pulls&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr-raw/znuny/Znuny?&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/pulls?q=is%3Apr+is%3Aclosed&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues-pr-closed-raw/znuny/Znuny?color=brightgreen&quot; /&gt;&lt;/a&gt; &lt;br /&gt; &lt;a href=&quot;https://github.com/znuny/Znuny&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/languages/count/znuny/Znuny?style=flat&amp;amp;label=languages&amp;amp;color=brightgreen&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/znuny/Znuny/graphs/contributors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/contributors/znuny/Znuny?&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/XTud3WWZTs&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/836533233885773825?style=flat&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h1&gt;Znuny&lt;/h1&gt; 
&lt;p&gt;Znuny is a continuation of the ((OTRS)) Community Edition (version 6.0.30) which was declared end of life (EOL) at the end of December 2020.&lt;/p&gt; 
&lt;p&gt;The primary goal for this project is to provide a maintained and stable version of the well known ticket system and improve it with new features.&lt;/p&gt; 
&lt;p&gt;The second goal is to reestablish a connection to the community.&lt;/p&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;The project is distributed under the GNU General Public License (GPL v3) - see the accompanying &lt;a href=&quot;https://raw.githubusercontent.com/znuny/Znuny/dev/COPYING&quot;&gt;COPYING&lt;/a&gt; file for general license information. If you need more details you can have a look &lt;a href=&quot;https://snyk.io/learn/what-is-gpl-license-gplv3-explained/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Documentation&lt;/h1&gt; 
&lt;p&gt;You can find documentation &lt;a href=&quot;https://doc.znuny.org/&quot;&gt;here&lt;/a&gt;. The source code of Znuny is publicly available on &lt;a href=&quot;https://github.com/znuny/znuny&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You want to get in touch?&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.znuny.org&quot;&gt;Project website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://community.znuny.org&quot;&gt;Community forum&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/XTud3WWZTs&quot;&gt;Discord Server&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.znuny.com&quot;&gt;Commercial services&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Software requirements&lt;/h1&gt; 
&lt;p&gt;Operating system&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Linux (Debian or Red Hat preferred)&lt;/li&gt; 
 &lt;li&gt;Perl 5.16.0 or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Web server&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Apache 2 + mod_perl2 or higher (recommended)&lt;/li&gt; 
 &lt;li&gt;Web server with CGI support (CGI is not recommended)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Databases&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MySQL 8.0 or higher&lt;/li&gt; 
 &lt;li&gt;MariaDB 10.3 or higher&lt;/li&gt; 
 &lt;li&gt;PostgreSQL 12.0 or higher&lt;/li&gt; 
 &lt;li&gt;Oracle 19c or higher&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Browsers&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;These browsers are NOT supported: 
  &lt;ul&gt; 
   &lt;li&gt;Internet Explorer before version 11&lt;/li&gt; 
   &lt;li&gt;Firefox before version 31&lt;/li&gt; 
   &lt;li&gt;Safari before version 6&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Vendor&lt;/h1&gt; 
&lt;p&gt;This project is mainly funded by Znuny GmbH, Berlin. If you need professional support or consulting, feel free to contact us.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.znuny.com&quot;&gt;Znuny Website&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>acl-org/ACLPUB</title>
      <link>https://github.com/acl-org/ACLPUB</link>
      <description>&lt;p&gt;The official tool for creating proceedings for conferences of the Association for Computational Linguistics (ACL).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ACLPUB package&lt;/h1&gt; 
&lt;p&gt;This is the official ACLPUB package for *ACL conferences. Its primary role is to package up the PDF files, BibTeX files, and optional extra files into a format that can be ingested by the &lt;a href=&quot;https://www.aclweb.org/anthology/&quot;&gt;ACL Anthology&lt;/a&gt;. Documentation can found under &lt;code&gt;docs/&lt;/code&gt; and view &lt;a href=&quot;https://acl-org.github.io/ACLPUB/&quot;&gt;on the web&lt;/a&gt;. The latest version can always be found &lt;a href=&quot;https://github.com/acl-org/ACLPUB&quot;&gt;on Github&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Instructions for START&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;http://softconf.com&quot;&gt;Softconf&#39;s&lt;/a&gt; &lt;a href=&quot;http://softconf.com/about/start-v2-mainmenu-26&quot;&gt;STARTv2&lt;/a&gt; system is the main system used for conference management within the ACL community. It includes extensive integration around the ACLPUB package. Information about how to use ACLPUB within START &lt;a href=&quot;https://acl-org.github.io/ACLPUB/start.html&quot;&gt;can be found here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;ACLPUB can also be run from the command line, which facilitates use with third-party conference management software.&lt;/p&gt; 
&lt;h2&gt;Instructions for submitting to the Anthology&lt;/h2&gt; 
&lt;p&gt;Instructions for submitting proceedings to the Anthology &lt;a href=&quot;https://acl-org.github.io/ACLPUB/anthology.html&quot;&gt;can be found here&lt;/a&gt;. These instructions were simplified in March of 2020 to accommodate the Anthology&#39;s &lt;a href=&quot;https://www.aclweb.org/anthology/info/ids/&quot;&gt;new ID format&lt;/a&gt;). For more complete information on the overall process, please see the Anthology&#39;s &lt;a href=&quot;https://www.aclweb.org/anthology/info/contrib/&quot;&gt;Information for Submitters&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Branch Convention&lt;/h2&gt; 
&lt;p&gt;The following branches have special import:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;code&gt;master&lt;/code&gt; branch is used for main development and contains the official stable release.&lt;/li&gt; 
 &lt;li&gt;The &lt;code&gt;start&lt;/code&gt; branch reflects the current code being used in START. It is brought in sync with &lt;code&gt;master&lt;/code&gt; at regular intervals.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;History&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;2005: The ACLPUB package and documentation were built in by Jason Eisner and Philipp Koehn, based in part on scripts by David Yarowsky that had been used for several years previously.&lt;/li&gt; 
 &lt;li&gt;2019: the code underwent substantial modernization and revision by David Chiang and Dan Gildea.&lt;/li&gt; 
 &lt;li&gt;2020: revisions were put in place to work with the Anthology&#39;s &lt;a href=&quot;https://www.aclweb.org/anthology/info/ids/&quot;&gt;new ID format&lt;/a&gt; by Matt Post.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>tseemann/prokka</title>
      <link>https://github.com/tseemann/prokka</link>
      <description>&lt;p&gt;⚡ ♒ Rapid prokaryotic genome annotation&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://travis-ci.org/tseemann/prokka&quot;&gt;&lt;img src=&quot;https://travis-ci.org/tseemann/prokka.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.gnu.org/licenses/gpl-3.0&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-GPL%20v3-blue.svg?sanitize=true&quot; alt=&quot;License: GPL v3&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://doi.org/10.1093/bioinformatics/btu153&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/DOI/10.1093/bioinformatics/btu153.svg?sanitize=true&quot; alt=&quot;DOI:10.1093/bioinformatics/btu153&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/Language-Perl_5-steelblue.svg?sanitize=true&quot; alt=&quot;Don&#39;t judge me&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Prokka: rapid prokaryotic genome annotation&lt;/h1&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;Whole genome annotation is the process of identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;h3&gt;Bioconda&lt;/h3&gt; 
&lt;p&gt;If you use &lt;a href=&quot;https://conda.io/docs/install/quick.html&quot;&gt;Conda&lt;/a&gt; you can use the &lt;a href=&quot;https://bioconda.github.io/&quot;&gt;Bioconda channel&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;conda install -c conda-forge -c bioconda -c defaults prokka
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Brew&lt;/h3&gt; 
&lt;p&gt;If you are using the &lt;a href=&quot;http://brew.sh/&quot;&gt;MacOS Brew&lt;/a&gt; or &lt;a href=&quot;http://brew.sh/linuxbrew/&quot;&gt;LinuxBrew&lt;/a&gt; packaging system:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;brew install brewsci/bio/prokka
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker&lt;/h3&gt; 
&lt;p&gt;Maintained by &lt;a href=&quot;https://hub.docker.com/u/staphb&quot;&gt;https://hub.docker.com/u/staphb&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;
docker pull staphb/prokka:latest
docker run staphb/prokka:latest prokka -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Singularity&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;singularity build prokka.sif docker://staphb/prokka:latest
singularity exec prokka.sif prokka -h
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu/Debian/Mint&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo apt-get install libdatetime-perl libxml-simple-perl libdigest-md5-perl git default-jre bioperl
sudo cpan Bio::Perl
git clone https://github.com/tseemann/prokka.git $HOME/prokka
$HOME/prokka/bin/prokka --setupdb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Centos/Fedora/RHEL&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo yum install git perl-Time-Piece perl-XML-Simple perl-Digest-MD5 perl-App-cpanminus git java perl-CPAN perl-Module-Build
sudo cpanm Bio::Perl
git clone https://github.com/tseemann/prokka.git $HOME/prokka
$HOME/prokka/bin/prokka --setupdb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;MacOS&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;sudo cpan Time::Piece XML::Simple Digest::MD5 Bio::Perl
git clone https://github.com/tseemann/prokka.git $HOME/prokka
$HOME/prokka/bin/prokka --setupdb
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Test&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Type &lt;code&gt;prokka&lt;/code&gt; and it should output its help screen.&lt;/li&gt; 
 &lt;li&gt;Type &lt;code&gt;prokka --version&lt;/code&gt; and you should see an output like &lt;code&gt;prokka 1.x&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Type &lt;code&gt;prokka --listdb&lt;/code&gt; and it will show you what databases it has installed to use.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Invoking Prokka&lt;/h2&gt; 
&lt;h3&gt;Beginner&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Vanilla (but with free toppings)
% prokka contigs.fa

# Look for a folder called PROKKA_yyyymmdd (today&#39;s date) and look at stats
% cat PROKKA_yyyymmdd/*.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Moderate&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Choose the names of the output files
% prokka --outdir mydir --prefix mygenome contigs.fa

# Visualize it in Artemis
% art mydir/mygenome.gff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Specialist&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Have curated genomes I want to use to annotate from
% prokka --proteins MG1655.gbk --outdir mutant --prefix K12_mut contigs.fa

# Look at tabular features
% less -S mutant/K12_mut.tsv
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Expert&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# It&#39;s not just for bacteria, people
% prokka --kingdom Archaea --outdir mydir --genus Pyrococcus --locustag PYCC

# Search for your favourite gene
% exonerate --bestn 1 zetatoxin.fasta mydir/PYCC_06072012.faa | less
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Wizard&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Watch and learn
% prokka --outdir mydir --locustag EHEC --proteins NewToxins.faa --evalue 0.001 --gram neg --addgenes contigs.fa

# Check to see if anything went really wrong
% less mydir/EHEC_06072012.err

# Add final details using Sequin
% sequin mydir/EHEC_0607201.sqn
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;NCBI Genbank submitter&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Register your BioProject (e.g. PRJNA123456) and your locus_tag prefix (e.g. EHEC) first!
% prokka --compliant --centre UoN --outdir PRJNA123456 --locustag EHEC --prefix EHEC-Chr1 contigs.fa

# Check to see if anything went really wrong
% less PRJNA123456/EHEC-Chr1.err

# Add final details using Sequin
% sequin PRJNA123456/EHEC-Chr1.sqn
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;European Nucleotide Archive (ENA) submitter&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# Register your BioProject (e.g. PRJEB12345) and your locus_tag (e.g. EHEC) prefix first!
% prokka --compliant --centre UoN --outdir PRJEB12345 --locustag EHEC --prefix EHEC-Chr1 contigs.fa

# Check to see if anything went really wrong
% less PRJNA123456/EHEC-Chr1.err

# Install and run Sanger Pathogen group&#39;s Prokka GFF3 to EMBL converter
# available from https://github.com/sanger-pathogens/gff3toembl
# Find the closest NCBI taxonomy id (e.g. 562 for Escherichia coli)
% gff3_to_embl -i &quot;Submitter, A.&quot; \
    -m &quot;Escherichia coli EHEC annotated using Prokka.&quot; \
    -g linear -c PROK -n 11 -f PRJEB12345/EHEC-Chr1.embl \
    &quot;Escherichia coli&quot; 562 PRJEB12345 &quot;Escherichia coli strain EHEC&quot; PRJEB12345/EHEC-Chr1.gff

# Download and run the latest EMBL validator prior to submitting the EMBL flat file
# from http://central.maven.org/maven2/uk/ac/ebi/ena/sequence/embl-api-validator/
# which at the time of writing is v1.1.129
% curl -L -O http://central.maven.org/maven2/uk/ac/ebi/ena/sequence/embl-api-validator/1.1.129/embl-api-validator-1.1.129.jar
% java -jar embl-api-validator-1.1.129.jar -r PRJEB12345/EHEC-Chr1.embl

# Compress the file ready to upload to ENA, and calculate MD5 checksum
% gzip PRJEB12345/EHEC-Chr1.embl
% md5sum PRJEB12345/EHEC-Chr1.embl.gz
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Crazy Person&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;# No stinking Perl script is going to control me
% prokka \
        --outdir $HOME/genomes/Ec_POO247 --force \
        --prefix Ec_POO247 --addgenes --locustag ECPOOp \
        --increment 10 --gffver 2 --centre CDC  --compliant \
        --genus Escherichia --species coli --strain POO247 --plasmid pECPOO247 \
        --kingdom Bacteria --gcode 11 --usegenus \
        --proteins /opt/prokka/db/trusted/Ecocyc-17.6 \
        --evalue 1e-9 --rfam \
        plasmid-closed.fna
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Output Files&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Extension&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.gff&lt;/td&gt; 
   &lt;td&gt;This is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.gbk&lt;/td&gt; 
   &lt;td&gt;This is a standard Genbank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-Genbank, with one record for each sequence.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.fna&lt;/td&gt; 
   &lt;td&gt;Nucleotide FASTA file of the input contig sequences.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.faa&lt;/td&gt; 
   &lt;td&gt;Protein FASTA file of the translated CDS sequences.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.ffn&lt;/td&gt; 
   &lt;td&gt;Nucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.sqn&lt;/td&gt; 
   &lt;td&gt;An ASN1 format &quot;Sequin&quot; file for submission to Genbank. It needs to be edited to set the correct taxonomy, authors, related publication etc.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.fsa&lt;/td&gt; 
   &lt;td&gt;Nucleotide FASTA file of the input contig sequences, used by &quot;tbl2asn&quot; to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.tbl&lt;/td&gt; 
   &lt;td&gt;Feature Table file, used by &quot;tbl2asn&quot; to create the .sqn file.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.err&lt;/td&gt; 
   &lt;td&gt;Unacceptable annotations - the NCBI discrepancy report.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.log&lt;/td&gt; 
   &lt;td&gt;Contains all the output that Prokka produced during its run. This is a record of what settings you used, even if the --quiet option was enabled.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.txt&lt;/td&gt; 
   &lt;td&gt;Statistics relating to the annotated features found.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;.tsv&lt;/td&gt; 
   &lt;td&gt;Tab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Command line options&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;General:
  --help            This help
  --version         Print version and exit
  --citation        Print citation for referencing Prokka
  --quiet           No screen output (default OFF)
  --debug           Debug mode: keep all temporary files (default OFF)
Setup:
  --listdb          List all configured databases
  --setupdb         Index all installed databases
  --cleandb         Remove all database indices
  --depends         List all software dependencies
Outputs:
  --outdir [X]      Output folder [auto] (default &#39;&#39;)
  --force           Force overwriting existing output folder (default OFF)
  --prefix [X]      Filename output prefix [auto] (default &#39;&#39;)
  --addgenes        Add &#39;gene&#39; features for each &#39;CDS&#39; feature (default OFF)
  --locustag [X]    Locus tag prefix (default &#39;PROKKA&#39;)
  --increment [N]   Locus tag counter increment (default &#39;1&#39;)
  --gffver [N]      GFF version (default &#39;3&#39;)
  --compliant       Force Genbank/ENA/DDJB compliance: --genes --mincontiglen 200 --centre XXX (default OFF)
  --centre [X]      Sequencing centre ID. (default &#39;&#39;)
Organism details:
  --genus [X]       Genus name (default &#39;Genus&#39;)
  --species [X]     Species name (default &#39;species&#39;)
  --strain [X]      Strain name (default &#39;strain&#39;)
  --plasmid [X]     Plasmid name or identifier (default &#39;&#39;)
Annotations:
  --kingdom [X]     Annotation mode: Archaea|Bacteria|Mitochondria|Viruses (default &#39;Bacteria&#39;)
  --gcode [N]       Genetic code / Translation table (set if --kingdom is set) (default &#39;0&#39;)
  --prodigaltf [X]  Prodigal training file (default &#39;&#39;)
  --gram [X]        Gram: -/neg +/pos (default &#39;&#39;)
  --usegenus        Use genus-specific BLAST databases (needs --genus) (default OFF)
  --proteins [X]    Fasta file of trusted proteins to first annotate from (default &#39;&#39;)
  --hmms [X]        Trusted HMM to first annotate from (default &#39;&#39;)
  --metagenome      Improve gene predictions for highly fragmented genomes (default OFF)
  --rawproduct      Do not clean up /product annotation (default OFF)
Computation:
  --fast            Fast mode - skip CDS /product searching (default OFF)
  --cpus [N]        Number of CPUs to use [0=all] (default &#39;8&#39;)
  --mincontiglen [N] Minimum contig size [NCBI needs 200] (default &#39;1&#39;)
  --evalue [n.n]    Similarity e-value cut-off (default &#39;1e-06&#39;)
  --rfam            Enable searching for ncRNAs with Infernal+Rfam (SLOW!) (default &#39;0&#39;)
  --norrna          Don&#39;t run rRNA search (default OFF)
  --notrna          Don&#39;t run tRNA search (default OFF)
  --rnammer         Prefer RNAmmer over Barrnap for rRNA prediction (default OFF)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Option: --proteins&lt;/h3&gt; 
&lt;p&gt;The &lt;code&gt;--proteins&lt;/code&gt; option is recommended when you have good quality reference genomes and want to ensure gene naming is consistent. Some species use specific terminology which will be often lost if you rely on the default Swiss-Prot database included with Prokka.&lt;/p&gt; 
&lt;p&gt;If you have Genbank or Protein FASTA file(s) that you want to annotate genes from as the first priority, use the &lt;code&gt;--proteins myfile.gbk&lt;/code&gt;. Please make sure it has a recognisable file extension like &lt;code&gt;.gb&lt;/code&gt; or &lt;code&gt;.gbk&lt;/code&gt; or auto-detect will fail. The use of Genbank is recommended over FASTA, because it will provide &lt;code&gt;/gene&lt;/code&gt; and &lt;code&gt;/EC_number&lt;/code&gt; annotations that a typical &lt;code&gt;.faa&lt;/code&gt; file will not provide, unless you have specially formatted it for Prokka.&lt;/p&gt; 
&lt;h3&gt;Option: --prodigaltf&lt;/h3&gt; 
&lt;p&gt;Instead of letting &lt;code&gt;prodigal&lt;/code&gt; train its gene model on the contigs you provide, you can pre-train it on some good closed reference genomes first using the &lt;code&gt;prodigal -t&lt;/code&gt; option. Once you&#39;ve done that, provide &lt;code&gt;prokka&lt;/code&gt; the training file using the &lt;code&gt;--prodgialtf&lt;/code&gt; option.&lt;/p&gt; 
&lt;h3&gt;Option: --rawproduct&lt;/h3&gt; 
&lt;p&gt;Prokka annotates proteins by using sequence similarity to other proteins in its database, or the databases the user provides via &lt;code&gt;--proteins&lt;/code&gt;. By default, Prokka tries to &quot;cleans&quot; the &lt;code&gt;/product&lt;/code&gt; names to ensure they are compliant with Genbank/ENA conventions. Some of the main things it does is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;set vague names to &lt;code&gt;hypothetical protein&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;consistifies terms like &lt;code&gt;possible&lt;/code&gt;, &lt;code&gt;probable&lt;/code&gt;, &lt;code&gt;predicted&lt;/code&gt;, ... to &lt;code&gt;putative&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;removes EC, COG and locus_tag identifiers&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Full details can be found in the &lt;code&gt;cleanup_product()&lt;/code&gt; function in the &lt;code&gt;prokka&lt;/code&gt; script. If you feel your annotations are being ruined, try using the &lt;code&gt;--rawproduct&lt;/code&gt; option, and please &lt;a href=&quot;https://github.com/tseemann/prokka/issues/&quot;&gt;file an issue&lt;/a&gt; if you find an example of where it is &quot;behaving badly&quot; and I will fix it.&lt;/p&gt; 
&lt;h2&gt;Databases&lt;/h2&gt; 
&lt;h3&gt;The Core (BLAST+) Databases&lt;/h3&gt; 
&lt;p&gt;Prokka uses a variety of databases when trying to assign function to the predicted CDS features. It takes a hierarchical approach to make it fast.&lt;br /&gt; A small, core set of well characterized proteins are first searched using BLAST+. This combination of small database and fast search typically completes about 70% of the workload. Then a series of slower but more sensitive HMM databases are searched using HMMER3.&lt;/p&gt; 
&lt;p&gt;The three core databases, applied in order, are:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://isfinder.biotoul.fr/&quot;&gt;ISfinder&lt;/a&gt;: Only the tranposase (protein) sequences; the whole transposon is not annotated.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/bioproject/313047&quot;&gt;NCBI Bacterial Antimicrobial Resistance Reference Gene Database&lt;/a&gt;: Antimicrobial resistance genes curated by NCBI.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://www.uniprot.org/uniprot/?query=reviewed:yes&quot;&gt;UniProtKB (SwissProt)&lt;/a&gt;: For each &lt;code&gt;--kingdom&lt;/code&gt; we include curated proteins with evidence that (i) from Bacteria (or Archaea or Viruses); (ii) not be &quot;Fragment&quot; entries; and (iii) have an evidence level (&quot;PE&quot;) of 2 or lower, which corresponds to experimental mRNA or proteomics evidence.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Making a Core Databases&lt;/h4&gt; 
&lt;p&gt;If you want to modify these core databases, the included script &lt;code&gt;prokka-uniprot_to_fasta_db&lt;/code&gt;, along with the official &lt;code&gt;uniprot_sprot.dat&lt;/code&gt;, can be used to generate a new database to put in &lt;code&gt;/opt/prokka/db/kingdom/&lt;/code&gt;. If you add new ones, the command &lt;code&gt;prokka --listdb&lt;/code&gt; will show you whether it has been detected properly.&lt;/p&gt; 
&lt;h4&gt;The Genus Databases&lt;/h4&gt; 
&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; This is no longer recommended. Please use &lt;code&gt;--proteins&lt;/code&gt; instead.&lt;/p&gt; 
&lt;p&gt;If you enable &lt;code&gt;--usegenus&lt;/code&gt; and also provide a Genus via &lt;code&gt;--genus&lt;/code&gt; then it will first use a BLAST database which is Genus specific. Prokka comes with a set of databases for the most common Bacterial genera; type prokka &lt;code&gt;--listdb&lt;/code&gt; to see what they are.&lt;/p&gt; 
&lt;h4&gt;Adding a Genus Databases&lt;/h4&gt; 
&lt;p&gt;If you have a set of Genbank files and want to create a new Genus database, Prokka comes with a tool called &lt;code&gt;prokka-genbank_to_fasta_db&lt;/code&gt; to help. For example, if you had four annotated &quot;Coccus&quot; genomes, you could do the following:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;% prokka-genbank_to_fasta_db Coccus1.gbk Coccus2.gbk Coccus3.gbk Coccus4.gbk &amp;gt; Coccus.faa
% cd-hit -i Coccus.faa -o Coccus -T 0 -M 0 -g 1 -s 0.8 -c 0.9
% rm -fv Coccus.faa Coccus.bak.clstr Coccus.clstr
% makeblastdb -dbtype prot -in Coccus
% mv Coccus.p* /path/to/prokka/db/genus/
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;The HMM Databases&lt;/h3&gt; 
&lt;p&gt;Prokka comes with a bunch of HMM libraries for HMMER3. They are mostly Bacteria-specific. They are searched after the core and genus databases. You can add more simply by putting them in &lt;code&gt;/opt/prokka/db/hmm&lt;/code&gt;. Type &lt;code&gt;prokka --listdb&lt;/code&gt; to confirm they are recognised.&lt;/p&gt; 
&lt;h3&gt;FASTA database format&lt;/h3&gt; 
&lt;p&gt;Prokka understands two annotation tag formats, a plain one and a detailed one.&lt;/p&gt; 
&lt;p&gt;The plain one is a standard FASTA-like line with the ID after the &lt;code&gt;&amp;gt;&lt;/code&gt; sign, and the protein &lt;code&gt;/product&lt;/code&gt; after the ID (the &quot;description&quot; part of the line):&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;SeqID product
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The detailed one consists of a special encoded three-part description line. The parts are the &lt;code&gt;/EC_number&lt;/code&gt;, the &lt;code&gt;/gene&lt;/code&gt; code, then the &lt;code&gt;/product&lt;/code&gt; - and they are separated by a special &quot;~~~&quot; sequence:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;SeqID EC_number~~~gene~~~product~~~COG
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Here are some examples. Note that not all parts need to be present, but the &quot;~~~&quot; should still be there:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt;YP_492693.1 2.1.1.48~~~ermC~~~rRNA adenine N-6-methyltransferase~~~COG1234
MNEKNIKHSQNFITSKHNIDKIMTNIRLNEHDNIFEIGSGKGHFTLELVQRCNFVTAIEI
DHKLCKTTENKLVDHDNFQVLNKDILQFKFPKNQSYKIFGNIPYNISTDIIRKIVF*
&amp;gt;YP_492697.1 ~~~traB~~~transfer complex protein TraB~~~
MIKKFSLTTVYVAFLSIVLSNITLGAENPGPKIEQGLQQVQTFLTGLIVAVGICAGVWIV
LKKLPGIDDPMVKNEMFRGVGMVLAGVAVGAALVWLVPWVYNLFQ*
&amp;gt;YP_492694.1 ~~~~~~transposase~~~
MNYFRYKQFNKDVITVAVGYYLRYALSYRDISEILRGRGVNVHHSTVYRWVQEYAPILYQ
QSINTAKNTLKGIECIYALYKKNRRSLQIYGFSPCHEISIMLAS*
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The same description lines apply to HMM models, except the &quot;NAME&quot; and &quot;DESC&quot; fields are used:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;NAME  PRK00001
ACC   PRK00001
DESC  2.1.1.48~~~ermC~~~rRNA adenine N-6-methyltransferase~~~COG1234
LENG  284
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;FAQ&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Where does the name &quot;Prokka&quot; come from?&lt;/strong&gt;&lt;br /&gt; Prokka is a contraction of &quot;prokaryotic annotation&quot;. It&#39;s also relatively unique within Google, and also rhymes with a native Australian marsupial called the quokka.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can I annotate by eukaryote genome with Prokka?&lt;/strong&gt;&lt;br /&gt; No. Prokka is specifically designed for Bacteria, Archaea and Viruses. It can&#39;t handle multi-exon gene models; I would recommend using MAKER 2 for that purpose.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Why does Prokka keeps on crashing when it gets to the &quot;tbl2asn&quot; stage?&lt;/strong&gt;&lt;br /&gt; It seems that the tbl2asn program from NCBI &quot;expires&quot; after 6-12 months, and refuses to run. Unfortunately you need to install a newer version which you can download from &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/genbank/tbl2asn2/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;The hmmscan step seems to hang and do nothing?&lt;/strong&gt;&lt;br /&gt; The problem here is GNU Parallel. It seems the Debian package for hmmer has modified it to require the &lt;code&gt;--gnu&lt;/code&gt; option to behave in the &#39;default&#39; way. There is no clear reason for this. The only way to restore normal behaviour is to edit the prokka script and change &lt;code&gt;parallel&lt;/code&gt; to &lt;code&gt;parallel --gnu&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Why does prokka fail when it gets to hmmscan?&lt;/strong&gt;&lt;br /&gt; Unfortunately HMMER keeps changing its database format, and they aren&#39;t upward compatible. If you upgraded HMMER (from 3.0 to 3.1 say) then you need to &quot;re-press&quot; the files. This can be done as follows:&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;cd /path/to/prokka/db/hmm
mkdir new
for D in *.hmm ; do hmmconvert $D &amp;gt; new/$D ; done
cd new
for D in *.hmm ; do hmmpress $D ; done
mv * ..
rmdir new
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Why can&#39;t I load Prokka .GBK files into Mauve?&lt;/strong&gt;&lt;br /&gt; Mauve uses BioJava to parse GenBank files, and it is very picky about Genbank files. It does not like long contig names, like those from Velvet or Spades. One solution is to use &lt;code&gt;--centre XXX&lt;/code&gt; in Prokka and it will rename all your contigs to be NCBI (and Mauve) compliant. It does not like the ACCESSION and VERSION strings that Prokka produces via the &quot;tbl2asn&quot; tool. The following Unix command will fix them: &lt;code&gt;egrep -v &#39;^(ACCESSION|VERSION)&#39; prokka.gbk &amp;gt; mauve.gbk&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;How can I make my GFF not have the contig sequences in it?&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code&gt;sed &#39;/^##FASTA/Q&#39; prokka.gff &amp;gt; nosequence.gff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Bugs&lt;/h2&gt; 
&lt;p&gt;Submit problems or requests to the &lt;a href=&quot;https://github.com/tseemann/prokka/issues&quot;&gt;Issue Tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Changes&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://github.com/tseemann/prokka/releases&quot;&gt;release notes&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Read the &lt;a href=&quot;https://raw.githubusercontent.com/tseemann/prokka/master/doc/ChangeLog.txt&quot;&gt;ChangeLog.txt&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Look at the &lt;a href=&quot;https://github.com/tseemann/prokka/commits/master&quot;&gt;Github commits&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;Seemann T.&lt;br /&gt; &lt;em&gt;Prokka: rapid prokaryotic genome annotation&lt;/em&gt;&lt;br /&gt; &lt;strong&gt;Bioinformatics&lt;/strong&gt; 2014 Jul 15;30(14):2068-9. &lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/24642063&quot;&gt;PMID:24642063&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Dependencies&lt;/h2&gt; 
&lt;h3&gt;Mandatory&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;BioPerl&lt;/strong&gt;&lt;br /&gt; Used for input/output of various file formats&lt;br /&gt; &lt;em&gt;Stajich et al, The Bioperl toolkit: Perl modules for the life sciences. Genome Res. 2002 Oct;12(10):1611-8.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;GNU Parallel&lt;/strong&gt;&lt;br /&gt; A shell tool for executing jobs in parallel using one or more computers&lt;br /&gt; &lt;em&gt;O. Tange, GNU Parallel - The Command-Line Power Tool, ;login: The USENIX Magazine, Feb 2011:42-47.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;BLAST+&lt;/strong&gt;&lt;br /&gt; Used for similarity searching against protein sequence libraries&lt;br /&gt; &lt;em&gt;Camacho C et al. BLAST+: architecture and applications. BMC Bioinformatics. 2009 Dec 15;10:421.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prodigal&lt;/strong&gt;&lt;br /&gt; Finds protein-coding features (CDS)&lt;br /&gt; &lt;em&gt;Hyatt D et al. Prodigal: prokaryotic gene recognition and translation initiation site identification. BMC Bioinformatics. 2010 Mar 8;11:119.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;TBL2ASN&lt;/strong&gt; Prepare sequence records for Genbank submission &lt;a href=&quot;https://www.ncbi.nlm.nih.gov/genbank/tbl2asn2/&quot;&gt;Tbl2asn home page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Recommended&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Aragorn&lt;/strong&gt;&lt;br /&gt; Finds transfer RNA features (tRNA)&lt;br /&gt; &lt;em&gt;Laslett D, Canback B. ARAGORN, a program to detect tRNA genes and tmRNA genes in nucleotide sequences. Nucleic Acids Res. 2004 Jan 2;32(1):11-6.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Barrnap&lt;/strong&gt;&lt;br /&gt; Used to predict ribosomal RNA features (rRNA). My licence-free replacement for RNAmmmer.&lt;br /&gt; &lt;em&gt;Manuscript under preparation.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HMMER3&lt;/strong&gt;&lt;br /&gt; Used for similarity searching against protein family profiles&lt;br /&gt; &lt;em&gt;Finn RD et al. HMMER web server: interactive sequence similarity searching. Nucleic Acids Res. 2011 Jul;39(Web Server issue):W29-37.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Optional&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;minced&lt;/strong&gt;&lt;br /&gt; Finds CRISPR arrays &lt;a href=&quot;https://github.com/ctSkennerton/minced&quot;&gt;Minced home page&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;RNAmmer&lt;/strong&gt;&lt;br /&gt; Finds ribosomal RNA features (rRNA)&lt;br /&gt; &lt;em&gt;Lagesen K et al. RNAmmer: consistent and rapid annotation of ribosomal RNA genes. Nucleic Acids Res. 2007;35(9):3100-8.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;SignalP&lt;/strong&gt;&lt;br /&gt; Finds signal peptide features in CDS (sig_peptide)&lt;br /&gt; &lt;em&gt;Petersen TN et al. SignalP 4.0: discriminating signal peptides from transmembrane regions. Nat Methods. 2011 Sep 29;8(10):785-6.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Infernal&lt;/strong&gt;&lt;br /&gt; Used for similarity searching against ncRNA family profiles&lt;br /&gt; &lt;em&gt;D. L. Kolbe, S. R. Eddy. Fast Filtering for RNA Homology Search. Bioinformatics, 27:3102-3109, 2011.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Licence&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/tseemann/prokka/master/doc/LICENSE.Prokka&quot;&gt;GPL v3&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Author&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Torsten Seemann&lt;/li&gt; 
 &lt;li&gt;Web: &lt;a href=&quot;https://tseemann.github.io/&quot;&gt;https://tseemann.github.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Twitter: &lt;a href=&quot;https://twitter.com/torstenseemann&quot;&gt;@torstenseemann&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Blog: &lt;a href=&quot;https://thegenomefactory.blogspot.com/&quot;&gt;The Genome Factory&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
