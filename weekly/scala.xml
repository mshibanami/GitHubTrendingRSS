<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub Scala Weekly Trending</title>
    <description>Weekly Trending of Scala in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:49:03 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>apache/spark</title>
      <link>https://github.com/apache/spark</link>
      <description>&lt;p&gt;Apache Spark - A unified analytics engine for large-scale data processing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Spark&lt;/h1&gt; 
&lt;p&gt;Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R (Deprecated), and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Official version: &lt;a href=&quot;https://spark.apache.org/&quot;&gt;https://spark.apache.org/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Development version: &lt;a href=&quot;https://apache.github.io/spark/&quot;&gt;https://apache.github.io/spark/&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_main.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/apache/spark&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg?sanitize=true&quot; alt=&quot;PySpark Coverage&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/pyspark/&quot;&gt;&lt;img src=&quot;https://static.pepy.tech/personalized-badge/pyspark?period=month&amp;amp;units=international_system&amp;amp;left_color=black&amp;amp;right_color=orange&amp;amp;left_text=PyPI%20downloads&quot; alt=&quot;PyPI Downloads&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Online Documentation&lt;/h2&gt; 
&lt;p&gt;You can find the latest Spark documentation, including a programming guide, on the &lt;a href=&quot;https://spark.apache.org/documentation.html&quot;&gt;project web page&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; 
&lt;h2&gt;Build Pipeline Status&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Branch&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;master&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_non_ansi.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_non_ansi.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_uds.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_uds.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_rockdb_as_ui_backend.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_rockdb_as_ui_backend.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_macos15.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_macos15.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_arm.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_maven_java21_arm.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_coverage.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_coverage.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_pypy3.10.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_pypy3.10.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.10.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.10.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_classic_only.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_classic_only.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_arm.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_arm.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_macos.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.11_macos.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_numpy_2.1.3.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_numpy_2.1.3.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.12.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.12.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13_nogil.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_3.13_nogil.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_minimum.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_minimum.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_ps_minimum.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_ps_minimum.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect35.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect35.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_python_connect.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_sparkr_window.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_sparkr_window.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/publish_snapshot.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/publish_snapshot.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;branch-4.0&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_non_ansi.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_non_ansi.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven_java21.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_maven_java21.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python_pypy3.10.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch40_python_pypy3.10.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;branch-3.5&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch35.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch35.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/apache/spark/actions/workflows/build_branch35_python.yml&quot;&gt;&lt;img src=&quot;https://github.com/apache/spark/actions/workflows/build_branch35_python.yml/badge.svg?sanitize=true&quot; alt=&quot;GitHub Actions Build&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Building Spark&lt;/h2&gt; 
&lt;p&gt;Spark is built using &lt;a href=&quot;https://maven.apache.org/&quot;&gt;Apache Maven&lt;/a&gt;. To build Spark and its example programs, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./build/mvn -DskipTests clean package
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(You do not need to do this if you downloaded a pre-built package.)&lt;/p&gt; 
&lt;p&gt;More detailed documentation is available from the project site, at &lt;a href=&quot;https://spark.apache.org/docs/latest/building-spark.html&quot;&gt;&quot;Building Spark&quot;&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For general development tips, including info on developing Spark using an IDE, see &lt;a href=&quot;https://spark.apache.org/developer-tools.html&quot;&gt;&quot;Useful Developer Tools&quot;&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Interactive Scala Shell&lt;/h2&gt; 
&lt;p&gt;The easiest way to start using Spark is through the Scala shell:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/spark-shell
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Try the following command, which should return 1,000,000,000:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;scala&amp;gt; spark.range(1000 * 1000 * 1000).count()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Interactive Python Shell&lt;/h2&gt; 
&lt;p&gt;Alternatively, if you prefer Python, you can use the Python shell:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/pyspark
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And run the following command, which should also return 1,000,000,000:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;&amp;gt;&amp;gt;&amp;gt; spark.range(1000 * 1000 * 1000).count()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example Programs&lt;/h2&gt; 
&lt;p&gt;Spark also comes with several sample programs in the &lt;code&gt;examples&lt;/code&gt; directory. To run one of them, use &lt;code&gt;./bin/run-example &amp;lt;class&amp;gt; [params]&lt;/code&gt;. For example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;will run the Pi example locally.&lt;/p&gt; 
&lt;p&gt;You can set the MASTER environment variable when running examples to submit examples to a cluster. This can be spark:// URL, &quot;yarn&quot; to run on YARN, and &quot;local&quot; to run locally with one thread, or &quot;local[N]&quot; to run locally with N threads. You can also use an abbreviated class name if the class is in the &lt;code&gt;examples&lt;/code&gt; package. For instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;MASTER=spark://host:7077 ./bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Many of the example programs print usage help if no params are given.&lt;/p&gt; 
&lt;h2&gt;Running Tests&lt;/h2&gt; 
&lt;p&gt;Testing first requires &lt;a href=&quot;https://raw.githubusercontent.com/apache/spark/master/#building-spark&quot;&gt;building Spark&lt;/a&gt;. Once Spark is built, tests can be run using:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./dev/run-tests
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Please see the guidance on how to &lt;a href=&quot;https://spark.apache.org/developer-tools.html#individual-tests&quot;&gt;run tests for a module, or individual tests&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md&lt;/p&gt; 
&lt;h2&gt;A Note About Hadoop Versions&lt;/h2&gt; 
&lt;p&gt;Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported storage systems. Because the protocols have changed in different versions of Hadoop, you must build Spark against the same version that your cluster runs.&lt;/p&gt; 
&lt;p&gt;Please refer to the build documentation at &lt;a href=&quot;https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn&quot;&gt;&quot;Specifying the Hadoop Version and Enabling YARN&quot;&lt;/a&gt; for detailed guidance on building for a particular distribution of Hadoop, including building for particular Hive and Hive Thriftserver distributions.&lt;/p&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;Please refer to the &lt;a href=&quot;https://spark.apache.org/docs/latest/configuration.html&quot;&gt;Configuration Guide&lt;/a&gt; in the online documentation for an overview on how to configure Spark.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Please review the &lt;a href=&quot;https://spark.apache.org/contributing.html&quot;&gt;Contribution to Spark guide&lt;/a&gt; for information on how to get started contributing to the project.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ucb-bar/berkeley-hardfloat</title>
      <link>https://github.com/ucb-bar/berkeley-hardfloat</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Berkeley Hardware Floating-Point Units&lt;/h1&gt; 
&lt;p&gt;This repository contains hardware floating-point units written in Chisel. This library contains parameterized floating-point units for fused multiply-add operations, conversions between integer and floating-point numbers, and conversions between floating-point conversions with different precision.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: These units are works in progress. They may not be yet completely free of bugs, nor are they fully optimized.&lt;/p&gt; 
&lt;h2&gt;Recoded Format&lt;/h2&gt; 
&lt;p&gt;The floating-point units in this repository work on an internal recoded format (exponent has an additional bit) to handle subnormal numbers more efficiently in a microprocessor. A more detailed explanation will come soon, but in the mean time here are some example mappings for single-precision numbers.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;IEEE format                           Recoded format
----------------------------------    -----------------------------------
s 00000000 00000000000000000000000    s 000------ 00000000000000000000000
s 00000000 00000000000000000000001    s 001101011 00000000000000000000000
s 00000000 0000000000000000000001f    s 001101100 f0000000000000000000000
s 00000000 000000000000000000001ff    s 001101101 ff000000000000000000000
    ...              ...                   ...              ... 
s 00000000 001ffffffffffffffffffff    s 001111111 ffffffffffffffffffff000
s 00000000 01fffffffffffffffffffff    s 010000000 fffffffffffffffffffff00
s 00000000 1ffffffffffffffffffffff    s 010000001 ffffffffffffffffffffff0
s 00000001 fffffffffffffffffffffff    s 010000010 fffffffffffffffffffffff
s 00000010 fffffffffffffffffffffff    s 010000011 fffffffffffffffffffffff
    ...              ...                   ...              ... 
s 11111101 fffffffffffffffffffffff    s 101111110 fffffffffffffffffffffff
s 11111110 fffffffffffffffffffffff    s 101111111 fffffffffffffffffffffff
s 11111111 00000000000000000000000    s 110------ -----------------------
s 11111111 fffffffffffffffffffffff    s 111------ fffffffffffffffffffffff
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Unit-Testing&lt;/h2&gt; 
&lt;p&gt;To unit-test these floating-point units, you need the berkeley-testfloat-3 package.&lt;/p&gt; 
&lt;p&gt;To test floating-point units with the C simulator:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>bitlap/sbt-dependency-analyzer</title>
      <link>https://github.com/bitlap/sbt-dependency-analyzer</link>
      <description>&lt;p&gt;üìà SBT Dependency Analyzer for IntelliJ IDEAÔºàIntelliJ IDEAÁöÑSBT‰æùËµñÂàÜÊûêÊèí‰ª∂Ôºâ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sbt Dependency Analyzer for IntelliJ IDEA&lt;/h1&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/bitlap/sbt-dependency-analyzer/master/logo.svg?sanitize=true&quot; width=&quot;250&quot; height=&quot;150&quot; alt=&quot;Sbt Dependency Analyzer Logo&quot; align=&quot;right&quot; /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/bitlap/sbt-dependency-analyzer/actions/workflows/ScalaCI.yml&quot;&gt;&lt;img src=&quot;https://github.com/bitlap/sbt-dependency-analyzer/actions/workflows/ScalaCI.yml/badge.svg?sanitize=true&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://plugins.jetbrains.com/plugin/22427-sbt-dependency-analyzer/versions&quot;&gt;&lt;img src=&quot;https://img.shields.io/jetbrains/plugin/v/22427-sbt-dependency-analyzer?label=Version&quot; alt=&quot;Plugin Version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://plugins.jetbrains.com/plugin/22427-sbt-dependency-analyzer&quot;&gt;&lt;img src=&quot;https://img.shields.io/jetbrains/plugin/d/22427?label=Downloads&quot; alt=&quot;Plugin Downloads&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;English | &lt;a href=&quot;https://raw.githubusercontent.com/bitlap/sbt-dependency-analyzer/master/README-CN.md&quot;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;‚≠ê Found this plugin useful? Please give it a Star on &lt;a href=&quot;https://github.com/bitlap/sbt-dependency-analyzer&quot;&gt;GitHub&lt;/a&gt; to show your support!&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;üöÄ Overview&lt;/h2&gt; 
&lt;p&gt;The Sbt Dependency Analyzer plugin for IntelliJ IDEA provides powerful visual tools to help you understand, manage, and troubleshoot your Sbt project&#39;s dependencies with ease. Gain clear insights into your library dependencies and their relationships, directly within your IDE.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Supports Community Edition, Ultimate, and Android Studio.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;‚ú® Key Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency Tree Visualization&lt;/strong&gt;: View a hierarchical tree of all your project&#39;s dependencies.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conflict Identification&lt;/strong&gt;: Quickly spot and resolve version conflicts between libraries.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency Search&lt;/strong&gt;: Easily find specific dependencies across your project.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inter-Module Dependency Analysis&lt;/strong&gt;: Visualize how different modules in your project depend on each other.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JAR Size Indicators&lt;/strong&gt;: See the size of dependency JARs to better manage your project&#39;s footprint.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Navigate to Declaration&lt;/strong&gt;: Click on a &lt;em&gt;user-defined&lt;/em&gt; dependency to jump directly to its declaration in &lt;code&gt;build.sbt&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dependency Exclusion (Experimental)&lt;/strong&gt;: 
  &lt;ul&gt; 
   &lt;li&gt;Select a &lt;em&gt;transitive&lt;/em&gt; dependency to exclude it from a &lt;em&gt;user-defined&lt;/em&gt; dependency.&lt;/li&gt; 
   &lt;li&gt;Select a &lt;em&gt;user-defined&lt;/em&gt; dependency to remove it entirely.&lt;/li&gt; 
   &lt;li&gt;&lt;em&gt;Available since plugin version &lt;code&gt;0.5.0-242.21829.142&lt;/code&gt;.&lt;/em&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Install the Plugin&lt;/strong&gt;: Go to &lt;code&gt;Settings/Preferences&lt;/code&gt; &amp;gt; &lt;code&gt;Plugins&lt;/code&gt; &amp;gt; &lt;code&gt;Marketplace&lt;/code&gt;, search for &quot;Sbt Dependency Analyzer&quot;, and install it.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Automatic Setup&lt;/strong&gt;: Upon first analysis, the plugin will automatically generate a &lt;code&gt;project/sdap.sbt&lt;/code&gt; file if needed. This file adds the required &lt;code&gt;addDependencyTreePlugin&lt;/code&gt; statement. &lt;strong&gt;Please do not modify or delete this file&lt;/strong&gt; once created.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Plugin Dependency&lt;/strong&gt;: This plugin leverages the &lt;code&gt;sbt-dependency-tree&lt;/code&gt; functionality, which is bundled with recent sbt versions (though not be enabled by default, &lt;a href=&quot;https://github.com/sbt/sbt/pull/5880&quot;&gt;sbt issue&lt;/a&gt;).&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;üìñ Usage&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Default Keyboard Shortcut&lt;/strong&gt;: &lt;code&gt;Ctrl&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;L&lt;/code&gt; (Windows/Linux) / &lt;code&gt;Command&lt;/code&gt; + &lt;code&gt;Shift&lt;/code&gt; + &lt;code&gt;L&lt;/code&gt; (macOS)&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Simply open your Sbt project in IntelliJ IDEA and use the shortcut to generate and view the dependency analysis.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://plugins.jetbrains.com/files/22427/screenshot_064531dc-a3fa-4a8e-9437-7e76defa1f48&quot; alt=&quot;Dependency Analysis View&quot; /&gt; &lt;em&gt;The interactive dependency graph provides a clear overview of your project&#39;s structure.&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;‚öôÔ∏è Configuration &lt;a id=&quot;settings&quot;&gt;&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Fine-tune the plugin&#39;s behavior and potentially speed up analysis via &lt;code&gt;Settings/Preferences&lt;/code&gt; &amp;gt; &lt;code&gt;Tools&lt;/code&gt; &amp;gt; &lt;code&gt;Sbt Dependency Analyzer&lt;/code&gt;:&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/bitlap/sbt-dependency-analyzer/master/docs/settings.png&quot; width=&quot;400&quot; height=&quot;280&quot; alt=&quot;Plugin Settings Panel&quot; align=&quot;right&quot; /&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;File Cache Timeout&lt;/strong&gt;: Adjust how long (in seconds) the plugin uses cached dependency graph files (&lt;code&gt;.dot&lt;/code&gt;) before re-running the &lt;code&gt;dependencyDot&lt;/code&gt; command. (Default: &lt;code&gt;3600&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Organization&lt;/strong&gt;: Predefine your project&#39;s organization value here to avoid the plugin needing to query sbt for it.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Disable Scopes&lt;/strong&gt;: Improve analysis speed by disabling dependency scopes (e.g., &lt;code&gt;Test&lt;/code&gt;, &lt;code&gt;Provided&lt;/code&gt;) you are not interested in.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These settings are stored per IntelliJ project in &lt;code&gt;.idea/bitlap.sbt.dependency.analyzer.xml&lt;/code&gt;. Deleting this file will reset settings and clear the cache.&lt;/p&gt; 
&lt;h2&gt;‚ùó Troubleshooting&lt;/h2&gt; 
&lt;h3&gt;Issue: &quot;Caused by: java.io.IOException: Could not create lock for ...&quot;&lt;/h3&gt; 
&lt;p&gt;This error can occur due to conflicts between the plugin&#39;s use of the sbt shell and IntelliJ IDEA&#39;s internal project reload/build mechanisms.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Use the &lt;strong&gt;sbt shell within IntelliJ IDEA&lt;/strong&gt; for reloading (&lt;code&gt;sbt reload&lt;/code&gt;) or building (&lt;code&gt;sbt compile&lt;/code&gt;) your project, instead of the IDE&#39;s built-in buttons. &lt;img src=&quot;https://raw.githubusercontent.com/bitlap/sbt-dependency-analyzer/master/docs/sbtShellUseForReload.jpg&quot; width=&quot;500&quot; height=&quot;230&quot; alt=&quot;Using SBT Shell for reload and compile&quot; align=&quot;center&quot; /&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Issue: Cannot analyze dependencies between modules&lt;/h3&gt; 
&lt;p&gt;The plugin may fail to correctly parse inter-module dependencies if it cannot determine the project organization.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Ensure the plugin knows your project&#39;s organization by either: 
  &lt;ol&gt; 
   &lt;li&gt;Setting the &lt;strong&gt;Organization&lt;/strong&gt; value in the plugin&#39;s &lt;a href=&quot;https://raw.githubusercontent.com/bitlap/sbt-dependency-analyzer/master/#settings&quot;&gt;settings&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Defining the &lt;code&gt;organization&lt;/code&gt; setting in your &lt;code&gt;build.sbt&lt;/code&gt; globally using &lt;code&gt;ThisBuild / organization&lt;/code&gt; or &lt;code&gt;inThisBuild(...)&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Submodules not declared within the &lt;code&gt;dependsOn&lt;/code&gt; clause of the root project will not be parsed.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;üîç Technical Details&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;The plugin executes several sbt commands (&lt;code&gt;organization&lt;/code&gt;, &lt;code&gt;moduleName&lt;/code&gt;, &lt;code&gt;dependencyDot&lt;/code&gt;, &lt;code&gt;reload&lt;/code&gt;, &lt;code&gt;update&lt;/code&gt;) to gather dependency information. Significant optimizations are in place to minimize the number and impact of these commands.&lt;/li&gt; 
 &lt;li&gt;The plugin has replicated the Kotlin code from the &lt;a href=&quot;https://github.com/JetBrains/intellij-community&quot;&gt;intellij-community&lt;/a&gt; project on the UI and compiles it using the &lt;a href=&quot;https://github.com/bitlap/kotlin-plugin&quot;&gt;kotlin-plugin&lt;/a&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; 
&lt;p&gt;Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests on &lt;a href=&quot;https://github.com/bitlap/sbt-dependency-analyzer&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;üôè Acknowledgments&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;JetBrains Support&lt;/strong&gt;: This project is developed using JetBrains IntelliJ IDEA. We extend our gratitude to JetBrains for providing a free license, which significantly supports its development.&lt;/p&gt; 
&lt;a href=&quot;https://raw.githubusercontent.com/bitlap/sbt-dependency-analyzer/master/www.jetbrains.com&quot;&gt; &lt;img src=&quot;https://resources.jetbrains.com/storage/products/company/brand/logos/jb_beam.svg?_gl=1*8f2ovk*_ga*NTY2NTA4Mzg1LjE2NzU3MzgzMTI.*_ga_9J976DJZ68*MTcwMzIwOTE4NS4xODUuMS4xNzAzMjA5NDYzLjI4LjAuMA..&amp;amp;_ga=2.177269094.2105719560.1703209186-566508385.1675738312&quot; alt=&quot;IntelliJ IDEA logo.&quot; /&gt; &lt;/a&gt; 
&lt;br /&gt;</description>
    </item>
    
    <item>
      <title>chipsalliance/rocket-chip</title>
      <link>https://github.com/chipsalliance/rocket-chip</link>
      <description>&lt;p&gt;Rocket Chip Generator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rocket Chip Generator &lt;span&gt;üöÄ&lt;/span&gt; &lt;img src=&quot;https://github.com/chipsalliance/rocket-chip/workflows/Continuous%20Integration/badge.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/h1&gt; 
&lt;p&gt;This repository contains the Rocket chip generator necessary to instantiate the RISC-V Rocket Core. For more information on Rocket Chip, please consult our &lt;a href=&quot;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html&quot;&gt;technical report&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;RocketChip Dev Meeting&lt;/h2&gt; 
&lt;p&gt;RocketChip development meetings happen every 2 weeks on Wednesday 17:00 ‚Äì 18:00am CST (Pacific Time - Los Angeles) with meeting notes &lt;a href=&quot;https://docs.google.com/document/d/1NjDnf-i10QE0y-qI94A67uCspDRdCIS_IRTm4jc0Ycc&quot;&gt;here&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Click &lt;a href=&quot;https://calendar.google.com/calendar/ical/c_699527d804418f900468a49b413d1f9c08e13c0f3f872ce551fc0470d4cdf983%40group.calendar.google.com/public/basic.ics&quot;&gt;here&lt;/a&gt; to subscribe Meeting Schedule(iCal format)&lt;/li&gt; 
 &lt;li&gt;Click &lt;a href=&quot;https://calendar.google.com/calendar/embed?src=c_699527d804418f900468a49b413d1f9c08e13c0f3f872ce551fc0470d4cdf983%40group.calendar.google.com&quot;&gt;here&lt;/a&gt; to view Meeting Schedule via Google Calendar&lt;/li&gt; 
 &lt;li&gt;Click &lt;a href=&quot;https://sifive.zoom.us/j/93899365000?pwd=UG1HSFJ4ODFzR2dhMHU2bUNqbXc3Zz09&quot;&gt;here&lt;/a&gt; to join Zoom meeting (ID: 93899365000, passcode: 754340)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For possible time adjustments, they will be negotiated in Slack and published in the calendar.&lt;/p&gt; 
&lt;h2&gt;Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#quick&quot;&gt;Quick instructions&lt;/a&gt; for those who want to dive directly into the details without knowing exactly what&#39;s in the repository.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#what&quot;&gt;What&#39;s in the Rocket chip generator repository?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#how&quot;&gt;How should I use the Rocket chip generator?&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#emulator&quot;&gt;Using the cycle-accurate Verilator simulation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#fpga&quot;&gt;Mapping a Rocket core down to an FPGA&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#vlsi&quot;&gt;Pushing a Rocket core through the VLSI tools&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#param&quot;&gt;How can I parameterize my Rocket chip?&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#debug&quot;&gt;Debugging with GDB&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#ide&quot;&gt;Building Rocket Chip with an IDE&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/#contributors&quot;&gt;Contributors&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a name=&quot;quick&quot;&gt;&lt;/a&gt; Quick Instructions&lt;/h2&gt; 
&lt;h3&gt;Checkout The Code&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ucb-bar/rocket-chip.git
$ cd rocket-chip
$ git submodule update --init
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Install Necessary Dependencies&lt;/h3&gt; 
&lt;p&gt;You may need to install some additional packages to use this repository. Rather than list all dependencies here, please see the appropriate section of the READMEs for each of the subprojects:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/freechipsproject/rocket-tools/raw/master/README.md&quot;&gt;rocket-tools &quot;Ubuntu Packages Needed&quot;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ucb-bar/chisel3#installation&quot;&gt;chisel3 &quot;Installation&quot;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Building The Project&lt;/h3&gt; 
&lt;p&gt;Generating verilog&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make verilog
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Generating verilog for a specific Config&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ make verilog CONFIG=DefaultSmallConfig
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Keeping Your Repo Up-to-Date&lt;/h3&gt; 
&lt;p&gt;If you are trying to keep your repo up to date with this GitHub repo, you also need to keep the submodules and tools up to date.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ # Get the newest versions of the files in this repo
$ git pull origin master
$ # Make sure the submodules have the correct versions
$ git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If rocket-tools version changes, you should recompile and install rocket-tools according to the directions in the &lt;a href=&quot;https://github.com/freechipsproject/rocket-tools/raw/master/README.md&quot;&gt;rocket-tools/README&lt;/a&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ cd rocket-tools
$ ./build.sh
$ ./build-rv32ima.sh (if you are using RV32)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&lt;a name=&quot;what&quot;&gt;&lt;/a&gt; What&#39;s in the Rocket chip generator repository?&lt;/h2&gt; 
&lt;p&gt;The rocket-chip repository is a meta-repository that points to several sub-repositories using &lt;a href=&quot;http://git-scm.com/book/en/Git-Tools-Submodules&quot;&gt;Git submodules&lt;/a&gt;. Those repositories contain tools needed to generate and test SoC designs. This respository also contains code that is used to generate RTL. Hardware generation is done using &lt;a href=&quot;http://chisel.eecs.berkeley.edu&quot;&gt;Chisel&lt;/a&gt;, a hardware construction language embedded in Scala. The rocket-chip generator is a Scala program that invokes the Chisel compiler in order to emit RTL describing a complete SoC. The following sections describe the components of this repository.&lt;/p&gt; 
&lt;h3&gt;&lt;a name=&quot;what_submodules&quot;&gt;&lt;/a&gt;Git Submodules&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://git-scm.com/book/en/v2/Git-Tools-Submodules&quot;&gt;Git submodules&lt;/a&gt; allow you to keep a Git repository as a subdirectory of another Git repository. For projects being co-developed with the Rocket Chip Generator, we have often found it expedient to track them as submodules, allowing for rapid exploitation of new features while keeping commit histories separate. As submoduled projects adopt stable public APIs, we transition them to external dependencies. Here are the submodules that are currently being tracked in the rocket-chip repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;chisel3&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/chisel3&quot;&gt;https://github.com/ucb-bar/chisel3&lt;/a&gt;): The Rocket Chip Generator uses &lt;a href=&quot;http://chisel.eecs.berkeley.edu&quot;&gt;Chisel&lt;/a&gt; to generate RTL.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;firrtl&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/firrtl&quot;&gt;https://github.com/ucb-bar/firrtl&lt;/a&gt;): &lt;a href=&quot;http://bar.eecs.berkeley.edu/projects/2015-firrtl.html&quot;&gt;Firrtl (Flexible Internal Representation for RTL)&lt;/a&gt; is the intermediate representation of RTL constructions used by Chisel3. The Chisel3 compiler generates a Firrtl representation, from which the final product (Verilog code, C code, etc) is generated.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;hardfloat&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/berkeley-hardfloat&quot;&gt;https://github.com/ucb-bar/berkeley-hardfloat&lt;/a&gt;): Hardfloat holds Chisel code that generates parameterized IEEE 754-2008 compliant floating-point units used for fused multiply-add operations, conversions between integer and floating-point numbers, and conversions between floating-point conversions with different precision.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;rocket-tools&lt;/strong&gt; (&lt;a href=&quot;https://github.com/freechipsproject/rocket-tools&quot;&gt;https://github.com/freechipsproject/rocket-tools&lt;/a&gt;): We tag a version of RISC-V software tools that work with the RTL committed in this repository.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;torture&lt;/strong&gt; (&lt;a href=&quot;https://github.com/ucb-bar/riscv-torture&quot;&gt;https://github.com/ucb-bar/riscv-torture&lt;/a&gt;): This module is used to generate and execute constrained random instruction streams that can be used to stress-test both the core and uncore portions of the design.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a name=&quot;what_packages&quot;&gt;&lt;/a&gt;Scala Packages&lt;/h3&gt; 
&lt;p&gt;In addition to submodules that track independent Git repositories, the rocket-chip code base is itself factored into a number of Scala packages. These packages are all found within the src/main/scala directory. Some of these packages provide Scala utilities for generator configuration, while other contain the actual Chisel RTL generators themselves. Here is a brief description of what can be found in each package:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;amba&lt;/strong&gt; This RTL package uses diplomacy to generate bus implementations of AMBA protocols, including AXI4, AHB-lite, and APB.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;config&lt;/strong&gt; This utility package provides Scala interfaces for configuring a generator via a dynamically-scoped parameterization library.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;coreplex&lt;/strong&gt; This RTL package generates a complete coreplex by gluing together a variety of components from other packages, including: tiled Rocket cores, a system bus network, coherence agents, debug devices, interrupt handlers, externally-facing peripherals, clock-crossers and converters from TileLink to external bus protocols (e.g. AXI or AHB).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;devices&lt;/strong&gt; This RTL package contains implementations for peripheral devices, including the Debug module and various TL slaves.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;diplomacy&lt;/strong&gt; This utility package extends Chisel by allowing for two-phase hardware elaboration, in which certain parameters are dynamically negotiated between modules. For more information about diplomacy, see &lt;a href=&quot;https://carrv.github.io/2017/papers/cook-diplomacy-carrv2017.pdf&quot;&gt;this paper&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;groundtest&lt;/strong&gt; This RTL package generates synthesizable hardware testers that emit randomized memory access streams in order to stress-tests the uncore memory hierarchy.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;jtag&lt;/strong&gt; This RTL package provides definitions for generating JTAG bus interfaces.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;regmapper&lt;/strong&gt; This utility package generates slave devices with a standardized interface for accessing their memory-mapped registers.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;rocket&lt;/strong&gt; This RTL package generates the Rocket in-order pipelined core, as well as the L1 instruction and data caches. This library is intended to be used by a chip generator that instantiates the core within a memory system and connects it to the outside world.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tile&lt;/strong&gt; This RTL package contains components that can be combined with cores to construct tiles, such as FPUs and accelerators.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;tilelink&lt;/strong&gt; This RTL package uses diplomacy to generate bus implementations of the TileLink protocol. It also contains a variety of adapters and protocol converters.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;system&lt;/strong&gt; This top-level utility package invokes Chisel to elaborate a particular configuration of a coreplex, along with the appropriate testing collateral.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;unittest&lt;/strong&gt; This utility package contains a framework for generateing synthesizable hardware testers of individual modules.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;util&lt;/strong&gt; This utility package provides a variety of common Scala and Chisel constructs that are re-used across multiple other packages,&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;&lt;a name=&quot;what_else&quot;&gt;&lt;/a&gt;Other Resources&lt;/h3&gt; 
&lt;p&gt;Outside of Scala, we also provide a variety of resources to create a complete SoC implementation and test the generated designs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;bootrom&lt;/strong&gt; Sources for the first-stage bootloader included in the BootROM.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;csrc&lt;/strong&gt; C sources for use with Verilator simulation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;docs&lt;/strong&gt; Documentation, tutorials, etc for specific parts of the codebase.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;emulator&lt;/strong&gt; Directory in which Verilator simulations are compiled and run.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;regression&lt;/strong&gt; Defines continuous integration and nightly regression suites.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;scripts&lt;/strong&gt; Utilities for parsing the output of simulations or manipulating the contents of source files.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vsim&lt;/strong&gt; Directory in which Synopsys VCS simulations are compiled and run.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;vsrc&lt;/strong&gt; Verilog sources containing interfaces, harnesses and VPI.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;a name=&quot;ide&quot;&gt;&lt;/a&gt; IDEs Support&lt;/h2&gt; 
&lt;p&gt;The Rocket Chip Scala build uses &lt;a href=&quot;https://github.com/com-lihaoyi/mill&quot;&gt;mill&lt;/a&gt; as build tool.&lt;/p&gt; 
&lt;p&gt;IDEs like &lt;a href=&quot;https://www.jetbrains.com/idea/&quot;&gt;IntelliJ&lt;/a&gt; and &lt;a href=&quot;https://code.visualstudio.com/&quot;&gt;VSCode&lt;/a&gt; are popular in the Scala community and work with Rocket Chip.&lt;/p&gt; 
&lt;p&gt;The Rocket Chip currently uses &lt;code&gt;nix&lt;/code&gt; to configure the build and/or development environment, you need to install it first depending on your OS distro.&lt;/p&gt; 
&lt;p&gt;Then follow the steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Generate BSP config by running:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mill mill.bsp.BSP/install
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Patch the &lt;code&gt;argv&lt;/code&gt; in &lt;code&gt;.bsp/mill-bsp.json&lt;/code&gt;, from&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;name&quot;:&quot;mill-bsp&quot;,&quot;argv&quot;:[&quot;/usr/bin/mill&quot;,&quot;--bsp&quot;,&quot;--disable-ticker&quot;,&quot;--color&quot;,&quot;false&quot;,&quot;--jobs&quot;,&quot;1&quot;],&quot;millVersion&quot;:&quot;0.10.9&quot;,&quot;bspVersion&quot;:&quot;2.0.0&quot;,&quot;languages&quot;:[&quot;scala&quot;,&quot;java&quot;]}
&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;to&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;name&quot;:&quot;mill-bsp&quot;,&quot;argv&quot;:[&quot;/usr/bin/nix&quot;,&quot;develop&quot;,&quot;-c&quot;,&quot;mill&quot;,&quot;--bsp&quot;,&quot;--disable-ticker&quot;,&quot;--color&quot;,&quot;false&quot;,&quot;--jobs&quot;,&quot;1&quot;],&quot;millVersion&quot;:&quot;0.10.9&quot;,&quot;bspVersion&quot;:&quot;2.0.0&quot;,&quot;languages&quot;:[&quot;scala&quot;,&quot;java&quot;]}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;For IntelliJ users&lt;/h3&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Install and configure &lt;a href=&quot;https://plugins.jetbrains.com/plugin/1347-scala&quot;&gt;Scala&lt;/a&gt; plugin.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;BSP should be automatically run. If it doesn&#39;t, click &lt;code&gt;bsp&lt;/code&gt; on the right bar, then right-click on your project to reload.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;For VSCode users&lt;/h3&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Install and configure &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=scalameta.metals&quot;&gt;Metals&lt;/a&gt; extension.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Execute VSCode command &lt;code&gt;Metals: Import build&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;&lt;a name=&quot;contributors&quot;&gt;&lt;/a&gt; Contributors&lt;/h2&gt; 
&lt;p&gt;Contributing guidelines can be found in &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/rocket-chip/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;A list of contributors can be found &lt;a href=&quot;https://github.com/chipsalliance/rocket-chip/graphs/contributors&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;&lt;a name=&quot;attribution&quot;&gt;&lt;/a&gt; Attribution&lt;/h2&gt; 
&lt;p&gt;If used for research, please cite Rocket Chip by the technical report:&lt;/p&gt; 
&lt;p&gt;Krste Asanoviƒá, Rimas Avi≈æienis, Jonathan Bachrach, Scott Beamer, David Biancolin, Christopher Celio, Henry Cook, Palmer Dabbelt, John Hauser, Adam Izraelevitz, Sagar Karandikar, Benjamin Keller, Donggyu Kim, John Koenig, Yunsup Lee, Eric Love, Martin Maas, Albert Magyar, Howard Mao, Miquel Moreto, Albert Ou, David Patterson, Brian Richards, Colin Schmidt, Stephen Twigg, Huy Vo, and Andrew Waterman, &lt;em&gt;&lt;a href=&quot;https://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.html&quot;&gt;The Rocket Chip Generator&lt;/a&gt;&lt;/em&gt;, Technical Report UCB/EECS-2016-17, EECS Department, University of California, Berkeley, April 2016&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>chipsalliance/rocket-chip-inclusive-cache</title>
      <link>https://github.com/chipsalliance/rocket-chip-inclusive-cache</link>
      <description>&lt;p&gt;An RTL generator for a last-level shared inclusive TileLink cache controller&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rocket Chip SoC Inclusive Cache Generator&lt;/h1&gt; 
&lt;p&gt;This &lt;code&gt;block&lt;/code&gt; package contains an RTL generator for creating instances of a coherent, last-level, inclusive cache. The &lt;code&gt;InclusiveCache&lt;/code&gt; controller enforces coherence among a set of caching clients using an invalidation-based coherence policy implemetated on top of the the TileLink 1.8.1 coherence messaging protocol. This policy is implemented using a full-map of directory bits stored with each cache block&#39;s metadata tag.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;InclusiveCache&lt;/code&gt; is a TileLink adapter; it can be used as a drop-in replacement for Rocket-Chip&#39;s &lt;code&gt;tilelink.BroadcastHub&lt;/code&gt; coherence manager. It additionally supplies a SW-controlled interface for flusing cache blocks based on physical addresses.&lt;/p&gt; 
&lt;p&gt;The following parameters of the cache are easily &lt;code&gt;Config&lt;/code&gt;-urable: size, ways, banking and sub-banking factors, external bandwidth, network interface buffering.&lt;/p&gt; 
&lt;p&gt;Stand-alone unit tests coming soon.&lt;/p&gt; 
&lt;p&gt;This repository is a replacement for &lt;a href=&quot;https://github.com/sifive/block-inclusivecache-sifive&quot;&gt;https://github.com/sifive/block-inclusivecache-sifive&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>akka/akka</title>
      <link>https://github.com/akka/akka</link>
      <description>&lt;p&gt;A platform to build and run apps that are elastic, agile, and resilient. SDK, libraries, and hosted environments.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Akka&lt;/h1&gt; 
&lt;p&gt;The Akka family of projects is managed by teams at &lt;a href=&quot;https://akka.io/&quot;&gt;Akka&lt;/a&gt; with help from the community.&lt;/p&gt; 
&lt;p&gt;We believe that writing correct concurrent &amp;amp; distributed, resilient and elastic applications is too hard. Most of the time it&#39;s because we are using the wrong tools and the wrong level of abstraction.&lt;/p&gt; 
&lt;p&gt;Akka is here to change that.&lt;/p&gt; 
&lt;p&gt;Using the Actor Model we raise the abstraction level and provide a better platform to build correct concurrent and scalable applications. This model is a perfect match for the principles laid out in the &lt;a href=&quot;https://www.reactivemanifesto.org/&quot;&gt;Reactive Manifesto&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For resilience, we adopt the &quot;Let it crash&quot; model which the telecom industry has used with great success to build applications that self-heal and systems that never stop.&lt;/p&gt; 
&lt;p&gt;Actors also provide the abstraction for transparent distribution and the basis for truly scalable and fault-tolerant applications.&lt;/p&gt; 
&lt;p&gt;Learn more at &lt;a href=&quot;https://akka.io/&quot;&gt;akka.io&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Reference Documentation&lt;/h2&gt; 
&lt;p&gt;The reference documentation is available at &lt;a href=&quot;https://doc.akka.io&quot;&gt;doc.akka.io&lt;/a&gt;, for &lt;a href=&quot;https://doc.akka.io/libraries/akka-core/current/?language=scala&quot;&gt;Scala&lt;/a&gt; and &lt;a href=&quot;https://doc.akka.io/libraries/akka-core/current/?language=java&quot;&gt;Java&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Current versions of all Akka libraries&lt;/h2&gt; 
&lt;p&gt;The current versions of all Akka libraries are listed on the &lt;a href=&quot;https://doc.akka.io/libraries/akka-dependencies/current/&quot;&gt;Akka Dependencies&lt;/a&gt; page. Releases of the Akka core libraries in this repository are listed on the &lt;a href=&quot;https://github.com/akka/akka/releases&quot;&gt;GitHub releases&lt;/a&gt; page.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;You can join these groups and chats to discuss and ask Akka related questions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Forums: &lt;a href=&quot;https://discuss.akka.io&quot;&gt;discuss.akka.io&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Issue tracker: &lt;a href=&quot;https://github.com/akka/akka/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/github%3A-issues-blue.svg?style=flat-square&quot; alt=&quot;github: akka/akka&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In addition to that, you may enjoy following:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Questions tagged &lt;a href=&quot;https://stackoverflow.com/questions/tagged/akka&quot;&gt;#akka on StackOverflow&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Contributions are &lt;em&gt;very&lt;/em&gt; welcome!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you see an issue that you&#39;d like to see fixed, or want to shape out some ideas, the best way to make it happen is to help out by submitting a pull request implementing it. We welcome contributions from all, even you are not yet familiar with this project, We are happy to get you started, and will guide you through the process once you&#39;ve submitted your PR.&lt;/p&gt; 
&lt;p&gt;Refer to the &lt;a href=&quot;https://github.com/akka/akka/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; file for more details about the workflow, and general hints on how to prepare your pull request. You can also ask for clarifications or guidance in GitHub issues directly, or in the akka/dev chat if a more real time communication would be of benefit.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Akka is licensed under the Business Source License 1.1, please see the &lt;a href=&quot;https://akka.io/bsl-license-faq&quot;&gt;Akka License FAQ&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Tests and documentation are under a separate license, see the LICENSE file in each documentation and test root directory for details.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>chipsalliance/chisel</title>
      <link>https://github.com/chipsalliance/chisel</link>
      <description>&lt;p&gt;Chisel: A Modern Hardware Design Language&lt;/p&gt;&lt;hr&gt;&lt;a href=&quot;https://www.chisel-lang.org&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/docs/src/images/chisel_logo.svg?sanitize=true&quot; height=&quot;60&quot; /&gt; &lt;/a&gt; 
&lt;a href=&quot;https://www.chipsalliance.org&quot;&gt; &lt;img align=&quot;right&quot; src=&quot;https://raw.githubusercontent.com/chipsalliance/.github/main/profile/images/chips_alliance.svg?sanitize=true&quot; height=&quot;60&quot; /&gt; &lt;/a&gt; 
&lt;p&gt;The &lt;strong&gt;Constructing Hardware in a Scala Embedded Language&lt;/strong&gt; (&lt;a href=&quot;https://www.chisel-lang.org&quot;&gt;&lt;strong&gt;Chisel&lt;/strong&gt;&lt;/a&gt;) is an open-source hardware description language (HDL) used to describe digital electronics and circuits at the register-transfer level that facilitates &lt;strong&gt;advanced circuit generation and design reuse for both ASIC and FPGA digital logic designs&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Chisel adds hardware construction primitives to the &lt;a href=&quot;https://www.scala-lang.org&quot;&gt;Scala&lt;/a&gt; programming language, providing designers with the power of a modern programming language to write complex, parameterizable circuit generators that produce synthesizable Verilog. This generator methodology enables the creation of re-usable components and libraries, such as the FIFO queue and arbiters in the &lt;a href=&quot;https://www.chisel-lang.org/api/latest/#chisel3.util.package&quot;&gt;Chisel Standard Library&lt;/a&gt;, raising the level of abstraction in design while retaining fine-grained control.&lt;/p&gt; 
&lt;p&gt;For more information on the benefits of Chisel see: &lt;a href=&quot;https://stackoverflow.com/questions/53007782/what-benefits-does-chisel-offer-over-classic-hardware-description-languages&quot;&gt;&quot;What benefits does Chisel offer over classic Hardware Description Languages?&quot;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Chisel is powered by &lt;a href=&quot;https://github.com/chipsalliance/firrtl-spec&quot;&gt;FIRRTL (Flexible Intermediate Representation for RTL)&lt;/a&gt;, a hardware compiler framework implemented by &lt;a href=&quot;https://github.com/llvm/circt&quot;&gt;LLVM CIRCT&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Chisel is &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/LICENSE&quot;&gt;permissively licensed&lt;/a&gt; (Apache 2.0) under the guidance of &lt;a href=&quot;https://www.chipsalliance.org&quot;&gt;CHIPS Alliance&lt;/a&gt;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#what-does-chisel-code-look-like&quot;&gt;What does Chisel code look like?&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#led-blink&quot;&gt;LED blink&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#fir-filter&quot;&gt;FIR Filter&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#getting-started&quot;&gt;Getting Started&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#bootcamp-interactive-tutorial&quot;&gt;Bootcamp Interactive Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#a-textbook-on-chisel&quot;&gt;A Textbook on Chisel&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#build-your-own-chisel-projects&quot;&gt;Build Your Own Chisel Projects&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#guide-for-new-contributors&quot;&gt;Guide For New Contributors&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#design-verification&quot;&gt;Design Verification&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#documentation&quot;&gt;Documentation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#useful-resources&quot;&gt;Useful Resources&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#chisel-dev-meeting&quot;&gt;Chisel Dev Meeting&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#data-types-overview&quot;&gt;Data Types Overview&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#contributor-documentation&quot;&gt;Contributor Documentation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#useful-resources-for-contributors&quot;&gt;Useful Resources for Contributors&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#compiling-and-testing-chisel&quot;&gt;Compiling and Testing Chisel&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#running-projects-against-local-chisel&quot;&gt;Running Projects Against Local Chisel&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#chisel-architecture-overview&quot;&gt;Chisel Architecture Overview&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#chisel-sub-projects&quot;&gt;Chisel Sub-Projects&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#which-version-should-i-use&quot;&gt;Which version should I use?&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#roadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href=&quot;https://gitter.im/freechipsproject/chisel3?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&quot;&gt;&lt;img src=&quot;https://matrix.to/img/matrix-badge.svg?sanitize=true&quot; alt=&quot;Join the chat at https://gitter.im/freechipsproject/chisel3&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://javadoc.io/doc/org.chipsalliance/chisel_2.13/latest&quot;&gt;&lt;img src=&quot;https://www.javadoc.io/badge/org.chipsalliance/chisel_2.13.svg?color=blue&amp;amp;label=Scaladoc&quot; alt=&quot;Scaladoc&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://github.com/chipsalliance/chisel/actions/workflows/test.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt; &lt;a href=&quot;https://github.com/chipsalliance/chisel/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/tag/chipsalliance/chisel.svg?include_prereleases&amp;amp;sort=semver&quot; alt=&quot;GitHub tag (latest SemVer)&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://index.scala-lang.org/chipsalliance/chisel/chisel&quot;&gt;&lt;img src=&quot;https://index.scala-lang.org/chipsalliance/chisel/chisel/latest-by-scala-version.svg?platform=jvm&quot; alt=&quot;Scala version support&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What does Chisel code look like?&lt;/h2&gt; 
&lt;h3&gt;LED blink&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import chisel3._
import chisel3.util.Counter
import circt.stage.ChiselStage

class Blinky(freq: Int, startOn: Boolean = false) extends Module {
  val io = IO(new Bundle {
    val led0 = Output(Bool())
  })
  // Blink LED every second using Chisel built-in util.Counter
  val led = RegInit(startOn.B)
  val (_, counterWrap) = Counter(true.B, freq / 2)
  when(counterWrap) {
    led := ~led
  }
  io.led0 := led
}

object Main extends App {
  // These lines generate the Verilog output
  println(
    ChiselStage.emitSystemVerilog(
      new Blinky(1000),
      firtoolOpts = Array(&quot;-disable-all-randomization&quot;, &quot;-strip-debug-info&quot;)
    )
  )
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Should output the following Verilog:&lt;/p&gt; 
&lt;!--
Note that you can regenerate the HTML below by using VSCode with extensions:
* Markdown All in One: https://marketplace.visualstudio.com/items?itemName=yzhang.markdown-all-in-one
* Verilog-HDL/SystemVerilog/Bluespec SystemVerilog: https://marketplace.visualstudio.com/items?itemName=mshr-h.VerilogHDL

You then generate the Verilog and place it in a syntax highlighted code block in this file, eg.
```verilog
...
```
You can then run the command: &gt; Markdown All in One: Print current document to HTML
Then you can open the generated HTML and copy-paste
--&gt; 
&lt;details&gt; 
 &lt;summary&gt;Click to expand!&lt;/summary&gt; 
 &lt;p&gt;&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-verilog&quot;&gt;&lt;span class=&quot;hljs-comment&quot;&gt;// Generated by CIRCT firtool-1.37.0&lt;/span&gt;
&lt;span class=&quot;hljs-keyword&quot;&gt;module&lt;/span&gt; Blinky(
  &lt;span class=&quot;hljs-keyword&quot;&gt;input&lt;/span&gt;  clock,
         reset,
  &lt;span class=&quot;hljs-keyword&quot;&gt;output&lt;/span&gt; io_led0
);

  &lt;span class=&quot;hljs-keyword&quot;&gt;reg&lt;/span&gt;       led;
  &lt;span class=&quot;hljs-keyword&quot;&gt;reg&lt;/span&gt; [&lt;span class=&quot;hljs-number&quot;&gt;8&lt;/span&gt;:&lt;span class=&quot;hljs-number&quot;&gt;0&lt;/span&gt;] counterWrap_c_value;
  &lt;span class=&quot;hljs-keyword&quot;&gt;always&lt;/span&gt; @(&lt;span class=&quot;hljs-keyword&quot;&gt;posedge&lt;/span&gt; clock) &lt;span class=&quot;hljs-keyword&quot;&gt;begin&lt;/span&gt;
    &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (reset) &lt;span class=&quot;hljs-keyword&quot;&gt;begin&lt;/span&gt;
      led &amp;lt;= &lt;span class=&quot;hljs-number&quot;&gt;1&#39;h0&lt;/span&gt;;
      counterWrap_c_value &amp;lt;= &lt;span class=&quot;hljs-number&quot;&gt;9&#39;h0&lt;/span&gt;;
    &lt;span class=&quot;hljs-keyword&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;begin&lt;/span&gt;
      &lt;span class=&quot;hljs-keyword&quot;&gt;automatic&lt;/span&gt; &lt;span class=&quot;hljs-keyword&quot;&gt;logic&lt;/span&gt; counterWrap = counterWrap_c_value == &lt;span class=&quot;hljs-number&quot;&gt;9&#39;h1F3&lt;/span&gt;;
      led &amp;lt;= counterWrap ^ led;
      &lt;span class=&quot;hljs-keyword&quot;&gt;if&lt;/span&gt; (counterWrap)
        counterWrap_c_value &amp;lt;= &lt;span class=&quot;hljs-number&quot;&gt;9&#39;h0&lt;/span&gt;;
      &lt;span class=&quot;hljs-keyword&quot;&gt;else&lt;/span&gt;
        counterWrap_c_value &amp;lt;= counterWrap_c_value + &lt;span class=&quot;hljs-number&quot;&gt;9&#39;h1&lt;/span&gt;;
    &lt;span class=&quot;hljs-keyword&quot;&gt;end&lt;/span&gt;
  &lt;span class=&quot;hljs-keyword&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;hljs-comment&quot;&gt;// always @(posedge)&lt;/span&gt;
  &lt;span class=&quot;hljs-keyword&quot;&gt;assign&lt;/span&gt; io_led0 = led;
&lt;span class=&quot;hljs-keyword&quot;&gt;endmodule&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;FIR Filter&lt;/h3&gt; 
&lt;p&gt;Consider an FIR filter that implements a convolution operation, as depicted in this block diagram:&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/docs/src/images/fir_filter.svg?sanitize=true&quot; width=&quot;512&quot; /&gt; 
&lt;p&gt;While Chisel provides similar base primitives as synthesizable Verilog, and &lt;em&gt;could&lt;/em&gt; be used as such:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// 3-point moving sum implemented in the style of a FIR filter
class MovingSum3(bitWidth: Int) extends Module {
  val io = IO(new Bundle {
    val in = Input(UInt(bitWidth.W))
    val out = Output(UInt(bitWidth.W))
  })

  val z1 = RegNext(io.in)
  val z2 = RegNext(z1)

  io.out := (io.in * 1.U) + (z1 * 1.U) + (z2 * 1.U)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;the power of Chisel comes from the ability to create generators, such as an FIR filter that is defined by the list of coefficients:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// Generalized FIR filter parameterized by the convolution coefficients
class FirFilter(bitWidth: Int, coeffs: Seq[UInt]) extends Module {
  val io = IO(new Bundle {
    val in = Input(UInt(bitWidth.W))
    val out = Output(UInt(bitWidth.W))
  })
  // Create the serial-in, parallel-out shift register
  val zs = Reg(Vec(coeffs.length, UInt(bitWidth.W)))
  zs(0) := io.in
  for (i &amp;lt;- 1 until coeffs.length) {
    zs(i) := zs(i-1)
  }

  // Do the multiplies
  val products = VecInit.tabulate(coeffs.length)(i =&amp;gt; zs(i) * coeffs(i))

  // Sum up the products
  io.out := products.reduce(_ + _)
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;and use and re-use them across designs:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val movingSum3Filter = Module(new FirFilter(8, Seq(1.U, 1.U, 1.U)))  // same 3-point moving sum filter as before
val delayFilter = Module(new FirFilter(8, Seq(0.U, 1.U)))  // 1-cycle delay as a FIR filter
val triangleFilter = Module(new FirFilter(8, Seq(1.U, 2.U, 3.U, 2.U, 1.U)))  // 5-point FIR filter with a triangle impulse response
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The above can be converted to Verilog using &lt;code&gt;ChiselStage&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import chisel3.stage.ChiselGeneratorAnnotation
import circt.stage.{ChiselStage, FirtoolOption}

(new ChiselStage).execute(
  Array(&quot;--target&quot;, &quot;systemverilog&quot;),
  Seq(ChiselGeneratorAnnotation(() =&amp;gt; new FirFilter(8, Seq(1.U, 1.U, 1.U))),
    FirtoolOption(&quot;--disable-all-randomization&quot;))
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you may generate some Verilog directly for inspection:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val verilogString = chisel3.getVerilogString(new FirFilter(8, Seq(0.U, 1.U)))
println(verilogString)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Bootcamp Interactive Tutorial&lt;/h3&gt; 
&lt;p&gt;The &lt;a href=&quot;https://mybinder.org/v2/gh/freechipsproject/chisel-bootcamp/master&quot;&gt;&lt;strong&gt;online Chisel Bootcamp&lt;/strong&gt;&lt;/a&gt; is the recommended way to get started with and learn Chisel. &lt;strong&gt;No setup is required&lt;/strong&gt; (it runs in the browser), nor does it assume any prior knowledge of Scala.&lt;/p&gt; 
&lt;p&gt;The &lt;a href=&quot;https://github.com/ucb-bar/chisel-tutorial&quot;&gt;&lt;strong&gt;classic Chisel tutorial&lt;/strong&gt;&lt;/a&gt; contains small exercises and runs on your computer.&lt;/p&gt; 
&lt;h3&gt;A Textbook on Chisel&lt;/h3&gt; 
&lt;p&gt;If you like a textbook to learn Chisel and also a bit of digital design in general, you may be interested in reading &lt;a href=&quot;http://www.imm.dtu.dk/~masca/chisel-book.html&quot;&gt;&lt;strong&gt;Digital Design with Chisel&lt;/strong&gt;&lt;/a&gt;. It is available in English, Chinese, Japanese, and Vietnamese.&lt;/p&gt; 
&lt;h3&gt;Build Your Own Chisel Projects&lt;/h3&gt; 
&lt;p&gt;Please see &lt;a href=&quot;https://www.chisel-lang.org/docs/installation&quot;&gt;the Installation page&lt;/a&gt; of the Chisel website for information about how to use Chisel locally.&lt;/p&gt; 
&lt;p&gt;When you&#39;re ready to build your own circuits in Chisel, &lt;strong&gt;we recommend starting from the &lt;a href=&quot;https://github.com/chipsalliance/chisel-template&quot;&gt;Chisel Template&lt;/a&gt; repository&lt;/strong&gt;, which provides a pre-configured project, example design, and testbench. Follow the &lt;a href=&quot;https://github.com/chipsalliance/chisel-template&quot;&gt;chisel-template README&lt;/a&gt; to get started.&lt;/p&gt; 
&lt;p&gt;If you insist on setting up your own project from scratch, your project needs to depend on both the chisel-plugin (Scalac plugin) and the chisel library. For example, in SBT this could be expressed as:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// build.sbt
scalaVersion := &quot;2.13.16&quot;
val chiselVersion = &quot;7.1.0&quot;
addCompilerPlugin(&quot;org.chipsalliance&quot; % &quot;chisel-plugin&quot; % chiselVersion cross CrossVersion.full)
libraryDependencies += &quot;org.chipsalliance&quot; %% &quot;chisel&quot; % chiselVersion
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For Chisel prior to v5.0.0, Chisel was published using a different artifact name:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;// build.sbt
scalaVersion := &quot;2.13.10&quot;
addCompilerPlugin(&quot;edu.berkeley.cs&quot; % &quot;chisel3-plugin&quot; % &quot;3.6.0&quot; cross CrossVersion.full)
libraryDependencies += &quot;edu.berkeley.cs&quot; %% &quot;chisel3&quot; % &quot;3.6.0&quot;
// We also recommend using chiseltest for writing unit tests
libraryDependencies += &quot;edu.berkeley.cs&quot; %% &quot;chiseltest&quot; % &quot;0.6.0&quot; % &quot;test&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Guide For New Contributors&lt;/h3&gt; 
&lt;p&gt;If you are trying to make a contribution to this project, please read &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Design Verification&lt;/h3&gt; 
&lt;p&gt;These simulation-based verification tools are available for Chisel:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/svsim&quot;&gt;&lt;strong&gt;svsim&lt;/strong&gt;&lt;/a&gt; is the lightweight testing library for Chisel, included in this repository.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ucb-bar/chiseltest&quot;&gt;&lt;strong&gt;chiseltest (Chisel 6.0 and before)&lt;/strong&gt;&lt;/a&gt; is the batteries-included testing and formal verification library for Chisel-based RTL designs and a replacement for the former PeekPokeTester, providing the same base constructs but with a streamlined interface and concurrency support with &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;join&lt;/code&gt; with internal and Verilator integration for simulations.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;h3&gt;Useful Resources&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/freechipsproject/chisel-cheatsheet/releases/latest/download/chisel_cheatsheet.pdf&quot;&gt;&lt;strong&gt;Cheat Sheet&lt;/strong&gt;&lt;/a&gt;, a 2-page reference of the base Chisel syntax and libraries&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.chisel-lang.org/api/latest/index.html&quot;&gt;&lt;strong&gt;ScalaDoc (latest)&lt;/strong&gt;&lt;/a&gt;, a listing, description, and examples of the functionality exposed by Chisel, &lt;a href=&quot;https://www.chisel-lang.org/api/&quot;&gt;older versions&lt;/a&gt; are also available&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gitter.im/freechipsproject/chisel3&quot;&gt;&lt;strong&gt;Gitter&lt;/strong&gt;&lt;/a&gt;, where you can ask questions or discuss anything Chisel&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.chisel-lang.org&quot;&gt;&lt;strong&gt;Website&lt;/strong&gt;&lt;/a&gt; (&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/website&quot;&gt;source&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://scastie.scala-lang.org/CsDO7Q3TQHmBWJfKEB85Tw&quot;&gt;&lt;strong&gt;Scastie (v6.0.0)&lt;/strong&gt;&lt;/a&gt; - cannot generate Verilog (firtool does not work in Scastie)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://scastie.scala-lang.org/1XICrlaZQs6ZvxpuKdFdDw&quot;&gt;&lt;strong&gt;Scastie (v3.6.0)&lt;/strong&gt;&lt;/a&gt; - generates Verilog with legacy Scala FIRRTL Compiler&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.asic-world.com/verilog/veritut.html&quot;&gt;&lt;strong&gt;asic-world&lt;/strong&gt;&lt;/a&gt; If you aren&#39;t familiar with verilog, this is a good tutorial.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you are migrating from Chisel2, see &lt;a href=&quot;https://www.chisel-lang.org/chisel3/docs/appendix/chisel3-vs-chisel2.html&quot;&gt;the migration guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Chisel Dev Meeting&lt;/h3&gt; 
&lt;p&gt;Chisel/FIRRTL development meetings happen every Monday from 9:00-10:00 am PT.&lt;/p&gt; 
&lt;p&gt;Call-in info and meeting notes are available &lt;a href=&quot;https://docs.google.com/document/d/1BLP2DYt59DqI-FgFCcjw8Ddl4K-WU0nHmQu0sZ_wAGo/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Data Types Overview&lt;/h3&gt; 
&lt;p&gt;These are the base data types for defining circuit components:&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/docs/src/images/type_hierarchy.svg?sanitize=true&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Contributor Documentation&lt;/h2&gt; 
&lt;p&gt;This section describes how to get started contributing to Chisel itself, including how to test your version locally against other projects that pull in Chisel using &lt;a href=&quot;https://www.scala-sbt.org/1.x/docs/Library-Dependencies.html&quot;&gt;sbt&#39;s managed dependencies&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Useful Resources for Contributors&lt;/h3&gt; 
&lt;p&gt;The &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/#useful-resources&quot;&gt;Useful Resources&lt;/a&gt; for users are also helpful for contributors.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/1gMtABxBEDFbCFXN_-dPyvycNAyFROZKwk-HMcnxfTnU/edit?usp=sharing&quot;&gt;&lt;strong&gt;Chisel Breakdown Slides&lt;/strong&gt;&lt;/a&gt;, an introductory talk about Chisel&#39;s internals&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Compiling and Testing Chisel&lt;/h3&gt; 
&lt;p&gt;You must first install required dependencies to build Chisel locally, please see &lt;a href=&quot;https://www.chisel-lang.org/docs/installation&quot;&gt;the installation instructions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Clone and build the Chisel library:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/chipsalliance/chisel.git
cd chisel
./mill chisel[].compile
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In order to run the following unit tests, you will need several tools on your &lt;code&gt;PATH&lt;/code&gt;, namely &lt;a href=&quot;https://www.veripool.org/verilator/&quot;&gt;verilator&lt;/a&gt;, &lt;a href=&quot;https://yosyshq.net/yosys/&quot;&gt;yosys&lt;/a&gt;, &lt;a href=&quot;https://github.com/chipsalliance/espresso&quot;&gt;espresso&lt;/a&gt;, &lt;a href=&quot;https://github.com/MikePopoloski/slang&quot;&gt;slang&lt;/a&gt;, and &lt;a href=&quot;https://llvm.org/docs/CommandGuide/FileCheck.html&quot;&gt;filecheck&lt;/a&gt;. Check that each is installed on your &lt;code&gt;PATH&lt;/code&gt; by running &lt;code&gt;which verilator&lt;/code&gt; and so on.&lt;/p&gt; 
&lt;p&gt;If the compilation succeeded and the dependencies noted above are installed, you can then run the included unit tests by invoking:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./mill chisel[].test
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Projects Against Local Chisel&lt;/h3&gt; 
&lt;p&gt;To use the development version of Chisel (&lt;code&gt;main&lt;/code&gt; branch), you will need to build from source and publish locally. The repository version can be found by running &lt;code&gt;./mill show unipublish.publishVersion&lt;/code&gt;. As of the time of writing it was: &lt;code&gt;7.0.0-M2+431-4798bea7-SNAPSHOT&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;To publish your version of Chisel to the local Ivy repository, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;./mill unipublish.publishLocal
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The compiled version gets placed in &lt;code&gt;~/.ivy2/local/org.chipsalliance/&lt;/code&gt;. If you need to un-publish your local copy of Chisel, remove the directory generated in &lt;code&gt;~/.ivy2/local/org.chipsalliance/&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;In order to have your projects use this version of Chisel, you should update the &lt;code&gt;libraryDependencies&lt;/code&gt; setting in your project&#39;s build.sbt file to use the current version, for example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val chiselVersion = &quot;7.0.0-M2+431-4798bea7-SNAPSHOT&quot;
addCompilerPlugin(&quot;org.chipsalliance&quot; % &quot;chisel-plugin&quot; % chiselVersion cross CrossVersion.full)
libraryDependencies += &quot;org.chipsalliance&quot; %% &quot;chisel&quot; % chiselVersion
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Chisel Architecture Overview&lt;/h3&gt; 
&lt;p&gt;The Chisel compiler consists of these main parts:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;The frontend&lt;/strong&gt;, &lt;code&gt;chisel3.*&lt;/code&gt;, which is the publicly visible &quot;API&quot; of Chisel and what is used in Chisel RTL. These just add data to the...&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The Builder&lt;/strong&gt;, &lt;code&gt;chisel3.internal.Builder&lt;/code&gt;, which maintains global state (like the currently open Module) and contains commands, generating...&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The intermediate data structures&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.*&lt;/code&gt;, which are syntactically very similar to Firrtl. Once the entire circuit has been elaborated, the top-level object (a &lt;code&gt;Circuit&lt;/code&gt;) is then passed to...&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;The Firrtl emitter&lt;/strong&gt;, &lt;code&gt;chisel3.firrtl.Emitter&lt;/code&gt;, which turns the intermediate data structures into a string that can be written out into a Firrtl file for further processing.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Also included is:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;The standard library&lt;/strong&gt; of circuit generators, &lt;code&gt;chisel3.util.*&lt;/code&gt;. These contain commonly used interfaces and constructors (like &lt;code&gt;Decoupled&lt;/code&gt;, which wraps a signal with a ready-valid pair) as well as fully parameterizable circuit generators (like arbiters and multiplexors).&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chisel Stage&lt;/strong&gt;, &lt;code&gt;chisel3.stage.*&lt;/code&gt;, which contains compilation and test functions that are invoked in the standard Verilog generation and simulation testing infrastructure. These can also be used as part of custom flows.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Chisel Sub-Projects&lt;/h3&gt; 
&lt;p&gt;Chisel consists of several Scala projects; each is its own separate compilation unit:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/core&quot;&gt;&lt;code&gt;core&lt;/code&gt;&lt;/a&gt; is the bulk of the source code of Chisel, depends on &lt;code&gt;firrtl&lt;/code&gt;, &lt;code&gt;svsim&lt;/code&gt;, and &lt;code&gt;macros&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/firrtl&quot;&gt;&lt;code&gt;firrtl&lt;/code&gt;&lt;/a&gt; is the vestigial remains of the old Scala FIRRTL compiler, much if it will likely be absorbed into &lt;code&gt;core&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/macros&quot;&gt;&lt;code&gt;macros&lt;/code&gt;&lt;/a&gt; is most of the macros used in Chisel, no internal dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/plugin&quot;&gt;&lt;code&gt;plugin&lt;/code&gt;&lt;/a&gt; is the compiler plugin, no internal dependencies&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/src/main&quot;&gt;&lt;code&gt;src/main&lt;/code&gt;&lt;/a&gt; is the &quot;main&quot; that brings it all together and includes a &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/src/main/scala/chisel3/util&quot;&gt;&lt;code&gt;util&lt;/code&gt;&lt;/a&gt; library, which depends on &lt;code&gt;core&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/svsim&quot;&gt;&lt;code&gt;svsim&lt;/code&gt;&lt;/a&gt; is a low-level library for compiling and controlling SystemVerilog simulations, currently targeting Verilator and VCS as backends&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Code that touches lots of APIs that are private to the &lt;code&gt;chisel3&lt;/code&gt; package should belong in &lt;code&gt;core&lt;/code&gt;, while code that is pure Chisel should belong in &lt;code&gt;src/main&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Which version should I use?&lt;/h3&gt; 
&lt;p&gt;We encourage Chisel users (as opposed to Chisel developers), to use the latest release version of Chisel. This &lt;a href=&quot;https://github.com/chipsalliance/chisel-template&quot;&gt;chisel-template&lt;/a&gt; repository is kept up-to-date, depending on the most recent version of Chisel. The recommended version is also captured near the top of this README, and in the &lt;a href=&quot;https://github.com/chipsalliance/chisel/releases&quot;&gt;Github releases&lt;/a&gt; section of this repo. If you encounter an issue with a released version of Chisel, please file an issue on GitHub mentioning the Chisel version and provide a simple test case (if possible). Try to reproduce the issue with the associated latest minor release (to verify that the issue hasn&#39;t been already addressed).&lt;/p&gt; 
&lt;p&gt;For more information on our versioning policy and what versions of the various Chisel ecosystem projects work together, see &lt;a href=&quot;https://www.chisel-lang.org/chisel3/docs/appendix/versioning.html&quot;&gt;Chisel Project Versioning&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you&#39;re developing a Chisel library (or &lt;code&gt;chisel3&lt;/code&gt; itself), you&#39;ll probably want to work closer to the tip of the development trunk. By default, the main branch of the chisel repository is configured to build and publish its version of the code as &lt;code&gt;&amp;lt;version&amp;gt;+&amp;lt;n&amp;gt;-&amp;lt;commit hash&amp;gt;-SNAPSHOT&lt;/code&gt;. Updated SNAPSHOTs are published on every push to main. You are encouraged to do your development against the latest SNAPSHOT, but note that neither API nor ABI compatibility is guaranteed so your code may break at any time.&lt;/p&gt; 
&lt;h3&gt;Roadmap&lt;/h3&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/chipsalliance/chisel/main/ROADMAP.md&quot;&gt;Roadmap&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>delta-io/delta</title>
      <link>https://github.com/delta-io/delta</link>
      <description>&lt;p&gt;An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://docs.delta.io/latest/_static/delta-lake-white.png&quot; width=&quot;200&quot; alt=&quot;Delta Lake Logo&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/delta-io/delta/actions/workflows/test.yaml&quot;&gt;&lt;img src=&quot;https://github.com/delta-io/delta/actions/workflows/test.yaml/badge.svg?sanitize=true&quot; alt=&quot;Test&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/LICENSE.txt&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-Apache%202-brightgreen.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/delta-spark/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/delta-spark.svg?sanitize=true&quot; alt=&quot;PyPI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypistats.org/packages/delta-spark&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/dm/delta-spark&quot; alt=&quot;PyPI - Downloads&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Delta Lake is an open-source storage framework that enables building a &lt;a href=&quot;http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf&quot;&gt;Lakehouse architecture&lt;/a&gt; with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;See the &lt;a href=&quot;https://docs.delta.io&quot;&gt;Delta Lake Documentation&lt;/a&gt; for details.&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/quick-start.html&quot;&gt;Quick Start Guide&lt;/a&gt; to get started with Scala, Java and Python.&lt;/li&gt; 
 &lt;li&gt;Note, this repo is one of many Delta Lake repositories in the &lt;a href=&quot;https://github.com/delta-io&quot;&gt;delta.io&lt;/a&gt; organizations including &lt;a href=&quot;https://github.com/delta-io/delta&quot;&gt;delta&lt;/a&gt;, &lt;a href=&quot;https://github.com/delta-io/delta-rs&quot;&gt;delta-rs&lt;/a&gt;, &lt;a href=&quot;https://github.com/delta-io/delta-sharing&quot;&gt;delta-sharing&lt;/a&gt;, &lt;a href=&quot;https://github.com/delta-io/kafka-delta-ingest&quot;&gt;kafka-delta-ingest&lt;/a&gt;, and &lt;a href=&quot;https://github.com/delta-io/website&quot;&gt;website&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The following are some of the more popular Delta Lake integrations, refer to &lt;a href=&quot;https://delta.io/integrations/&quot;&gt;delta.io/integrations&lt;/a&gt; for the complete list:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/&quot;&gt;Apache Spark‚Ñ¢&lt;/a&gt;: This connector allows Apache Spark‚Ñ¢ to read from and write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/delta-io/delta/tree/master/connectors/flink&quot;&gt;Apache Flink (Preview)&lt;/a&gt;: This connector allows Apache Flink to write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://prestodb.io/docs/current/connector/deltalake.html&quot;&gt;PrestoDB&lt;/a&gt;: This connector allows PrestoDB to read from Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://trino.io/docs/current/connector/delta-lake.html&quot;&gt;Trino&lt;/a&gt;: This connector allows Trino to read from and write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/delta-standalone.html&quot;&gt;Delta Standalone&lt;/a&gt;: This library allows Scala and Java-based projects (including Apache Flink, Apache Hive, Apache Beam, and PrestoDB) to read from and write to Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/hive-integration.html&quot;&gt;Apache Hive&lt;/a&gt;: This connector allows Apache Hive to read from Delta Lake.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.rs/deltalake/latest/deltalake/&quot;&gt;Delta Rust API&lt;/a&gt;: This library allows Rust (with Python and Ruby bindings) low level access to Delta tables and is intended to be used with data processing frameworks like datafusion, ballista, rust-dataframe, vega, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;br /&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;&lt;em&gt;Table of Contents&lt;/em&gt;&lt;/strong&gt;&lt;/summary&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#latest-binaries&quot;&gt;Latest binaries&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#api-documentation&quot;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#compatibility&quot;&gt;Compatibility&lt;/a&gt; 
   &lt;ul&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#api-compatibility&quot;&gt;API Compatibility&lt;/a&gt;&lt;/li&gt; 
    &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#data-storage-compatibility&quot;&gt;Data Storage Compatibility&lt;/a&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#roadmap&quot;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#building&quot;&gt;Building&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#transaction-protocol&quot;&gt;Transaction Protocol&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#requirements-for-underlying-storage-systems&quot;&gt;Requirements for Underlying Storage Systems&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#concurrency-control&quot;&gt;Concurrency Control&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#reporting-issues&quot;&gt;Reporting issues&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#contributing&quot;&gt;Contributing&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#license&quot;&gt;License&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#community&quot;&gt;Community&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h2&gt;Latest Binaries&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/&quot;&gt;online documentation&lt;/a&gt; for the latest release.&lt;/p&gt; 
&lt;h2&gt;API Documentation&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/delta-apidoc.html&quot;&gt;Scala API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/api/java/index.html&quot;&gt;Java API docs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.delta.io/latest/api/python/index.html&quot;&gt;Python API docs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Compatibility&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://docs.delta.io/latest/delta-standalone.html&quot;&gt;Delta Standalone&lt;/a&gt; library is a single-node Java library that can be used to read from and write to Delta tables. Specifically, this library provides APIs to interact with a table‚Äôs metadata in the transaction log, implementing the Delta Transaction Log Protocol to achieve the transactional guarantees of the Delta Lake format.&lt;/p&gt; 
&lt;h3&gt;API Compatibility&lt;/h3&gt; 
&lt;p&gt;There are two types of APIs provided by the Delta Lake project.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Direct Java/Scala/Python APIs - The classes and methods documented in the &lt;a href=&quot;https://docs.delta.io/latest/delta-apidoc.html&quot;&gt;API docs&lt;/a&gt; are considered as stable public APIs. All other classes, interfaces, methods that may be directly accessible in code are considered internal, and they are subject to change across releases.&lt;/li&gt; 
 &lt;li&gt;Spark-based APIs - You can read Delta tables through the &lt;code&gt;DataFrameReader&lt;/code&gt;/&lt;code&gt;Writer&lt;/code&gt; (i.e. &lt;code&gt;spark.read&lt;/code&gt;, &lt;code&gt;df.write&lt;/code&gt;, &lt;code&gt;spark.readStream&lt;/code&gt; and &lt;code&gt;df.writeStream&lt;/code&gt;). Options to these APIs will remain stable within a major release of Delta Lake (e.g., 1.x.x).&lt;/li&gt; 
 &lt;li&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/releases.html&quot;&gt;online documentation&lt;/a&gt; for the releases and their compatibility with Apache Spark versions.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Data Storage Compatibility&lt;/h3&gt; 
&lt;p&gt;Delta Lake guarantees backward compatibility for all Delta Lake tables (i.e., newer versions of Delta Lake will always be able to read tables written by older versions of Delta Lake). However, we reserve the right to break forward compatibility as new features are introduced to the transaction protocol (i.e., an older version of Delta Lake may not be able to read a table produced by a newer version).&lt;/p&gt; 
&lt;p&gt;Breaking changes in the protocol are indicated by incrementing the minimum reader/writer version in the &lt;code&gt;Protocol&lt;/code&gt; &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/spark/src/test/scala/org/apache/spark/sql/delta/ActionSerializerSuite.scala&quot;&gt;action&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Roadmap&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;For the high-level Delta Lake roadmap, see &lt;a href=&quot;http://delta.io/roadmap&quot;&gt;Delta Lake 2022H1 roadmap&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;For the detailed timeline, see the &lt;a href=&quot;https://github.com/delta-io/delta/milestones&quot;&gt;project roadmap&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Transaction Protocol&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/PROTOCOL.md&quot;&gt;Delta Transaction Log Protocol&lt;/a&gt; document provides a specification of the transaction protocol.&lt;/p&gt; 
&lt;h2&gt;Requirements for Underlying Storage Systems&lt;/h2&gt; 
&lt;p&gt;Delta Lake ACID guarantees are predicated on the atomicity and durability guarantees of the storage system. Specifically, we require the storage system to provide the following.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;Atomic visibility&lt;/strong&gt;: There must be a way for a file to be visible in its entirety or not visible at all.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mutual exclusion&lt;/strong&gt;: Only one writer must be able to create (or rename) a file at the final destination.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Consistent listing&lt;/strong&gt;: Once a file has been written in a directory, all future listings for that directory must return that file.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://docs.delta.io/latest/delta-storage.html&quot;&gt;online documentation on Storage Configuration&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Concurrency Control&lt;/h2&gt; 
&lt;p&gt;Delta Lake ensures &lt;em&gt;serializability&lt;/em&gt; for concurrent reads and writes. Please see &lt;a href=&quot;https://docs.delta.io/latest/delta-concurrency.html&quot;&gt;Delta Lake Concurrency Control&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Reporting issues&lt;/h2&gt; 
&lt;p&gt;We use &lt;a href=&quot;https://github.com/delta-io/delta/issues&quot;&gt;GitHub Issues&lt;/a&gt; to track community reported issues. You can also &lt;a href=&quot;https://raw.githubusercontent.com/delta-io/delta/master/#community&quot;&gt;contact&lt;/a&gt; the community for getting answers.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We welcome contributions to Delta Lake. See our &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;We also adhere to the &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/CODE_OF_CONDUCT.md&quot;&gt;Delta Lake Code of Conduct&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building&lt;/h2&gt; 
&lt;p&gt;Delta Lake is compiled using &lt;a href=&quot;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&quot;&gt;SBT&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To compile, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt compile
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To generate artifacts, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt package
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute tests, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute a single test suite, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt spark/&#39;testOnly org.apache.spark.sql.delta.optimize.OptimizeCompactionSQLSuite&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To execute a single test within and a single test suite, run&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;build/sbt spark/&#39;testOnly *.OptimizeCompactionSQLSuite -- -z &quot;optimize command: on partitioned table - all partitions&quot;&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Refer to &lt;a href=&quot;https://www.scala-sbt.org/1.x/docs/Command-Line-Reference.html&quot;&gt;SBT docs&lt;/a&gt; for more commands.&lt;/p&gt; 
&lt;h2&gt;Running python tests locally&lt;/h2&gt; 
&lt;h3&gt;Setup Environment&lt;/h3&gt; 
&lt;h4&gt;Install Conda (Skip if you already installed it)&lt;/h4&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://www.anaconda.com/download/&quot;&gt;Conda Download&lt;/a&gt; to install Anaconda.&lt;/p&gt; 
&lt;h4&gt;Create an environment from environment file&lt;/h4&gt; 
&lt;p&gt;Follow &lt;a href=&quot;https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-from-file&quot;&gt;Create Environment From Environment file&lt;/a&gt; to create a Conda environment from &lt;code&gt;&amp;lt;repo-root&amp;gt;/python/environment.yml&lt;/code&gt; and activate the newly created &lt;code&gt;delta_python_tests&lt;/code&gt; environment.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;# Note the `--file` argument should be a fully qualified path. Using `~` in file
# path doesn&#39;t work. Example valid path: `/Users/macuser/delta/python/environment.yml`

conda env create --name delta_python_tests --file=&amp;lt;absolute_path_to_delta_repo&amp;gt;/python/environment.yml`
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;JDK Setup&lt;/h4&gt; 
&lt;p&gt;Build needs JDK 1.8. Make sure to setup &lt;code&gt;JAVA_HOME&lt;/code&gt; that points to JDK 1.8.&lt;/p&gt; 
&lt;h4&gt;Running tests&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;conda activate delta_python_tests
python3 &amp;lt;delta-root&amp;gt;/python/run-tests.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IntelliJ Setup&lt;/h2&gt; 
&lt;p&gt;IntelliJ is the recommended IDE to use when developing Delta Lake. To import Delta Lake as a new project:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Clone Delta Lake into, for example, &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In IntelliJ, select &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;New Project&lt;/code&gt; &amp;gt; &lt;code&gt;Project from Existing Sources...&lt;/code&gt; and select &lt;code&gt;~/delta&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Under &lt;code&gt;Import project from external model&lt;/code&gt; select &lt;code&gt;sbt&lt;/code&gt;. Click &lt;code&gt;Next&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Under &lt;code&gt;Project JDK&lt;/code&gt; specify a valid Java &lt;code&gt;1.8&lt;/code&gt; JDK and opt to use SBT shell for &lt;code&gt;project reload&lt;/code&gt; and &lt;code&gt;builds&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Click &lt;code&gt;Finish&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In your terminal, run &lt;code&gt;build/sbt clean package&lt;/code&gt;. Make sure you use Java &lt;code&gt;1.8&lt;/code&gt;. The build will generate files that are necessary for Intellij to index the repository.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Setup Verification&lt;/h3&gt; 
&lt;p&gt;After waiting for IntelliJ to index, verify your setup by running a test suite in IntelliJ.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Search for and open &lt;code&gt;DeltaLogSuite&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Next to the class declaration, right click on the two green arrows and select &lt;code&gt;Run &#39;DeltaLogSuite&#39;&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;p&gt;If you see errors of the form&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Error:(46, 28) object DeltaSqlBaseParser is not a member of package io.delta.sql.parser
import io.delta.sql.parser.DeltaSqlBaseParser._
...
Error:(91, 22) not found: type DeltaSqlBaseParser
    val parser = new DeltaSqlBaseParser(tokenStream)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;then follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Ensure you are using Java &lt;code&gt;1.8&lt;/code&gt;. You can set this using&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;export JAVA_HOME=`/usr/libexec/java_home -v 1.8`
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Compile using the SBT CLI: &lt;code&gt;build/sbt clean compile&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; &amp;gt; &lt;code&gt;Project Structure...&lt;/code&gt; &amp;gt; &lt;code&gt;Modules&lt;/code&gt; &amp;gt; &lt;code&gt;delta-spark&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;In the right panel under &lt;code&gt;Source Folders&lt;/code&gt; remove any &lt;code&gt;target&lt;/code&gt; folders, e.g. &lt;code&gt;target/scala-2.12/src_managed/main [generated]&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Click &lt;code&gt;Apply&lt;/code&gt; and then re-run your test.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Apache License 2.0, see &lt;a href=&quot;https://github.com/delta-io/delta/raw/master/LICENSE.txt&quot;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Community&lt;/h2&gt; 
&lt;p&gt;There are two mediums of communication within the Delta Lake community.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Public Slack Channel 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://go.delta.io/slack&quot;&gt;Register here&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://delta-users.slack.com/&quot;&gt;Login here&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/company/deltalake&quot;&gt;Linkedin page&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/deltalake&quot;&gt;Youtube channel&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Public &lt;a href=&quot;https://groups.google.com/forum/#!forum/delta-users&quot;&gt;Mailing list&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>awslabs/deequ</title>
      <link>https://github.com/awslabs/deequ</link>
      <description>&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &quot;unit tests for data&quot;, which measure data quality in large datasets.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deequ - Unit Tests for Data&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/license/awslabs/deequ.svg?sanitize=true&quot; alt=&quot;GitHub license&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/awslabs/deequ/issues&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/issues/awslabs/deequ.svg?sanitize=true&quot; alt=&quot;GitHub issues&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/awslabs/deequ/actions/workflows/maven.yml?query=branch%3Amaster&quot;&gt;&lt;img src=&quot;https://github.com/awslabs/deequ/actions/workflows/maven.yml/badge.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ&quot;&gt;&lt;img src=&quot;https://maven-badges.herokuapp.com/maven-central/com.amazon.deequ/deequ/badge.svg?sanitize=true&quot; alt=&quot;Maven Central&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Deequ is a library built on top of Apache Spark for defining &quot;unit tests for data&quot;, which measure data quality in large datasets. We are happy to receive feedback and &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/CONTRIBUTING.md&quot;&gt;contributions&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Python users may also be interested in PyDeequ, a Python interface for Deequ. You can find PyDeequ on &lt;a href=&quot;https://github.com/awslabs/python-deequ&quot;&gt;GitHub&lt;/a&gt;, &lt;a href=&quot;https://pydeequ.readthedocs.io/en/latest/README.html&quot;&gt;readthedocs&lt;/a&gt;, and &lt;a href=&quot;https://pypi.org/project/pydeequ/&quot;&gt;PyPI&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Requirements and Installation&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; depends on Java 8. Deequ version 2.x only runs with Spark 3.1, and vice versa. If you rely on a previous Spark version, please use a Deequ 1.x version (legacy version is maintained in legacy-spark-3.0 branch). We provide legacy releases compatible with Apache Spark versions 2.2.x to 3.0.x. The Spark 2.2.x and 2.3.x releases depend on Scala 2.11 and the Spark 2.4.x, 3.0.x, and 3.1.x releases depend on Scala 2.12.&lt;/p&gt; 
&lt;p&gt;Available via &lt;a href=&quot;http://mvnrepository.com/artifact/com.amazon.deequ/deequ&quot;&gt;maven central&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Choose the latest release that matches your Spark version from the &lt;a href=&quot;https://repo1.maven.org/maven2/com/amazon/deequ/deequ/&quot;&gt;available versions&lt;/a&gt;. Add the release as a dependency to your project. For example, for Spark 3.1.x:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Maven&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;com.amazon.deequ&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;deequ&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;2.0.0-spark-3.1&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;sbt&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;libraryDependencies += &quot;com.amazon.deequ&quot; % &quot;deequ&quot; % &quot;2.0.0-spark-3.1&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt;&#39;s purpose is to &quot;unit-test&quot; data to find errors early, before the data gets fed to consuming systems or machine learning algorithms. In the following, we will walk you through a toy example to showcase the most basic usage of our library. An executable version of the example is available &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/BasicExample.scala&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Deequ&lt;/strong&gt; works on tabular data, e.g., CSV files, database tables, logs, flattened json files, basically anything that you can fit into a Spark dataframe. For this example, we assume that we work on some kind of &lt;code&gt;Item&lt;/code&gt; data, where every item has an id, a productName, a description, a priority and a count of how often it has been viewed.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;case class Item(
  id: Long,
  productName: String,
  description: String,
  priority: String,
  numViews: Long
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Our library is built on &lt;a href=&quot;https://spark.apache.org/&quot;&gt;Apache Spark&lt;/a&gt; and is designed to work with very large datasets (think billions of rows) that typically live in a distributed filesystem or a data warehouse. For the sake of simplicity in this example, we just generate a few toy records though.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;val rdd = spark.sparkContext.parallelize(Seq(
  Item(1, &quot;Thingy A&quot;, &quot;awesome thing.&quot;, &quot;high&quot;, 0),
  Item(2, &quot;Thingy B&quot;, &quot;available at http://thingb.com&quot;, null, 0),
  Item(3, null, null, &quot;low&quot;, 5),
  Item(4, &quot;Thingy D&quot;, &quot;checkout https://thingd.ca&quot;, &quot;low&quot;, 10),
  Item(5, &quot;Thingy E&quot;, null, &quot;high&quot;, 12)))

val data = spark.createDataFrame(rdd)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Most applications that work with data have implicit assumptions about that data, e.g., that attributes have certain types, do not contain NULL values, and so on. If these assumptions are violated, your application might crash or produce wrong outputs. The idea behind &lt;strong&gt;deequ&lt;/strong&gt; is to explicitly state these assumptions in the form of a &quot;unit-test&quot; for data, which can be verified on a piece of data at hand. If the data has errors, we can &quot;quarantine&quot; and fix it, before we feed it to an application.&lt;/p&gt; 
&lt;p&gt;The main entry point for defining how you expect your data to look is the &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationSuite.scala&quot;&gt;VerificationSuite&lt;/a&gt; from which you can add &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/checks/Check.scala&quot;&gt;Checks&lt;/a&gt; that define constraints on attributes of the data. In this example, we test for the following properties of our data:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;there are 5 rows in total&lt;/li&gt; 
 &lt;li&gt;values of the &lt;code&gt;id&lt;/code&gt; attribute are never NULL and unique&lt;/li&gt; 
 &lt;li&gt;values of the &lt;code&gt;productName&lt;/code&gt; attribute are never NULL&lt;/li&gt; 
 &lt;li&gt;the &lt;code&gt;priority&lt;/code&gt; attribute can only contain &quot;high&quot; or &quot;low&quot; as value&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;numViews&lt;/code&gt; should not contain negative values&lt;/li&gt; 
 &lt;li&gt;at least half of the values in &lt;code&gt;description&lt;/code&gt; should contain a url&lt;/li&gt; 
 &lt;li&gt;the median of &lt;code&gt;numViews&lt;/code&gt; should be less than or equal to 10&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In code this looks as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import com.amazon.deequ.VerificationSuite
import com.amazon.deequ.checks.{Check, CheckLevel, CheckStatus}


val verificationResult = VerificationSuite()
  .onData(data)
  .addCheck(
    Check(CheckLevel.Error, &quot;unit testing my data&quot;)
      .hasSize(_ == 5) // we expect 5 rows
      .isComplete(&quot;id&quot;) // should never be NULL
      .isUnique(&quot;id&quot;) // should not contain duplicates
      .isComplete(&quot;productName&quot;) // should never be NULL
      // should only contain the values &quot;high&quot; and &quot;low&quot;
      .isContainedIn(&quot;priority&quot;, Array(&quot;high&quot;, &quot;low&quot;))
      .isNonNegative(&quot;numViews&quot;) // should not contain negative values
      // at least half of the descriptions should contain a url
      .containsURL(&quot;description&quot;, _ &amp;gt;= 0.5)
      // half of the items should have less than 10 views
      .hasApproxQuantile(&quot;numViews&quot;, 0.5, _ &amp;lt;= 10))
    .run()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;After calling &lt;code&gt;run&lt;/code&gt;, &lt;strong&gt;deequ&lt;/strong&gt; translates your test to a series of Spark jobs, which it executes to compute metrics on the data. Afterwards it invokes your assertion functions (e.g., &lt;code&gt;_ == 5&lt;/code&gt; for the size check) on these metrics to see if the constraints hold on the data. We can inspect the &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/VerificationResult.scala&quot;&gt;VerificationResult&lt;/a&gt; to see if the test found errors:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import com.amazon.deequ.constraints.ConstraintStatus


if (verificationResult.status == CheckStatus.Success) {
  println(&quot;The data passed the test, everything is fine!&quot;)
} else {
  println(&quot;We found errors in the data:\n&quot;)

  val resultsForAllConstraints = verificationResult.checkResults
    .flatMap { case (_, checkResult) =&amp;gt; checkResult.constraintResults }

  resultsForAllConstraints
    .filter { _.status != ConstraintStatus.Success }
    .foreach { result =&amp;gt; println(s&quot;${result.constraint}: ${result.message.get}&quot;) }
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If we run the example, we get the following output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;We found errors in the data:

CompletenessConstraint(Completeness(productName)): Value: 0.8 does not meet the requirement!
PatternConstraint(containsURL(description)): Value: 0.4 does not meet the requirement!
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The test found that our assumptions are violated! Only 4 out of 5 (80%) of the values of the &lt;code&gt;productName&lt;/code&gt; attribute are non-null and only 2 out of 5 (40%) values of the &lt;code&gt;description&lt;/code&gt; attribute did contain a url. Fortunately, we ran a test and found the errors, somebody should immediately fix the data :)&lt;/p&gt; 
&lt;h2&gt;More examples&lt;/h2&gt; 
&lt;p&gt;Our library contains much more functionality than what we showed in the basic example. We are in the process of adding &lt;a href=&quot;https://raw.githubusercontent.com/awslabs/deequ/master/src/main/scala/com/amazon/deequ/examples/&quot;&gt;more examples&lt;/a&gt; for its advanced features. So far, we showcase the following functionality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/metrics_repository_example.md&quot;&gt;Persistence and querying of computed metrics of the data with a MetricsRepository&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/data_profiling_example.md&quot;&gt;Data profiling&lt;/a&gt; of large data sets&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/anomaly_detection_example.md&quot;&gt;Anomaly detection&lt;/a&gt; on data quality metrics over time&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/constraint_suggestion_example.md&quot;&gt;Automatic suggestion of constraints&lt;/a&gt; for large datasets&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/awslabs/deequ/raw/master/src/main/scala/com/amazon/deequ/examples/algebraic_states_example.md&quot;&gt;Incremental metrics computation on growing data and metric updates on partitioned data&lt;/a&gt; (advanced)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;DQDL (Data Quality Definition Language)&lt;/h2&gt; 
&lt;p&gt;Deequ also supports &lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/dqdl.html&quot;&gt;DQDL&lt;/a&gt;, a declarative language for defining data quality rules. DQDL allows you to express data quality constraints in a simple, readable format.&lt;/p&gt; 
&lt;h3&gt;Supported DQDL Rules&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;RowCount&lt;/strong&gt;: &lt;code&gt;RowCount &amp;lt; 100&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Completeness&lt;/strong&gt;: &lt;code&gt;Completeness &quot;column&quot; &amp;gt; 0.9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IsComplete&lt;/strong&gt;: &lt;code&gt;IsComplete &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Uniqueness&lt;/strong&gt;: &lt;code&gt;Uniqueness &quot;column&quot; = 1.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IsUnique&lt;/strong&gt;: &lt;code&gt;IsUnique &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ColumnCorrelation&lt;/strong&gt;: &lt;code&gt;ColumnCorrelation &quot;col1&quot; &quot;col2&quot; &amp;gt; 0.8&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DistinctValuesCount&lt;/strong&gt;: &lt;code&gt;DistinctValuesCount &quot;column&quot; = 5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Entropy&lt;/strong&gt;: &lt;code&gt;Entropy &quot;column&quot; &amp;gt; 2.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mean&lt;/strong&gt;: &lt;code&gt;Mean &quot;column&quot; between 10 and 50&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;StandardDeviation&lt;/strong&gt;: &lt;code&gt;StandardDeviation &quot;column&quot; &amp;lt; 5.0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sum&lt;/strong&gt;: &lt;code&gt;Sum &quot;column&quot; = 100&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;UniqueValueRatio&lt;/strong&gt;: &lt;code&gt;UniqueValueRatio &quot;column&quot; &amp;gt; 0.7&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CustomSql&lt;/strong&gt;: &lt;code&gt;CustomSql &quot;SELECT COUNT(*) FROM primary&quot; &amp;gt; 0&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;IsPrimaryKey&lt;/strong&gt;: &lt;code&gt;IsPrimaryKey &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ColumnLength&lt;/strong&gt;: &lt;code&gt;ColumnLength &quot;column&quot; between 1 and 5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ColumnExists&lt;/strong&gt;: &lt;code&gt;ColumnExists &quot;column&quot;&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Scala Example&lt;/h3&gt; 
&lt;p&gt;ScalaDQDLExample.scala&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-scala&quot;&gt;import com.amazon.deequ.dqdl.EvaluateDataQuality
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName(&quot;DQDL Example&quot;)
  .master(&quot;local[*]&quot;)
  .getOrCreate()

import spark.implicits._

// Sample data
val df = Seq(
  (&quot;1&quot;, &quot;a&quot;, &quot;c&quot;),
  (&quot;2&quot;, &quot;a&quot;, &quot;c&quot;),
  (&quot;3&quot;, &quot;a&quot;, &quot;c&quot;),
  (&quot;4&quot;, &quot;b&quot;, &quot;d&quot;)
).toDF(&quot;item&quot;, &quot;att1&quot;, &quot;att2&quot;)

// Define rules using DQDL syntax
val ruleset = &quot;&quot;&quot;Rules=[IsUnique &quot;item&quot;, RowCount &amp;lt; 10, Completeness &quot;item&quot; &amp;gt; 0.8, Uniqueness &quot;item&quot; = 1.0]&quot;&quot;&quot;

// Evaluate data quality
val results = EvaluateDataQuality.process(df, ruleset)
results.show()
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Java Example&lt;/h3&gt; 
&lt;p&gt;JavaDQDLExample.java&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-java&quot;&gt;import com.amazon.deequ.dqdl.EvaluateDataQuality;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

SparkSession spark = SparkSession.builder()
    .appName(&quot;DQDL Java Example&quot;)
    .master(&quot;local[*]&quot;)
    .getOrCreate();

// Create sample data
Dataset&amp;lt;Row&amp;gt; df = spark.sql(
    &quot;SELECT * FROM VALUES &quot; +
    &quot;(&#39;1&#39;, &#39;a&#39;, &#39;c&#39;), &quot; +
    &quot;(&#39;2&#39;, &#39;a&#39;, &#39;c&#39;), &quot; +
    &quot;(&#39;3&#39;, &#39;a&#39;, &#39;c&#39;), &quot; +
    &quot;(&#39;4&#39;, &#39;b&#39;, &#39;d&#39;) &quot; +
    &quot;AS t(item, att1, att2)&quot;
);

// Define rules using DQDL syntax
String ruleset = &quot;Rules=[IsUnique \&quot;item\&quot;, RowCount &amp;lt; 10, Completeness \&quot;item\&quot; &amp;gt; 0.8, Uniqueness \&quot;item\&quot; = 1.0]&quot;;

// Evaluate data quality
Dataset&amp;lt;Row&amp;gt; results = EvaluateDataQuality.process(df, ruleset);
results.show();
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;p&gt;If you would like to reference this package in a research paper, please cite:&lt;/p&gt; 
&lt;p&gt;Sebastian Schelter, Dustin Lange, Philipp Schmidt, Meltem Celikel, Felix Biessmann, and Andreas Grafberger. 2018. &lt;a href=&quot;http://www.vldb.org/pvldb/vol11/p1781-schelter.pdf&quot;&gt;Automating large-scale data quality verification&lt;/a&gt;. Proc. VLDB Endow. 11, 12 (August 2018), 1781-1794.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This library is licensed under the Apache 2.0 License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>rtyley/bfg-repo-cleaner</title>
      <link>https://github.com/rtyley/bfg-repo-cleaner</link>
      <description>&lt;p&gt;Removes large or troublesome blobs like git-filter-branch does, but faster. And written in Scala&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BFG Repo-Cleaner&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/ci.yml&quot;&gt;&lt;img src=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/ci.yml/badge.svg?sanitize=true&quot; alt=&quot;CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/rtyley/bfg-repo-cleaner/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Removes large or troublesome blobs like git-filter-branch does, but faster - and written in Scala&lt;/em&gt; - &lt;a href=&quot;https://j.mp/fund-bfg&quot;&gt;Fund the BFG&lt;/a&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ bfg --strip-blobs-bigger-than 1M --replace-text banned.txt repo.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The BFG is a simpler, faster (&lt;a href=&quot;https://docs.google.com/spreadsheet/ccc?key=0AsR1d5Zpes8HdER3VGU1a3dOcmVHMmtzT2dsS2xNenc&quot;&gt;10 - 720x&lt;/a&gt; faster) alternative to &lt;code&gt;git-filter-branch&lt;/code&gt; for cleansing bad data out of your Git repository:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Removing &lt;strong&gt;Crazy Big Files&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Removing &lt;strong&gt;Passwords, Credentials&lt;/strong&gt; &amp;amp; other &lt;strong&gt;Private data&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Main documentation for The BFG is here : &lt;strong&gt;&lt;a href=&quot;https://rtyley.github.io/bfg-repo-cleaner/&quot;&gt;https://rtyley.github.io/bfg-repo-cleaner/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>joernio/joern</title>
      <link>https://github.com/joernio/joern</link>
      <description>&lt;p&gt;Open-source code analysis platform for C/C++/Java/Binary/Javascript/Python/Kotlin based on code property graphs. Discord https://discord.gg/vv4MH284Hc&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Joern - The Bug Hunter&#39;s Workbench&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/joernio/joern/actions/workflows/release.yml&quot;&gt;&lt;img src=&quot;https://github.com/joernio/joern/actions/workflows/release.yml/badge.svg?sanitize=true&quot; alt=&quot;release&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://index.scala-lang.org/joernio/joern&quot;&gt;&lt;img src=&quot;https://index.scala-lang.org/joernio/joern/latest.svg?sanitize=true&quot; alt=&quot;Joern SBT&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/joernio/joern/releases/&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/downloads/joernio/joern/total.svg?sanitize=true&quot; alt=&quot;Github All Releases&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.com/invite/vv4MH284Hc&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/-Discord-lime?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;color=black&quot; alt=&quot;Gitter&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Joern is a platform for analyzing source code, bytecode, and binary executables. It generates code property graphs (CPGs), a graph representation of code for cross-language code analysis. Code property graphs are stored in a custom graph database. This allows code to be mined using search queries formulated in a Scala-based domain-specific query language. Joern is developed with the goal of providing a useful tool for vulnerability discovery and research in static program analysis.&lt;/p&gt; 
&lt;p&gt;Website: &lt;a href=&quot;https://joern.io&quot;&gt;https://joern.io&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Documentation: &lt;a href=&quot;https://docs.joern.io/&quot;&gt;https://docs.joern.io/&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Specification: &lt;a href=&quot;https://cpg.joern.io&quot;&gt;https://cpg.joern.io&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;News / Changelog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Joern v4.0.0 &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/changelog/4.0.0-flatgraph.md&quot;&gt;migrates from overflowdb to flatgraph&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Joern v2.0.0 &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/changelog/2.0.0-scala3.md&quot;&gt;upgrades from Scala2 to Scala3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Joern v1.2.0 removes the &lt;code&gt;overflowdb.traversal.Traversal&lt;/code&gt; class. This change is not completely backwards compatible. See &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/changelog/traversal_removal.md&quot;&gt;here&lt;/a&gt; for a detailed writeup.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;JDK 21 (other versions &lt;em&gt;might&lt;/em&gt; work, but have not been properly tested)&lt;/li&gt; 
 &lt;li&gt;&lt;em&gt;optional&lt;/em&gt;: gcc and g++ (for auto-discovery of C/C++ system header files if included/used in your C/C++ code)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;wget https://github.com/joernio/joern/releases/latest/download/joern-install.sh
chmod +x ./joern-install.sh
sudo ./joern-install.sh
joern

     ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó
     ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë
     ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë
‚ñà‚ñà   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë
‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù
Version: 2.0.1
Type `help` to begin

joern&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the installation script fails for any reason, try&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./joern-install --interactive
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Development Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://jdk.java.net/&quot;&gt;java&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.scala-sbt.org&quot;&gt;sbt&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run unit and integration tests locally&lt;/h2&gt; 
&lt;p&gt;Unit tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sbt test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Integration tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sbt joerncli/stage querydb/createDistribution
python -m pip install requests pexpect # wexpect on Windows
python -u ./testDistro.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Docker based execution&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v /tmp:/tmp -v $(pwd):/app:rw -w /app -t ghcr.io/joernio/joern joern
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To run joern in server mode:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v /tmp:/tmp -v $(pwd):/app:rw -w /app -t ghcr.io/joernio/joern joern --server
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Almalinux 9 requires the CPU to support SSE4.2. For kvm64 VM use the Almalinux 8 version instead.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;docker run --rm -it -v /tmp:/tmp -v $(pwd):/app:rw -w /app -t ghcr.io/joernio/joern-alma8 joern
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Releases&lt;/h2&gt; 
&lt;p&gt;A new release is &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/.github/workflows/release.yml&quot;&gt;created automatically&lt;/a&gt; once per day. Contributers can also manually run the &lt;a href=&quot;https://github.com/joernio/joern/actions/workflows/release.yml&quot;&gt;release workflow&lt;/a&gt; if they need the release sooner.&lt;/p&gt; 
&lt;h2&gt;Developers&lt;/h2&gt; 
&lt;h3&gt;Contribution Guidelines&lt;/h3&gt; 
&lt;p&gt;Thank you for taking time to contribute to Joern! Here are a few guidelines to ensure your pull request will get merged as soon as possible:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Try to make use of the templates as far as possible, however they may not suit all needs. The minimum we would like to see is: 
  &lt;ul&gt; 
   &lt;li&gt;A title that briefly describes the change and purpose of the PR, preferably with the affected module in square brackets, e.g. &lt;code&gt;[javasrc2cpg] Addition Operator Fix&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;A short description of the changes in the body of the PR. This could be in bullet points or paragraphs.&lt;/li&gt; 
   &lt;li&gt;A link or reference to the related issue, if any exists.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Do not: 
  &lt;ul&gt; 
   &lt;li&gt;Immediately CC/@/email spam other contributors, the team will review the PR and assign the most appropriate contributor to review the PR. Joern is maintained by industry partners and researchers alike, for the most part with their own goals and priorities, and additional help is largely volunteer work. If your PR is going stale, then reach out to us in follow-up comments with @&#39;s asking for an explanation of priority or planning of when it may be addressed (if ever, depending on quality).&lt;/li&gt; 
   &lt;li&gt;Leave the description body empty, this makes reviewing the purpose of the PR difficult.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Remember to: 
  &lt;ul&gt; 
   &lt;li&gt;Remember to format your code, i.e. run &lt;code&gt;sbt scalafmt Test/scalafmt&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Add a unit test to verify your change.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;IDE setup&lt;/h3&gt; 
&lt;h4&gt;Intellij IDEA&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.jetbrains.com/idea/download&quot;&gt;Download Intellij Community&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Install and run it&lt;/li&gt; 
 &lt;li&gt;Install the &lt;a href=&quot;https://plugins.jetbrains.com/plugin/1347-scala&quot;&gt;Scala Plugin&lt;/a&gt; - just search and install from within Intellij.&lt;/li&gt; 
 &lt;li&gt;Important: open &lt;code&gt;sbt&lt;/code&gt; in your local joern repository, run &lt;code&gt;compile&lt;/code&gt; and keep it open - this will allow us to use the BSP build in the next step&lt;/li&gt; 
 &lt;li&gt;Back to Intellij: open project: select your local joern clone: select to open as &lt;code&gt;BSP project&lt;/code&gt; (i.e. &lt;em&gt;not&lt;/em&gt; &lt;code&gt;sbt project&lt;/code&gt;!)&lt;/li&gt; 
 &lt;li&gt;Await the import and indexing to complete, then you can start, e.g. &lt;code&gt;Build -&amp;gt; build project&lt;/code&gt; or run a test&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;VSCode&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install VSCode and Docker&lt;/li&gt; 
 &lt;li&gt;Install the plugin &lt;code&gt;ms-vscode-remote.remote-containers&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Open Joern project folder in VSCode 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure-sphere/app-development/container-build-vscode#build-and-debug-the-project&quot;&gt;Option 1&lt;/a&gt;: Visual Studio Code detects the new files and opens a message box saying: &lt;code&gt;Folder contains a Dev Container configuration file. Reopen to folder to develop in a container.&lt;/code&gt;. Select the &lt;code&gt;Reopen in Container&lt;/code&gt; button to reopen the folder in the container created by the &lt;code&gt;.devcontainer/Dockerfile&lt;/code&gt; file.&lt;/li&gt; 
   &lt;li&gt;Option 2: press &lt;code&gt;Ctrl + Shift + P&lt;/code&gt; then select &lt;code&gt;Dev Containers: Reopen in Container&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Press &lt;code&gt;Ctrl + Shift + P&lt;/code&gt; then select &lt;code&gt;Metals: Import build&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;After &lt;code&gt;Metals: Import build&lt;/code&gt; succeeds, you are ready to start writing code for Joern&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;QueryDB (queries plugin)&lt;/h2&gt; 
&lt;p&gt;Quick way to develop and test QueryDB:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sbt stage
./querydb-install.sh
./joern-scan --list-query-names
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The last command prints all available queries - add your own in querydb, run the above commands again to see that your query got deployed. More details in the &lt;a href=&quot;https://raw.githubusercontent.com/joernio/joern/master/querydb/README.md&quot;&gt;separate querydb readme&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ucb-bar/chipyard</title>
      <link>https://github.com/ucb-bar/chipyard</link>
      <description>&lt;p&gt;An Agile RISC-V SoC Design Framework with in-order cores, out-of-order cores, accelerators, and more&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://github.com/ucb-bar/chipyard/raw/main/docs/_static/images/chipyard-logo-full.png&quot; alt=&quot;CHIPYARD&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Chipyard Framework &lt;a href=&quot;https://github.com/ucb-bar/chipyard/actions&quot;&gt;&lt;img src=&quot;https://github.com/ucb-bar/chipyard/actions/workflows/chipyard-run-tests.yml/badge.svg?sanitize=true&quot; alt=&quot;Test&quot; /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;h2&gt;Quick Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Latest Documentation&lt;/strong&gt;: &lt;a href=&quot;https://chipyard.readthedocs.io/&quot;&gt;https://chipyard.readthedocs.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;User Question Forum&lt;/strong&gt;: &lt;a href=&quot;https://groups.google.com/forum/#!forum/chipyard&quot;&gt;https://groups.google.com/forum/#!forum/chipyard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Bugs and Feature Requests&lt;/strong&gt;: &lt;a href=&quot;https://github.com/ucb-bar/chipyard/issues&quot;&gt;https://github.com/ucb-bar/chipyard/issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Using Chipyard&lt;/h2&gt; 
&lt;p&gt;To get started using Chipyard, see the documentation on the Chipyard documentation site: &lt;a href=&quot;https://chipyard.readthedocs.io/&quot;&gt;https://chipyard.readthedocs.io/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;What is Chipyard&lt;/h2&gt; 
&lt;p&gt;Chipyard is an open source framework for agile development of Chisel-based systems-on-chip. It will allow you to leverage the Chisel HDL, Rocket Chip SoC generator, and other &lt;a href=&quot;https://berkeley.edu&quot;&gt;Berkeley&lt;/a&gt; projects to produce a &lt;a href=&quot;https://riscv.org/&quot;&gt;RISC-V&lt;/a&gt; SoC with everything from MMIO-mapped peripherals to custom accelerators. Chipyard contains processor cores (&lt;a href=&quot;https://github.com/freechipsproject/rocket-chip&quot;&gt;Rocket&lt;/a&gt;, &lt;a href=&quot;https://github.com/riscv-boom/riscv-boom&quot;&gt;BOOM&lt;/a&gt;, &lt;a href=&quot;https://github.com/openhwgroup/cva6/&quot;&gt;CVA6 (Ariane)&lt;/a&gt;), vector units (&lt;a href=&quot;https://github.com/ucb-bar/saturn-vectors&quot;&gt;Saturn&lt;/a&gt;, &lt;a href=&quot;https://github.com/pulp-platform/ara&quot;&gt;Ara&lt;/a&gt;), accelerators (&lt;a href=&quot;https://github.com/ucb-bar/gemmini&quot;&gt;Gemmini&lt;/a&gt;, &lt;a href=&quot;http://nvdla.org/&quot;&gt;NVDLA&lt;/a&gt;), memory systems, and additional peripherals and tooling to help create a full featured SoC. Chipyard supports multiple concurrent flows of agile hardware development, including software RTL simulation, FPGA-accelerated simulation (&lt;a href=&quot;https://fires.im&quot;&gt;FireSim&lt;/a&gt;), automated VLSI flows (&lt;a href=&quot;https://github.com/ucb-bar/hammer&quot;&gt;Hammer&lt;/a&gt;), and software workload generation for bare-metal and Linux-based systems (&lt;a href=&quot;https://github.com/firesim/FireMarshal/&quot;&gt;FireMarshal&lt;/a&gt;). Chipyard is actively developed in the &lt;a href=&quot;http://bar.eecs.berkeley.edu&quot;&gt;Berkeley Architecture Research Group&lt;/a&gt; in the &lt;a href=&quot;https://eecs.berkeley.edu&quot;&gt;Electrical Engineering and Computer Sciences Department&lt;/a&gt; at the &lt;a href=&quot;https://berkeley.edu&quot;&gt;University of California, Berkeley&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Resources&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Chipyard Documentation: &lt;a href=&quot;https://chipyard.readthedocs.io/&quot;&gt;https://chipyard.readthedocs.io/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chipyard (x FireSim) Tutorial: &lt;a href=&quot;https://fires.im/tutorial-recent/&quot;&gt;https://fires.im/tutorial-recent/&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Chipyard Basics slides: &lt;a href=&quot;https://fires.im/asplos23-slides-pdf/02_chipyard_basics.pdf&quot;&gt;https://fires.im/asplos23-slides-pdf/02_chipyard_basics.pdf&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Need help?&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Join the Chipyard Mailing List: &lt;a href=&quot;https://groups.google.com/forum/#!forum/chipyard&quot;&gt;https://groups.google.com/forum/#!forum/chipyard&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;If you find a bug or would like propose a feature, post an issue on this repo: &lt;a href=&quot;https://github.com/ucb-bar/chipyard/issues&quot;&gt;https://github.com/ucb-bar/chipyard/issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/ucb-bar/chipyard/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Attribution and Chipyard-related Publications&lt;/h2&gt; 
&lt;p&gt;If used for research, please cite Chipyard by the following publication:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@article{chipyard,
  author={Amid, Alon and Biancolin, David and Gonzalez, Abraham and Grubb, Daniel and Karandikar, Sagar and Liew, Harrison and Magyar,   Albert and Mao, Howard and Ou, Albert and Pemberton, Nathan and Rigge, Paul and Schmidt, Colin and Wright, John and Zhao, Jerry and Shao, Yakun Sophia and Asanovi\&#39;{c}, Krste and Nikoli\&#39;{c}, Borivoje},
  journal={IEEE Micro},
  title={Chipyard: Integrated Design, Simulation, and Implementation Framework for Custom SoCs},
  year={2020},
  volume={40},
  number={4},
  pages={10-21},
  doi={10.1109/MM.2020.2996616},
  ISSN={1937-4143},
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Chipyard&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;A. Amid, et al. &lt;em&gt;IEEE Micro&#39;20&lt;/em&gt; &lt;a href=&quot;https://ieeexplore.ieee.org/document/9099108&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;A. Amid, et al. &lt;em&gt;DAC&#39;20&lt;/em&gt; &lt;a href=&quot;https://ieeexplore.ieee.org/document/9218756&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;A. Amid, et al. &lt;em&gt;ISCAS&#39;21&lt;/em&gt; &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9401515&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;These additional publications cover many of the internal components used in Chipyard. However, for the most up-to-date details, users should refer to the Chipyard docs.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Generators&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Rocket Chip&lt;/strong&gt;: K. Asanovic, et al., &lt;em&gt;UCB EECS TR&lt;/em&gt;. &lt;a href=&quot;http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-17.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;BOOM&lt;/strong&gt;: C. Celio, et al., &lt;em&gt;Hot Chips 30&lt;/em&gt;. &lt;a href=&quot;https://old.hotchips.org/hc30/1conf/1.03_Berkeley_BROOM_HC30.Berkeley.Celio.v02.pdf&quot;&gt;PDF&lt;/a&gt;. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;SonicBOOM (BOOMv3)&lt;/strong&gt;: J. Zhao, et al., &lt;em&gt;CARRV&#39;20&lt;/em&gt;. &lt;a href=&quot;https://carrv.github.io/2020/papers/CARRV2020_paper_15_Zhao.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;COBRA (BOOM Branch Prediction)&lt;/strong&gt;: J. Zhao, et al., &lt;em&gt;ISPASS&#39;21&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/9408173&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Gemmini&lt;/strong&gt;: H. Genc, et al., &lt;em&gt;DAC&#39;21&lt;/em&gt;. &lt;a href=&quot;https://arxiv.org/pdf/1911.09925&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sims&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;FireSim&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;ISCA&#39;18&lt;/em&gt;. &lt;a href=&quot;https://sagark.org/assets/pubs/firesim-isca2018.pdf&quot;&gt;PDF&lt;/a&gt;. 
    &lt;ul&gt; 
     &lt;li&gt;&lt;strong&gt;FireSim Micro Top Picks&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;IEEE Micro, Top Picks 2018&lt;/em&gt;. &lt;a href=&quot;https://sagark.org/assets/pubs/firesim-micro-top-picks2018.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;FASED&lt;/strong&gt;: D. Biancolin, et al., &lt;em&gt;FPGA&#39;19&lt;/em&gt;. &lt;a href=&quot;https://people.eecs.berkeley.edu/~biancolin/papers/fased-fpga19.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;Golden Gate&lt;/strong&gt;: A. Magyar, et al., &lt;em&gt;ICCAD&#39;19&lt;/em&gt;. &lt;a href=&quot;https://davidbiancolin.github.io/papers/goldengate-iccad19.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;FirePerf&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;ASPLOS&#39;20&lt;/em&gt;. &lt;a href=&quot;https://sagark.org/assets/pubs/fireperf-asplos2020.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
     &lt;li&gt;&lt;strong&gt;FireSim ISCA@50 Retrospective&lt;/strong&gt;: S. Karandikar, et al., &lt;em&gt;ISCA@50 Retrospective: 1996-2020&lt;/em&gt;. &lt;a href=&quot;https://sites.coecis.cornell.edu/isca50retrospective/files/2023/06/Karandikar_2018_FireSim.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Chisel&lt;/strong&gt;: J. Bachrach, et al., &lt;em&gt;DAC&#39;12&lt;/em&gt;. &lt;a href=&quot;https://people.eecs.berkeley.edu/~krste/papers/chisel-dac2012.pdf&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;FIRRTL&lt;/strong&gt;: A. Izraelevitz, et al., &lt;em&gt;ICCAD&#39;17&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/8203780&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Chisel DSP&lt;/strong&gt;: A. Wang, et al., &lt;em&gt;DAC&#39;18&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/8465790&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;FireMarshal&lt;/strong&gt;: N. Pemberton, et al., &lt;em&gt;ISPASS&#39;21&lt;/em&gt;. &lt;a href=&quot;https://ieeexplore.ieee.org/document/9408192&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VLSI&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Hammer&lt;/strong&gt;: E. Wang, et al., &lt;em&gt;ISQED&#39;20&lt;/em&gt;. &lt;a href=&quot;https://www.isqed.org/English/Archives/2020/Technical_Sessions/113.html&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Hammer&lt;/strong&gt;: H. Liew, et al., &lt;em&gt;DAC&#39;22&lt;/em&gt;. &lt;a href=&quot;https://dl.acm.org/doi/abs/10.1145/3489517.3530672&quot;&gt;PDF&lt;/a&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgements&lt;/h2&gt; 
&lt;p&gt;This work is supported by the NSF CCRI ENS Chipyard Award #2016662.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>hyperledger-labs/splice</title>
      <link>https://github.com/hyperledger-labs/splice</link>
      <description>&lt;p&gt;Reference applications for funding, operating, and incentivizing the use of a decentralized, public Canton synchronizer. Includes the Amulet reference application for creating native payment utilities for Canton synchronizers and Daml applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Splice&lt;/h1&gt; 
&lt;h1&gt;Short Description&lt;/h1&gt; 
&lt;p&gt;Reference applications for funding, operating, and incentivizing the use of a decentralized, public Canton synchronizer. Includes the Amulet reference application for creating native payment utilities for Canton synchronizers and Daml applications.&lt;/p&gt; 
&lt;h1&gt;Scope of Lab&lt;/h1&gt; 
&lt;h2&gt;Abstract&lt;/h2&gt; 
&lt;p&gt;Splice is a set of reference applications designed to allow entities to operate, fund, and govern publicly available decentralized Canton synchronizers that provide connectivity and interoperability infrastructure for the Canton Network, as well as to provide bootstrapping rewards and incentives to early users of that service. The Canton Network is the set of all applications, built using the Daml blockchain application platform, that form shared blockchain state via the Canton Protocol.&lt;/p&gt; 
&lt;p&gt;Splice introduces a reference method for operating a publicly available decentralized Canton synchronizer. Each node in the decentralized synchronizer is operated by an entity known in Splice as a &quot;Super Validator&quot;. Splice refers to a group of Super Validators actively operating nodes in a decentralized synchronizer at any point in time as the &quot;decentralized synchronizer operator&quot; (dso). The Splice code uses a code construct called a &quot;decentralized synchronizer operator party&quot; (dso party) to accumulate signatures from and take actions on behalf of the currently-active set of Super Validators.&lt;/p&gt; 
&lt;p&gt;Splice aims to help Super Validator operating groups create a transparent economic ecosystem that will, over time, fund operations of and extensions to multiple public synchronization services in the Canton Network.&lt;/p&gt; 
&lt;h2&gt;Context&lt;/h2&gt; 
&lt;p&gt;Daml is a platform built by Digital Asset designed for deploying blockchains and developing blockchain applications. It includes a smart contract language, a set of APIs for calling smart contract code, a transaction processing engine, a query-optimized database for accessing smart contract state, and development tooling including an SDK. The open source and enterprise distributions of Daml also include a blockchain protocol, Canton, that creates synchronized state among nodes running the Daml platform.&lt;/p&gt; 
&lt;p&gt;The Canton blockchain guarantees secure state synchronization between participant nodes by running a per-transaction consensus protocol using encrypted messages through a synchronization infrastructure called synchronizers (or sometimes synchronization domains). Synchronizers serve three functions:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;guaranteeing consistent message order and timestamps among stakeholders;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;guaranteeing delivery of messages to the stakeholders; and&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;aggregating validation confirmations from stakeholders that the proposed transactions use valid inputs and produce valid outputs.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Together these allow ordered, two-phase atomic transaction processing, with confirmation of transaction validation and transaction commits. Canton does this while keeping data private to the stakeholders in each transaction.&lt;/p&gt; 
&lt;p&gt;Daml applications may choose any Canton synchronizer on a per-transaction basis to help them advance the shared blockchain state that forms on the participant nodes. The collection of all Daml applications that synchronize their state via the Canton protocol, and the set of Canton synchronizers they use to do this, together form the Canton Network.&lt;/p&gt; 
&lt;p&gt;Operating groups may charge a fee for traffic that uses their synchronizer. The fee may be metered in USD per megabyte, and levied via an on-chain payment utility. This utility is an implementation of the Splice reference application called Amulet.&lt;/p&gt; 
&lt;p&gt;The Amulet reference application specifies how to implement on-chain payments using ‚Äúamulets‚Äù which represent the ability to pay an operating group to synchronize a transaction with a payload of a given size. Together with the full suite of Splice applications, amulets allow a group of entities to deploy, operate, fund, and govern a decentralized Canton synchronizer, and incentivize application developers and their customers to use that synchronizer to create blockchain state among their nodes. Each operating group configures an Amulet implementation to charge for use of the synchronizer, and distribute their own named version of amulets as rewards to incentivize early use of the service.&lt;/p&gt; 
&lt;p&gt;To provide a mapping between the value of its specific amulet and fiat currencies, the synchronizer operating group may use tooling included in Splice to jointly vote on a fee rate in megabytes per USD, and a nominal conversion rate between USD and the amulet configured by that operating group. This provides a base intrinsic value for the amulet used by the operating group: each amulet represents the value of creating and maintaining high guarantees of synchronized state across multiple computers, for a given data volume of messages shared among those computers. Should a market for that operating group‚Äôs amulet develop, the operating group may align the on-ledger price to the market price at its discretion. Synchronizer operating groups can incentivize use of their synchronizer by issuing their amulets to members of the operating group, to application providers, and to Canton participant node operators (‚ÄúValidators‚Äù).&lt;/p&gt; 
&lt;p&gt;Daml applications may choose to use their own, privately operated, Canton synchronizer to create shared state across the Canton participant nodes interacting with those applications, or they may use any shared synchronizer offered by a third party operating group.&lt;/p&gt; 
&lt;h2&gt;Dependent Projects&lt;/h2&gt; 
&lt;p&gt;Splice has grown out of the Daml blockchain ecosystem and its Canton protocol. Open Source versions of Daml are maintained by Digital Asset under an Apache 2.0 License.&lt;/p&gt; 
&lt;p&gt;Splice expands on Daml‚Äôs privacy-preserving blockchain technology by providing governance, network bootstrapping and rewards mechanisms as well as payment solutions for any public, decentralized synchronizer built using the Daml blockchain platform. Splice uses the Daml smart contract language to represent the stakeholders and business operations involved in configuring and implementing amulets and the applications that use them. Splice repositories are designed to be deployed in conjunction with Canton synchronizers and participant nodes. Any organization that operates a Canton synchronizer uses the Daml platform in addition to the tooling and applications provided by Splice. This use of the Daml platform could include use of closed-source elements of Daml, at the operating group‚Äôs discretion.&lt;/p&gt; 
&lt;h2&gt;Motivation&lt;/h2&gt; 
&lt;p&gt;Splice aims to create a transparent economic ecosystem that will, over time, fund operations of and extensions to a public synchronization service. Success for Splice would mean that synchronization infrastructure becomes so widely available that any application will be able to use Daml to define a subset of state to be synchronized, distribute Daml code for generating that state to the computers to be synchronized, and use a synchronizer to coordinate creation of that shared state across any set of computers running Daml.&lt;/p&gt; 
&lt;p&gt;For a synchronization infrastructure to be this widely adopted, application developers need to know that they can trust it to operate in a transparent and predictable way, and at a better ‚Äúprice per value‚Äù point than comparable integration and synchronization technologies. Application developers and users also need reasons to try out the network before wide adoption creates strong network effects, so Splice includes a system of incentives and fees that reward early adopters.&lt;/p&gt; 
&lt;p&gt;Splice aims to make a high level of trust possible in the following ways:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Make operations and governance of Canton synchronizers transparent and trackable through built-in governance tooling.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Keep synchronizer transaction costs low via integrated payment, operations monitoring and governance automation, so operating groups can adjust their operations as needed to maintain a stable, long-term service offering.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Manage these tools and applications via an Apache 2.0 license and an open source community within the Hyperledger Foundation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Splice‚Äôs interlocking system of incentives and fees includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Tools for using amulets as a payment utility, including a secure payment redirect feature that allows applications to call an amulet wallet to complete payments with strong security guarantees.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Rewards denominated in amulets which strongly incentivize early providers of synchronization infrastructure, application providers who make solutions available in the early stages of the network, and Validator operators, who run blockchain nodes in the early stages of the network.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Traffic acquisition tooling that allows users to purchase access to synchronization infrastructure using amulets.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Handles for amulet users, to make it easy to find counterparties on chain and exchange amulets with them.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Status&lt;/h2&gt; 
&lt;p&gt;The code that forms Splice was first implemented in June 2023 as part of a TestNet synchronizer operated by the members of an initial set of Super Validators. At that time Splice was a set of applications with various names including Canton Coin and Canton Name Service. The TestNet synchronizer has operated continuously since that time, with regular tests and software upgrades, using the code proposed for Splice to implement its operations, governance and payment applications. In late December 2023, Digital Asset and 46 separate financial institutions demonstrated 30 decentralized application deployments that synchronized roughly 350 financial transactions via the TestNet synchronizer.&lt;/p&gt; 
&lt;p&gt;The group of Super Validators operating this TestNet currently includes Digital Asset and three other organizations, with four additional organizations currently in the process of applying to join.&lt;/p&gt; 
&lt;p&gt;Digital Asset has funded an engineering team to develop the Splice reference applications. Digital Asset is working to build a team of collaborators who can decentralize responsibility for further development as part of the Open Source Software process.&lt;/p&gt; 
&lt;h2&gt;Solution&lt;/h2&gt; 
&lt;p&gt;Splice consists of:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Daml models defining the behavior of amulets, including their use as a means of payment for traffic across a decentralized synchronizer, and their use as an incentive mechanism to encourage early adoption of Canton synchronization infrastructure. Amulet behaviors include a minting rate and a burn rate, and various fee mechanisms. These Daml models consist of complex smart contract code written in the Daml language. The minting rate describes a configurable issuance curve over time, while the burn rate consists of several fees paid by destroying (‚Äúburning‚Äù) amulets to reduce its total supply. Minted amulets are distributed as rewards to synchronizer operating groups, application providers, and Canton participant node operators (‚ÄúValidators‚Äù).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Five reference applications that provide the foundation for configuring and implementing an incentivizing economic ecosystem using amulets. Each application includes Daml models, automation elements and UI components:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;A directory of handles for amulet users called the Name Service&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;An Amulet Wallet for controlling amulets on behalf of a given user, sending and receiving transfers of amulets under the control of that user, handling payment redirects from within applications, and allowing authorized OAuth2 users to interact with wallets under their control.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;A Payment Scan service that collects records of amulet transactions and makes them visible via an API and an application UI.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;A Synchronizer Governance app, which a synchronizer operating group can use for setting fees for its synchronizer, monitoring its operation, setting the nominal price of its amulets in USD, and implementing votes to change governance, operations and the nominal price for its amulet. This app will be accessible to any entity or group that chooses to operate synchronization infrastructure.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;A Traffic Acquisition app, which allows users to purchase traffic across a Canton synchronizer, and configure automatic refills of a desired traffic balance as the user submits transactions via the synchronizer.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Amulet behaviors are controlled by the underlying Daml models. These models can be used to create an on-chain instance of an amulet following the Amulet pattern, but the models do not control actual operations and governance of an on-chain amulet. Actual definition, configuration and governance of operating group-specific amulets takes place by setting configuration variables on these models via governance votes in a specific on-chain context.&lt;/p&gt; 
&lt;p&gt;The Amulet models expose a large number of configuration variables to the Governance app (fifty-five variables in the current version). These allow synchronizer operating groups to use the Governance app to modify minting and burning behaviors, including but not limited to issuance curves, issuance round timing, fees, and rewards. This makes it possible to separate the governance of a particular implementation of the Amulet reference application from development of the Splice code base.&lt;/p&gt; 
&lt;p&gt;We expect that contributors interested in Splice may contribute, for example, extensions to the APIs for the reference applications, including extensions to Amulet Wallet and the Payment Scan; extensions and enhancements to the Synchronizer Governance application; changes to Traffic Acquisition behavior and APIs, and enhanced UI features in the Wallet and the Name Service. These will be accepted based on the Hyperledger Foundation process for open source projects.&lt;/p&gt; 
&lt;p&gt;Any group operating a Canton synchronizer may accept and implement new versions of the Splice reference applications at its own discretion. The amulet configuration used by any Canton synchronizer operating group can be made publicly available (visible) by that operating group.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The following diagram shows how the Splice applications interact with the Daml platform from Digital Asset.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/hyperledger-labs/splice/main/images/Splice-Canton-Decentralized-Synchronizer.jpg&quot; alt=&quot;SV Node Architecture&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;Notes:&lt;/h3&gt; 
&lt;p&gt;The Validator module contains the Wallet module and the Traffic Acquisition module.&lt;/p&gt; 
&lt;p&gt;The SV App module contains the Amulet smart contract code and the Amulet configuration variables, as well as the Synchronizer Governance app.&lt;/p&gt; 
&lt;h2&gt;Repository Status&lt;/h2&gt; 
&lt;p&gt;This repository is in the process of being contributed from the private repository of Digital Asset.&lt;/p&gt; 
&lt;p&gt;As of now, it has the following limitations:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;No CI or other periodic testing is running directly on this repository. This repository is currently updated daily as a copy of Digital Asset&#39;s &lt;a href=&quot;https://github.com/digital-asset/decentralized-canton-sync&quot;&gt;open source repository&lt;/a&gt;. Over the coming months, we will migrate the CI environment to this repository, and shift our development efforts to be directly against it.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The code currently still refers to terms which are either copyrighted by Digital Asset, or otherwise should be removed from this reposotiry, such as Canton and Canton Coin. We are in the process of a thorough renaming, and will complete that over the coming months. Note that this repository will still vendor Canton from its &lt;a href=&quot;https://github.com/digital-asset/canton&quot;&gt;open source repository&lt;/a&gt; as some of its code is reused by the Splice apps for convenience.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>lichess-org/lila</title>
      <link>https://github.com/lichess-org/lila</link>
      <description>&lt;p&gt;‚ôû lichess.org: the forever free, adless and open source chess server ‚ôû&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&quot;https://lichess.org&quot;&gt;lichess.org&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/lichess-org/lila/actions/workflows/server.yml&quot;&gt;&lt;img src=&quot;https://github.com/lichess-org/lila/actions/workflows/server.yml/badge.svg?sanitize=true&quot; alt=&quot;Build server&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/lichess-org/lila/actions/workflows/assets.yml&quot;&gt;&lt;img src=&quot;https://github.com/lichess-org/lila/actions/workflows/assets.yml/badge.svg?sanitize=true&quot; alt=&quot;Build assets&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://crowdin.com/project/lichess&quot;&gt;&lt;img src=&quot;https://d322cqt584bo4o.cloudfront.net/lichess/localized.svg?sanitize=true&quot; alt=&quot;Crowdin&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://mastodon.online/@lichess&quot;&gt;&lt;img src=&quot;https://img.shields.io/mastodon/follow/109298525492334687?domain=mastodon.online&quot; alt=&quot;Mastodon&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://bsky.app/profile/lichess.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Bluesky-0285FF?logo=bluesky&amp;amp;logoColor=fff&quot; alt=&quot;Bluesky&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/lichess&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/280713822073913354?label=Discord&amp;amp;logo=discord&amp;amp;style=flat&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/lichess-org/lila/master/public/images/home-bicolor.png&quot; alt=&quot;Lichess homepage&quot; title=&quot;Lichess comes with light and dark theme, this screenshot shows both.&quot; /&gt; 
&lt;p&gt;Lila (li[chess in sca]la) is a free online chess game server focused on &lt;a href=&quot;https://lichess.org/games&quot;&gt;realtime&lt;/a&gt; gameplay and ease of use.&lt;/p&gt; 
&lt;p&gt;It features a &lt;a href=&quot;https://lichess.org/games/search&quot;&gt;search engine&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/ief49lif&quot;&gt;computer analysis&lt;/a&gt; distributed with &lt;a href=&quot;https://github.com/lichess-org/fishnet&quot;&gt;fishnet&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/tournament&quot;&gt;tournaments&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/simul&quot;&gt;simuls&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/forum&quot;&gt;forums&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/team&quot;&gt;teams&lt;/a&gt;, &lt;a href=&quot;https://lichess.org/training&quot;&gt;tactic trainer&lt;/a&gt;, a &lt;a href=&quot;https://lichess.org/mobile&quot;&gt;mobile app&lt;/a&gt;, and a &lt;a href=&quot;https://lichess.org/study&quot;&gt;shared analysis board&lt;/a&gt;. The UI is available in more than &lt;a href=&quot;https://crowdin.com/project/lichess&quot;&gt;140 languages&lt;/a&gt; thanks to the community.&lt;/p&gt; 
&lt;p&gt;Lichess is written in &lt;a href=&quot;https://www.scala-lang.org/&quot;&gt;Scala 3&lt;/a&gt;, and relies on the &lt;a href=&quot;https://www.playframework.com/&quot;&gt;Play 2.8&lt;/a&gt; framework. &lt;a href=&quot;https://com-lihaoyi.github.io/scalatags/&quot;&gt;scalatags&lt;/a&gt; is used for templating. Pure chess logic is contained in the &lt;a href=&quot;https://github.com/lichess-org/scalachess&quot;&gt;scalachess&lt;/a&gt; submodule. The server is fully asynchronous, making heavy use of Scala Futures and &lt;a href=&quot;https://akka.io&quot;&gt;Akka streams&lt;/a&gt;. WebSocket connections are handled by a &lt;a href=&quot;https://github.com/lichess-org/lila-ws&quot;&gt;separate server&lt;/a&gt; that communicates using &lt;a href=&quot;https://redis.io/&quot;&gt;redis&lt;/a&gt;. Lichess talks to &lt;a href=&quot;https://stockfishchess.org/&quot;&gt;Stockfish&lt;/a&gt; deployed in an &lt;a href=&quot;https://github.com/lichess-org/fishnet&quot;&gt;AI cluster&lt;/a&gt; of donated servers. It uses &lt;a href=&quot;https://www.mongodb.com&quot;&gt;MongoDB&lt;/a&gt; to store more than 4.7 billion games, which are indexed by &lt;a href=&quot;https://github.com/elastic/elasticsearch&quot;&gt;elasticsearch&lt;/a&gt;. HTTP requests and WebSocket connections can be proxied by &lt;a href=&quot;https://nginx.org&quot;&gt;nginx&lt;/a&gt;. The web client is written in &lt;a href=&quot;https://www.typescriptlang.org/&quot;&gt;TypeScript&lt;/a&gt; and &lt;a href=&quot;https://github.com/snabbdom/snabbdom&quot;&gt;snabbdom&lt;/a&gt;, using &lt;a href=&quot;https://sass-lang.com/&quot;&gt;Sass&lt;/a&gt; to generate CSS. All rated games are published in a &lt;a href=&quot;https://database.lichess.org&quot;&gt;free PGN database&lt;/a&gt;. Browser testing done with &lt;a href=&quot;https://www.browserstack.com&quot;&gt;Browserstack&lt;/a&gt;. Proxy detection done with &lt;a href=&quot;https://www.ip2location.com/database/ip2proxy&quot;&gt;IP2Proxy database&lt;/a&gt;. Please help us &lt;a href=&quot;https://crowdin.com/project/lichess&quot;&gt;translate Lichess with Crowdin&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;See &lt;a href=&quot;https://lichess.org/source&quot;&gt;lichess.org/source&lt;/a&gt; for a list of repositories.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/lichess&quot;&gt;Join us on Discord&lt;/a&gt; for more info. Use &lt;a href=&quot;https://github.com/lichess-org/lila/issues&quot;&gt;GitHub issues&lt;/a&gt; for bug reports and feature requests.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;pre&gt;&lt;code&gt;./lila.sh # thin wrapper around sbt
run
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The Wiki describes &lt;a href=&quot;https://github.com/lichess-org/lila/wiki/Lichess-Development-Onboarding&quot;&gt;how to setup a development environment&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;HTTP API&lt;/h2&gt; 
&lt;p&gt;Feel free to use the &lt;a href=&quot;https://lichess.org/api&quot;&gt;Lichess API&lt;/a&gt; in your applications and websites.&lt;/p&gt; 
&lt;h2&gt;Supported browsers&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Name&lt;/th&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Notes&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Chromium / Chrome&lt;/td&gt; 
   &lt;td&gt;last 10&lt;/td&gt; 
   &lt;td&gt;Full support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Firefox&lt;/td&gt; 
   &lt;td&gt;75+&lt;/td&gt; 
   &lt;td&gt;Full support (fastest local analysis since FF 79)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Edge&lt;/td&gt; 
   &lt;td&gt;91+&lt;/td&gt; 
   &lt;td&gt;Full support (reasonable support for 79+)&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Opera&lt;/td&gt; 
   &lt;td&gt;66+&lt;/td&gt; 
   &lt;td&gt;Reasonable support&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Safari&lt;/td&gt; 
   &lt;td&gt;11.1+&lt;/td&gt; 
   &lt;td&gt;Reasonable support&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Older browsers (including any version of Internet Explorer) will not work. For your own sake, please upgrade. Security and performance, think about it!&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Lila is licensed under the GNU Affero General Public License 3 or any later version at your choice. See &lt;a href=&quot;https://github.com/lichess-org/lila/raw/master/COPYING.md&quot;&gt;copying&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h2&gt;Production architecture (as of July 2022)&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/lichess-org/lila/master/public/images/architecture.png&quot; alt=&quot;Lichess production server architecture diagram&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Credits&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://lichess.org/thanks&quot;&gt;lichess.org/thanks&lt;/a&gt; and the contributors here:&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/lichess-org/lila/graphs/contributors&quot;&gt;&lt;img src=&quot;https://contrib.rocks/image?repo=lichess-org/lila&quot; alt=&quot;GitHub contributors&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Competence development program&lt;/h2&gt; 
&lt;p&gt;Lichess would like to support its contributors in their competence development by covering costs of relevant training materials and activities. This is a small way to further empower contributors who have given their time to Lichess and to enable or improve additional contributions to Lichess in the future. For more information, including how to apply, check &lt;a href=&quot;https://lichess.org/page/competence-development&quot;&gt;Competence Development for Lichess contributors&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>scala/scala</title>
      <link>https://github.com/scala/scala</link>
      <description>&lt;p&gt;Scala 2 compiler and standard library. Scala 2 bugs at https://github.com/scala/bug; Scala 3 at https://github.com/scala/scala3&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;This is Scala 2! Welcome!&lt;/h1&gt; 
&lt;p&gt;This is the home of the &lt;a href=&quot;https://www.scala-lang.org&quot;&gt;Scala 2&lt;/a&gt; standard library, compiler, and language spec.&lt;/p&gt; 
&lt;p&gt;For Scala 3, visit &lt;a href=&quot;https://github.com/scala/scala3&quot;&gt;scala/scala3&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;How to contribute&lt;/h1&gt; 
&lt;p&gt;Issues and bug reports for Scala 2 are located in &lt;a href=&quot;https://github.com/scala/bug&quot;&gt;scala/bug&lt;/a&gt;. That tracker is also where new contributors may find issues to work on: &lt;a href=&quot;https://github.com/scala/bug/labels/good%20first%20issue&quot;&gt;good first issues&lt;/a&gt;, &lt;a href=&quot;https://github.com/scala/bug/labels/help%20wanted&quot;&gt;help wanted&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For coordinating broader efforts, we also use the &lt;a href=&quot;https://github.com/scala/scala-dev/issues&quot;&gt;scala/scala-dev tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To contribute here, please open a &lt;a href=&quot;https://help.github.com/articles/using-pull-requests/#fork--pull&quot;&gt;pull request&lt;/a&gt; from your fork of this repository.&lt;/p&gt; 
&lt;p&gt;Be aware that we can&#39;t accept additions to the standard library, only modifications to existing code. Binary compatibility forbids adding new public classes or public methods. Additions are made to &lt;a href=&quot;https://github.com/scala/scala-library-next&quot;&gt;scala-library-next&lt;/a&gt; instead.&lt;/p&gt; 
&lt;p&gt;We require that you sign the &lt;a href=&quot;https://contribute.akka.io/contribute/cla/scala&quot;&gt;Scala CLA&lt;/a&gt; before we can merge any of your work, to protect Scala&#39;s future as open source software.&lt;/p&gt; 
&lt;p&gt;The general workflow is as follows.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Find/file an issue in scala/bug (or submit a well-documented PR right away!).&lt;/li&gt; 
 &lt;li&gt;Fork the scala/scala repo.&lt;/li&gt; 
 &lt;li&gt;Push your changes to a branch in your forked repo. For coding guidelines, go &lt;a href=&quot;https://github.com/scala/scala#coding-guidelines&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Submit a pull request to scala/scala from your forked repo.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For more information on building and developing the core of Scala, read the rest of this README, especially for &lt;a href=&quot;https://github.com/scala/scala#get-ready-to-contribute&quot;&gt;setting up your machine&lt;/a&gt;!&lt;/p&gt; 
&lt;h1&gt;Get in touch!&lt;/h1&gt; 
&lt;p&gt;In order to get in touch with other Scala contributors, join the #scala-contributors channel on the &lt;a href=&quot;https://discord.com/invite/scala&quot;&gt;Scala Discord&lt;/a&gt; chat, or post on &lt;a href=&quot;https://contributors.scala-lang.org&quot;&gt;contributors.scala-lang.org&lt;/a&gt; (Discourse).&lt;/p&gt; 
&lt;p&gt;If you need some help with your PR at any time, please feel free to @-mention anyone from the list below, and we will do our best to help you out:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;/th&gt; 
   &lt;th&gt;username&lt;/th&gt; 
   &lt;th&gt;talk to me about...&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/lrytz&quot; height=&quot;50px&quot; title=&quot;Lukas Rytz&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/lrytz&quot;&gt;&lt;code&gt;@lrytz&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;back end, optimizer, named &amp;amp; default arguments, reporters&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/retronym&quot; height=&quot;50px&quot; title=&quot;Jason Zaugg&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/retronym&quot;&gt;&lt;code&gt;@retronym&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;compiler performance, weird compiler bugs, lambdas&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/SethTisue&quot; height=&quot;50px&quot; title=&quot;Seth Tisue&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/SethTisue&quot;&gt;&lt;code&gt;@SethTisue&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;getting started, build, CI, community build, Jenkins, docs, library, REPL&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/dwijnand&quot; height=&quot;50px&quot; title=&quot;Dale Wijnand&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/dwijnand&quot;&gt;&lt;code&gt;@dwijnand&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;pattern matcher, MiMa, partest&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/som-snytt&quot; height=&quot;50px&quot; title=&quot;Som Snytt&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/som-snytt&quot;&gt;&lt;code&gt;@som-snytt&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;warnings/lints/errors, REPL, compiler options, compiler internals, partest&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/Ichoran&quot; height=&quot;50px&quot; title=&quot;Rex Kerr&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/Ichoran&quot;&gt;&lt;code&gt;@Ichoran&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;collections library, performance&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/viktorklang&quot; height=&quot;50px&quot; title=&quot;Viktor Klang&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/viktorklang&quot;&gt;&lt;code&gt;@viktorklang&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;concurrency, futures&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/sjrd&quot; height=&quot;50px&quot; title=&quot;S√©bastien Doeraene&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/sjrd&quot;&gt;&lt;code&gt;@sjrd&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;interactions with Scala.js&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/NthPortal&quot; height=&quot;50px&quot; title=&quot;Princess | April&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/NthPortal&quot;&gt;&lt;code&gt;@NthPortal&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;library, concurrency, &lt;code&gt;scala.math&lt;/code&gt;, &lt;code&gt;LazyList&lt;/code&gt;, &lt;code&gt;Using&lt;/code&gt;, warnings&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/bishabosha&quot; height=&quot;50px&quot; title=&quot;Jamie Thompson&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/bishabosha&quot;&gt;&lt;code&gt;@bishabosha&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;TASTy reader&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/joroKr21&quot; height=&quot;50px&quot; title=&quot;Georgi Krastev&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/joroKr21&quot;&gt;&lt;code&gt;@joroKr21&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;higher-kinded types, implicits, variance&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;P.S.: If you have some spare time to help out around here, we would be delighted to add your name to this list!&lt;/p&gt; 
&lt;h1&gt;Branches&lt;/h1&gt; 
&lt;p&gt;Target the oldest branch you would like your changes to end up in. We periodically merge forward from 2.12.x to 2.13.x. Most changes should target 2.13.x, as 2.12.x is now under minimal maintenance.&lt;/p&gt; 
&lt;p&gt;If your change is difficult to merge forward, you may be asked to also submit a separate PR targeting the newer branch.&lt;/p&gt; 
&lt;p&gt;If your change is version-specific and shouldn&#39;t be merged forward, put &lt;code&gt;[nomerge]&lt;/code&gt; in the PR name.&lt;/p&gt; 
&lt;p&gt;If your change is a backport from a newer branch and thus doesn&#39;t need to be merged forward, put &lt;code&gt;[backport]&lt;/code&gt; in the PR name.&lt;/p&gt; 
&lt;h2&gt;Choosing a branch&lt;/h2&gt; 
&lt;p&gt;Most changes should target 2.13.x. We are increasingly reluctant to target 2.12.x unless there is a special reason (e.g. if an especially bad bug is found, or if there is commercial sponsorship). See &lt;a href=&quot;https://www.scala-lang.org/development/#scala-2-maintenance&quot;&gt;Scala 2 maintenance&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Repository structure&lt;/h1&gt; 
&lt;p&gt;Most importantly:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;scala/
+--build.sbt                 The main sbt build definition
+--project/                  The rest of the sbt build
+--src/                      All sources
   +---/library              Scala Standard Library
   +---/reflect              Scala Reflection
   +---/compiler             Scala Compiler
+--test/                     The Scala test suite
   +---/files                Partest tests
   +---/junit                JUnit tests
   +---/scalacheck           ScalaCheck tests
+--spec/                     The Scala language specification
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;but also:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;scala/
   +---/library-aux          Scala Auxiliary Library, for bootstrapping and documentation purposes
   +---/interactive          Scala Interactive Compiler, for clients such as an IDE (aka Presentation Compiler)
   +---/manual               Scala&#39;s runner scripts &quot;man&quot; (manual) pages
   +---/partest              Scala&#39;s internal parallel testing framework
   +---/partest-javaagent    Partest&#39;s helper java agent
   +---/repl                 Scala REPL core
   +---/repl-frontend        Scala REPL frontend
   +---/scaladoc             Scala&#39;s documentation tool
   +---/scalap               Scala&#39;s class file decompiler
   +---/testkit              Scala&#39;s unit-testing kit
+--admin/                    Scripts for the CI jobs and releasing
+--doc/                      Additional licenses and copyrights
+--scripts/                  Scripts for the CI jobs and releasing
+--tools/                    Scripts useful for local development
+--build/                    Build products
+--dist/                     Build products
+--target/                   Build products
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Get ready to contribute&lt;/h1&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;You need the following tools:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Java SDK. The baseline version is 8 for both 2.12.x and 2.13.x. It is almost always fine to use a later SDK (such as 17 or 21) for local development. CI will verify against the baseline version.&lt;/li&gt; 
 &lt;li&gt;sbt&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;MacOS and Linux work. Windows may work if you use Cygwin. Community help with keeping the build working on Windows and documenting any needed setup is appreciated.&lt;/p&gt; 
&lt;h2&gt;Tools we use&lt;/h2&gt; 
&lt;p&gt;We are grateful for the following OSS licenses:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.ej-technologies.com/products/jprofiler/overview.html&quot;&gt;JProfiler Java profiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.yourkit.com/java/profiler/&quot;&gt;YourKit Java Profiler&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.jetbrains.com/idea/download/&quot;&gt;IntelliJ IDEA&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://develocity.scala-lang.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&amp;amp;labelColor=02303A&quot; alt=&quot;Revved up by Develocity&quot; /&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Build setup&lt;/h2&gt; 
&lt;h3&gt;Basics&lt;/h3&gt; 
&lt;p&gt;During ordinary development, a new Scala build is built by the previously released version, known as the &quot;reference compiler&quot; or, slangily, as &quot;STARR&quot; (stable reference release). Building with STARR is sufficient for most kinds of changes.&lt;/p&gt; 
&lt;p&gt;However, a full build of Scala is &lt;em&gt;bootstrapped&lt;/em&gt;. Bootstrapping has two steps: first, build with STARR; then, build again using the freshly built compiler, leaving STARR behind. This guarantees that every Scala version can build itself.&lt;/p&gt; 
&lt;p&gt;If you change the code generation part of the Scala compiler, your changes will only show up in the bytecode of the library and compiler after a bootstrap. Our CI does a bootstrapped build.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Bootstrapping locally&lt;/strong&gt;: To perform a bootstrap, run &lt;code&gt;restarrFull&lt;/code&gt; within an sbt session. This will build and publish the Scala distribution to your local artifact repository and then switch sbt to use that version as its new &lt;code&gt;scalaVersion&lt;/code&gt;. You may then revert back with &lt;code&gt;reload&lt;/code&gt;. Note &lt;code&gt;restarrFull&lt;/code&gt; will also write the STARR version to &lt;code&gt;buildcharacter.properties&lt;/code&gt; so you can switch back to it with &lt;code&gt;restarr&lt;/code&gt; without republishing. This will switch the sbt session to use the &lt;code&gt;build-restarr&lt;/code&gt; and &lt;code&gt;target-restarr&lt;/code&gt; directories instead of &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;target&lt;/code&gt;, which avoids wiping out classfiles and incremental metadata. IntelliJ will continue to be configured to compile and run tests using the starr version in &lt;code&gt;versions.properties&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;For history on how the current scheme was arrived at, see &lt;a href=&quot;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&quot;&gt;https://groups.google.com/d/topic/scala-internals/gp5JsM1E0Fo/discussion&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Building with fatal warnings&lt;/strong&gt;: To make warnings in the project fatal (i.e. turn them into errors), run &lt;code&gt;set Global / fatalWarnings := true&lt;/code&gt; in sbt (replace &lt;code&gt;Global&lt;/code&gt; with the name of a module‚Äîsuch as &lt;code&gt;reflect&lt;/code&gt;‚Äîto only make warnings fatal for that module). To disable fatal warnings again, either &lt;code&gt;reload&lt;/code&gt; sbt, or run &lt;code&gt;set Global / fatalWarnings := false&lt;/code&gt; (again, replace &lt;code&gt;Global&lt;/code&gt; with the name of a module if you only enabled fatal warnings for that module). CI always has fatal warnings enabled.&lt;/p&gt; 
&lt;h3&gt;Using the sbt build&lt;/h3&gt; 
&lt;p&gt;Once you&#39;ve started an &lt;code&gt;sbt&lt;/code&gt; session you can run one of the core commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;compile&lt;/code&gt; compiles all sub-projects (library, reflect, compiler, scaladoc, etc)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;scala&lt;/code&gt; / &lt;code&gt;scalac&lt;/code&gt; run the REPL / compiler directly from sbt (accept options / arguments)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;enableOptimizer&lt;/code&gt; reloads the build with the Scala optimizer enabled. Our releases are built this way. Enable this when working on compiler performance improvements. When the optimizer is enabled the build will be slower and incremental builds can be incorrect.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;setupPublishCore&lt;/code&gt; runs &lt;code&gt;enableOptimizer&lt;/code&gt; and configures a version number based on the current Git SHA. Often used as part of bootstrapping: &lt;code&gt;sbt setupPublishCore publishLocal &amp;amp;&amp;amp; sbt -Dstarr.version=&amp;lt;VERSION&amp;gt; testAll&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dist/mkBin&lt;/code&gt; generates runner scripts (&lt;code&gt;scala&lt;/code&gt;, &lt;code&gt;scalac&lt;/code&gt;, etc) in &lt;code&gt;build/quick/bin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;dist/mkPack&lt;/code&gt; creates a build in the Scala distribution format in &lt;code&gt;build/pack&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;junit/test&lt;/code&gt; runs the JUnit tests; &lt;code&gt;junit/testOnly *Foo&lt;/code&gt; runs a subset&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;scalacheck/test&lt;/code&gt; runs scalacheck tests, use &lt;code&gt;testOnly&lt;/code&gt; to run a subset&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;partest&lt;/code&gt; runs partest tests (accepts options, try &lt;code&gt;partest --help&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;publishLocal&lt;/code&gt; publishes a distribution locally (can be used as &lt;code&gt;scalaVersion&lt;/code&gt; in other sbt projects) 
  &lt;ul&gt; 
   &lt;li&gt;Optionally &lt;code&gt;set baseVersionSuffix := &quot;bin-abcd123-SNAPSHOT&quot;&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the git hash of the revision being published. You can also use something custom like &lt;code&gt;&quot;bin-mypatch&quot;&lt;/code&gt;. This changes the version number from &lt;code&gt;2.13.2-SNAPSHOT&lt;/code&gt; to something more stable (&lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;Note that the &lt;code&gt;-bin&lt;/code&gt; string marks the version binary compatible. Using it in sbt will cause the &lt;code&gt;scalaBinaryVersion&lt;/code&gt; to be &lt;code&gt;2.13&lt;/code&gt;. If the version is not binary compatible, we recommend using &lt;code&gt;-pre&lt;/code&gt;, e.g., &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;Optionally &lt;code&gt;set ThisBuild / Compile / packageDoc / publishArtifact := false&lt;/code&gt; to skip generating / publishing API docs (speeds up the process).&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If a command results in an error message like &lt;code&gt;a module is not authorized to depend on itself&lt;/code&gt;, it may be that a global sbt plugin is causing a cyclical dependency. Try disabling global sbt plugins (perhaps by temporarily commenting them out in &lt;code&gt;~/.sbt/1.0/plugins/plugins.sbt&lt;/code&gt;).&lt;/p&gt; 
&lt;h4&gt;Sandbox&lt;/h4&gt; 
&lt;p&gt;We recommend keeping local test files in the &lt;code&gt;sandbox&lt;/code&gt; directory which is listed in the &lt;code&gt;.gitignore&lt;/code&gt; of the Scala repo.&lt;/p&gt; 
&lt;h3&gt;IDE setup&lt;/h3&gt; 
&lt;p&gt;In IntelliJ IDEA, use &quot;File - Open...&quot;, select the project folder and use &quot;Open as: sbt project&quot;.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;JUnit tests can be launched / debugged from the IDE, including tests that run the compiler (e.g., &lt;code&gt;QuickfixTest&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Building in IntelliJ interoperates with the sbt build; the compiler script in &lt;code&gt;build/quick/bin&lt;/code&gt; (generated by running &lt;code&gt;sbt dist/mkBin&lt;/code&gt;) runs the classfiles built via the IDE.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;In VSCode / Metals, open the project directory and import the project as ususal.&lt;/p&gt; 
&lt;h1&gt;Coding guidelines&lt;/h1&gt; 
&lt;p&gt;Our guidelines for contributing are explained in &lt;a href=&quot;https://raw.githubusercontent.com/scala/scala/2.13.x/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;. It contains useful information on our coding standards, testing, documentation, how we use git and GitHub and how to get your code reviewed.&lt;/p&gt; 
&lt;p&gt;You may also want to check out the following resources:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The &lt;a href=&quot;https://scala-lang.org/contribute/hacker-guide.html&quot;&gt;&quot;Scala Hacker Guide&quot;&lt;/a&gt; covers some of the same ground as this README, but in greater detail and in a more tutorial style, using a running example.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.scala-lang.org&quot;&gt;Scala documentation site&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Scala CI&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://travis-ci.com/scala/scala&quot;&gt;&lt;img src=&quot;https://travis-ci.com/scala/scala.svg?branch=2.13.x&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Once you submit a PR your commits will be automatically tested by the Scala CI.&lt;/p&gt; 
&lt;p&gt;Our CI setup is always evolving. See &lt;a href=&quot;https://github.com/scala/scala-dev/issues/751&quot;&gt;scala/scala-dev#751&lt;/a&gt; for more details on how things currently work and how we expect they might change.&lt;/p&gt; 
&lt;p&gt;If you see a spurious failure on Jenkins, you can post &lt;code&gt;/rebuild&lt;/code&gt; as a PR comment. The &lt;a href=&quot;https://github.com/scala/scabot&quot;&gt;scabot README&lt;/a&gt; lists all available commands.&lt;/p&gt; 
&lt;p&gt;If you&#39;d like to test your patch before having everything polished for review, you can have Travis CI build your branch (make sure you have a fork and have Travis CI enabled for branch builds on it first, and then push your branch). Also feel free to submit a draft PR. In case your draft branch contains a large number of commits (that you didn&#39;t clean up / squash yet for review), consider adding &lt;code&gt;[ci: last-only]&lt;/code&gt; to the PR title. That way only the last commit will be tested, saving some energy and CI-resources. Note that inactive draft PRs will be closed eventually, which does not mean the change is being rejected.&lt;/p&gt; 
&lt;p&gt;CI performs a compiler bootstrap. The first task, &lt;code&gt;validatePublishCore&lt;/code&gt;, publishes a build of your commit to the temporary repository &lt;a href=&quot;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&quot;&gt;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots&lt;/a&gt;. Note that this build is not yet bootstrapped, its bytecode is built using the current STARR. The version number is &lt;code&gt;2.13.2-bin-abcd123-SNAPSHOT&lt;/code&gt; where &lt;code&gt;abcd123&lt;/code&gt; is the commit hash. For binary incompatible builds, the version number is &lt;code&gt;2.14.0-pre-abcd123-SNAPSHOT&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;You can use Scala builds in the validation repository locally by adding a resolver and specifying the corresponding &lt;code&gt;scalaVersion&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ sbt
&amp;gt; set resolvers += &quot;pr&quot; at &quot;https://scala-ci.typesafe.com/artifactory/scala-pr-validation-snapshots/&quot;
&amp;gt; set scalaVersion := &quot;2.13.17-bin-abcd123-SNAPSHOT&quot;
&amp;gt; console
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;&quot;Nightly&quot; builds&lt;/h2&gt; 
&lt;p&gt;The Scala CI publishes these to &lt;a href=&quot;https://scala-ci.typesafe.com/artifactory/scala-integration/&quot;&gt;https://scala-ci.typesafe.com/artifactory/scala-integration/&lt;/a&gt; .&lt;/p&gt; 
&lt;p&gt;Using a nightly build in sbt and other tools is explained on this &lt;a href=&quot;https://docs.scala-lang.org/overviews/core/nightlies.html&quot;&gt;doc page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Although we casually refer to these as &quot;nightly&quot; builds, they aren&#39;t actually built nightly, but &quot;mergely&quot;. That is to say, a build is published for every merged PR.&lt;/p&gt; 
&lt;h2&gt;Scala CI internals&lt;/h2&gt; 
&lt;p&gt;The Scala CI runs as a Jenkins instance on &lt;a href=&quot;https://scala-ci.typesafe.com/&quot;&gt;scala-ci.typesafe.com&lt;/a&gt;, configured by a chef cookbook at &lt;a href=&quot;https://github.com/scala/scala-jenkins-infra&quot;&gt;scala/scala-jenkins-infra&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The build bot that watches PRs, triggers testing builds and applies the &quot;reviewed&quot; label after an LGTM comment is in the &lt;a href=&quot;https://github.com/scala/scabot&quot;&gt;scala/scabot&lt;/a&gt; repo.&lt;/p&gt; 
&lt;h2&gt;Community build&lt;/h2&gt; 
&lt;p&gt;The Scala community build is an important method for testing Scala releases. A community build can be launched for any Scala commit, even before the commit&#39;s PR has been merged. That commit is then used to build a large number of open-source projects from source and run their test suites.&lt;/p&gt; 
&lt;p&gt;To request a community build run on your PR, just ask in a comment on the PR and a Scala team member (probably @SethTisue) will take care of it. (&lt;a href=&quot;https://github.com/scala/community-builds/wiki#can-i-run-it-against-a-pull-request-in-scalascala&quot;&gt;details&lt;/a&gt;)&lt;/p&gt; 
&lt;p&gt;Community builds run on the Scala Jenkins instance. The jobs are named &lt;code&gt;..-integrate-community-build&lt;/code&gt;. See the &lt;a href=&quot;https://github.com/scala/community-builds&quot;&gt;scala/community-builds&lt;/a&gt; repo.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OpenXiangShan/XiangShan</title>
      <link>https://github.com/OpenXiangShan/XiangShan</link>
      <description>&lt;p&gt;Open-source high-performance RISC-V processor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;XiangShan&lt;/h1&gt; 
&lt;p&gt;XiangShan (È¶ôÂ±±) is an open-source high-performance RISC-V processor project.&lt;/p&gt; 
&lt;p&gt;‰∏≠ÊñáËØ¥Êòé&lt;a href=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/readme.zh-cn.md&quot;&gt;Âú®Ê≠§&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h2&gt;Documentation&lt;/h2&gt; 
&lt;p&gt;XiangShan&#39;s documentation is available at &lt;a href=&quot;https://docs.xiangshan.cc&quot;&gt;docs.xiangshan.cc&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;XiangShan Design Document for Kunminghu V2R2 has been published separately. You can find it at &lt;a href=&quot;https://docs.xiangshan.cc/projects/design/&quot;&gt;docs.xiangshan.cc/projects/design&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;XiangShan User Guide has been published separately. You can find it at &lt;a href=&quot;https://docs.xiangshan.cc/projects/user-guide/&quot;&gt;docs.xiangshan.cc/projects/user-guide&lt;/a&gt; or &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-User-Guide/releases&quot;&gt;XiangShan-User-Guide/releases&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We are using &lt;a href=&quot;https://hosted.weblate.org/projects/openxiangshan/&quot;&gt;Weblate&lt;/a&gt; to translate documentation into English and other languages. Your contributions are welcome‚Äîcome and help us improve it!&lt;/p&gt; 
&lt;p&gt;All XiangShan documents are licensed under the CC-BY-4.0.&lt;/p&gt; 
&lt;h2&gt;Publications&lt;/h2&gt; 
&lt;h3&gt;MICRO 2022: Towards Developing High Performance RISC-V Processors Using Agile Methodology&lt;/h3&gt; 
&lt;p&gt;Our paper introduces XiangShan and the practice of agile development methodology on high performance RISC-V processors. It covers some representative tools we have developed and used to accelerate the chip development process, including design, functional verification, debugging, performance validation, etc. This paper is awarded all three available badges for artifact evaluation (Available, Functional, and Reproduced).&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/images/artifacts_available_dl.jpg&quot; alt=&quot;Artifacts Available&quot; /&gt; &lt;img src=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/images/artifacts_evaluated_functional_dl.jpg&quot; alt=&quot;Artifacts Evaluated ‚Äî Functional&quot; /&gt; &lt;img src=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/images/results_reproduced_dl.jpg&quot; alt=&quot;Results Reproduced&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/micro2022-xiangshan.pdf&quot;&gt;Paper PDF&lt;/a&gt; | &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9923860&quot;&gt;IEEE Xplore&lt;/a&gt; | &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/micro2022-xiangshan.bib&quot;&gt;BibTeX&lt;/a&gt; | &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan-doc/raw/main/publications/micro2022-xiangshan-slides.pdf&quot;&gt;Presentation Slides&lt;/a&gt; | &lt;a href=&quot;https://www.bilibili.com/video/BV1FB4y1j7Jy&quot;&gt;Presentation Video&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Follow us&lt;/h2&gt; 
&lt;p&gt;Wechat/ÂæÆ‰ø°ÔºöÈ¶ôÂ±±ÂºÄÊ∫êÂ§ÑÁêÜÂô®&lt;/p&gt; 
&lt;div align=&quot;left&quot;&gt;
 &lt;img width=&quot;340&quot; height=&quot;117&quot; src=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/wechat.png&quot; /&gt;
&lt;/div&gt; 
&lt;p&gt;Zhihu/Áü•‰πéÔºö&lt;a href=&quot;https://www.zhihu.com/people/openxiangshan&quot;&gt;È¶ôÂ±±ÂºÄÊ∫êÂ§ÑÁêÜÂô®&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Weibo/ÂæÆÂçöÔºö&lt;a href=&quot;https://weibo.com/u/7706264932&quot;&gt;È¶ôÂ±±ÂºÄÊ∫êÂ§ÑÁêÜÂô®&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;You can contact us through &lt;a href=&quot;mailto:xiangshan-all@ict.ac.cn&quot;&gt;our mailing list&lt;/a&gt;. All mails from this list will be archived &lt;a href=&quot;https://www.mail-archive.com/xiangshan-all@ict.ac.cn/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;The first stable micro-architecture of XiangShan is called Yanqihu (ÈõÅÊ†ñÊπñ) and is &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan/tree/yanqihu&quot;&gt;on the yanqihu branch&lt;/a&gt;, which has been developed since June 2020.&lt;/p&gt; 
&lt;p&gt;The second stable micro-architecture of XiangShan is called Nanhu (ÂçóÊπñ) and is &lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan/tree/nanhu&quot;&gt;on the nanhu branch&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The current version of XiangShan, also known as Kunminghu (ÊòÜÊòéÊπñ), is still under development on the master branch.&lt;/p&gt; 
&lt;p&gt;The micro-architecture overview of Kunminghu (ÊòÜÊòéÊπñ) is shown below.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/images/xs-arch-kunminghu.svg?sanitize=true&quot; alt=&quot;xs-arch-kunminghu&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Sub-directories Overview&lt;/h2&gt; 
&lt;p&gt;Some of the key directories are shown below.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;.
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îî‚îÄ‚îÄ main/scala         # design files
‚îÇ       ‚îú‚îÄ‚îÄ device         # virtual device for simulation
‚îÇ       ‚îú‚îÄ‚îÄ system         # SoC wrapper
‚îÇ       ‚îú‚îÄ‚îÄ top            # top module
‚îÇ       ‚îú‚îÄ‚îÄ utils          # utilization code
‚îÇ       ‚îî‚îÄ‚îÄ xiangshan      # main design code
‚îÇ           ‚îî‚îÄ‚îÄ transforms # some useful firrtl transforms
‚îú‚îÄ‚îÄ scripts                # scripts for agile development
‚îú‚îÄ‚îÄ fudian                 # floating unit submodule of XiangShan
‚îú‚îÄ‚îÄ huancun                # L2/L3 cache submodule of XiangShan
‚îú‚îÄ‚îÄ difftest               # difftest co-simulation framework
‚îî‚îÄ‚îÄ ready-to-run           # pre-built simulation images
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;IDE Support&lt;/h2&gt; 
&lt;h3&gt;bsp&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;make bsp
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;IDEA&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;make idea
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Generate Verilog&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Run &lt;code&gt;make verilog&lt;/code&gt; to generate verilog code. This generates multiple &lt;code&gt;.sv&lt;/code&gt; files in the &lt;code&gt;build/rtl/&lt;/code&gt; folder (e.g., &lt;code&gt;build/rtl/XSTop.sv&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Run Programs by Simulation&lt;/h2&gt; 
&lt;h3&gt;Prepare environment&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Set environment variable &lt;code&gt;NEMU_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&quot;https://github.com/OpenXiangShan/NEMU&quot;&gt;NEMU project&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Set environment variable &lt;code&gt;NOOP_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the XiangShan project.&lt;/li&gt; 
 &lt;li&gt;Set environment variable &lt;code&gt;AM_HOME&lt;/code&gt; to the &lt;strong&gt;absolute path&lt;/strong&gt; of the &lt;a href=&quot;https://github.com/OpenXiangShan/nexus-am&quot;&gt;AM project&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Install &lt;code&gt;mill&lt;/code&gt;. Refer to &lt;a href=&quot;https://mill-build.org/mill/cli/installation-ide.html#_bootstrap_scripts&quot;&gt;the Manual section in this guide&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Clone this project and run &lt;code&gt;make init&lt;/code&gt; to initialize submodules.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Run with simulator&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://verilator.org/guide/latest/&quot;&gt;Verilator&lt;/a&gt;, the open-source Verilog simulator.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make emu&lt;/code&gt; to build the C++ simulator &lt;code&gt;./build/emu&lt;/code&gt; with Verilator.&lt;/li&gt; 
 &lt;li&gt;Refer to &lt;code&gt;./build/emu --help&lt;/code&gt; for run-time arguments of the simulator.&lt;/li&gt; 
 &lt;li&gt;Refer to &lt;code&gt;Makefile&lt;/code&gt; and &lt;code&gt;verilator.mk&lt;/code&gt; for more information.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;make emu CONFIG=MinimalConfig EMU_THREADS=2 -j10
./build/emu -b 0 -e 0 -i ./ready-to-run/coremark-2-iteration.bin --diff ./ready-to-run/riscv64-nemu-interpreter-so
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Run with xspdb&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://github.com/XS-MLVP/picker&quot;&gt;picker&lt;/a&gt;, a verifaction tool that supports high-level languages.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make pdb&lt;/code&gt; to build XiangShan Python binaries.&lt;/li&gt; 
 &lt;li&gt;Run &lt;code&gt;make pdb-run&lt;/code&gt; to run XiangShan binaries.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Example output and interaction:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make pdb-run
[Info] Set PMEM_BASE to 0x80000000 (Current: 0x80000000)
[Info] Set FIRST_INST_ADDRESS to 0x80000000 (Current: 0x80000000)
Using simulated 32768B flash
[Info] reset dut complete
&amp;gt; XiangShan/scripts/pdb-run.py(13)run()
-&amp;gt; while True:
(XiangShan) xload ready-to-run/microbench.bin   # Load binary (Tab-compatible)
(XiangShan) xwatch_commit_pc 0x80000004         # set watch point,  
(XiangShan) xistep 3                            # Step to next three instruction commit, it will stop at watch point 
[Info] Find break point (Inst commit), break (step 2107 cycles) at cycle: 2207 (0x89f)
[Info] Find break point (Inst commit, Target commit), break (step 2108 cycles) at cycle: 2208 (0x8a0)
(XiangShan) xpc                                 # print pc info
PC[0]: 0x80000000    Instr: 0x00000093
PC[1]: 0x80000004    Instr: 0x00000113
PC[2]: 0x0    Instr: 0x0
...
PC[7]: 0x0    Instr: 0x0
(XiangShan) xistep 1000000                      # Execute to binary end
[Info] Find break point (Inst commit), break (step 2037 cycles) at cycle: 2207 (0x89f)
[Info] Find break point (Inst commit), break (step 2180 cycles) at cycle: 2207 (0x89f)
...
HIT GOOD LOOP at pc = 0xf0001cb0
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Troubleshooting Guide&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/OpenXiangShan/XiangShan/wiki/Troubleshooting-Guide&quot;&gt;Troubleshooting Guide&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;The implementation of XiangShan is inspired by several key papers. We list these papers in XiangShan document, see: &lt;a href=&quot;https://docs.xiangshan.cc/zh-cn/latest/acknowledgments/&quot;&gt;Acknowledgements&lt;/a&gt;. We very much encourage and expect that more academic innovations can be realised based on XiangShan in the future.&lt;/p&gt; 
&lt;h2&gt;LICENSE&lt;/h2&gt; 
&lt;p&gt;Copyright ¬© 2020-2025 Institute of Computing Technology, Chinese Academy of Sciences.&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 2021-2025 Beijing Institute of Open Source Chip&lt;/p&gt; 
&lt;p&gt;Copyright ¬© 2020-2022 by Peng Cheng Laboratory.&lt;/p&gt; 
&lt;p&gt;XiangShan is licensed under &lt;a href=&quot;https://raw.githubusercontent.com/OpenXiangShan/XiangShan/master/LICENSE&quot;&gt;Mulan PSL v2&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>scala/scala3</title>
      <link>https://github.com/scala/scala3</link>
      <description>&lt;p&gt;The Scala 3 compiler, also known as Dotty.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dotty&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/scala/scala3/actions?query=branch%3Amain&quot;&gt;&lt;img src=&quot;https://github.com/scala/scala3/workflows/Dotty/badge.svg?branch=main&quot; alt=&quot;Dotty CI&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.com/invite/scala&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/632150470000902164&quot; alt=&quot;Join the chat at https://discord.com/invite/scala&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://develocity.scala-lang.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Revved%20up%20by-Develocity-06A0CE?logo=Gradle&amp;amp;labelColor=02303A&quot; alt=&quot;Revved up by Develocity&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.scala-lang.org/scala3/&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Try it out&lt;/h1&gt; 
&lt;p&gt;To try it in your project see also the &lt;a href=&quot;https://docs.scala-lang.org/scala3/getting-started.html&quot;&gt;Getting Started User Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Building a Local Distribution&lt;/h1&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;sbt dist/Universal/packageBin&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Find the newly-built distributions in &lt;code&gt;dist/target/&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h1&gt;Code of Conduct&lt;/h1&gt; 
&lt;p&gt;Dotty uses the &lt;a href=&quot;https://www.scala-lang.org/conduct.html&quot;&gt;Scala Code of Conduct&lt;/a&gt; for all communication and discussion. This includes both GitHub, Discord and other more direct lines of communication such as email.&lt;/p&gt; 
&lt;h1&gt;How to Contribute&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.scala-lang.org/scala3/guides/contribution/contribution-intro.html&quot;&gt;Getting Started as Contributor&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/scala/scala3/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22&quot;&gt;Issues&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;License&lt;/h1&gt; 
&lt;p&gt;Dotty is licensed under the &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache License Version 2.0&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>scalapb/ScalaPB</title>
      <link>https://github.com/scalapb/ScalaPB</link>
      <description>&lt;p&gt;Protocol buffer compiler for Scala.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ScalaPB&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://index.scala-lang.org/scalapb/scalapb/scalapb-runtime&quot;&gt;&lt;img src=&quot;https://index.scala-lang.org/scalapb/scalapb/scalapb-runtime/latest-by-scala-version.svg?platform=jvm&quot; alt=&quot;ScalaPB runtime version support&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://gitter.im/ScalaPB/community&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/ScalaPB/community.svg?sanitize=true&quot; alt=&quot;Join the chat at https://gitter.im/ScalaPB/community&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/scalapb/ScalaPB/actions?query=workflow%3ACI&quot;&gt;&lt;img src=&quot;https://github.com/scalapb/ScalaPB/workflows/CI/badge.svg?sanitize=true&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;ScalaPB is a protocol buffer compiler (&lt;code&gt;protoc&lt;/code&gt;) plugin for Scala. It will generate Scala case classes, parsers and serializers for your protocol buffers.&lt;/p&gt; 
&lt;p&gt;ScalaPB generates case classes that can co-exist in the same project alongside the Java-generated code for ProtocolBuffer. This makes it easy to gradually migrate an existing project from the Java version of protocol buffers to Scala. This is achieved by having the ScalaPB generated code use the proto file as part of the package name (in contrast to Java which uses the file name in CamelCase as an outer class)&lt;/p&gt; 
&lt;p&gt;Each top-level message and enum is written to a separate Scala file. This results in a significant improvement in incremental compilations.&lt;/p&gt; 
&lt;p&gt;Another cool feature of ScalaPB is that it can optionally generate methods that convert a Java protocol buffer to a Scala protocol buffer and vice versa. This is useful if you are gradually migrating a large code base from Java protocol buffers to Scala. The optional Java conversion is required if you want to use &lt;code&gt;fromAscii&lt;/code&gt; (parsing ASCII representation of a protocol buffer). The current implementation delegates to the Java version.&lt;/p&gt; 
&lt;h1&gt;Highlights&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Supports proto2 and proto3&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Easily update nested structure in functional way using lenses&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Scala.js integration&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;GRPC integration&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Compatible with SparkSQL (through a helper library)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Conversion to and from JSON&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Support user-defined options (since 0.5.29)&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Support extensions (since 0.6.0)&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Versions&lt;/h1&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Version&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0.6.x&lt;/td&gt; 
   &lt;td&gt;Stable. Supports Protobuf 2.6.x to 3.5.x&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0.5.x&lt;/td&gt; 
   &lt;td&gt;Supports Protobuf 2.6.x and Protobuf 3.1.x.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;0.4.x&lt;/td&gt; 
   &lt;td&gt;Stable, unsupported. Works with Protobuf 2.6.x&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;Installing&lt;/h1&gt; 
&lt;p&gt;To automatically generate Scala case classes for your messages add ScalaPB&#39;s sbt plugin to your project. Create a file named &lt;code&gt;project/protoc.sbt&lt;/code&gt; containing the following line:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;addSbtPlugin(&quot;com.thesamet&quot; % &quot;sbt-protoc&quot; % &quot;1.0.6&quot;)

libraryDependencies += &quot;com.thesamet.scalapb&quot; %% &quot;compilerplugin&quot; % &quot;0.11.3&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the following line to your &lt;code&gt;build.sbt&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;Compile / PB.targets := Seq(
  scalapb.gen() -&amp;gt; (Compile / sourceManaged).value / &quot;scalapb&quot;
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For additional configuration options, see &lt;a href=&quot;https://scalapb.github.io/docs/sbt-settings&quot;&gt;ScalaPB SBT Settings&lt;/a&gt; documentation&lt;/p&gt; 
&lt;h1&gt;Using ScalaPB&lt;/h1&gt; 
&lt;p&gt;Documentation is available at &lt;a href=&quot;https://scalapb.github.io/&quot;&gt;ScalaPB website&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Questions?&lt;/h1&gt; 
&lt;p&gt;See &lt;a href=&quot;https://scalapb.github.io/docs/contact&quot;&gt;contacting us&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Testing&lt;/h1&gt; 
&lt;p&gt;ScalaPB uses ScalaCheck to aggressively test the generated code. The test generates many different sets of proto files. The sets are growing in complexity: number of files, references to messages from other protos, message nesting and so on. Then, test data is generated to populate this protocol schema, then we check that the ScalaPB generated code behaves exactly like the reference implementation in Java.&lt;/p&gt; 
&lt;p&gt;Running the tests:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;sbt test
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The tests take a few minutes to run. There is a smaller test suite called &lt;code&gt;e2e&lt;/code&gt; that uses the sbt plugin to compile the protos and runs a series of ScalaChecks on the outputs. To run it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;./e2e.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Sponsors&lt;/h1&gt; 
&lt;p&gt;We are very thankful to our sponsors:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/bikaras&quot;&gt;Evgeny Rubtsov (bikaras)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/andrew-selvia&quot;&gt;Andrew Selvia&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ASRagab&quot;&gt;Ahmad Ragab&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>apache/incubator-gluten</title>
      <link>https://github.com/apache/incubator-gluten</link>
      <description>&lt;p&gt;Gluten is a middle layer responsible for offloading JVM-based SQL engines&#39; execution to native engines.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/image/gluten-logo.svg?sanitize=true&quot; alt=&quot;Gluten&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;Apache Gluten (Incubating): A Middle Layer for Offloading JVM-based SQL Engines&#39; Execution to Native Engines&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.bestpractices.dev/projects/8452&quot;&gt;&lt;img src=&quot;https://www.bestpractices.dev/projects/8452/badge&quot; alt=&quot;OpenSSF Best Practices&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;1. Introduction&lt;/h1&gt; 
&lt;h2&gt;Problem Statement&lt;/h2&gt; 
&lt;p&gt;Apache Spark is a stable, mature project that has been developed for many years. It is one of the best frameworks to scale out for processing petabyte-scale datasets. However, the Spark community has had to address performance challenges that require various optimizations over time. As a key optimization in Spark 2.0, Whole Stage Code Generation is introduced to replace Volcano Model, which achieves 2x speedup. Henceforth, most optimizations are at query plan level. Single operator&#39;s performance almost stops growing.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/47296334/199853029-b6d0ea19-f8e4-4f62-9562-2838f7f159a7.png&quot; width=&quot;800&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;On the other side, native SQL engines have been developed for a few years, such as Clickhouse, Arrow and Velox, etc. With features like native execution, columnar data format and vectorized data processing, these native engines can outperform Spark&#39;s JVM based SQL engine. However, they only support single node execution.&lt;/p&gt; 
&lt;h2&gt;Gluten&#39;s Basic Design&lt;/h2&gt; 
&lt;p&gt;‚ÄúGluten‚Äù is Latin for &quot;glue&quot;. The main goal of Gluten project is to glue native engines with SparkSQL. Thus, we can benefit from high scalability of Spark SQL framework and high performance of native engines.&lt;/p&gt; 
&lt;p&gt;The basic design rule is that we would reuse Spark&#39;s whole control flow and as much JVM code as possible but offload the compute-intensive data processing to native side. Here is what Gluten does basically:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Transform Spark‚Äôs physical plan to Substrait plan, then transform it to native engine&#39;s plan.&lt;/li&gt; 
 &lt;li&gt;Offload performance-critical data processing to native engine.&lt;/li&gt; 
 &lt;li&gt;Define clear JNI interfaces for native SQL engines.&lt;/li&gt; 
 &lt;li&gt;Switch available native backends easily.&lt;/li&gt; 
 &lt;li&gt;Reuse Spark‚Äôs distributed control flow.&lt;/li&gt; 
 &lt;li&gt;Manage data sharing between JVM and native.&lt;/li&gt; 
 &lt;li&gt;Extensible to support more native engines.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Target User&lt;/h2&gt; 
&lt;p&gt;Gluten&#39;s target user is anyone who aspires to accelerate SparkSQL fundamentally. As a plugin to Spark, Gluten doesn&#39;t require any change for dataframe API or SQL query, but only requires user to make correct configuration. See Gluten configuration properties &lt;a href=&quot;https://github.com/apache/incubator-gluten/raw/main/docs/Configuration.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;References&lt;/h2&gt; 
&lt;p&gt;You can click below links for more related information.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=0Q6gHT_N-1U&quot;&gt;Gluten Intro Video at Data AI Summit 2022&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://medium.com/intel-analytics-software/accelerate-spark-sql-queries-with-gluten-9000b65d1b4e&quot;&gt;Gluten Intro Article at Medium.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cn.kyligence.io/blog/gluten-spark/&quot;&gt;Gluten Intro Article at Kyligence.io(in Chinese)&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://engineering.fb.com/2023/03/09/open-source/velox-open-source-execution-engine/&quot;&gt;Velox Intro from Meta&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;2. Architecture&lt;/h1&gt; 
&lt;p&gt;The overview chart is like below. Substrait provides a well-defined cross-language specification for data compute operations (see more details &lt;a href=&quot;https://substrait.io/&quot;&gt;here&lt;/a&gt;). Spark physical plan is transformed to Substrait plan. Then Substrait plan is passed to native through JNI call. On native side, the native operator chain will be built out and offloaded to native engine. Gluten will return Columnar Batch to Spark and Spark Columnar API (since Spark-3.0) will be used at execution time. Gluten uses Apache Arrow data format as its basic data format, so the returned data to Spark JVM is ArrowColumnarBatch.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/47296334/199617207-1140698a-4d53-462d-9bc7-303d14be060b.png&quot; width=&quot;800&quot; /&gt; &lt;/p&gt; Currently, Gluten only supports Clickhouse backend &amp;amp; Velox backend. Velox is a C++ database acceleration library which provides reusable, extensible and high-performance data processing components. More details can be found from https://github.com/facebookincubator/velox/. Gluten can also be extended to support more backends. 
&lt;p&gt;There are several key components in Gluten:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Query Plan Conversion&lt;/strong&gt;: converts Spark&#39;s physical plan to Substrait plan.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unified Memory Management&lt;/strong&gt;: controls native memory allocation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Columnar Shuffle&lt;/strong&gt;: shuffles Gluten columnar data. The shuffle service still reuses the one in Spark core. A kind of columnar exchange operator is implemented to support Gluten columnar data format.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fallback Mechanism&lt;/strong&gt;: supports falling back to Vanilla spark for unsupported operators. Gluten ColumnarToRow (C2R) and RowToColumnar (R2C) will convert Gluten columnar data and Spark&#39;s internal row data if needed. Both C2R and R2C are implemented in native code as well&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt;: collected from Gluten native engine to help identify bugs, performance bottlenecks, etc. The metrics are displayed in Spark UI.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Shim Layer&lt;/strong&gt;: supports multiple Spark versions. We plan to only support Spark&#39;s latest 2 or 3 releases. Currently, Spark-3.2, Spark-3.3 &amp;amp; Spark-3.4 (experimental) are supported.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;3. User Guide&lt;/h1&gt; 
&lt;p&gt;Here is a basic configuration to enable Gluten in Spark.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;export GLUTEN_JAR=/PATH/TO/GLUTEN_JAR
spark-shell \
  --master yarn --deploy-mode client \
  --conf spark.plugins=org.apache.gluten.GlutenPlugin \
  --conf spark.memory.offHeap.enabled=true \
  --conf spark.memory.offHeap.size=20g \
  --conf spark.driver.extraClassPath=${GLUTEN_JAR} \
  --conf spark.executor.extraClassPath=${GLUTEN_JAR} \
  --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager
  ...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are two ways to acquire Gluten jar for the above configuration.&lt;/p&gt; 
&lt;h3&gt;Use Released Jar&lt;/h3&gt; 
&lt;p&gt;Please download a tar package &lt;a href=&quot;https://downloads.apache.org/incubator/gluten/&quot;&gt;here&lt;/a&gt;, then extract out Gluten jar from it. Additionally, Gluten offers nightly builds based on the main branch, which are available for early testing. You can find these release jars at this link: &lt;a href=&quot;https://nightlies.apache.org/gluten/&quot;&gt;Apache Gluten Nightlies&lt;/a&gt;. It was verified on Centos-7, Centos-8, Centos-9, Ubuntu-20.04 and Ubuntu-22.04.&lt;/p&gt; 
&lt;h3&gt;Build From Source&lt;/h3&gt; 
&lt;p&gt;For &lt;strong&gt;Velox&lt;/strong&gt; backend, please refer to &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/get-started/Velox.md&quot;&gt;Velox.md&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/get-started/build-guide.md&quot;&gt;build-guide.md&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;For &lt;strong&gt;ClickHouse&lt;/strong&gt; backend, please refer to &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/get-started/ClickHouse.md&quot;&gt;ClickHouse.md&lt;/a&gt;. ClickHouse backend is developed by &lt;a href=&quot;https://kyligence.io/&quot;&gt;Kyligence&lt;/a&gt;, please visit &lt;a href=&quot;https://github.com/Kyligence/ClickHouse&quot;&gt;https://github.com/Kyligence/ClickHouse&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;p&gt;Gluten jar will be generated under &lt;code&gt;/PATH/TO/GLUTEN/package/target/&lt;/code&gt; after the build.&lt;/p&gt; 
&lt;h3&gt;Configurations&lt;/h3&gt; 
&lt;p&gt;Common configurations used by Gluten is listed in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/Configuration.md&quot;&gt;Configuration.md&lt;/a&gt;. Velox specific configurations is listed in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/velox-configuration.md&quot;&gt;velox-configuration.md&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Some of the spark configurations are hornored by Gluten Velox backend, some of them are ignored, and many are transparent to Gluten. The detail can be found in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/velox-spark-configuration.md&quot;&gt;velox-spark-configuration.md&lt;/a&gt; and parquet write ones can be found in &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/velox-parquet-write-configuration.md&quot;&gt;velox-parquet-write-configuration.md&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;4. Gluten Website&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://gluten.apache.org/&quot;&gt;https://gluten.apache.org/&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;5. Contribution&lt;/h1&gt; 
&lt;p&gt;Welcome to contribute to Gluten project! See &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; about how to make contributions.&lt;/p&gt; 
&lt;h1&gt;6. Community&lt;/h1&gt; 
&lt;p&gt;Gluten successfully became Apache incubator project in March 2024. Here are several ways to contact us:&lt;/p&gt; 
&lt;h2&gt;GitHub&lt;/h2&gt; 
&lt;p&gt;Welcome to report any issue or create any discussion related to Gluten in GitHub. Please do a search from GitHub issue list before creating a new one to avoid repetition.&lt;/p&gt; 
&lt;h2&gt;Mail Lists&lt;/h2&gt; 
&lt;p&gt;For any technical discussion, please send email to &lt;a href=&quot;mailto:dev@gluten.apache.org&quot;&gt;dev@gluten.apache.org&lt;/a&gt;. You can go to &lt;a href=&quot;https://lists.apache.org/list.html?dev@gluten.apache.org&quot;&gt;archives&lt;/a&gt; for getting historical discussions. Please click &lt;a href=&quot;mailto:dev-subscribe@gluten.apache.org&quot;&gt;here&lt;/a&gt; to subscribe the mail list.&lt;/p&gt; 
&lt;h2&gt;Slack Channel (English communication)&lt;/h2&gt; 
&lt;p&gt;Please click &lt;a href=&quot;https://github.com/apache/incubator-gluten/discussions/8429&quot;&gt;here&lt;/a&gt; to get invitation for ASF Slack workspace where you can find &quot;incubator-gluten&quot; channel.&lt;/p&gt; 
&lt;p&gt;The ASF Slack login entry: &lt;a href=&quot;https://the-asf.slack.com/&quot;&gt;https://the-asf.slack.com/&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;WeChat Group (Chinese communication)&lt;/h2&gt; 
&lt;p&gt;For PRC developers/users, please contact weitingchen at apache.org or zhangzc at apache.org for getting invited to the WeChat group.&lt;/p&gt; 
&lt;h1&gt;7. Performance&lt;/h1&gt; 
&lt;p&gt;We use Decision Support Benchmark1 (TPC-H like) to evaluate Gluten&#39;s performance. Decision Support Benchmark1 is a query set modified from &lt;a href=&quot;http://tpc.org/tpch/default5.asp&quot;&gt;TPC-H benchmark&lt;/a&gt;. We use Parquet file format for Velox testing &amp;amp; MergeTree file format for Clickhouse testing, compared to Parquet file format as baseline. See &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/tools/workload/tpch&quot;&gt;Decision Support Benchmark1&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The below test environment: single node with 2TB data; Spark-3.3.2 for both baseline and Gluten. The Decision Support Benchmark1 result (tested in Jun. 2023) shows an overall speedup of 2.71x and up to 14.53x speedup in a single query with Gluten Velox backend used.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/image/velox_decision_support_bench1_22queries_performance.png&quot; alt=&quot;Performance&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The below testing environment: a 8-nodes AWS cluster with 1TB data; Spark-3.1.1 for both baseline and Gluten. The Decision Support Benchmark1 result shows an average speedup of 2.12x and up to 3.48x speedup with Gluten Clickhouse backend.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/docs/image/clickhouse_decision_support_bench1_22queries_performance.png&quot; alt=&quot;Performance&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;8. Qualification Tool&lt;/h1&gt; 
&lt;p&gt;The Qualification Tool is a utility to analyze Spark event log files and assess the compatibility and performance of SQL workloads with Gluten. This tool helps users understand how their workloads can benefit from Gluten.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Analyzes Spark SQL execution plans for compatibility with Gluten.&lt;/li&gt; 
 &lt;li&gt;Supports various types of event log files, including single files, folders, compressed files, and rolling event logs.&lt;/li&gt; 
 &lt;li&gt;Generates detailed reports highlighting supported and unsupported operations.&lt;/li&gt; 
 &lt;li&gt;Provides metrics on SQL execution times and operator impact.&lt;/li&gt; 
 &lt;li&gt;Offers configurable options such as threading, output directory, and date-based filtering.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;p&gt;To use the Qualification Tool, follow the instructions in its &lt;a href=&quot;https://raw.githubusercontent.com/apache/incubator-gluten/main/tools/qualification-tool/README.MD&quot;&gt;README&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Example Command&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;java -jar target/qualification-tool-1.3.0-SNAPSHOT-jar-with-dependencies.jar -f /path/to/eventlog
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For detailed usage instructions and advanced options, see the Qualification Tool README.&lt;/p&gt; 
&lt;h1&gt;9. License&lt;/h1&gt; 
&lt;p&gt;Gluten is licensed under &lt;a href=&quot;https://www.apache.org/licenses/LICENSE-2.0&quot;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;10. Acknowledgements&lt;/h1&gt; 
&lt;p&gt;Gluten was initiated by Intel and Kyligence in 2022. Several companies are also actively participating in the development, such as BIGO, Meituan, Alibaba Cloud, NetEase, Baidu, Microsoft, IBM, Google, etc.&lt;/p&gt; 
&lt;a href=&quot;https://github.com/apache/incubator-gluten/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=apache/incubator-gluten&amp;amp;columns=25&quot; /&gt; &lt;/a&gt; 
&lt;h5&gt;* LEGAL NOTICE: Your use of this software and any required dependent software (the &quot;Software Package&quot;) is subject to the terms and conditions of the software license agreements for the Software Package, which may also include notices, disclaimers, or license terms for third party or open source software included in or with the Software Package, and your use indicates your acceptance of all such terms. Please refer to the &quot;TPP.txt&quot; or other similarly-named text file included with the Software Package for additional details.&lt;/h5&gt;</description>
    </item>
    
    <item>
      <title>airbnb/chronon</title>
      <link>https://github.com/airbnb/chronon</link>
      <description>&lt;p&gt;Chronon is a data platform for serving for AI/ML applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chronon: A Data Platform for AI/ML&lt;/h1&gt; 
&lt;p&gt;Chronon is a platform that abstracts away the complexity of data computation and serving for AI/ML applications. Users define features as transformation of raw data, then Chronon can perform batch and streaming computation, scalable backfills, low-latency serving, guaranteed correctness and consistency, as well as a host of observability and monitoring tools.&lt;/p&gt; 
&lt;p&gt;It allows you to utilize all of the data within your organization, from batch tables, event streams or services to power your AI/ML projects, without needing to worry about all the complex orchestration that this would usually entail.&lt;/p&gt; 
&lt;p&gt;More information about Chronon can be found at &lt;a href=&quot;https://chronon.ai/&quot;&gt;chronon.ai&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://chronon.ai/_images/intro.png&quot; alt=&quot;High Level&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Platform Features&lt;/h2&gt; 
&lt;h3&gt;Online Serving&lt;/h3&gt; 
&lt;p&gt;Chronon offers an API for realtime fetching which returns up-to-date values for your features. It supports:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Managed pipelines for batch and realtime feature computation and updates to the serving backend&lt;/li&gt; 
 &lt;li&gt;Low latency serving of computed features&lt;/li&gt; 
 &lt;li&gt;Scalable for high fanout feature sets&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Backfills&lt;/h3&gt; 
&lt;p&gt;ML practitioners often need historical views of feature values for model training and evaluation. Chronon&#39;s backfills are:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Scalable for large time windows&lt;/li&gt; 
 &lt;li&gt;Resilient to highly skewed data&lt;/li&gt; 
 &lt;li&gt;Point-in-time accurate such that consistency with online serving is guaranteed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Observability, monitoring and data quality&lt;/h3&gt; 
&lt;p&gt;Chronon offers visibility into:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Data freshness - ensure that online values are being updated in realtime&lt;/li&gt; 
 &lt;li&gt;Online/Offline consistency - ensure that backfill data for model training and evaluation is consistent with what is being observed in online serving&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Complex transformations and windowed aggregations&lt;/h3&gt; 
&lt;p&gt;Chronon supports a range of aggregation types. For a full list see the documentation &lt;a href=&quot;https://chronon.ai/Aggregations.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;These aggregations can all be configured to be computed over arbitrary window sizes.&lt;/p&gt; 
&lt;h1&gt;Quickstart&lt;/h1&gt; 
&lt;p&gt;This section walks you through the steps to create a training dataset with Chronon, using a fabricated underlying raw dataset.&lt;/p&gt; 
&lt;p&gt;Includes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Example implementation of the main API components for defining features - &lt;code&gt;GroupBy&lt;/code&gt; and &lt;code&gt;Join&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;The workflow for authoring these entities.&lt;/li&gt; 
 &lt;li&gt;The workflow for backfilling training data.&lt;/li&gt; 
 &lt;li&gt;The workflows for uploading and serving this data.&lt;/li&gt; 
 &lt;li&gt;The workflow for measuring consistency between backfilled training data and online inference data.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Does not include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A deep dive on the various concepts and terminologies in Chronon. For that, please see the &lt;a href=&quot;https://chronon.ai/authoring_features/GroupBy.html&quot;&gt;Introductory&lt;/a&gt; documentation.&lt;/li&gt; 
 &lt;li&gt;Running streaming jobs.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Setup&lt;/h2&gt; 
&lt;p&gt;To get started with the Chronon, all you need to do is download the &lt;a href=&quot;https://github.com/airbnb/chronon/raw/main/docker-compose.yml&quot;&gt;docker-compose.yml&lt;/a&gt; file and run it locally:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -o docker-compose.yml https://chronon.ai/docker-compose.yml
docker-compose up
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Once you see some data printed with a &lt;code&gt;only showing top 20 rows&lt;/code&gt; notice, you&#39;re ready to proceed with the tutorial.&lt;/p&gt; 
&lt;h2&gt;Introduction&lt;/h2&gt; 
&lt;p&gt;In this example, let&#39;s assume that we&#39;re a large online retailer, and we&#39;ve detected a fraud vector based on users making purchases and later returning items. We want to train a model that will be called when the &lt;strong&gt;checkout&lt;/strong&gt; flow commences and predicts whether this transaction is likely to result in a fraudulent return.&lt;/p&gt; 
&lt;h2&gt;Raw data sources&lt;/h2&gt; 
&lt;p&gt;Fabricated raw data is included in the &lt;a href=&quot;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/data&quot;&gt;data&lt;/a&gt; directory. It includes four tables:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Users - includes basic information about users such as account created date; modeled as a batch data source that updates daily&lt;/li&gt; 
 &lt;li&gt;Purchases - a log of all purchases by users; modeled as a log table with a streaming (i.e. Kafka) event-bus counterpart&lt;/li&gt; 
 &lt;li&gt;Returns - a log of all returns made by users; modeled as a log table with a streaming (i.e. Kafka) event-bus counterpart&lt;/li&gt; 
 &lt;li&gt;Checkouts - a log of all checkout events; &lt;strong&gt;this is the event that drives our model predictions&lt;/strong&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Start a shell session in the Docker container&lt;/h3&gt; 
&lt;p&gt;In a new terminal window, run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;docker-compose exec main bash
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will open a shell within the chronon docker container.&lt;/p&gt; 
&lt;h2&gt;Chronon Development&lt;/h2&gt; 
&lt;p&gt;Now that the setup steps are complete, we can start creating and testing various Chronon objects to define transformation and aggregations, and generate data.&lt;/p&gt; 
&lt;h3&gt;Step 1 - Define some features&lt;/h3&gt; 
&lt;p&gt;Let&#39;s start with three feature sets, built on top of our raw input sources.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note: These python definitions are already in your &lt;code&gt;chronon&lt;/code&gt; image. There&#39;s nothing for you to run until &lt;a href=&quot;https://raw.githubusercontent.com/airbnb/chronon/main/#step-3---backfilling-data&quot;&gt;Step 3 - Backfilling Data&lt;/a&gt; when you&#39;ll run computation for these definitions.&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Feature set 1: Purchases data features&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We can aggregate the purchases log data to the user level, to give us a view into this user&#39;s previous activity on our platform. Specifically, we can compute &lt;code&gt;SUM&lt;/code&gt;s &lt;code&gt;COUNT&lt;/code&gt;s and &lt;code&gt;AVERAGE&lt;/code&gt;s of their previous purchase amounts over various windows.&lt;/p&gt; 
&lt;p&gt;Because this feature is built upon a source that includes both a table and a topic, its features can be computed in both batch and streaming.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;source = Source(
    events=EventSource(
        table=&quot;data.purchases&quot;, # This points to the log table with historical purchase events
        topic=None, # Streaming is not currently part of quickstart, but this would be where you define the topic for realtime events
        query=Query(
            selects=select(&quot;user_id&quot;,&quot;purchase_price&quot;), # Select the fields we care about
            time_column=&quot;ts&quot;) # The event time
    ))

window_sizes = [Window(length=day, timeUnit=TimeUnit.DAYS) for day in [3, 14, 30]] # Define some window sizes to use below

v1 = GroupBy(
    sources=[source],
    keys=[&quot;user_id&quot;], # We are aggregating by user
    aggregations=[Aggregation(
            input_column=&quot;purchase_price&quot;,
            operation=Operation.SUM,
            windows=window_sizes
        ), # The sum of purchases prices in various windows
        Aggregation(
            input_column=&quot;purchase_price&quot;,
            operation=Operation.COUNT,
            windows=window_sizes
        ), # The count of purchases in various windows
        Aggregation(
            input_column=&quot;purchase_price&quot;,
            operation=Operation.AVERAGE,
            windows=window_sizes
        ) # The average purchases by user in various windows
    ],
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the whole code file here: &lt;a href=&quot;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/group_bys/quickstart/purchases.py&quot;&gt;purchases GroupBy&lt;/a&gt;. This is also in your docker image. We&#39;ll be running computation for it and the other GroupBys in &lt;a href=&quot;https://raw.githubusercontent.com/airbnb/chronon/main/#step-3---backfilling-data&quot;&gt;Step 3 - Backfilling Data&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Feature set 2: Returns data features&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;We perform a similar set of aggregations on returns data in the &lt;a href=&quot;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/group_bys/quickstart/returns.py&quot;&gt;returns GroupBy&lt;/a&gt;. The code is not included here because it looks similar to the above example.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Feature set 3: User data features&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Turning User data into features is a littler simpler, primarily because there are no aggregations to include. In this case, the primary key of the source data is the same as the primary key of the feature, so we&#39;re simply extracting column values rather than performing aggregations over rows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;source = Source(
    entities=EntitySource(
        snapshotTable=&quot;data.users&quot;, # This points to a table that contains daily snapshots of the entire product catalog
        query=Query(
            selects=select(&quot;user_id&quot;,&quot;account_created_ds&quot;,&quot;email_verified&quot;), # Select the fields we care about
        )
    ))

v1 = GroupBy(
    sources=[source],
    keys=[&quot;user_id&quot;], # Primary key is the same as the primary key for the source table
    aggregations=None # In this case, there are no aggregations or windows to define
) 
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Taken from the &lt;a href=&quot;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/group_bys/quickstart/users.py&quot;&gt;users GroupBy&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Step 2 - Join the features together&lt;/h3&gt; 
&lt;p&gt;Next, we need the features that we previously defined backfilled in a single table for model training. This can be achieved using the &lt;code&gt;Join&lt;/code&gt; API.&lt;/p&gt; 
&lt;p&gt;For our use case, it&#39;s very important that features are computed as of the correct timestamp. Because our model runs when the checkout flow begins, we&#39;ll want to be sure to use the corresponding timestamp in our backfill, such that features values for model training logically match what the model will see in online inference.&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;Join&lt;/code&gt; is the API that drives feature backfills for training data. It primarilly performs the following functions:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Combines many features together into a wide view (hence the name &lt;code&gt;Join&lt;/code&gt;).&lt;/li&gt; 
 &lt;li&gt;Defines the primary keys and timestamps for which feature backfills should be performed. Chronon can then guarantee that feature values are correct as of this timestamp.&lt;/li&gt; 
 &lt;li&gt;Performs scalable backfills.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Here is what our join looks like:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;source = Source(
    events=EventSource(
        table=&quot;data.checkouts&quot;, 
        query=Query(
            selects=select(&quot;user_id&quot;), # The primary key used to join various GroupBys together
            time_column=&quot;ts&quot;,
            ) # The event time used to compute feature values as-of
    ))

v1 = Join(  
    left=source,
    right_parts=[JoinPart(group_by=group_by) for group_by in [purchases_v1, refunds_v1, users]] # Include the three GroupBys
)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Taken from the &lt;a href=&quot;https://github.com/airbnb/chronon/raw/main/api/py/test/sample/joins/quickstart/training_set.py&quot;&gt;training_set Join&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The &lt;code&gt;left&lt;/code&gt; side of the join is what defines the timestamps and primary keys for the backfill (notice that it is built on top of the &lt;code&gt;checkout&lt;/code&gt; event, as dictated by our use case).&lt;/p&gt; 
&lt;p&gt;Note that this &lt;code&gt;Join&lt;/code&gt; combines the above three &lt;code&gt;GroupBy&lt;/code&gt;s into one data definition. In the next step, we&#39;ll run the command to execute computation for this whole pipeline.&lt;/p&gt; 
&lt;h3&gt;Step 3 - Backfilling Data&lt;/h3&gt; 
&lt;p&gt;Once the join is defined, we compile it using this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;compile.py --conf=joins/quickstart/training_set.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This converts it into a thrift definition that we can submit to spark with the following command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;run.py --conf production/joins/quickstart/training_set.v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The output of the backfill would contain the user_id and ts columns from the left source, as well as the 11 feature columns from the three GroupBys that we created.&lt;/p&gt; 
&lt;p&gt;Feature values would be computed for each user_id and ts on the left side, with guaranteed temporal accuracy. So, for example, if one of the rows on the left was for &lt;code&gt;user_id = 123&lt;/code&gt; and &lt;code&gt;ts = 2023-10-01 10:11:23.195&lt;/code&gt;, then the &lt;code&gt;purchase_price_avg_30d&lt;/code&gt; feature would be computed for that user with a precise 30 day window ending on that timestamp.&lt;/p&gt; 
&lt;p&gt;You can now query the backfilled data using the spark sql shell:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;spark-sql
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;And then:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sql&quot;&gt;spark-sql&amp;gt; SELECT user_id, quickstart_returns_v1_refund_amt_sum_30d, quickstart_purchases_v1_purchase_price_sum_14d, quickstart_users_v1_email_verified from default.quickstart_training_set_v1 limit 100;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Note that this only selects a few columns. You can also run a &lt;code&gt;select * from default.quickstart_training_set_v1 limit 100&lt;/code&gt; to see all columns, however, note that the table is quite wide and the results might not be very readable on your screen.&lt;/p&gt; 
&lt;p&gt;To exit the sql shell you can run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;spark-sql&amp;gt; quit;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Online Flows&lt;/h2&gt; 
&lt;p&gt;Now that we&#39;ve created a join and backfilled data, the next step would be to train a model. That is not part of this tutorial, but assuming it was complete, the next step after that would be to productionize the model online. To do this, we need to be able to fetch feature vectors for model inference. That&#39;s what this next section covers.&lt;/p&gt; 
&lt;h3&gt;Uploading data&lt;/h3&gt; 
&lt;p&gt;In order to serve online flows, we first need the data uploaded to the online KV store. This is different than the backfill that we ran in the previous step in two ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;The data is not a historic backfill, but rather the most up-to-date feature values for each primary key.&lt;/li&gt; 
 &lt;li&gt;The datastore is a transactional KV store suitable for point lookups. We use MongoDB in the docker image, however you are free to integrate with a database of your choice.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Upload the purchases GroupBy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;run.py --mode upload --conf production/group_bys/quickstart/purchases.v1 --ds  2023-12-01

spark-submit --class ai.chronon.quickstart.online.Spark2MongoLoader --master local[*] /srv/onlineImpl/target/scala-2.12/mongo-online-impl-assembly-0.1.0-SNAPSHOT.jar default.quickstart_purchases_v1_upload mongodb://admin:admin@mongodb:27017/?authSource=admin
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Upload the returns GroupBy:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;run.py --mode upload --conf production/group_bys/quickstart/returns.v1 --ds  2023-12-01

spark-submit --class ai.chronon.quickstart.online.Spark2MongoLoader --master local[*] /srv/onlineImpl/target/scala-2.12/mongo-online-impl-assembly-0.1.0-SNAPSHOT.jar default.quickstart_returns_v1_upload mongodb://admin:admin@mongodb:27017/?authSource=admin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Upload Join Metadata&lt;/h3&gt; 
&lt;p&gt;If we want to use the &lt;code&gt;FetchJoin&lt;/code&gt; api rather than &lt;code&gt;FetchGroupby&lt;/code&gt;, then we also need to upload the join metadata:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;run.py --mode metadata-upload --conf production/joins/quickstart/training_set.v2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This makes it so that the online fetcher knows how to take a request for this join and break it up into individual GroupBy requests, returning the unified vector, similar to how the Join backfill produces the wide view table with all features.&lt;/p&gt; 
&lt;h3&gt;Fetching Data&lt;/h3&gt; 
&lt;p&gt;With the above entities defined, you can now easily fetch feature vectors with a simple API call.&lt;/p&gt; 
&lt;p&gt;Fetching a join:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;run.py --mode fetch --type join --name quickstart/training_set.v2 -k &#39;{&quot;user_id&quot;:&quot;5&quot;}&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also fetch a single GroupBy (this would not require the Join metadata upload step performed earlier):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;run.py --mode fetch --type group-by --name quickstart/purchases.v1 -k &#39;{&quot;user_id&quot;:&quot;5&quot;}&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For production, the Java client is usually embedded directly into services.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-Java&quot;&gt;Map&amp;lt;String, String&amp;gt; keyMap = new HashMap&amp;lt;&amp;gt;();
keyMap.put(&quot;user_id&quot;, &quot;123&quot;);
Fetcher.fetch_join(new Request(&quot;quickstart/training_set_v1&quot;, keyMap))
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;sample response&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;&amp;gt; &#39;{&quot;purchase_price_avg_3d&quot;:14.3241, &quot;purchase_price_avg_14d&quot;:11.89352, ...}&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note: This java code is not runnable in the docker env, it is just an illustrative example.&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Log fetches and measure online/offline consistency&lt;/h2&gt; 
&lt;p&gt;As discussed in the introductory sections of this &lt;a href=&quot;https://github.com/airbnb/chronon?tab=readme-ov-file#platform-features&quot;&gt;README&lt;/a&gt;, one of Chronon&#39;s core guarantees is online/offline consistency. This means that the data that you use to train your model (offline) matches the data that the model sees for production inference (online).&lt;/p&gt; 
&lt;p&gt;A key element of this is temporal accuracy. This can be phrased as: &lt;strong&gt;when backfilling features, the value that is produced for any given &lt;code&gt;timestamp&lt;/code&gt; provided by the left side of the join should be the same as what would have been returned online if that feature was fetched at that particular &lt;code&gt;timestamp&lt;/code&gt;&lt;/strong&gt;.&lt;/p&gt; 
&lt;p&gt;Chronon not only guarantees this temporal accuracy, but also offers a way to measure it.&lt;/p&gt; 
&lt;p&gt;The measurement pipeline starts with the logs of the online fetch requests. These logs include the primary keys and timestamp of the request, along with the fetched feature values. Chronon then passes the keys and timestamps to a Join backfill as the left side, asking the compute engine to backfill the feature values. It then compares the backfilled values to actual fetched values to measure consistency.&lt;/p&gt; 
&lt;p&gt;Step 1: log fetches&lt;/p&gt; 
&lt;p&gt;First, make sure you&#39;ve ran a few fetch requests. Run:&lt;/p&gt; 
&lt;p&gt;&lt;code&gt;run.py --mode fetch --type join --name quickstart/training_set.v2 -k &#39;{&quot;user_id&quot;:&quot;5&quot;}&#39;&lt;/code&gt;&lt;/p&gt; 
&lt;p&gt;A few times to generate some fetches.&lt;/p&gt; 
&lt;p&gt;With that complete, you can run this to create a usable log table (these commands produce a logging hive table with the correct schema):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;spark-submit --class ai.chronon.quickstart.online.MongoLoggingDumper --master local[*] /srv/onlineImpl/target/scala-2.12/mongo-online-impl-assembly-0.1.0-SNAPSHOT.jar default.chronon_log_table mongodb://admin:admin@mongodb:27017/?authSource=admin
compile.py --conf group_bys/quickstart/schema.py
run.py --mode backfill --conf production/group_bys/quickstart/schema.v1
run.py --mode log-flattener --conf production/joins/quickstart/training_set.v2 --log-table default.chronon_log_table --schema-table default.quickstart_schema_v1
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This creates a &lt;code&gt;default.quickstart_training_set_v2_logged&lt;/code&gt; table that contains the results of each of the fetch requests that you previously made, along with the timestamp at which you made them and the &lt;code&gt;user&lt;/code&gt; that you requested.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Once you run the above command, it will create and &quot;close&quot; the log partitions, meaning that if you make additional fetches on the same day (UTC time) it will not append. If you want to go back and generate more requests for online/offline consistency, you can drop the table (run &lt;code&gt;DROP TABLE default.quickstart_training_set_v2_logged&lt;/code&gt; in a &lt;code&gt;spark-sql&lt;/code&gt; shell) before rerunning the above command.&lt;/p&gt; 
&lt;p&gt;Now you can compute consistency metrics with this command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;run.py --mode consistency-metrics-compute --conf production/joins/quickstart/training_set.v2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This job will take the primary key(s) and timestamps from the log table (&lt;code&gt;default.quickstart_training_set_v2_logged&lt;/code&gt; in this case), and uses those to create and run a join backfill. It then compares the backfilled results to the actual logged values that were fetched online&lt;/p&gt; 
&lt;p&gt;It produces two output tables:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;code&gt;default.quickstart_training_set_v2_consistency&lt;/code&gt;: A human readable table that you can query to see the results of the consistency checks. 
  &lt;ol&gt; 
   &lt;li&gt;You can enter a sql shell by running &lt;code&gt;spark-sql&lt;/code&gt; from your docker bash sesion, then query the table.&lt;/li&gt; 
   &lt;li&gt;Note that it has many columns (multiple metrics per feature), so you might want to run a &lt;code&gt;DESC default.quickstart_training_set_v2_consistency&lt;/code&gt; first, then select a few columns that you care about to query.&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;default.quickstart_training_set_v2_consistency_upload&lt;/code&gt;: A list of KV bytes that is uploaded to the online KV store, that can be used to power online data quality monitoring flows. Not meant to be human readable.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Conclusion&lt;/h2&gt; 
&lt;p&gt;Using chronon for your feature engineering work simplifies and improves your ML Workflow in a number of ways:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;You can define features in one place, and use those definitions both for training data backfills and for online serving.&lt;/li&gt; 
 &lt;li&gt;Backfills are automatically point-in-time correct, which avoids label leakage and inconsistencies between training data and online inference.&lt;/li&gt; 
 &lt;li&gt;Orchestration for batch and streaming pipelines to keep features up to date is made simple.&lt;/li&gt; 
 &lt;li&gt;Chronon exposes easy endpoints for feature fetching.&lt;/li&gt; 
 &lt;li&gt;Consistency is guaranteed and measurable.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;For a more detailed view into the benefits of using Chronon, see &lt;a href=&quot;https://github.com/airbnb/chronon/tree/main?tab=readme-ov-file#benefits-of-chronon-over-other-approaches&quot;&gt;Benefits of Chronon documentation&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Benefits of Chronon over other approaches&lt;/h1&gt; 
&lt;p&gt;Chronon offers the most value to AI/ML practitioners who are trying to build &quot;online&quot; models that are serving requests in real-time as opposed to batch workflows.&lt;/p&gt; 
&lt;p&gt;Without Chronon, engineers working on these projects need to figure out how to get data to their models for training/eval as well as production inference. As the complexity of data going into these models increases (multiple sources, complex transformation such as windowed aggregations, etc), so does the infrastructure challenge of supporting this data plumbing.&lt;/p&gt; 
&lt;p&gt;Generally, we observed ML practitioners taking one of two approaches:&lt;/p&gt; 
&lt;h2&gt;The log-and-wait approach&lt;/h2&gt; 
&lt;p&gt;With this approach, users start with the data that is available in the online serving environment from which the model inference will run. Log relevant features to the data warehouse. Once enough data has accumulated, train the model on the logs, and serve with the same data.&lt;/p&gt; 
&lt;p&gt;Pros:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Features used to train the model are guaranteed to be available at serving time&lt;/li&gt; 
 &lt;li&gt;The model can access service call features&lt;/li&gt; 
 &lt;li&gt;The model can access data from the the request context&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Cons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;It might take a long to accumulate enough data to train the model&lt;/li&gt; 
 &lt;li&gt;Performing windowed aggregations is not always possible (running large range queries against production databases doesn&#39;t scale, same for event streams)&lt;/li&gt; 
 &lt;li&gt;Cannot utilize the wealth of data already in the data warehouse&lt;/li&gt; 
 &lt;li&gt;Maintaining data transformation logic in the application layer is messy&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;The replicate offline-online approach&lt;/h2&gt; 
&lt;p&gt;With this approach, users train the model with data from the data warehouse, then figure out ways to replicate those features in the online environment.&lt;/p&gt; 
&lt;p&gt;Pros:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;You can use a broad set of data for training&lt;/li&gt; 
 &lt;li&gt;The data warehouse is well suited for large aggregations and other computationally intensive transformation&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Cons:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Often very error prone, resulting in inconsistent data between training and serving&lt;/li&gt; 
 &lt;li&gt;Requires maintaining a lot of complicated infrastructure to even get started with this approach,&lt;/li&gt; 
 &lt;li&gt;Serving features with realtime updates gets even more complicated, especially with large windowed aggregations&lt;/li&gt; 
 &lt;li&gt;Unlikely to scale well to many models&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;The Chronon approach&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;With Chronon you can use any data available in your organization, including everything in the data warehouse, any streaming source, service calls, etc, with guaranteed consistency between online and offline environments. It abstracts away the infrastructure complexity of orchestrating and maintining this data plumbing, so that users can simply define features in a simple API, and trust Chronon to handle the rest.&lt;/p&gt; 
&lt;h1&gt;Contributing&lt;/h1&gt; 
&lt;p&gt;We welcome contributions to the Chronon project! Please read &lt;a href=&quot;https://raw.githubusercontent.com/airbnb/chronon/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING&lt;/a&gt; for details.&lt;/p&gt; 
&lt;h1&gt;Support&lt;/h1&gt; 
&lt;p&gt;Use the GitHub issue tracker for reporting bugs or feature requests. Join our &lt;a href=&quot;https://join.slack.com/t/chrononworkspace/shared_invite/zt-2r621b6hw-pm552u71Y257Vtpt4RTiyg&quot;&gt;community Slack workspace&lt;/a&gt; for discussions, tips, and support.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gitbucket/gitbucket</title>
      <link>https://github.com/gitbucket/gitbucket</link>
      <description>&lt;p&gt;A Git platform powered by Scala with easy installation, high extensibility &amp; GitHub API compatibility&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitBucket &lt;a href=&quot;https://gitter.im/gitbucket/gitbucket&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/gitbucket/gitbucket.svg?sanitize=true&quot; alt=&quot;Gitter chat&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gitbucket/gitbucket/actions/workflows/build.yml&quot;&gt;&lt;img src=&quot;https://github.com/gitbucket/gitbucket/actions/workflows/build.yml/badge.svg?sanitize=true&quot; alt=&quot;build&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://index.scala-lang.org/gitbucket/gitbucket/gitbucket&quot;&gt;&lt;img src=&quot;https://index.scala-lang.org/gitbucket/gitbucket/gitbucket/latest-by-scala-version.svg?sanitize=true&quot; alt=&quot;gitbucket Scala version support&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gitbucket/gitbucket/raw/master/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt;&lt;/h1&gt; 
&lt;p&gt;GitBucket is a Git web platform powered by Scala offering:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy installation&lt;/li&gt; 
 &lt;li&gt;Intuitive UI&lt;/li&gt; 
 &lt;li&gt;High extensibility by plugins&lt;/li&gt; 
 &lt;li&gt;API compatibility with GitHub&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;img src=&quot;https://gitbucket.github.io/img/screenshots/screenshot-repository_viewer.png&quot; alt=&quot;GitBucket&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;The current version of GitBucket provides many features such as:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Public / Private Git repositories (with http/https and ssh access)&lt;/li&gt; 
 &lt;li&gt;GitLFS support&lt;/li&gt; 
 &lt;li&gt;Repository viewer including an online file editor&lt;/li&gt; 
 &lt;li&gt;Issues, Pull Requests and Wiki for repositories&lt;/li&gt; 
 &lt;li&gt;Activity timeline and email notifications&lt;/li&gt; 
 &lt;li&gt;Account and group management with LDAP integration&lt;/li&gt; 
 &lt;li&gt;a Plug-in system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;GitBucket requires &lt;strong&gt;Java 17&lt;/strong&gt;. You have to install it, if it is not already installed.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Download the latest &lt;strong&gt;gitbucket.war&lt;/strong&gt; from &lt;a href=&quot;https://github.com/gitbucket/gitbucket/releases&quot;&gt;the releases page&lt;/a&gt; and run it by &lt;code&gt;java -jar gitbucket.war&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Go to &lt;code&gt;http://[hostname]:8080/&lt;/code&gt; and log in with ID: &lt;strong&gt;root&lt;/strong&gt; / Pass: &lt;strong&gt;root&lt;/strong&gt;.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You can also deploy &lt;code&gt;gitbucket.war&lt;/code&gt; to a servlet container which supports Servlet 3.0 (like Jetty, Tomcat, JBoss, etc). Note that GitBucket doesn&#39;t support Jakarta EE yet.&lt;/p&gt; 
&lt;p&gt;For more information about installation on Mac or Windows Server (with IIS), or configuration of Apache or Nginx and also integration with other tools or services such as Jenkins or Slack, see &lt;a href=&quot;https://github.com/gitbucket/gitbucket/wiki&quot;&gt;Wiki&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;To upgrade GitBucket, replace &lt;code&gt;gitbucket.war&lt;/code&gt; with the new version, after stopping GitBucket. All GitBucket data is stored in &lt;code&gt;HOME/.gitbucket&lt;/code&gt; by default. So if you want to back up GitBucket&#39;s data, copy this directory to the backup location.&lt;/p&gt; 
&lt;h2&gt;Plugins&lt;/h2&gt; 
&lt;p&gt;GitBucket has a plug-in system that allows extra functionality. Officially the following plug-ins are provided:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gitbucket/gitbucket-gist-plugin&quot;&gt;gitbucket-gist-plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gitbucket/gitbucket-emoji-plugin&quot;&gt;gitbucket-emoji-plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gitbucket/gitbucket-pages-plugin&quot;&gt;gitbucket-pages-plugin&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/gitbucket/gitbucket-notifications-plugin&quot;&gt;gitbucket-notifications-plugin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;You can find more plugins made by the community at &lt;a href=&quot;https://gitbucket-plugins.github.io/&quot;&gt;GitBucket community plugins&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building and Development&lt;/h2&gt; 
&lt;p&gt;If you want to try the development version of GitBucket, or want to contribute to the project, please see the &lt;a href=&quot;https://github.com/gitbucket/gitbucket/raw/master/doc/readme.md&quot;&gt;Developer&#39;s Guide&lt;/a&gt;. It provides instructions on building from source and on setting up an IDE for debugging. It also contains documentation of the core concepts used within the project.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;If you have any questions about GitBucket, see &lt;a href=&quot;https://github.com/gitbucket/gitbucket/wiki&quot;&gt;Wiki&lt;/a&gt; and check issues whether there is a same question or request in the past.&lt;/li&gt; 
 &lt;li&gt;If you can&#39;t find same question and report, send it to our &lt;a href=&quot;https://gitter.im/gitbucket/gitbucket&quot;&gt;Gitter chat room&lt;/a&gt; before raising an issue.&lt;/li&gt; 
 &lt;li&gt;The highest priority of GitBucket is the ease of installation and API compatibility with GitHub, so your feature request might be rejected if they go against those principles.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What&#39;s New in 4.44.x&lt;/h2&gt; 
&lt;h2&gt;4.44.0 - 23 Sep 2025&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Enhanced branch protection which supports the following settings: 
  &lt;ul&gt; 
   &lt;li&gt;Prevent pushes from non-allowed users&lt;/li&gt; 
   &lt;li&gt;Whether to apply restrictions to administrator users as well&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improve logging for initialization errors&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Note that you have to migrate h2 database file if you will upgrade GitBucket from 4.42 or before to 4.43 or later and you are using the default h2 database because h2 1.x and h2.x don&#39;t have compatibility: &lt;a href=&quot;https://www.h2database.com/html/migration-to-v2.html&quot;&gt;https://www.h2database.com/html/migration-to-v2.html&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;It can&#39;t be done automatically using GitBucket&#39;s auto migration mechanism because it relies on database itself. So, users who use h2 will have to dump and recreate their database manually with the following steps:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Export database using the current version of H2
$ curl -O https://repo1.maven.org/maven2/com/h2database/h2/1.4.199/h2-1.4.199.jar
$ java -cp h2-1.4.199.jar org.h2.tools.Script -url &quot;jdbc:h2:~/.gitbucket/data&quot; -user sa -password sa -script dump.sql

# Recreate database using the new version of H2
$ curl -O https://repo1.maven.org/maven2/com/h2database/h2/2.3.232/h2-2.3.232.jar
$ java -cp h2-2.3.232.jar org.h2.tools.RunScript -url &quot;jdbc:h2:~/.gitbucket/data&quot; -user sa -password sa -script dump.sql
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition, if &lt;code&gt;~/.gitbucket/database.conf&lt;/code&gt; has the following configuration, remove &lt;code&gt;;MVCC=true&lt;/code&gt; from &lt;code&gt;url&lt;/code&gt;.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;db {
  url = &quot;jdbc:h2:${DatabaseHome};MVCC=true&quot; // =&amp;gt; &quot;jdbc:h2:${DatabaseHome}&quot;
  ...
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/gitbucket/gitbucket/master/CHANGELOG.md&quot;&gt;change log&lt;/a&gt; for all the past updates.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>databricks/Spark-The-Definitive-Guide</title>
      <link>https://github.com/databricks/Spark-The-Definitive-Guide</link>
      <description>&lt;p&gt;Spark: The Definitive Guide&#39;s Code Repository&lt;/p&gt;&lt;hr&gt;</description>
    </item>
    
  </channel>
</rss>
