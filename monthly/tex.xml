<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub TeX Monthly Trending</title>
    <description>Monthly Trending of TeX in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:59:52 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>dnl-blkv/mcdowell-cv</title>
      <link>https://github.com/dnl-blkv/mcdowell-cv</link>
      <description>&lt;p&gt;A Nice-looking CV template made into LaTeX&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;McDowell CV&lt;/h1&gt; 
&lt;p&gt;McDowell CV is a LuaLaTeX class for building neat and space-efficient CVs using the design originally proposed by Gayle L. McDowell at &lt;a href=&quot;http://www.careercup.com/resume&quot;&gt;http://www.careercup.com/resume&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The class is based on &lt;code&gt;article&lt;/code&gt; class. The paper format is set to U.S. letterpaper by default. A template showing an example usage of the class is included.&lt;/p&gt; 
&lt;h2&gt;Screenshot&lt;/h2&gt; 
&lt;img src=&quot;https://github.com/dnl-blkv/mcdowell-cv/raw/master/McDowell_CV.png&quot; width=&quot;240px&quot; /&gt; 
&lt;h2&gt;Use Cases&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;A great tool making it easy to build CVs: &lt;a href=&quot;https://latexresu.me/&quot;&gt;https://latexresu.me/&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Class Options&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;calibri&lt;/code&gt; - sets calibri as the main font. Otherwise the default font is Times New Roman since version 1.1.0.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Commands&lt;/h2&gt; 
&lt;p&gt;The class features the following commands:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;\name{name}&lt;/code&gt; - defines the applicant&#39;s name to be printed by &lt;code&gt;\printheader&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;\address{address}&lt;/code&gt; - defines the applicant&#39;s address to be printed by &lt;code&gt;\printheader&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;\contacts{contacts}&lt;/code&gt; - defines the applicant&#39;s contacts to be printed by &lt;code&gt;\printheader&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;\makecvheader&lt;/code&gt; - prints the CV header consisting of the name (see the &lt;code&gt;\name&lt;/code&gt; command), address (see the &lt;code&gt;\address&lt;/code&gt; command) and contacts (see the &lt;code&gt;\contacts&lt;/code&gt; command).&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Environments&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;\begin{cvsection}{sectionname}&lt;/code&gt; - prints a section with a header consisting of the name in bold small caps and a page-wide horizontal line below.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;\begin{cvsubsection}[linesnum]{left}{center}{right}{content}&lt;/code&gt; - prints a subsection with header consisting of the &lt;code&gt;left&lt;/code&gt;, &lt;code&gt;center&lt;/code&gt; and &lt;code&gt;right&lt;/code&gt; titles. The optional &lt;code&gt;linesnum&lt;/code&gt; argument defines the amount of lines in the header. The argument only affects the vertical spacing between the environment header and content thus eliminating the effect of &lt;em&gt;tabu&lt;/em&gt; package vertical spacing bug.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Build Instructions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Make sure &lt;code&gt;lualatex&lt;/code&gt; (see &lt;a href=&quot;https://www.luatex.org/download.html&quot;&gt;https://www.luatex.org/download.html&lt;/a&gt;) is installed on your machine and is available in the terminal or a command line client of your choice.&lt;/li&gt; 
 &lt;li&gt;In the terminal or a command line client of your choice, go to the folder containing &lt;code&gt;McDowell_CV_Template.tex&lt;/code&gt; and &lt;code&gt;mcdowellcv.cls&lt;/code&gt;, and run the following command: &lt;code&gt;lualatex McDowell_CV_Template.tex&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>posquit0/Awesome-CV</title>
      <link>https://github.com/posquit0/Awesome-CV</link>
      <description>&lt;p&gt;üìÑ Awesome CV is LaTeX template for your outstanding job application&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/posquit0/Awesome-CV&quot; title=&quot;AwesomeCV Documentation&quot;&gt; &lt;img alt=&quot;AwesomeCV&quot; src=&quot;https://github.com/posquit0/Awesome-CV/raw/master/icon.png&quot; width=&quot;200px&quot; height=&quot;200px&quot; /&gt; &lt;/a&gt; &lt;br /&gt; Awesome CV &lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; LaTeX template for your outstanding job application &lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;a href=&quot;https://www.paypal.me/posquit0&quot;&gt; &lt;img alt=&quot;Donate&quot; src=&quot;https://img.shields.io/badge/Donate-PayPal-blue.svg?sanitize=true&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://github.com/posquit0/Awesome-CV/actions/workflows/main.yml&quot;&gt; &lt;img alt=&quot;GitHub Actions&quot; src=&quot;https://github.com/posquit0/Awesome-CV/actions/workflows/main.yml/badge.svg?sanitize=true&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&quot;&gt; &lt;img alt=&quot;Example Resume&quot; src=&quot;https://img.shields.io/badge/resume-pdf-green.svg?sanitize=true&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/cv.pdf&quot;&gt; &lt;img alt=&quot;Example CV&quot; src=&quot;https://img.shields.io/badge/cv-pdf-green.svg?sanitize=true&quot; /&gt; &lt;/a&gt; 
 &lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&quot;&gt; &lt;img alt=&quot;Example Coverletter&quot; src=&quot;https://img.shields.io/badge/coverletter-pdf-green.svg?sanitize=true&quot; /&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;br /&gt; 
&lt;h2&gt;What is Awesome CV?&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Awesome CV&lt;/strong&gt; is LaTeX template for a &lt;strong&gt;CV(Curriculum Vitae)&lt;/strong&gt;, &lt;strong&gt;R√©sum√©&lt;/strong&gt; or &lt;strong&gt;Cover Letter&lt;/strong&gt; inspired by &lt;a href=&quot;https://www.sharelatex.com/templates/cv-or-resume/fancy-cv&quot;&gt;Fancy CV&lt;/a&gt;. It is easy to customize your own template, especially since it is really written by a clean, semantic markup.&lt;/p&gt; 
&lt;h2&gt;Donate&lt;/h2&gt; 
&lt;p&gt;Please help keep this project alive! Donations are welcome and will go towards further development of this project.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;PayPal: paypal.me/posquit0
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;em&gt;Thank you for your support!&lt;/em&gt;&lt;/p&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;h4&gt;R√©sum√©&lt;/h4&gt; 
&lt;p&gt;You can see &lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Page. 1&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Page. 2&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-0.png&quot; alt=&quot;R√©sum√©&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume.pdf&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/resume-1.png&quot; alt=&quot;R√©sum√©&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h4&gt;Cover Letter&lt;/h4&gt; 
&lt;p&gt;You can see &lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Without Sections&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;With Sections&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-0.png&quot; alt=&quot;Cover Letter(Traditional)&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter.pdf&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/posquit0/Awesome-CV/master/examples/coverletter-1.png&quot; alt=&quot;Cover Letter(Awesome)&quot; /&gt;&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.overleaf.com/latex/templates/awesome-cv/tvmzpvdjfqxp&quot;&gt;&lt;strong&gt;Edit R√©sum√© on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.overleaf.com/latex/templates/awesome-cv-cover-letter/pfzzjspkthbk&quot;&gt;&lt;strong&gt;Edit Cover Letter on OverLeaf.com&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note:&lt;/em&gt; Above services do not guarantee up-to-date source code of Awesome CV&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;How to Use&lt;/h2&gt; 
&lt;h4&gt;Requirements&lt;/h4&gt; 
&lt;p&gt;A full TeX distribution is assumed. &lt;a href=&quot;http://tex.stackexchange.com/q/55437&quot;&gt;Various distributions for different operating systems (Windows, Mac, *nix) are available&lt;/a&gt; but TeX Live is recommended. You can &lt;a href=&quot;https://tex.stackexchange.com/q/1092&quot;&gt;install TeX from upstream&lt;/a&gt; (recommended; most up-to-date) or use &lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt; if you really want that. (It&#39;s generally a few years behind.)&lt;/p&gt; 
&lt;p&gt;If you don&#39;t want to install the dependencies on your system, this can also be obtained via &lt;a href=&quot;https://docker.com&quot;&gt;Docker&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;Usage&lt;/h4&gt; 
&lt;p&gt;At a command prompt, run&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;xelatex {your-cv}.tex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Or using docker:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm --user $(id -u):$(id -g) -i -w &quot;/doc&quot; -v &quot;$PWD&quot;:/doc texlive/texlive:latest make
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In either case, this should result in the creation of &lt;code&gt;{your-cv}.pdf&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Credit&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.latex-project.org&quot;&gt;&lt;strong&gt;LaTeX&lt;/strong&gt;&lt;/a&gt; is a fantastic typesetting program that a lot of people use these days, especially the math and computer science people in academia.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/braniii/fontawesome&quot;&gt;&lt;strong&gt;FontAwesome6 LaTeX Package&lt;/strong&gt;&lt;/a&gt; is a LaTeX package that provides access to the &lt;a href=&quot;https://fontawesome.com/v6/icons&quot;&gt;Font Awesome 6&lt;/a&gt; icon set.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/google/roboto&quot;&gt;&lt;strong&gt;Roboto&lt;/strong&gt;&lt;/a&gt; is the default font on Android and ChromeOS, and the recommended font for Google‚Äôs visual language, Material Design.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/adobe-fonts/source-sans-pro&quot;&gt;&lt;strong&gt;Source Sans Pro&lt;/strong&gt;&lt;/a&gt; is a set of OpenType fonts that have been designed to work well in user interface (UI) environments.&lt;/p&gt; 
&lt;h2&gt;Contact&lt;/h2&gt; 
&lt;p&gt;You are free to take my &lt;code&gt;.tex&lt;/code&gt; file and modify it to create your own resume. Please don&#39;t use my resume for anything else without my permission, though!&lt;/p&gt; 
&lt;p&gt;If you have any questions, feel free to join me at &lt;a href=&quot;irc://irc.freenode.net/posquit0&quot;&gt;&lt;code&gt;#posquit0&lt;/code&gt; on Freenode&lt;/a&gt; and ask away. Click &lt;a href=&quot;https://kiwiirc.com/client/irc.freenode.net/posquit0&quot;&gt;here&lt;/a&gt; to connect.&lt;/p&gt; 
&lt;p&gt;Good luck!&lt;/p&gt; 
&lt;h2&gt;Maintainers&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/posquit0&quot;&gt;posquit0&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/OJFord&quot;&gt;OJFord&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;See Also&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/posquit0/hugo-awesome-identity&quot;&gt;Awesome Identity&lt;/a&gt; - A single-page Hugo theme to introduce yourself.&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>rstudio/cheatsheets</title>
      <link>https://github.com/rstudio/cheatsheets</link>
      <description>&lt;p&gt;Posit Cheat Sheets - Can also be found at https://posit.co/resources/cheatsheets/.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Posit Cheatsheets&lt;/h2&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/rstudio/cheatsheets/main/pngs/rstudio-ide.png&quot; width=&quot;364&quot; height=&quot;288&quot; align=&quot;right&quot; /&gt; 
&lt;p&gt;The cheatsheets make it easy to learn about and use some of our favorite packages. They are published in their respective PDF versions here: &lt;a href=&quot;https://posit.co/resources/cheatsheets/&quot;&gt;https://posit.co/resources/cheatsheets/&lt;/a&gt;, some are also available in the RStudio IDE under Help &amp;gt; Cheatsheets.&lt;/p&gt; 
&lt;p&gt;We are also starting to make some cheatsheets available in a more accessible, text-based HTML format. These are available at &lt;a href=&quot;https://rstudio.github.io/cheatsheets/&quot;&gt;https://rstudio.github.io/cheatsheets/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;This repository contains the source files of the current, archived and translated versions.&lt;/p&gt; 
&lt;p&gt;The cheatsheets use the creative commons copyright. Please see the LICENSE document for more details.&lt;/p&gt; 
&lt;h2&gt;Translations&lt;/h2&gt; 
&lt;p&gt;If you wish to contribute to this effort by translating a cheatsheet, please feel free to use the source Keynote file. To submit a translation, please use a Pull Request via GitHub. See the &lt;a href=&quot;https://github.com/rstudio/cheatsheets/raw/main/.github/CONTRIBUTING.md&quot;&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;HTML cheatsheets&lt;/h2&gt; 
&lt;p&gt;If you wish to provide an HTML cheatsheet version, please create a Pull Request with a new &lt;code&gt;.qmd&lt;/code&gt; file in the &lt;code&gt;html/&lt;/code&gt; directory of this repository. Use one of the existing &lt;code&gt;qmd&lt;/code&gt; files there as a starting point/template. These should not be duplicates of the pdf versions - they should be text-based so they are more accessible to people with visual impairments. Use of images should be minimized, and any images should include appropriate alternative text.&lt;/p&gt; 
&lt;h2&gt;Tips for making a new cheatsheet&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Cheatsheets are not meant to be text or documentation!&lt;/strong&gt; They are scannable visual aids that use layout and visual mnemonics to help people zoom to the functions they need. Think of cheatsheets as a quick reference, with the emphasis on quick. Here&#39;s an analogy:&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;A cheatsheet is more like a well-organized computer menu bar that leads you to a command than like a manual that documents each command.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;Everything about your cheatsheet should be designed to lead users to essential information &lt;em&gt;quickly&lt;/em&gt;. If you are summarizing the documentation manual, you are doing it wrong! Here are some tips to help you do it right:&lt;/p&gt; 
&lt;h3&gt;Getting Started&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;RStudio cheatsheets are hosted at &lt;a href=&quot;https://github.com/rstudio/cheatsheets&quot;&gt;https://github.com/rstudio/cheatsheets&lt;/a&gt;. You can submit new cheatsheets to the repository with a pull request. See the &lt;a href=&quot;https://github.com/rstudio/cheatsheets/raw/main/.github/CONTRIBUTING.md&quot;&gt;contributing guidelines&lt;/a&gt; for more information.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The files &lt;a href=&quot;https://github.com/rstudio/cheatsheets/raw/main/keynotes/0-template.key&quot;&gt;keynotes/0-template.key&lt;/a&gt; and &lt;a href=&quot;https://github.com/rstudio/cheatsheets/raw/main/powerpoints/0-template.pptx&quot;&gt;powerpoints/0-template.ppt&lt;/a&gt; are official templates that contain some helpful tips.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You may find it easiest to create a new cheatsheet by duplicating the most recent Keynote / Powerpoint cheatsheet and then heavily editing it‚Äîthat&#39;s what I do!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Process&lt;/h3&gt; 
&lt;p&gt;Budget more time than you expect to make the sheets. So far, I&#39;ve found this process to be the least time consuming:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Identify which functions to include&lt;/strong&gt; by reading the package web page and vignettes. I try to limit my cheatsheets to the essentials.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Organize the functions&lt;/strong&gt; into meaningful, self-explanatory groups. Each group should address a common problem or task.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Think about how to visualize the purpose of each function.&lt;/strong&gt; Visual mnemonics are easier to scan than text, which all looks the same.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Think about&lt;/strong&gt; what &lt;strong&gt;key mental models&lt;/strong&gt;, definitions, or explanations the cheatsheet should contain in addition to the functions. Ideally, use these to explain the visualizations.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sketch out several possible layouts&lt;/strong&gt; for the sheet. Take care to put the more basic and/or pre-requisite content above and to the left of other content. Try to keep related content on the same side of the page. often your final layout will itself be a &quot;mental map&quot; for the topic of the cheatsheet.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Type out all of the explanations and function descriptions&lt;/strong&gt; that you plan to include. Lay them out. Use placeholders for the visuals. Verify that everything fits. White space is very important. Use it to make the sheet scannable and to isolate content groups. Retain white space, even if it means smaller text.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make the visuals.&lt;/strong&gt; They take the longest, so I save them for last or make them as I do step 6.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tweak until happy.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Visual Design&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use the existing theme&lt;/strong&gt; that you see in the cheatsheets. It is cohesive and black and white printer friendly.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Choose a highlight color&lt;/strong&gt; to use throughout your cheatsheet, and repeat this highlight color in the background of the top right corner. Ideally you could find a color that is different enough from the other cheatsheets that you can quickly tell yours apart when flipping through a booklet of cheatsheets.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Use a second color sparingly or not at all&lt;/strong&gt; to draw attention to where it is needed and to differentiate different groupings of content.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Include lots of white space.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visually differentiate groups of content.&lt;/strong&gt; Backgrounds, boxes, side bars, and headers are helpful here. It is very useful for the user to know immediately where one group of content begins and where one ends. Our &quot;gradation headers&quot; fail here, so think of better solutions if possible.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Align things&lt;/strong&gt; to guides, i.e. align things across the page. It helps define the white space and makes the cheat more orderly and professional.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make the text no smaller than ~10pt.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;If the letters are white on a colored background&lt;/strong&gt;, make the font thicker - semibold or bold.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Save bold text&lt;/strong&gt; for simple, important statements, or to draw scanning eyes to important words, such as words that identify the topic discussed. Don&#39;t make an entire paragraph bold text.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Content&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Include a hex sticker, IDE screenshot, or other branding material&lt;/strong&gt;. The cheatsheets have a second function as marketing material.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Include a &lt;a href=&quot;https://creativecommons.org/&quot;&gt;Creative Commons Copyright&lt;/a&gt;&lt;/strong&gt; to make the sheet easy to share. You&#39;ll find one baked into every cheatsheet and the template.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Be very concise&lt;/strong&gt; - rely on diagrams where possible.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pay attention to the details!&lt;/strong&gt; Your readers sure will... so be correct.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;If in doubt, leave it out.&lt;/strong&gt; There is a documentation manual after all.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Code comments inform, but fail&lt;/strong&gt; to draw the readers attention. It is better to use arrows, speech bubbles, etc. for important information. If it is not important information, leave it out.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple working examples are more helpful than documentation details.&lt;/strong&gt; They meet the user at his or her pain points, demonstrating code, and reminding users how to run it, with the least context shifting.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Add some concise text to &lt;strong&gt;help the user make sense of your sections and diagrams&lt;/strong&gt;. Images are best, but readers need to be able to interpret them.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Summary&lt;/h3&gt; 
&lt;p&gt;Your cheatsheet has two goals. First, to help users find essential information quickly, and second, to prevent confusion while doing the above. Your best strategy will be to limit the amount of information you put into the cheatsheet and to lay that information out intuitively and visually. This approach will make your cheatsheet equally useful as a teaching tool, programming tool, or marketing tool.&lt;/p&gt; 
&lt;p&gt;Cheatsheets fall squarely on the &lt;em&gt;human-facing side of software design&lt;/em&gt;. They focus on human attention. What does that mean? When you write documentation, your job is to fill in all of the relevant details‚Äîthat&#39;s a software facing job, you need to know the software to do it. You assume that interested humans will find their way to your details on their own (and understand them when they do!). When you make a cheatsheet, your job flips. You assume that the relevant details already exist in the documentation. Your job is to help interested humans find them and understand them. Your job is to guide the human&#39;s attention. Don&#39;t just write, design.&lt;/p&gt; 
&lt;h2&gt;Website&lt;/h2&gt; 
&lt;p&gt;This repo is deployed as a quarto website at &lt;a href=&quot;https://rstudio.github.io/cheatsheets/&quot;&gt;https://rstudio.github.io/cheatsheets/&lt;/a&gt;. It uses &lt;a href=&quot;https://rstudio.github.io/renv/&quot;&gt;renv&lt;/a&gt; to manage the dependencies to render the site (in particular the &lt;code&gt;html/*.qmd&lt;/code&gt; files that generate the HTML cheatsheets). Packages that are required to render these cheatsheets should be list in &lt;code&gt;DESCRIPTION&lt;/code&gt; so that they are reliably discovered by &lt;code&gt;renv::snapshot()&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;We prefer the Quarto cheatsheets to set &lt;code&gt;eval: true&lt;/code&gt; and &lt;code&gt;output: false&lt;/code&gt; in the &lt;code&gt;execute&lt;/code&gt; options (vs &lt;code&gt;eval: false&lt;/code&gt;) as this helps to ensure the code in them still works when they are rerun. Exceptions can be made on a per-chunk basis, and some (e.g., keras) are not really feasible to run all the time due to complex installation.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HarisIqbal88/PlotNeuralNet</title>
      <link>https://github.com/HarisIqbal88/PlotNeuralNet</link>
      <description>&lt;p&gt;Latex code for making neural networks diagrams&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PlotNeuralNet&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://doi.org/10.5281/zenodo.2526396&quot;&gt;&lt;img src=&quot;https://zenodo.org/badge/DOI/10.5281/zenodo.2526396.svg?sanitize=true&quot; alt=&quot;DOI&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Latex code for drawing neural networks for reports and presentation. Have a look into examples to see how they are made. Additionally, lets consolidate any improvements that you make and fix any bugs to help more people with this code.&lt;/p&gt; 
&lt;h2&gt;Examples&lt;/h2&gt; 
&lt;p&gt;Following are some network representations:&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/17570785/50308846-c2231880-049c-11e9-8763-3daa1024de78.png&quot; width=&quot;85%&quot; height=&quot;85%&quot; /&gt;&lt;/p&gt; 
&lt;h6 align=&quot;center&quot;&gt;FCN-8 (&lt;a href=&quot;https://www.overleaf.com/read/kkqntfxnvbsk&quot;&gt;view on Overleaf&lt;/a&gt;)&lt;/h6&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/17570785/50308873-e2eb6e00-049c-11e9-9587-9da6bdec011b.png&quot; width=&quot;85%&quot; height=&quot;85%&quot; /&gt;&lt;/p&gt; 
&lt;h6 align=&quot;center&quot;&gt;FCN-32 (&lt;a href=&quot;https://www.overleaf.com/read/wsxpmkqvjnbs&quot;&gt;view on Overleaf&lt;/a&gt;)&lt;/h6&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/17570785/50308911-03b3c380-049d-11e9-92d9-ce15669017ad.png&quot; width=&quot;85%&quot; height=&quot;85%&quot; /&gt;&lt;/p&gt; 
&lt;h6 align=&quot;center&quot;&gt;Holistically-Nested Edge Detection (&lt;a href=&quot;https://www.overleaf.com/read/jxhnkcnwhfxp&quot;&gt;view on Overleaf&lt;/a&gt;)&lt;/h6&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Install the following packages on Ubuntu.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Ubuntu 16.04&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ubuntu 18.04.2 Base on this &lt;a href=&quot;https://gist.github.com/rain1024/98dd5e2c6c8c28f9ea9d&quot;&gt;website&lt;/a&gt;, please install the following packages.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt-get install texlive-latex-base
sudo apt-get install texlive-fonts-recommended
sudo apt-get install texlive-fonts-extra
sudo apt-get install texlive-latex-extra
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; 
  &lt;ol&gt; 
   &lt;li&gt;Download and install &lt;a href=&quot;https://miktex.org/download&quot;&gt;MikTeX&lt;/a&gt;.&lt;/li&gt; 
   &lt;li&gt;Download and install bash runner on Windows, recommends &lt;a href=&quot;https://git-scm.com/download/win&quot;&gt;Git bash&lt;/a&gt; or Cygwin(&lt;a href=&quot;https://www.cygwin.com/&quot;&gt;https://www.cygwin.com/&lt;/a&gt;)&lt;/li&gt; 
  &lt;/ol&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Execute the example as followed.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd pyexamples/
bash ../tikzmake.sh test_simple
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Python interface&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; Add easy legend functionality&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; Add more layer shapes like TruncatedPyramid, 2DSheet etc&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; Add examples for RNN and likes.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Latex usage&lt;/h2&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/HarisIqbal88/PlotNeuralNet/master/examples&quot;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory for usage.&lt;/p&gt; 
&lt;h2&gt;Python usage&lt;/h2&gt; 
&lt;p&gt;First, create a new directory and a new Python file:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ mkdir my_project
$ cd my_project
vim my_arch.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Add the following code to your new file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import sys
sys.path.append(&#39;../&#39;)
from pycore.tikzeng import *

# defined your arch
arch = [
    to_head( &#39;..&#39; ),
    to_cor(),
    to_begin(),
    to_Conv(&quot;conv1&quot;, 512, 64, offset=&quot;(0,0,0)&quot;, to=&quot;(0,0,0)&quot;, height=64, depth=64, width=2 ),
    to_Pool(&quot;pool1&quot;, offset=&quot;(0,0,0)&quot;, to=&quot;(conv1-east)&quot;),
    to_Conv(&quot;conv2&quot;, 128, 64, offset=&quot;(1,0,0)&quot;, to=&quot;(pool1-east)&quot;, height=32, depth=32, width=2 ),
    to_connection( &quot;pool1&quot;, &quot;conv2&quot;),
    to_Pool(&quot;pool2&quot;, offset=&quot;(0,0,0)&quot;, to=&quot;(conv2-east)&quot;, height=28, depth=28, width=1),
    to_SoftMax(&quot;soft1&quot;, 10 ,&quot;(3,0,0)&quot;, &quot;(pool1-east)&quot;, caption=&quot;SOFT&quot;  ),
    to_connection(&quot;pool2&quot;, &quot;soft1&quot;),
    to_end()
    ]

def main():
    namefile = str(sys.argv[0]).split(&#39;.&#39;)[0]
    to_generate(arch, namefile + &#39;.tex&#39; )

if __name__ == &#39;__main__&#39;:
    main()
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now, run the program as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;bash ../tikzmake.sh my_arch
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>riscv/riscv-isa-manual</title>
      <link>https://github.com/riscv/riscv-isa-manual</link>
      <description>&lt;p&gt;RISC-V Instruction Set Manual&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RISC-V Instruction Set Manual&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/riscv/riscv-isa-manual/actions/workflows/isa-build.yml&quot;&gt;&lt;img src=&quot;https://github.com/riscv/riscv-isa-manual/actions/workflows/isa-build.yml/badge.svg?sanitize=true&quot; alt=&quot;RISC-V ISA Build&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;This repository contains the source files for the RISC-V Instruction Set Manual, which consists of the Privileged RISC-V Instruction Set Manual (LaTeX) and the Unprivileged RISC-V Instruction Set Manual (AsciiDoc). The preface of each document indicates the version of each standard that has been formally ratified by RISC-V International.&lt;/p&gt; 
&lt;p&gt;This work is licensed under a &lt;a href=&quot;https://creativecommons.org/licenses/by/4.0/&quot;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;. See the &lt;a href=&quot;https://raw.githubusercontent.com/riscv/riscv-isa-manual/main/LICENSE&quot;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; 
&lt;p&gt;The RISC-V Instruction Set Manual is organized into the following volumes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Volume I: User-Level ISA&lt;/li&gt; 
 &lt;li&gt;Volume II: Privileged Architecture&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Official and Draft Versions&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Official versions&lt;/strong&gt; of the specifications are available at the &lt;a href=&quot;https://riscv.org/specifications/&quot;&gt;RISC-V International website&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Compiled versions of the most recent drafts&lt;/strong&gt; of the specifications can be found on the &lt;a href=&quot;https://github.com/riscv/riscv-isa-manual/releases/latest&quot;&gt;GitHub releases page&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTML snapshots of the latest commit&lt;/strong&gt; can be viewed at the following locations: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://riscv.github.io/riscv-isa-manual/snapshot/unprivileged/&quot;&gt;Unprivileged spec&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://riscv.github.io/riscv-isa-manual/snapshot/privileged/&quot;&gt;Privileged spec&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Older official versions&lt;/strong&gt; of the specifications are archived at the &lt;a href=&quot;https://github.com/riscv/riscv-isa-manual/releases/tag/archive&quot;&gt;GitHub releases archive&lt;/a&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The canonical list of &lt;strong&gt;open-source RISC-V implementations&#39; marchid CSR values&lt;/strong&gt; is available in the &lt;a href=&quot;https://github.com/riscv/riscv-isa-manual/raw/main/marchid.md&quot;&gt;marchid.md file&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;If you would like to contribute to this documentation, please refer to the &lt;a href=&quot;https://github.com/riscv/docs-dev-guide&quot;&gt;Documentation Developer&#39;s Guide&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;The recommended method for building the PDF files is to use the Docker Image, as described in the &lt;a href=&quot;https://github.com/riscv/riscv-docs-base-container-image&quot;&gt;RISC-V Docs Base Container Image repository&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Alternative build methods, such as local builds and GitHub Action builds, are also available and described in the Documentation Developer&#39;s Guide.&lt;/p&gt; 
&lt;h2&gt;Images not rendered for EPUB files&lt;/h2&gt; 
&lt;p&gt;If the eBook reader does not support embedded images, uncomment &lt;code&gt;:data-uri:&lt;/code&gt; lines in &lt;code&gt;src/riscv-privileged.adoc&lt;/code&gt; and &lt;code&gt;src/riscv-unprivileged.adoc&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Known devices that cannot handle embedded images&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;PocketBook InkPad 3&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Repo Activity&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://repobeats.axiom.co/api/embed/ccec87dc4502f2ed7c216b670b5ed8efc33a1d4c.svg?sanitize=true&quot; alt=&quot;Alt&quot; title=&quot;Repobeats analytics image&quot; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>vdumoulin/conv_arithmetic</title>
      <link>https://github.com/vdumoulin/conv_arithmetic</link>
      <description>&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Convolution arithmetic&lt;/h1&gt; 
&lt;p&gt;A technical report on convolution arithmetic in the context of deep learning.&lt;/p&gt; 
&lt;p&gt;The code and the images of this tutorial are free to use as regulated by the licence and subject to proper attribution:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;[1] Vincent Dumoulin, Francesco Visin - &lt;a href=&quot;https://arxiv.org/abs/1603.07285&quot;&gt;A guide to convolution arithmetic for deep learning&lt;/a&gt; (&lt;a href=&quot;https://gist.github.com/fvisin/165ca9935392fa9600a6c94664a01214&quot;&gt;BibTeX&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Convolution animations&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; 
&lt;table style=&quot;width:100%; table-layout:fixed;&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides.gif&quot; /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No padding, no strides&lt;/td&gt; 
   &lt;td&gt;Arbitrary padding, no strides&lt;/td&gt; 
   &lt;td&gt;Half padding, no strides&lt;/td&gt; 
   &lt;td&gt;Full padding, no strides&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No padding, strides&lt;/td&gt; 
   &lt;td&gt;Padding, strides&lt;/td&gt; 
   &lt;td&gt;Padding, strides (odd)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Transposed convolution animations&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; 
&lt;table style=&quot;width:100%; table-layout:fixed;&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides_transposed.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/arbitrary_padding_no_strides_transposed.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides_transposed.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/full_padding_no_strides_transposed.gif&quot; /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No padding, no strides, transposed&lt;/td&gt; 
   &lt;td&gt;Arbitrary padding, no strides, transposed&lt;/td&gt; 
   &lt;td&gt;Half padding, no strides, transposed&lt;/td&gt; 
   &lt;td&gt;Full padding, no strides, transposed&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides_transposed.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_transposed.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides_odd_transposed.gif&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No padding, strides, transposed&lt;/td&gt; 
   &lt;td&gt;Padding, strides, transposed&lt;/td&gt; 
   &lt;td&gt;Padding, strides, transposed (odd)&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Dilated convolution animations&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;N.B.: Blue maps are inputs, and cyan maps are outputs.&lt;/em&gt;&lt;/p&gt; 
&lt;table style=&quot;width:25%&quot; ; table-layout:fixed;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td&gt;&lt;img width=&quot;150px&quot; src=&quot;https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif&quot; /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;No padding, no stride, dilation&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h2&gt;Generating the Makefile&lt;/h2&gt; 
&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ ./bin/generate_makefile
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Generating the animations&lt;/h2&gt; 
&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make all_animations
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The animations will be output to the &lt;code&gt;gif&lt;/code&gt; directory. Individual animation steps will be output in PDF format to the &lt;code&gt;pdf&lt;/code&gt; directory and in PNG format to the &lt;code&gt;png&lt;/code&gt; directory.&lt;/p&gt; 
&lt;h2&gt;Compiling the document&lt;/h2&gt; 
&lt;p&gt;From the repository&#39;s root directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ make
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>SLAM-Handbook-contributors/slam-handbook-public-release</title>
      <link>https://github.com/SLAM-Handbook-contributors/slam-handbook-public-release</link>
      <description>&lt;p&gt;Release repo for our SLAM Handbook&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SLAM Handbook Public Release&lt;/h1&gt; 
&lt;p&gt;Together with a large number of experts in Simultaneous Localization and Mapping (SLAM) we are preparing the &lt;strong&gt;SLAM Handbook&lt;/strong&gt; to be published by Cambridge University Press. This book will cover the theoretical background of SLAM, its applications, and its future as spatial AI. We expect this handbook will be a good guideline for those working in this field.&lt;/p&gt; 
&lt;p&gt;Starting from the release of Part 1 in November 2024, Part 2 in March 2025, and Part 3 in May 2025, we have incrementally released the book to accommodate feedback from the public. Please use this repository&#39;s &lt;strong&gt;issue&lt;/strong&gt; and &lt;strong&gt;discussion&lt;/strong&gt; boards to report any issues or ideas regarding this book.&lt;/p&gt; 
&lt;h2&gt;How to Cite this Book?&lt;/h2&gt; 
&lt;p&gt;Please cite it as a book chapter. You can find the specific bibliography entry for each chapter by clicking the dropdown menu below.&lt;/p&gt; 
&lt;h3&gt;Part 1: Foundations of SLAM&lt;/h3&gt; 
&lt;details&gt;
 &lt;summary&gt;Part 1 Prelude&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-p1-prelude,
  title        = {Part1 Prelude},
  author       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter1: Factor Graphs for SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch1-fg4slam,
  title        = {Factor Graphs for SLAM},
  author       = {Frank Dellaert and Michael Kaess and Timothy Barfoot},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter2: Advanced State Variable Representations&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch2-state,
  title        = {Advanced State Variable Representations},
  author       = {Timothy Barfoot and Frank Dellaert and Michael Kaess and Jose Luis Blanco-Claraco},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter3: Robustness to Incorrect Data Association and Outliers&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch3-outlier,
  title        = {Robustness to Incorrect Data Association and Outliers},
  author       = {Heng Yang and Josh Mangelson and Yun Chang and Jingnan Shi and Luca Carlone},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter4: Differentiable Optimization&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch4-diffopt,
  title        = {Differentiable Optimization},
  author       = {Chen Wang and Krishna Murthy Jatavallabhula and Mustafa Mukadam},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter5: Dense Map Representation&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch5-map,
  title        = {Dense Map Representation},
  author       = {Victor Reijgwart and Jens Behley and Teresa Vidal-Calleja and Helen Oleynikova and Lionel Ott and Cyrill Stachniss and Ayoung Kim},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter6: Theoretical Properties&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch6-theory,
  title        = {Certifiably Optimal Solvers and Theoretical Properties of {SLAM}},
  author       = {David M. Rosen and Kasra Khosoussi and Connor Holmes and Gamini Dissanayake and Timothy Barfoot and Luca Carlone},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Part 2: SLAM in Practice&lt;/h3&gt; 
&lt;details&gt;
 &lt;summary&gt;Part 2 Prelude&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-p2-prelude,
  title        = {Part 2 Prelude},
  author       = {Ayoung Kim and Timothy Barfoot and Luca Carlone and Frank Dellaert and Daniel Cremers},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter7: Visual SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch7-vision,
  title        = {Visual {SLAM}},
  author       = {Jakob Engel and Juan D. Tard¬¥os and Javier Civera and Margarita Chli and Stefan Leutenegger and Frank Dellaert and Daniel Cremers},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter8: LiDAR SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch8-lidar,
  title        = {{LiDAR SLAM}},
  author       = {Jens Behley and Maurice Fallon and Shibo Zhao and Giseop Kim and Ji Zhang and Fu Zhang and Ayoung Kim},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter9: Radar SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch9-radar,
  title        = {Radar {SLAM}},
  author       = {Martin Magnusson and Christoffer Heckman and Henrik Andreasson and Ayoung Kim and Timothy Barfoot and Michael Kaess and Paul Newman},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter10: Event-based SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch10-event,
  title        = {Event-based {SLAM}},
  author       = {Guillermo Gallego and Javier Hidalgo-Carri¬¥o and Davide Scaramuzza},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter11: Inertial Odometry for SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch11-imu,
  title        = {Inertial Odometry for {SLAM}},
  author       = {Guoquan (Paul) Huang and C¬¥edric Le Gentil and Teresa Vidal-Calleja and Davide Scaramuzza and Frank Dellaert and Luca Carlone},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter12: Leg Odometry for SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch12-proprio,
  title        = {Leg Odometry for {SLAM}},
  author       = {Marco Camurri and Mat¬¥ƒ±as Mattamala},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;Part 3: From SLAM to Spatial AI&lt;/h3&gt; 
&lt;details&gt;
 &lt;summary&gt;Part 3 Prelude&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-p3-prelude,
  title        = {Part3 Prelude},
  author       = {Marc Pollefeys and Luca Carlone and Ayoung Kim and Frank Dellaert and Timothy Barfoot and Daniel Cremers},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter13: Boosting SLAM with Deep Learning&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch13-dl4slam,
  title        = {Boosting {SLAM} with Deep Learning},
  author       = {Zachary Teed and Jia Deng, Boris Chidlovskii and J¬¥erome Revaud and Felix Wimbauer and Daniel Cremers},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter14: Map Representations with Differentiable Volume Rendering&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch14-nerfgs,
  title        = {Map Representations with Differentiable Volume Rendering},
  author       = {Hidenobu Matsuki and Andrew J. Davison},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter15: Dynamic and Deformable SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch15-dyndef,
  title        = {Dynamic and Deformable {SLAM}},
  author       = {Lukas Schmid and Jose Maria Martinez Montiel and Shoudong Huang and Daniel Cremers and Jose Neira and Javier Civera},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter16: Metric-Semantic SLAM&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch16-semantic,
  title        = {Metric-Semantic {SLAM}},
  author       = {Arash Asgharivaskasi and Kevin Doherty and Jens Behley and Nathan Hughes and Yun Chang and John Leonard and Henrik I. Christensen and Luca Carlone and Nikolay Atanasov},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter17: Towards Open-World Spatial AI&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch17-openworld,
  title        = {Towards Open-World Spatial {AI}},
  author       = {Liam Paull and Sacha Morin and Dominic Maggio and Martin B¬®uchner and Cesar Cadena and Abhinav Valada and Luca Carlone},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;details&gt;
 &lt;summary&gt;Chapter18: The Computational Structure of Spatial AI Systems&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@inbook{sh-ch18-spatial-ai,
  title        = {The Computational Structure of Spatial {AI} Systems},
  author       = {Andrew J. Davison},
  booktitle    = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  publisher    = {Cambridge University Press},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h3&gt;To cite it as a book&lt;/h3&gt; 
&lt;p&gt;If you need to cite it as a book, use the following bib.&lt;/p&gt; 
&lt;details&gt;
 &lt;summary&gt;SLAM Handbook Bib&lt;/summary&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@book{slam-handbook,
  title        = {{SLAM Handbook.} From Localization and Mapping to Spatial Intelligence},
  editor       = {Luca Carlone and Ayoung Kim and Timothy Barfoot and Daniel Cremers and Frank Dellaert},
  publisher    = {Cambridge University Press},
  year         = {2025}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h2&gt;Contributors of the Book&lt;/h2&gt; 
&lt;p&gt;We deeply appreciate your contribution to this book! (in alphabetical order by their last name)&lt;/p&gt; 
&lt;p&gt;Henrik Andreasson&lt;br /&gt; Arash Asgharivaskasi&lt;br /&gt; Nikolay Atanasov&lt;br /&gt; Timothy Barfoot&lt;br /&gt; Jens Behley&lt;br /&gt; Jose Luis Blanco-Claraco&lt;br /&gt; Martin B√ºchner&lt;br /&gt; Cesar Cadena&lt;br /&gt; Marco Camurri&lt;br /&gt; Luca Carlone&lt;br /&gt; Yun Chang&lt;br /&gt; Boris Chidlovskii&lt;br /&gt; Margarita Chli&lt;br /&gt; Henrik Christensen&lt;br /&gt; Javier Civera&lt;br /&gt; Daniel Cremers&lt;br /&gt; Andrew J. Davison&lt;br /&gt; Frank Dellaert&lt;br /&gt; Jia Deng&lt;br /&gt; Gamini Dissanayake&lt;br /&gt; Kevin Doherty&lt;br /&gt; Jakob Engel&lt;br /&gt; Maurice Fallon&lt;br /&gt; Guillermo Gallego&lt;br /&gt; C√©dric Le Gentil&lt;br /&gt; Christoffer Heckman&lt;br /&gt; Javier Hidalgo-Carri√≥&lt;br /&gt; Connor Holmes&lt;br /&gt; Guoquan Huang&lt;br /&gt; Shoudong Huang&lt;br /&gt; Nathan Hughes&lt;br /&gt; Krishna Murthy Jatavallabhula&lt;br /&gt; Michael Kaess&lt;br /&gt; Kasra Khosoussi&lt;br /&gt; Ayoung Kim&lt;br /&gt; Giseop Kim&lt;br /&gt; John Leonard&lt;br /&gt; Stefan Leutenegger&lt;br /&gt; Dominic Maggio&lt;br /&gt; Martin Magnusson&lt;br /&gt; Joshua Mangelson&lt;br /&gt; Hidenobu Matsuki&lt;br /&gt; Matias Mattamala&lt;br /&gt; Jos√© M Mart√≠nez Montiel&lt;br /&gt; Sacha Morin&lt;br /&gt; Mustafa Mukadam&lt;br /&gt; Jose Neira&lt;br /&gt; Paul Newman&lt;br /&gt; Helen Oleynikova&lt;br /&gt; Lionel Ott&lt;br /&gt; Liam Paull&lt;br /&gt; Marc Pollefeys&lt;br /&gt; Victor Reijgwart&lt;br /&gt; Jerome Revaud&lt;br /&gt; David Rosen&lt;br /&gt; Davide Scaramuzza&lt;br /&gt; Lukas Schmid&lt;br /&gt; Jingnan Shi&lt;br /&gt; Cyrill Stachniss&lt;br /&gt; Niko Sunderhauf&lt;br /&gt; Juan D. Tard√≥s&lt;br /&gt; Zachary Teed&lt;br /&gt; Abhinav Valada&lt;br /&gt; Teresa Vidal-Calleja&lt;br /&gt; Chen Wang&lt;br /&gt; Felix Wimbauer&lt;br /&gt; Heng Yang&lt;br /&gt; Fu Zhang&lt;br /&gt; Ji Zhang&lt;br /&gt; Shibo Zhao&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>sb2nov/resume</title>
      <link>https://github.com/sb2nov/resume</link>
      <description>&lt;p&gt;Software developer resume in Latex&lt;/p&gt;&lt;hr&gt;&lt;p&gt;A single-page, one-column resume for software developers. It uses the base latex templates and fonts to provide ease of use and installation when trying to update the resume. The different sections are clearly documented and custom commands are used to provide consistent formatting. The three main sections in the resume are education, experience, and projects.&lt;/p&gt; 
&lt;h3&gt;Motivation&lt;/h3&gt; 
&lt;p&gt;I created this template as managing a resume on Google Docs was hard and changing any formatting was too difficult since it had to be applied in multiple places.&lt;/p&gt; 
&lt;p&gt;Most currently available templates either focus on two columns, or are multiple pages long that didn&#39;t work well for career fairs or online applications.&lt;/p&gt; 
&lt;h3&gt;Quick start&lt;/h3&gt; 
&lt;p&gt;Get started quickly using &lt;a href=&quot;https://www.overleaf.com/latex/templates/software-engineer-resume/gqxmqsvsbdjf&quot;&gt;Overleaf&lt;/a&gt; template.&lt;/p&gt; 
&lt;h3&gt;Build using Docker&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;docker build -t latex .
docker run --rm -i -v &quot;$PWD&quot;:/data latex pdflatex sourabh_bajaj_resume.tex
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Preview&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/sb2nov/resume/master/resume_preview.png&quot; alt=&quot;Resume Screenshot&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;License&lt;/h3&gt; 
&lt;p&gt;Format is MIT but all the data is owned by Sourabh Bajaj.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>billryan/resume</title>
      <link>https://github.com/billryan/resume</link>
      <description>&lt;p&gt;An elegant \LaTeX\ r√©sum√© template. Â§ßÈôÜÈïúÂÉè https://gods.coding.net/p/resume/git&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;R√©sum√©&lt;/h1&gt; 
&lt;p&gt;Hit branch &lt;a href=&quot;https://github.com/billryan/resume/tree/zh_CN&quot;&gt;zh_CN&lt;/a&gt; if you want a Simplified Chinese r√©sum√©.&lt;/p&gt; 
&lt;p&gt;‰∏≠ÊñáÁî®Êà∑ËØ∑ÂâçÂæÄ &lt;a href=&quot;https://github.com/billryan/resume/tree/zh_CN&quot;&gt;zh_CN&lt;/a&gt; ÂàÜÊîØ„ÄÇ&lt;/p&gt; 
&lt;p&gt;An elegant \LaTeX\ r√©sum√© template, compiled with \XeLaTeX. Inspired by&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zachscrivena/simple-resume-cv&quot;&gt;zachscrivena/simple-resume-cv&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.ctan.org/pkg/res&quot;&gt;res&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.jianxu.net/en/files/JianXu_CV.pdf&quot;&gt;JianXu&#39;s CV&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;http://www.stat.berkeley.edu/~paciorek/computingTips/Latex_template_creating_CV_.html&quot;&gt;paciorek&#39;s CV/Resume template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.sharelatex.com/blog/2011/03/27/how-to-write-a-latex-class-file-and-design-your-own-cv.html&quot;&gt;How to write a LaTeX class file and design your own CV (Part 1) - ShareLaTeX&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Easy to further customize or extend&lt;/li&gt; 
 &lt;li&gt;Full support for unicode characters (e.g. CJK) with \XeLaTeX\&lt;/li&gt; 
 &lt;li&gt;Perfect Simplified Chinese fonts supported with Adobefonts&lt;/li&gt; 
 &lt;li&gt;FontAwesome 4.6.3 support&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Fork this repository&lt;/li&gt; 
 &lt;li&gt;Add information about you directly in GitHub&lt;/li&gt; 
 &lt;li&gt;Compile TeX file to PDF with &lt;a href=&quot;https://latexonline.cc/&quot;&gt;LaTeX.Online&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Can also use Overleaf for online compilation with &lt;a href=&quot;https://www.overleaf.com/latex/templates/bill-ryans-elegant-latex-resume/xcqmhktmzmsw&quot;&gt;template&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Sample Output&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/25968335/131621921-65ab1862-1f56-47ef-9d58-8d5149bec841.png&quot; alt=&quot;English&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/25968335/131621960-1cafb3c2-114b-4e90-8b04-bd9b949a6e9d.png&quot; alt=&quot;English with photo&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/25968335/131621980-c004f2a6-4199-4676-8a97-5d2cb165402f.png&quot; alt=&quot;ÁÆÄ‰Ωì‰∏≠Êñá&quot; /&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/billryan/resume/files/3463503/resume.pdf&quot;&gt;English PDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/billryan/resume/files/3463501/resume_photo.pdf&quot;&gt;English with photo PDF&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/billryan/resume/files/3463502/resume-zh_CN.pdf&quot;&gt;ÁÆÄ‰Ωì‰∏≠Êñá PDF&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Usage&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Edit in Overleaf online Web &lt;a href=&quot;https://www.overleaf.com/latex/templates/bill-ryans-elegant-latex-resume/xcqmhktmzmsw&quot;&gt;template&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Compile tex on your Computer&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;If you only need a r√©sum√© in English or have installed Adobe Simplified Chinese on your OS, &lt;strong&gt;It would be better to clone only the master branch,&lt;/strong&gt; since the Simplified Chinese fonts files are too large.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;git clone https://github.com/billryan/resume.git --branch master --depth 1 --single-branch &amp;lt;folder&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;http://opensource.org/licenses/MIT&quot;&gt;The MIT License (MIT)&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Copyrighted fonts are not subjected to this License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>terryum/awesome-deep-learning-papers</title>
      <link>https://github.com/terryum/awesome-deep-learning-papers</link>
      <description>&lt;p&gt;The most cited deep learning papers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome - Most Cited Deep Learning Papers&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/sindresorhus/awesome&quot;&gt;&lt;img src=&quot;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&quot; alt=&quot;Awesome&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.&lt;/p&gt; 
&lt;p&gt;A curated list of the most cited deep learning papers (2012-2016)&lt;/p&gt; 
&lt;p&gt;We believe that there exist &lt;em&gt;classic&lt;/em&gt; deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a &lt;em&gt;curated list&lt;/em&gt; of the awesome deep learning papers which are considered as &lt;em&gt;must-reads&lt;/em&gt; in certain research domains.&lt;/p&gt; 
&lt;h2&gt;Background&lt;/h2&gt; 
&lt;p&gt;Before this list, there exist other &lt;em&gt;awesome deep learning lists&lt;/em&gt;, for example, &lt;a href=&quot;https://github.com/kjw0612/awesome-deep-vision&quot;&gt;Deep Vision&lt;/a&gt; and &lt;a href=&quot;https://github.com/kjw0612/awesome-rnn&quot;&gt;Awesome Recurrent Neural Networks&lt;/a&gt;. Also, after this list comes out, another awesome list for deep learning beginners, called &lt;a href=&quot;https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap&quot;&gt;Deep Learning Papers Reading Roadmap&lt;/a&gt;, has been created and loved by many deep learning researchers.&lt;/p&gt; 
&lt;p&gt;Although the &lt;em&gt;Roadmap List&lt;/em&gt; includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; here as a good starting point of overviewing deep learning researches.&lt;/p&gt; 
&lt;p&gt;To get the news for newly released papers everyday, follow my &lt;a href=&quot;https://twitter.com/TerryUm_ML&quot;&gt;twitter&lt;/a&gt; or &lt;a href=&quot;https://www.facebook.com/terryum.io/&quot;&gt;facebook page&lt;/a&gt;!&lt;/p&gt; 
&lt;h2&gt;Awesome list criteria&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;A list of &lt;strong&gt;top 100 deep learning papers&lt;/strong&gt; published from 2012 to 2016 is suggested.&lt;/li&gt; 
 &lt;li&gt;If a paper is added to the list, another paper (usually from *More Papers from 2016&quot; section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)&lt;/li&gt; 
 &lt;li&gt;Papers that are important, but failed to be included in the list, will be listed in &lt;em&gt;More than Top 100&lt;/em&gt; section.&lt;/li&gt; 
 &lt;li&gt;Please refer to &lt;em&gt;New Papers&lt;/em&gt; and &lt;em&gt;Old Papers&lt;/em&gt; sections for the papers published in recent 6 months or before 2012.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;em&gt;(Citation criteria)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&amp;lt; 6 months&lt;/strong&gt; : &lt;em&gt;New Papers&lt;/em&gt; (by discussion)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2016&lt;/strong&gt; : +60 citations or &quot;More Papers from 2016&quot;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2015&lt;/strong&gt; : +200 citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2014&lt;/strong&gt; : +400 citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2013&lt;/strong&gt; : +600 citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;2012&lt;/strong&gt; : +800 citations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;~2012&lt;/strong&gt; : &lt;em&gt;Old Papers&lt;/em&gt; (by discussion)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Please note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;We need your contributions!&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;If you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request. (Please read the &lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers/raw/master/Contributing.md&quot;&gt;contributing guide&lt;/a&gt; for further instructions, though just letting me know the title of papers can also be a big contribution to us.)&lt;/p&gt; 
&lt;p&gt;(Update) You can download all top-100 papers with &lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers/raw/master/fetch_papers.py&quot;&gt;this&lt;/a&gt; and collect all authors&#39; names with &lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers/raw/master/get_authors.py&quot;&gt;this&lt;/a&gt;. Also, &lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers/raw/master/top100papers.bib&quot;&gt;bib file&lt;/a&gt; for all top-100 papers are available. Thanks, doodhwala, &lt;a href=&quot;https://github.com/sunshinemyson&quot;&gt;Sven&lt;/a&gt; and &lt;a href=&quot;https://github.com/grepinsight&quot;&gt;grepinsight&lt;/a&gt;!&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#understanding--generalization--transfer&quot;&gt;Understanding / Generalization / Transfer&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#optimization--training-techniques&quot;&gt;Optimization / Training Techniques&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#unsupervised--generative-models&quot;&gt;Unsupervised / Generative Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#convolutional-neural-network-models&quot;&gt;Convolutional Network Models&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#image-segmentation--object-detection&quot;&gt;Image Segmentation / Object Detection&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#image--video--etc&quot;&gt;Image / Video / Etc&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#natural-language-processing--rnns&quot;&gt;Natural Language Processing / RNNs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#speech--other-domain&quot;&gt;Speech / Other Domain&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#reinforcement-learning--robotics&quot;&gt;Reinforcement Learning / Robotics&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#more-papers-from-2016&quot;&gt;More Papers from 2016&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;(More than Top 100)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#new-papers&quot;&gt;New Papers&lt;/a&gt; : Less than 6 months&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#old-papers&quot;&gt;Old Papers&lt;/a&gt; : Before 2012&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#hw--sw--dataset&quot;&gt;HW / SW / Dataset&lt;/a&gt; : Technical reports&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#book--survey--review&quot;&gt;Book / Survey / Review&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#video-lectures--tutorials--blogs&quot;&gt;Video Lectures / Tutorials / Blogs&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/terryum/awesome-deep-learning-papers/master/#appendix-more-than-top-100&quot;&gt;Appendix: More than Top 100&lt;/a&gt; : More papers not in the list&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Understanding / Generalization / Transfer&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Distilling the knowledge in a neural network&lt;/strong&gt; (2015), G. Hinton et al. &lt;a href=&quot;http://arxiv.org/pdf/1503.02531&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images&lt;/strong&gt; (2015), A. Nguyen et al. &lt;a href=&quot;http://arxiv.org/pdf/1412.1897&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;How transferable are features in deep neural networks?&lt;/strong&gt; (2014), J. Yosinski et al. &lt;a href=&quot;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;CNN features off-the-Shelf: An astounding baseline for recognition&lt;/strong&gt; (2014), A. Razavian et al. &lt;a href=&quot;http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learning and transferring mid-Level image representations using convolutional neural networks&lt;/strong&gt; (2014), M. Oquab et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Visualizing and understanding convolutional networks&lt;/strong&gt; (2014), M. Zeiler and R. Fergus &lt;a href=&quot;http://arxiv.org/pdf/1311.2901&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Decaf: A deep convolutional activation feature for generic visual recognition&lt;/strong&gt; (2014), J. Donahue et al. &lt;a href=&quot;http://arxiv.org/pdf/1310.1531&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Jason Yosinski](https://scholar.google.ca/citations?hl=en&amp;user=gxL1qj8AAAAJ) --&gt; 
&lt;h3&gt;Optimization / Training Techniques&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Training very deep networks&lt;/strong&gt; (2015), R. Srivastava et al. &lt;a href=&quot;http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Batch normalization: Accelerating deep network training by reducing internal covariate shift&lt;/strong&gt; (2015), S. Loffe and C. Szegedy &lt;a href=&quot;http://arxiv.org/pdf/1502.03167&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification&lt;/strong&gt; (2015), K. He et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dropout: A simple way to prevent neural networks from overfitting&lt;/strong&gt; (2014), N. Srivastava et al. &lt;a href=&quot;http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Adam: A method for stochastic optimization&lt;/strong&gt; (2014), D. Kingma and J. Ba &lt;a href=&quot;http://arxiv.org/pdf/1412.6980&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improving neural networks by preventing co-adaptation of feature detectors&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href=&quot;http://arxiv.org/pdf/1207.0580.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Random search for hyper-parameter optimization&lt;/strong&gt; (2012) J. Bergstra and Y. Bengio &lt;a href=&quot;http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers] [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Christian Szegedy](https://scholar.google.ca/citations?hl=en&amp;user=3QeF7mAAAAAJ), [Sergey Ioffe](https://scholar.google.ca/citations?user=S5zOyIkAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&amp;user=DhtAFkwAAAAJ), [Diederik P. Kingma](https://scholar.google.ca/citations?hl=en&amp;user=yyIoQu4AAAAJ)--&gt; 
&lt;h3&gt;Unsupervised / Generative Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Pixel recurrent neural networks&lt;/strong&gt; (2016), A. Oord et al. &lt;a href=&quot;http://arxiv.org/pdf/1601.06759v2.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Improved techniques for training GANs&lt;/strong&gt; (2016), T. Salimans et al. &lt;a href=&quot;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Unsupervised representation learning with deep convolutional generative adversarial networks&lt;/strong&gt; (2015), A. Radford et al. &lt;a href=&quot;https://arxiv.org/pdf/1511.06434v2&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DRAW: A recurrent neural network for image generation&lt;/strong&gt; (2015), K. Gregor et al. &lt;a href=&quot;http://arxiv.org/pdf/1502.04623&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generative adversarial nets&lt;/strong&gt; (2014), I. Goodfellow et al. &lt;a href=&quot;http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Auto-encoding variational Bayes&lt;/strong&gt; (2013), D. Kingma and M. Welling &lt;a href=&quot;http://arxiv.org/pdf/1312.6114&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Building high-level features using large scale unsupervised learning&lt;/strong&gt; (2013), Q. Le et al. &lt;a href=&quot;http://arxiv.org/pdf/1112.6209&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers] [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Ian Goodfellow](https://scholar.google.ca/citations?user=iYN86KEAAAAJ), [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)--&gt; 
&lt;h3&gt;Convolutional Neural Network Models&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Rethinking the inception architecture for computer vision&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Inception-v4, inception-resnet and the impact of residual connections on learning&lt;/strong&gt; (2016), C. Szegedy et al. &lt;a href=&quot;http://arxiv.org/pdf/1602.07261&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt; (2016), K. He et al. &lt;a href=&quot;https://arxiv.org/pdf/1603.05027v2.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep residual learning for image recognition&lt;/strong&gt; (2016), K. He et al. &lt;a href=&quot;http://arxiv.org/pdf/1512.03385&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Spatial transformer network&lt;/strong&gt; (2015), M. Jaderberg et al., &lt;a href=&quot;http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Going deeper with convolutions&lt;/strong&gt; (2015), C. Szegedy et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Very deep convolutional networks for large-scale image recognition&lt;/strong&gt; (2014), K. Simonyan and A. Zisserman &lt;a href=&quot;http://arxiv.org/pdf/1409.1556&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Return of the devil in the details: delving deep into convolutional nets&lt;/strong&gt; (2014), K. Chatfield et al. &lt;a href=&quot;http://arxiv.org/pdf/1405.3531&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OverFeat: Integrated recognition, localization and detection using convolutional networks&lt;/strong&gt; (2013), P. Sermanet et al. &lt;a href=&quot;http://arxiv.org/pdf/1312.6229&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Maxout networks&lt;/strong&gt; (2013), I. Goodfellow et al. &lt;a href=&quot;http://arxiv.org/pdf/1302.4389v4&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Network in network&lt;/strong&gt; (2013), M. Lin et al. &lt;a href=&quot;http://arxiv.org/pdf/1312.4400&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ImageNet classification with deep convolutional neural networks&lt;/strong&gt; (2012), A. Krizhevsky et al. &lt;a href=&quot;http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Christian Szegedy](https://scholar.google.ca/citations?hl=en&amp;user=3QeF7mAAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&amp;user=DhtAFkwAAAAJ), [Shaoqing Ren](https://scholar.google.ca/citations?hl=en&amp;user=AUhj438AAAAJ), [Jian Sun](https://scholar.google.ca/citations?hl=en&amp;user=ALVSZAYAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Yann LeCun](https://scholar.google.ca/citations?hl=en&amp;user=WLN3QrAAAAAJ)--&gt; 
&lt;h3&gt;Image: Segmentation / Object Detection&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;You only look once: Unified, real-time object detection&lt;/strong&gt; (2016), J. Redmon et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fully convolutional networks for semantic segmentation&lt;/strong&gt; (2015), J. Long et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/strong&gt; (2015), S. Ren et al. &lt;a href=&quot;http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Fast R-CNN&lt;/strong&gt; (2015), R. Girshick &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Rich feature hierarchies for accurate object detection and semantic segmentation&lt;/strong&gt; (2014), R. Girshick et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Spatial pyramid pooling in deep convolutional networks for visual recognition&lt;/strong&gt; (2014), K. He et al. &lt;a href=&quot;http://arxiv.org/pdf/1406.4729&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic image segmentation with deep convolutional nets and fully connected CRFs&lt;/strong&gt;, L. Chen et al. &lt;a href=&quot;https://arxiv.org/pdf/1412.7062&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learning hierarchical features for scene labeling&lt;/strong&gt; (2013), C. Farabet et al. &lt;a href=&quot;https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Ross Girshick](https://scholar.google.ca/citations?hl=en&amp;user=W8VIEZgAAAAJ), [Jeff Donahue](https://scholar.google.ca/citations?hl=en&amp;user=UfbuDH8AAAAJ), [Trevor Darrell](https://scholar.google.ca/citations?hl=en&amp;user=bh-uRFMAAAAJ)--&gt; 
&lt;h3&gt;Image / Video / Etc&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/strong&gt; (2016), C. Dong et al. &lt;a href=&quot;https://arxiv.org/pdf/1501.00092v3.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;A neural algorithm of artistic style&lt;/strong&gt; (2015), L. Gatys et al. &lt;a href=&quot;https://arxiv.org/pdf/1508.06576&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep visual-semantic alignments for generating image descriptions&lt;/strong&gt; (2015), A. Karpathy and L. Fei-Fei &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Show, attend and tell: Neural image caption generation with visual attention&lt;/strong&gt; (2015), K. Xu et al. &lt;a href=&quot;http://arxiv.org/pdf/1502.03044&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Show and tell: A neural image caption generator&lt;/strong&gt; (2015), O. Vinyals et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Long-term recurrent convolutional networks for visual recognition and description&lt;/strong&gt; (2015), J. Donahue et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;VQA: Visual question answering&lt;/strong&gt; (2015), S. Antol et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DeepFace: Closing the gap to human-level performance in face verification&lt;/strong&gt; (2014), Y. Taigman et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;:&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Large-scale video classification with convolutional neural networks&lt;/strong&gt; (2014), A. Karpathy et al. &lt;a href=&quot;http://vision.stanford.edu/pdf/karpathy14.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two-stream convolutional networks for action recognition in videos&lt;/strong&gt; (2014), K. Simonyan et al. &lt;a href=&quot;http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;3D convolutional neural networks for human action recognition&lt;/strong&gt; (2013), S. Ji et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Andrej Karpathy](https://scholar.google.ca/citations?user=l8WuQJgAAAAJ)--&gt; 
&lt;!--[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)--&gt; 
&lt;h3&gt;Natural Language Processing / RNNs&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Neural Architectures for Named Entity Recognition&lt;/strong&gt; (2016), G. Lample et al. &lt;a href=&quot;http://aclweb.org/anthology/N/N16/N16-1030.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Exploring the limits of language modeling&lt;/strong&gt; (2016), R. Jozefowicz et al. &lt;a href=&quot;http://arxiv.org/pdf/1602.02410&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Teaching machines to read and comprehend&lt;/strong&gt; (2015), K. Hermann et al. &lt;a href=&quot;http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Effective approaches to attention-based neural machine translation&lt;/strong&gt; (2015), M. Luong et al. &lt;a href=&quot;https://arxiv.org/pdf/1508.04025&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Conditional random fields as recurrent neural networks&lt;/strong&gt; (2015), S. Zheng and S. Jayasumana. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Memory networks&lt;/strong&gt; (2014), J. Weston et al. &lt;a href=&quot;https://arxiv.org/pdf/1410.3916&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural turing machines&lt;/strong&gt; (2014), A. Graves et al. &lt;a href=&quot;https://arxiv.org/pdf/1410.5401&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Neural machine translation by jointly learning to align and translate&lt;/strong&gt; (2014), D. Bahdanau et al. &lt;a href=&quot;http://arxiv.org/pdf/1409.0473&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Sequence to sequence learning with neural networks&lt;/strong&gt; (2014), I. Sutskever et al. &lt;a href=&quot;http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learning phrase representations using RNN encoder-decoder for statistical machine translation&lt;/strong&gt; (2014), K. Cho et al. &lt;a href=&quot;http://arxiv.org/pdf/1406.1078&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;A convolutional neural network for modeling sentences&lt;/strong&gt; (2014), N. Kalchbrenner et al. &lt;a href=&quot;http://arxiv.org/pdf/1404.2188v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Convolutional neural networks for sentence classification&lt;/strong&gt; (2014), Y. Kim &lt;a href=&quot;http://arxiv.org/pdf/1408.5882&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Glove: Global vectors for word representation&lt;/strong&gt; (2014), J. Pennington et al. &lt;a href=&quot;http://anthology.aclweb.org/D/D14/D14-1162.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed representations of sentences and documents&lt;/strong&gt; (2014), Q. Le and T. Mikolov &lt;a href=&quot;http://arxiv.org/pdf/1405.4053&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Distributed representations of words and phrases and their compositionality&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href=&quot;http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Efficient estimation of word representations in vector space&lt;/strong&gt; (2013), T. Mikolov et al. &lt;a href=&quot;http://arxiv.org/pdf/1301.3781&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Recursive deep models for semantic compositionality over a sentiment treebank&lt;/strong&gt; (2013), R. Socher et al. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generating sequences with recurrent neural networks&lt;/strong&gt; (2013), A. Graves. &lt;a href=&quot;https://arxiv.org/pdf/1308.0850&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Kyunghyun Cho](https://scholar.google.ca/citations?user=0RAmmIAAAAAJ), [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Richard Socher](https://scholar.google.ca/citations?hl=en&amp;user=FaOcyfMAAAAJ), [Tomas Mikolov](https://scholar.google.ca/citations?user=oBu8kMMAAAAJ), [Christopher D. Manning](https://scholar.google.ca/citations?user=1zmDOdwAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ)--&gt; 
&lt;h3&gt;Speech / Other Domain&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end attention-based large vocabulary speech recognition&lt;/strong&gt; (2016), D. Bahdanau et al. &lt;a href=&quot;https://arxiv.org/pdf/1508.04395&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep speech 2: End-to-end speech recognition in English and Mandarin&lt;/strong&gt; (2015), D. Amodei et al. &lt;a href=&quot;https://arxiv.org/pdf/1512.02595&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Speech recognition with deep recurrent neural networks&lt;/strong&gt; (2013), A. Graves &lt;a href=&quot;http://arxiv.org/pdf/1303.5778.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups&lt;/strong&gt; (2012), G. Hinton et al. &lt;a href=&quot;http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition&lt;/strong&gt; (2012) G. Dahl et al. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Acoustic modeling using deep belief networks&lt;/strong&gt; (2012), A. Mohamed et al. &lt;a href=&quot;http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Dong Yu](https://scholar.google.ca/citations?hl=en&amp;user=tMY31_gAAAAJ)--&gt; 
&lt;h3&gt;Reinforcement Learning / Robotics&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;End-to-end training of deep visuomotor policies&lt;/strong&gt; (2016), S. Levine et al. &lt;a href=&quot;http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection&lt;/strong&gt; (2016), S. Levine et al. &lt;a href=&quot;https://arxiv.org/pdf/1603.02199&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Asynchronous methods for deep reinforcement learning&lt;/strong&gt; (2016), V. Mnih et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep Reinforcement Learning with Double Q-Learning&lt;/strong&gt; (2016), H. Hasselt et al. &lt;a href=&quot;https://arxiv.org/pdf/1509.06461.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Mastering the game of Go with deep neural networks and tree search&lt;/strong&gt; (2016), D. Silver et al. &lt;a href=&quot;http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Continuous control with deep reinforcement learning&lt;/strong&gt; (2015), T. Lillicrap et al. &lt;a href=&quot;https://arxiv.org/pdf/1509.02971&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Human-level control through deep reinforcement learning&lt;/strong&gt; (2015), V. Mnih et al. &lt;a href=&quot;http://www.davidqiu.com:8888/research/nature14236.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Deep learning for detecting robotic grasps&lt;/strong&gt; (2015), I. Lenz et al. &lt;a href=&quot;http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Playing atari with deep reinforcement learning&lt;/strong&gt; (2013), V. Mnih et al. &lt;a href=&quot;http://arxiv.org/pdf/1312.5602.pdf&quot;&gt;[pdf]&lt;/a&gt;)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!--[Key researchers]  [Sergey Levine](https://scholar.google.ca/citations?user=8R35rCwAAAAJ), [Volodymyr Mnih](https://scholar.google.ca/citations?hl=en&amp;user=rLdfJ1gAAAAJ), [David Silver](https://scholar.google.ca/citations?user=-8DNE4UAAAAJ)--&gt; 
&lt;h3&gt;More Papers from 2016&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Layer Normalization&lt;/strong&gt; (2016), J. Ba et al. &lt;a href=&quot;https://arxiv.org/pdf/1607.06450v1.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Learning to learn by gradient descent by gradient descent&lt;/strong&gt; (2016), M. Andrychowicz et al. &lt;a href=&quot;http://arxiv.org/pdf/1606.04474v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Domain-adversarial training of neural networks&lt;/strong&gt; (2016), Y. Ganin et al. &lt;a href=&quot;http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;WaveNet: A Generative Model for Raw Audio&lt;/strong&gt; (2016), A. Oord et al. &lt;a href=&quot;https://arxiv.org/pdf/1609.03499v2&quot;&gt;[pdf]&lt;/a&gt; &lt;a href=&quot;https://deepmind.com/blog/wavenet-generative-model-raw-audio/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Colorful image colorization&lt;/strong&gt; (2016), R. Zhang et al. &lt;a href=&quot;https://arxiv.org/pdf/1603.08511&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Generative visual manipulation on the natural image manifold&lt;/strong&gt; (2016), J. Zhu et al. &lt;a href=&quot;https://arxiv.org/pdf/1609.03552&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Texture networks: Feed-forward synthesis of textures and stylized images&lt;/strong&gt; (2016), D Ulyanov et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SSD: Single shot multibox detector&lt;/strong&gt; (2016), W. Liu et al. &lt;a href=&quot;https://arxiv.org/pdf/1512.02325&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&amp;lt; 1MB model size&lt;/strong&gt; (2016), F. Iandola et al. &lt;a href=&quot;http://arxiv.org/pdf/1602.07360&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Eie: Efficient inference engine on compressed deep neural network&lt;/strong&gt; (2016), S. Han et al. &lt;a href=&quot;http://arxiv.org/pdf/1602.01528&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1&lt;/strong&gt; (2016), M. Courbariaux et al. &lt;a href=&quot;https://arxiv.org/pdf/1602.02830&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Dynamic memory networks for visual and textual question answering&lt;/strong&gt; (2016), C. Xiong et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Stacked attention networks for image question answering&lt;/strong&gt; (2016), Z. Yang et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Hybrid computing using a neural network with dynamic external memory&lt;/strong&gt; (2016), A. Graves et al. &lt;a href=&quot;https://www.gwern.net/docs/2016-graves.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Google&#39;s neural machine translation system: Bridging the gap between human and machine translation&lt;/strong&gt; (2016), Y. Wu et al. &lt;a href=&quot;https://arxiv.org/pdf/1609.08144&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;New papers&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Newly published papers (&amp;lt; 6 months) which are worth reading&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. &lt;a href=&quot;https://arxiv.org/pdf/1704.04861.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. &lt;a href=&quot;https://arxiv.org/pdf/1705.03122&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. &lt;a href=&quot;https://arxiv.org/pdf/1702.01932&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. &lt;a href=&quot;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. &lt;a href=&quot;https://arxiv.org/pdf/1703.10135.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep Photo Style Transfer (2017), F. Luan et al. &lt;a href=&quot;http://arxiv.org/pdf/1703.07511v1.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. &lt;a href=&quot;http://arxiv.org/pdf/1703.03864v1.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deformable Convolutional Networks (2017), J. Dai et al. &lt;a href=&quot;http://arxiv.org/pdf/1703.06211v2.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mask R-CNN (2017), K. He et al. &lt;a href=&quot;https://128.84.21.199/pdf/1703.06870&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. &lt;a href=&quot;http://arxiv.org/pdf/1703.05192v1.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., &lt;a href=&quot;http://arxiv.org/pdf/1702.07825v2.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. &lt;a href=&quot;http://arxiv.org/pdf/1702.06506v1.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. &lt;a href=&quot;https://arxiv.org/abs/1702.03275&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Wasserstein GAN (2017), M. Arjovsky et al. &lt;a href=&quot;https://arxiv.org/pdf/1701.07875v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. &lt;a href=&quot;https://arxiv.org/pdf/1611.03530&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Least squares generative adversarial networks (2016), X. Mao et al. &lt;a href=&quot;https://arxiv.org/abs/1611.04076v2&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Old Papers&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;Classic papers published before 2012&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep sparse rectifier neural networks (2011), X. Glorot et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Natural language processing (almost) from scratch (2011), R. Collobert et al. &lt;a href=&quot;http://arxiv.org/pdf/1103.0398&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Recurrent neural network based language model (2010), T. Mikolov et al. &lt;a href=&quot;http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning mid-level features for recognition (2010), Y. Boureau &lt;a href=&quot;http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A practical guide to training restricted boltzmann machines (2010), G. Hinton &lt;a href=&quot;http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning deep architectures for AI (2009), Y. Bengio. &lt;a href=&quot;http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. &lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Greedy layer-wise training of deep networks (2007), Y. Bengio et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. &lt;a href=&quot;http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A fast learning algorithm for deep belief nets (2006), G. Hinton et al. &lt;a href=&quot;http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Gradient-based learning applied to document recognition (1998), Y. LeCun et al. &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. &lt;a href=&quot;http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;HW / SW / Dataset&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. &lt;a href=&quot;https://arxiv.org/pdf/1606.05250.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;OpenAI gym (2016), G. Brockman et al. &lt;a href=&quot;https://arxiv.org/pdf/1606.01540&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. &lt;a href=&quot;http://arxiv.org/pdf/1603.04467&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.&lt;/li&gt; 
 &lt;li&gt;Torch7: A matlab-like environment for machine learning, R. Collobert et al. &lt;a href=&quot;https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc &lt;a href=&quot;http://arxiv.org/pdf/1412.4564&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. &lt;a href=&quot;http://arxiv.org/pdf/1409.0575&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. &lt;a href=&quot;http://arxiv.org/pdf/1408.5093&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Book / Survey / Review&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. &lt;a href=&quot;https://arxiv.org/pdf/1702.07800&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep Reinforcement Learning: An Overview (2017), Y. Li, &lt;a href=&quot;http://arxiv.org/pdf/1701.07274v2.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. &lt;a href=&quot;http://arxiv.org/pdf/1703.01619v1.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/index.html&quot;&gt;[html]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep learning (Book, 2016), Goodfellow et al. &lt;a href=&quot;http://www.deeplearningbook.org/&quot;&gt;[html]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;LSTM: A search space odyssey (2016), K. Greff et al. &lt;a href=&quot;https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&amp;amp;utm_medium=social&amp;amp;utm_source=plus.google.com&amp;amp;utm_campaign=buffer&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Tutorial on Variational Autoencoders (2016), C. Doersch. &lt;a href=&quot;https://arxiv.org/pdf/1606.05908&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep learning in neural networks: An overview (2015), J. Schmidhuber &lt;a href=&quot;http://arxiv.org/pdf/1404.7828&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Representation learning: A review and new perspectives (2013), Y. Bengio et al. &lt;a href=&quot;http://arxiv.org/pdf/1206.5538&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Video Lectures / Tutorials / Blogs&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;(Lectures)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University &lt;a href=&quot;http://cs231n.stanford.edu/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;CS224d, Deep Learning for Natural Language Processing, Stanford University &lt;a href=&quot;http://cs224d.stanford.edu/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford &lt;a href=&quot;https://github.com/oxford-cs-deepnlp-2017/lectures&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;(Tutorials)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;NIPS 2016 Tutorials, Long Beach &lt;a href=&quot;https://nips.cc/Conferences/2016/Schedule?type=Tutorial&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ICML 2016 Tutorials, New York City &lt;a href=&quot;http://techtalks.tv/icml/2016/tutorials/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;ICLR 2016 Videos, San Juan &lt;a href=&quot;http://videolectures.net/iclr2016_san_juan/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep Learning Summer School 2016, Montreal &lt;a href=&quot;http://videolectures.net/deeplearning2016_montreal/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Bay Area Deep Learning School 2016, Stanford &lt;a href=&quot;https://www.bayareadlschool.org/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;(Blogs)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;OpenAI &lt;a href=&quot;https://www.openai.com/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Distill &lt;a href=&quot;http://distill.pub/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Andrej Karpathy Blog &lt;a href=&quot;http://karpathy.github.io/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Colah&#39;s Blog &lt;a href=&quot;http://colah.github.io/&quot;&gt;[Web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;WildML &lt;a href=&quot;http://www.wildml.com/&quot;&gt;[Web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;FastML &lt;a href=&quot;http://www.fastml.com/&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;TheMorningPaper &lt;a href=&quot;https://blog.acolyer.org&quot;&gt;[web]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Appendix: More than Top 100&lt;/h3&gt; 
&lt;p&gt;&lt;em&gt;(2016)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. &lt;a href=&quot;https://arxiv.org/pdf/1603.06147&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. &lt;a href=&quot;http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html&quot;&gt;[html]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. &lt;a href=&quot;https://arxiv.org/pdf/1503.00949&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. &lt;a href=&quot;https://arxiv.org/pdf/1505.03540&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. &lt;a href=&quot;https://arxiv.org/pdf/1610.09038&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Adversarially learned inference (2016), V. Dumoulin et al. &lt;a href=&quot;https://ishmaelbelghazi.github.io/ALI/&quot;&gt;[web]&lt;/a&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.00704v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Understanding convolutional neural networks (2016), J. Koushik &lt;a href=&quot;https://arxiv.org/pdf/1605.09081v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. &lt;a href=&quot;https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Adaptive computation time for recurrent neural networks (2016), A. Graves &lt;a href=&quot;http://arxiv.org/pdf/1603.08983&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Densely connected convolutional networks (2016), G. Huang et al. &lt;a href=&quot;https://arxiv.org/pdf/1608.06993v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al.&lt;/li&gt; 
 &lt;li&gt;Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/gu16.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. &lt;a href=&quot;https://arxiv.org/pdf/1606.02858&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. &lt;a href=&quot;https://arxiv.org/pdf/1604.00788&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. &lt;a href=&quot;https://arxiv.org/pdf/1606.01781&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Bag of tricks for efficient text classification (2016), A. Joulin et al. &lt;a href=&quot;https://arxiv.org/pdf/1607.01759&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning to compose neural networks for question answering (2016), J. Andreas et al. &lt;a href=&quot;https://arxiv.org/pdf/1601.01705&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. &lt;a href=&quot;https://arxiv.org/pdf/1603.08155&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. &lt;a href=&quot;http://arxiv.org/pdf/1412.1842&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;What makes for effective detection proposals? (2016), J. Hosang et al. &lt;a href=&quot;https://arxiv.org/pdf/1502.05082&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. &lt;a href=&quot;http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep networks with stochastic depth (2016), G. Huang et al., &lt;a href=&quot;https://arxiv.org/pdf/1603.09382&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. &lt;a href=&quot;http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;(2015)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Exploring models and data for image question answering (2015), M. Ren et al. &lt;a href=&quot;http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. &lt;a href=&quot;http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Mind&#39;s eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;From captions to visual concepts and back (2015), H. Fang et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;.&lt;/li&gt; 
 &lt;li&gt;Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. &lt;a href=&quot;http://arxiv.org/pdf/1502.05698&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. &lt;a href=&quot;http://arxiv.org/pdf/1506.07285&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. &lt;a href=&quot;https://arxiv.org/pdf/1510.00149&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. &lt;a href=&quot;https://arxiv.org/pdf/1503.00075&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Character-aware neural language models (2015), Y. Kim et al. &lt;a href=&quot;https://arxiv.org/pdf/1508.06615&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Grammar as a foreign language (2015), O. Vinyals et al. &lt;a href=&quot;http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Trust Region Policy Optimization (2015), J. Schulman et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Beyond short snippents: Deep networks for video classification (2015) &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. &lt;a href=&quot;https://arxiv.org/pdf/1505.04366v1&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Understanding neural networks through deep visualization (2015), J. Yosinski et al. &lt;a href=&quot;https://arxiv.org/pdf/1506.06579&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Deep generative image models using aÔøº laplacian pyramid of adversarial networks (2015), E.Denton et al. &lt;a href=&quot;http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v37/chung15.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. &lt;a href=&quot;https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Pointer networks (2015), O. Vinyals et al. &lt;a href=&quot;http://papers.nips.cc/paper/5866-pointer-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. &lt;a href=&quot;https://arxiv.org/pdf/1506.02078&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Attention-based models for speech recognition (2015), J. Chorowski et al. &lt;a href=&quot;http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;End-to-end memory networks (2015), S. Sukbaatar et al. &lt;a href=&quot;http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Describing videos by exploiting temporal structure (2015), L. Yao et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A neural conversational model (2015), O. Vinyals and Q. Le. &lt;a href=&quot;https://arxiv.org/pdf/1506.05869.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (&lt;a href=&quot;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&quot;&gt;https://www.transacl.org/ojs/index.php/tacl/article/download/570/124&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. &lt;a href=&quot;http://aclweb.org/anthology/P/P15/P15-1033.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. &lt;a href=&quot;http://aclweb.org/anthology/D/D15/D15-1041.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. &lt;a href=&quot;http://aclweb.org/anthology/D/D15/D15-1176.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;em&gt;(~2014)&lt;/em&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. &lt;a href=&quot;https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Recurrent models of visual attention (2014), V. Mnih et al. &lt;a href=&quot;http://arxiv.org/pdf/1406.6247.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. &lt;a href=&quot;https://arxiv.org/pdf/1412.3555&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Addressing the rare word problem in neural machine translation (2014), M. Luong et al. &lt;a href=&quot;https://arxiv.org/pdf/1410.8206&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.&lt;/li&gt; 
 &lt;li&gt;Recurrent neural network regularization (2014), W. Zaremba et al. &lt;a href=&quot;http://arxiv.org/pdf/1409.2329&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Intriguing properties of neural networks (2014), C. Szegedy et al. &lt;a href=&quot;https://arxiv.org/pdf/1312.6199.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v32/graves14.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Scalable object detection using deep neural networks (2014), D. Erhan et al. &lt;a href=&quot;http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Regularization of neural networks using dropconnect (2013), L. Wan et al. &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. &lt;a href=&quot;https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. &lt;a href=&quot;http://www.aclweb.org/anthology/N13-1#page=784&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Large scale distributed deep networks (2012), J. Dean et al. &lt;a href=&quot;http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. &lt;a href=&quot;http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf&quot;&gt;[pdf]&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Acknowledgement&lt;/h2&gt; 
&lt;p&gt;Thank you for all your contributions. Please make sure to read the &lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers/raw/master/Contributing.md&quot;&gt;contributing guide&lt;/a&gt; before you make a pull request.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://creativecommons.org/publicdomain/zero/1.0/&quot;&gt;&lt;img src=&quot;http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg?sanitize=true&quot; alt=&quot;CC0&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;To the extent possible under law, &lt;a href=&quot;https://www.facebook.com/terryum.io/&quot;&gt;Terry T. Um&lt;/a&gt; has waived all copyright and related or neighboring rights to this work.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>OWASP/www-project-top-10-for-large-language-model-applications</title>
      <link>https://github.com/OWASP/www-project-top-10-for-large-language-model-applications</link>
      <description>&lt;p&gt;OWASP Top 10 for Large Language Model Apps (Part of the GenAI Security Project)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/actions/workflows/pages/pages-build-deployment&quot;&gt;&lt;img src=&quot;https://github.com/OWASP/www-project-top-10-for-large-language-model-applications/actions/workflows/pages/pages-build-deployment/badge.svg?branch=main&quot; alt=&quot;pages-build-deployment&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;www-project-top-10-for-large-language-model-applications&lt;/h1&gt; 
&lt;p&gt;OWASP Foundation Web Repository&lt;/p&gt; 
&lt;h1&gt;OWASP Top 10 for Large Language Model Applications&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/posts/wilsonsd_announcing-the-version-2-project-its-time-activity-7157734167244378113-s2v2?utm_source=share&amp;amp;utm_medium=member_ios&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/current_version-v2.0-purple&quot; alt=&quot;Current version in-flight&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://owasp.org/projects/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/owasp-flagship-blue.svg?sanitize=true&quot; alt=&quot;OWASP Flagship Status project&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/4.0/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/License-CC%20BY--SA%204.0-lightgrey.svg?sanitize=true&quot; alt=&quot;License: CC BY-SA 4.0&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://genai.owasp.org&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/officialsite-genai.owasp.org-032CFA.svg?sanitize=true&quot; alt=&quot;genai.owasp.org&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Welcome to the official repository for the OWASP Top 10 for Large Language Model Applications!&lt;/p&gt; 
&lt;h2&gt;About This Repository&lt;/h2&gt; 
&lt;p&gt;This repository contains the OWASP Top 10 for Large Language Model Applications, which is now housed under the comprehensive &lt;strong&gt;OWASP GenAI Security Project&lt;/strong&gt;. The OWASP GenAI Security Project is a global, open-source initiative dedicated to identifying, mitigating, and documenting security and safety risks associated with generative AI technologies.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Visit our main project site:&lt;/strong&gt; &lt;a href=&quot;https://genai.owasp.org&quot;&gt;genai.owasp.org&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Overview and Audience üó£Ô∏è&lt;/h2&gt; 
&lt;p&gt;The OWASP Top 10 for Large Language Model Applications is a standard awareness document for developers and web application security. It represents a broad consensus about the most critical security risks to Large Language Model (LLM) applications. There are other ongoing frameworks both inside and outside of OWASP that are not to be confused with this project and is currently scoped towards only LLM Application Security.&lt;/p&gt; 
&lt;p&gt;Our primary audience is developers, data scientists, and security experts tasked with designing and building applications and plugins leveraging LLM technologies. We aim to provide practical, actionable, and concise security guidance to help these professionals navigate the complex and evolving terrain of LLM application security.&lt;/p&gt; 
&lt;h2&gt;Key Focus üìñ&lt;/h2&gt; 
&lt;p&gt;The primary aim of this project is to provide a comprehensible and adoptable guide to navigate the potential security risks in LLM applications. Our Top 10 list serves as a starting point for developers and security professionals who are new to this domain, and as a reference for those who are more experienced.&lt;/p&gt; 
&lt;h2&gt;Mission Statement üöÄ&lt;/h2&gt; 
&lt;p&gt;Our mission is to make application security visible, so that people and organizations can make informed decisions about application security risks related to LLMs. While our list shares DNA with vulnerability types found in other OWASP Top 10 lists, we do not simply reiterate these vulnerabilities. Instead, we delve into these vulnerabilities&#39; unique implications when encountered in applications utilizing LLMs.&lt;/p&gt; 
&lt;p&gt;Our goal is to bridge the divide between general application security principles and the specific challenges posed by LLMs. The group&#39;s goals include exploring how conventional vulnerabilities may pose different risks or be exploited in novel ways within LLMs and how developers must adapt traditional remediation strategies for applications utilizing LLMs.&lt;/p&gt; 
&lt;h2&gt;Contribution üëã&lt;/h2&gt; 
&lt;p&gt;The first version of this list was contributed by Steve Wilson of Contrast Security. We encourage the community to contribute and help improve the project. If you have any suggestions, feedback or want to help improve the list, feel free to open an issue or send a pull request.&lt;/p&gt; 
&lt;p&gt;We have a working group channel on the &lt;a href=&quot;https://owasp.org/slack/invite&quot;&gt;OWASP Slack&lt;/a&gt;, so please sign up and then join us on the #project-top10-llm channel.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Learn how to contribute:&lt;/strong&gt; &lt;a href=&quot;https://genai.owasp.org/contribute/&quot;&gt;https://genai.owasp.org/contribute/&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This project is licensed under the terms of the &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/4.0/&quot;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support OWASP!&lt;/h2&gt; 
&lt;picture&gt; 
 &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;
      https://api.star-history.com/svg?repos=OWASP/www-project-top-10-for-large-language-model-applications&amp;amp;type=Date&amp;amp;theme=dark
    &quot; /&gt; 
 &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;
      https://api.star-history.com/svg?repos=OWASP/www-project-top-10-for-large-language-model-applications&amp;amp;type=Date
    &quot; /&gt; 
 &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=OWASP/www-project-top-10-for-large-language-model-applications&amp;amp;type=Date&quot; /&gt; 
&lt;/picture&gt;</description>
    </item>
    
    <item>
      <title>latex3/latex2e</title>
      <link>https://github.com/latex3/latex2e</link>
      <description>&lt;p&gt;The LaTeX2e kernel&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The LaTeX2e Kernel Code Repository&lt;/h1&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Important notice:&lt;/strong&gt; This repository holds the unpackaged sources of LaTeX2e as well as code under development in various branches. For this reason it is only of interest to a small number of developers in this form and building a working version from the sources is a non-trivial exercise.&lt;/p&gt; 
 &lt;p&gt;The normal way to obtain LaTeX is therefore not to get it from this repository, but through the packaged version available from &lt;a href=&quot;https://ctan.org&quot;&gt;https://ctan.org&lt;/a&gt; and automatically distributed as part of major TeX distributions such as TeXLive, MacTeX or MiKTeX.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;This repository hosts development of the core LaTeX distribution, which comprises:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The LaTeX kernel itself (&lt;code&gt;base&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;LaTeX team documentation (&lt;code&gt;doc&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Bundles which must be available (&lt;code&gt;required&lt;/code&gt;). These are 
  &lt;ul&gt; 
   &lt;li&gt;Essential tools (&lt;code&gt;tools&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;Core graphics and color support (&lt;code&gt;graphics&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;Key mathematics support (&lt;code&gt;amsmath&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;First aid for LaTeX (&lt;code&gt;firstaid&lt;/code&gt;)&lt;/li&gt; 
   &lt;li&gt;LaTeX laboratory (&lt;code&gt;latex-lab&lt;/code&gt;)&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The main public Git repository is hosted on &lt;a href=&quot;https://github.com/latex3/latex2e&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Note that Babel moved to its own repository in 2019: &lt;a href=&quot;https://github.com/latex3/babel&quot;&gt;GitHub&lt;/a&gt;; any issues related to Babel should be reported there.&lt;/p&gt; 
&lt;p&gt;From 2020 onwards LaTeX includes the L3 programming layer as part of the format. The code for this layer is hosted in its own repository: &lt;a href=&quot;https://github.com/latex3/latex3&quot;&gt;GitHub&lt;/a&gt;; any issues directly related to commands from that layer should preferably be reported there.&lt;/p&gt; 
&lt;h2&gt;LaTeX Version number&lt;/h2&gt; 
&lt;p&gt;The LaTeX version is defined in the file &lt;code&gt;ltvers.dtx&lt;/code&gt; in the two commands &lt;code&gt;\fmtversion&lt;/code&gt; (the main version) and &lt;code&gt;\patch@level&lt;/code&gt; (the patch level). A negative patch level indicates a pretest version.&lt;/p&gt; 
&lt;p&gt;Each component of the core distribution contains a &lt;code&gt;README&lt;/code&gt; file which is tagged with the appropriate release string prior to upload to CTAN.&lt;/p&gt; 
&lt;h2&gt;Issues&lt;/h2&gt; 
&lt;p&gt;Only issues &lt;em&gt;specifically related to these components&lt;/em&gt; should be logged &lt;a href=&quot;https://github.com/latex3/latex2e/issues&quot;&gt;with the team on GitHub&lt;/a&gt;. The LaTeX ecosystem is large, and there are &lt;em&gt;many&lt;/em&gt; (thousands) of additional packages not maintained by us: issues related to the use of those need to be reported to the relevant maintainers because we are usually unable to help in that case.&lt;/p&gt; 
&lt;p&gt;To help you making the right decision where to report an issue we ask to start your minimal example file showing the problem &lt;em&gt;always&lt;/em&gt; with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;\RequirePackage{latexbug}
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Please look at the report generated from &lt;code&gt;latexbug&lt;/code&gt; and if it indicates that you are using packages not maintained by the LaTeX Project check if your problem is still present after removing them. If so contact the maintainers of these third-party packages and file a bug report with them.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details on creating issue reports for the core LaTeX distribution are given in our &lt;a href=&quot;https://raw.githubusercontent.com/latex3/latex2e/develop/CONTRIBUTING.md&quot;&gt;CONTRIBUTING guide&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Support&lt;/h2&gt; 
&lt;p&gt;We are unable to provide advice/support here: community sites such as &lt;a href=&quot;http://tex.stackexchange.com&quot;&gt;TeX-LaTeX StackExchange&lt;/a&gt; and &lt;a href=&quot;http://latex-community.org&quot;&gt;The LaTeX Community&lt;/a&gt; are available for general help. See also &lt;a href=&quot;https://www.latex-project.org/help&quot;&gt;the help pages on our website&lt;/a&gt; for further suggestions.&lt;/p&gt; 
&lt;h2&gt;Code fixes&lt;/h2&gt; 
&lt;p&gt;Changes to the core LaTeX distribution have to be approached bearing in mind the importance of maintaining stability. This means that all changes have to be carefully weighed up, balancing the issues addressed by a change with the effects on existing documents.&lt;/p&gt; 
&lt;h2&gt;Development team&lt;/h2&gt; 
&lt;p&gt;The LaTeX kernel is developed by &lt;a href=&quot;https://latex-project.org&quot;&gt;The LaTeX Project&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Copyright&lt;/h2&gt; 
&lt;p&gt;This README file is&lt;/p&gt; 
&lt;p&gt;Copyright (C) 2019-2025 The LaTeX Project&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>zhanwen/MathModel</title>
      <link>https://github.com/zhanwen/MathModel</link>
      <description>&lt;p&gt;Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°ÔºåÊú¨ÁßëÁîüÊï∞Â≠¶Âª∫Ê®°„ÄÅÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ‰ºòÁßÄËÆ∫ÊñáÔºåÊï∞Â≠¶Âª∫Ê®°ÁÆóÊ≥ïÔºåLaTeXËÆ∫ÊñáÊ®°ÊùøÔºåÁÆóÊ≥ïÊÄùÁª¥ÂØºÂõæÔºåÂèÇËÄÉ‰π¶Á±çÔºåMatlabËΩØ‰ª∂ÊïôÁ®ãÔºåPPT&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Êï∞Â≠¶Âª∫Ê®°ËµÑÊ∫ê&lt;/h1&gt; 
&lt;h2&gt;Êõ¥Êñ∞Ê®°Âùó&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://pan.baidu.com/s/1uxhi5n47ZsLm9fU1xqpS3g&quot;&gt;2024Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;Ôºåkey:&lt;code&gt;opr4&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2025%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2025Âπ¥Êï∞Ê®°ÊÇâÁü•&amp;amp;ËÆ∫ÊñáÊ®°ÁâàÔºàWord/LatexÔºâ&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2024Âπ¥ÂõΩËµõËØïÈ¢ò&lt;/a&gt;&lt;/strong&gt;Ôºà24Âπ¥‰ºòÁßÄËÆ∫ÊñáÁ®çÂêéÊõ¥Êñ∞Ôºâ&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2024%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;2024Âπ¥Ëé∑Â•ñÂêçÂçï&lt;/a&gt;&lt;/strong&gt;(24.11.27ÂÖ¨Â∏É)&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel?tab=readme-ov-file#%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2024Âπ¥Êï∞Â≠¶Âª∫Ê®°ËµõÈ¢ò&lt;/a&gt;&lt;/strong&gt;(Â§áÊ≥®Ôºö2023Âπ¥ÊòØ11.14Âá∫ÁöÑÊØîËµõÁªìÊûú)&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ 2024ËµõÈ¢ò&lt;a href=&quot;https://pan.baidu.com/s/1JIH0EbDA0xFef53AIHMymA&quot;&gt;ÁôæÂ∫¶‰∫ëÁΩëÁõò&lt;/a&gt;Ôºåkey:&lt;code&gt;v44w&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ÊèêÈÜí &lt;strong&gt;&lt;a href=&quot;https://www.shumo.com/home/html/4561.html&quot;&gt;Ë¶ÅÂèäÊó∂ÂÖ≥Ê≥®ËÆ∫ÂùõËÆ®ËÆ∫ÂÜÖÂÆπ&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2024%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2024Âπ¥Êï∞Ê®°ÊÇâÁü•&amp;amp;ËÆ∫ÊñáÊ®°ÁâàÔºàWord/LatexÔºâ&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2023Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;ÊâÄÊúâÊñá‰ª∂‰∏ãËΩΩÂú∞ÂùÄÊõ¥Êñ∞Âà∞ÂæÆÁõòÔºö&lt;a href=&quot;https://share.weiyun.com/Lk0sE1o4&quot;&gt;ÂæÆÁõò‰∏ãËΩΩ&lt;/a&gt;ÔºåkeyÔºö&lt;code&gt;uzw9mf&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;ÂêåÊ≠•ÊâÄÊúâÊñá‰ª∂Âà∞&lt;a href=&quot;https://pan.baidu.com/s/1dOl-MRXtkLBU2l_UWPpYdA&quot;&gt;ÁôæÂ∫¶‰∫ëÁΩëÁõò&lt;/a&gt;Ôºåkey:&lt;code&gt;bxdy&lt;/code&gt;Ôºà2020Âπ¥6Êúà24Êó•Ôºâ&lt;/li&gt; 
 &lt;li&gt;ÁôæÂ∫¶‰∫ëÂ§áÁî®ÈìæÊé•1: &lt;a href=&quot;https://pan.baidu.com/s/1t6KIxwGky0p_1STrIxOQMQ&quot;&gt;ÁÇπÂáª‰∏ãËΩΩ&lt;/a&gt;Ôºåkey:&lt;code&gt;0rm9&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ÁôæÂ∫¶‰∫ëÂ§áÁî®ÈìæÊé•2: &lt;a href=&quot;https://pan.baidu.com/s/1nO2oCk2_Dt8ED7Ff1IZ61w&quot;&gt;ÁÇπÂáª‰∏ãËΩΩ&lt;/a&gt;Ôºåkey:&lt;code&gt;5s6y&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;ÁôæÂ∫¶‰∫ëÂ§áÁî®ÈìæÊé•3: &lt;a href=&quot;https://pan.baidu.com/s/1eOq1-YcU3OGvlsenNQrmXw&quot;&gt;ÁÇπÂáª‰∏ãËΩΩ&lt;/a&gt;Ôºåkey:&lt;code&gt;29vl&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;2025 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®° (Êñ∞)&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2025%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2025 LaTex ËÆ∫ÊñáÊ®°Áâà&lt;/a&gt;‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/README.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;ÔºàLatex Ê®°Áâà‰∏ÄËà¨Âú®ÂÆòÊñπÁªôÂá∫ word Ê®°ÁâàÂêéÊõ¥Êñ∞ÔºåÊó∂Èó¥Â§ßÊ¶ÇÂú®ÊØîËµõÂâç‰∏ÄÊòüÊúüÂ∑¶Âè≥Ôºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨‰∫åÂçÅ‰∫åÂ±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;ÔºàÂè™ÂÖÅËÆ∏2025Á∫ßË∑®Ê†°ÁªÑÈòüÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏Ä‰∏ã„ÄÇÔºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà15Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà16Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà17Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Âä†ÂØÜËµõÈ¢òÂºÄÂßã‰∏ãËΩΩÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà20Êó•8:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ËµõÈ¢òËß£ÂØÜÂØÜÁ†ÅÂÖ¨Â∏ÉÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà21Êó•8:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà21Êó•8:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†ËÆ∫ÊñáMD5Á†ÅÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà24Êó•12:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†PDFÊ†ºÂºèËÆ∫ÊñáÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà25Êó•14:00‚Äî‚Äî9Êúà26Êó•24:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÈôÑ‰ª∂‰∏ä‰º†ÂºÄÂßãÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà27Êó•08:00‚Äî‚Äî9Êúà28Êó•24:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Êü•ÁúãËÆ∫ÊñáÂºÄÂßãÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2025Âπ¥9Êúà27Êó•08:00‚Äî‚Äî9Êúà28Êó•12:00&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master#%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%8E%86%E5%8F%B2%E6%9B%B4%E6%96%B0%E7%89%88%E6%9C%AC&quot;&gt;&lt;strong&gt;ÂéÜÂπ¥Êï∞Ê®°ÊØîËµõÊó∂Èó¥Ë°®&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;2024 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;LaTex ËÆ∫ÊñáÊ®°Áâà‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/README.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;ÔºàLatex Ê®°Áâà‰∏ÄËà¨Âú®ÂÆòÊñπÁªôÂá∫ word Ê®°ÁâàÂêéÊõ¥Êñ∞ÔºåÊó∂Èó¥Â§ßÊ¶ÇÂú®ÊØîËµõÂâç‰∏ÄÊòüÊúüÂ∑¶Âè≥Ôºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨‰∫åÂçÅ‰∏ÄÂ±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;ÔºàÂè™ÂÖÅËÆ∏2024Á∫ßË∑®Ê†°ÁªÑÈòüÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏Ä‰∏ã„ÄÇÔºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà15Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà16Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà17Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Âä†ÂØÜËµõÈ¢òÂºÄÂßã‰∏ãËΩΩÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà20Êó•8:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ËµõÈ¢òËß£ÂØÜÂØÜÁ†ÅÂÖ¨Â∏ÉÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà21Êó•8:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà21Êó•8:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†ËÆ∫ÊñáMD5Á†ÅÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà24Êó•12:00‚Äî‚Äî9Êúà25Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†PDFÊ†ºÂºèËÆ∫ÊñáÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà25Êó•14:00‚Äî‚Äî9Êúà26Êó•24:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÈôÑ‰ª∂‰∏ä‰º†ÂºÄÂßãÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà27Êó•08:00‚Äî‚Äî9Êúà28Êó•24:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;Êü•ÁúãËÆ∫ÊñáÂºÄÂßãÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2024Âπ¥9Êúà27Êó•08:00‚Äî‚Äî9Êúà28Êó•12:00&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master#%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%8E%86%E5%8F%B2%E6%9B%B4%E6%96%B0%E7%89%88%E6%9C%AC&quot;&gt;&lt;strong&gt;ÂéÜÂπ¥Êï∞Ê®°ÊØîËµõÊó∂Èó¥Ë°®&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;2023 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;LaTex ËÆ∫ÊñáÊ®°Áâà‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/README.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;ÔºàLatex Ê®°Áâà‰∏ÄËà¨Âú®ÂÆòÊñπÁªôÂá∫ word Ê®°ÁâàÂêéÊõ¥Êñ∞ÔºåÊó∂Èó¥Â§ßÊ¶ÇÂú®ÊØîËµõÂâç‰∏ÄÊòüÊúüÂ∑¶Âè≥Ôºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨‰∫åÂçÅÂ±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;ÔºàÂè™ÂÖÅËÆ∏2023Á∫ßË∑®Ê†°ÁªÑÈòüÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏Ä‰∏ã„ÄÇÔºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2023Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà17Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2023Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà18Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2023Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà19Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2023Âπ¥9Êúà22Êó•8:00‚Äî‚Äî9Êúà26Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†ËÆ∫ÊñáMD5Á†ÅÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2023Âπ¥9Êúà25Êó•12:00‚Äî‚Äî9Êúà26Êó•12:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†PDFÊ†ºÂºèËÆ∫ÊñáÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2023Âπ¥9Êúà26Êó•14:00‚Äî‚Äî9Êúà27Êó•24:00&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master#%E4%BB%A5%E4%B8%8B%E6%98%AF%E5%8E%86%E5%8F%B2%E6%9B%B4%E6%96%B0%E7%89%88%E6%9C%AC&quot;&gt;&lt;strong&gt;ÂéÜÂπ¥Êï∞Ê®°ÊØîËµõÊó∂Èó¥Ë°®&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;‰∏ãËΩΩ‰∏é‰ΩøÁî®ÔºàÁî±‰∫éÊï¥‰∏™È°πÁõÆÁõ¥Êé•‰∏ãËΩΩÊØîËæÉÊÖ¢ÔºåÂèØ‰ª•ÁúãÊñπÂºèÂõõÔºâ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;ÊñπÂºè‰∏ÄÔºö‰ΩøÁî® &lt;code&gt;git&lt;/code&gt; ‰∏ãËΩΩ„ÄÇ&lt;br /&gt; &lt;code&gt;git clone https://github.com/zhanwen/MathModel.git&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;ÊñπÂºè‰∫åÔºöÁõ¥Êé•‰∏ãËΩΩÂéãÁº©ÂåÖ„ÄÇ&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/downloaddemo.gif&quot; height=&quot;250&quot; width=&quot;500&quot; align=&quot;center&quot; /&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊñπÂºè‰∏â 
  &lt;ul&gt; 
   &lt;li&gt;ÂèØ‰ª•Âçï‰∏™Êñá‰ª∂‰∏ãËΩΩÔºåÈÄâÊã©Ëá™Â∑±ÈúÄË¶ÅÁöÑÊüêÁØáËÆ∫ÊñáÔºåÁõ¥Êé•Âú®ÂØπÂ∫îÁöÑÈ°µÈù¢ÁÇπÂáª‰∏ãËΩΩÂç≥ÂèØ„ÄÇ&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/download3.gif&quot; height=&quot;250&quot; width=&quot;500&quot; align=&quot;center&quot; /&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÊñπÂºèÂõõÔºöÁôæÂ∫¶‰∫ë‰∏ãËΩΩÔºàÊé®ËçêÔºâ 
  &lt;ul&gt; 
   &lt;li&gt;‰ΩøÁî®ÁôæÂ∫¶‰∫ë‰∏ãËΩΩÔºåÊ≠£Â∏∏ÁöÑÂÆ¢Êà∑Á´Ø‰ºöÂá∫Áé∞ÈôêÈÄüÔºåÂØºËá¥‰∏ãËΩΩÁöÑÂæàÊÖ¢ÔºåËøôÈáåÁªôÂ§ßÂÆ∂Êé®Ëçê‰∏Ä‰∏™ÁªïËøáÁôæÂ∫¶‰∫ë‰∏ãËΩΩÈôêÈÄüÁöÑÊñπÂºè„ÄÇÂÖ∑‰ΩìÊÄé‰πà‰∏ãËΩΩÔºåËØ∑ÂèÇÁÖß &lt;a href=&quot;https://github.com/GangZhuo/BaiduPCS&quot;&gt;ÁªïËøáÈôêÈÄü&lt;/a&gt;„ÄÇ&lt;/li&gt; 
   &lt;li&gt;ËØ•È°πÁõÆÁöÑÁôæÂ∫¶‰∫ëÈìæÊé• &lt;a href=&quot;https://pan.baidu.com/s/1wcCcc8pICGx5mBiwP6jwnw&quot;&gt;https://pan.baidu.com/s/1wcCcc8pICGx5mBiwP6jwnw&lt;/a&gt;ÔºåÂØÜÁ†ÅÔºö&lt;code&gt;7sog&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Êõ¥Êñ∞ÔºöËøô‰∏™ÈìæÊé•Â§±ÊïàÊ≤°ÂΩ±ÂìçÔºåÁõ¥Êé•ËÆøÈóÆËøô‰∏™ÁΩëÂùÄ&lt;a href=&quot;https://github.com/GangZhuo/BaiduPCS&quot;&gt;ÁªïËøáÈôêÈÄü&lt;/a&gt;ÔºåÁúã README ÈÉ®ÂàÜÔºåÊúâÈíàÂØπ‰∏çÂêåÊìç‰ΩúÁ≥ªÁªüÂÆâË£ÖÊñπÊ≥ïÔºåÊ†πÊçÆËá™Â∑±ÁöÑÊìç‰ΩúÁ≥ªÁªüÂÆâË£ÖÂç≥ÂèØÔºåÂ§ßÈÉ®ÂàÜÊñπÂºèÈÉΩÊòØÂ∞ÜÈ°πÁõÆgit cloneÔºåÂú®ÂêéÁª≠„ÄÇ&lt;/strong&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂõΩËµõËØïÈ¢ò&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2024Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/A&quot;&gt;AÈ¢òÔºöÈ£éÁîµÂú∫ÊúâÂäüÂäüÁéá‰ºòÂåñÂàÜÈÖç&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/B&quot;&gt;BÈ¢òÔºöWLAN ÁªÑÁΩë‰∏≠ÁΩëÁªúÂêûÂêêÈáèÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/C&quot;&gt;CÈ¢òÔºöÊï∞ÊçÆÈ©±Âä®‰∏ãÁ£ÅÊÄßÂÖÉ‰ª∂ÁöÑÁ£ÅËäØÊçüËÄóÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/D&quot;&gt;DÈ¢òÔºöÂ§ßÊï∞ÊçÆÈ©±Âä®ÁöÑÂú∞ÁêÜÁªºÂêàÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/E&quot;&gt;EÈ¢òÔºöÈ´òÈÄüÂÖ¨Ë∑ØÂ∫îÊÄ•ËΩ¶ÈÅìÁ¥ßÊÄ•ÂêØÁî®Ê®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2024%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/F&quot;&gt;FÈ¢òÔºöX Â∞ÑÁ∫øËÑâÂÜ≤ÊòüÂÖâÂ≠êÂà∞ËææÊó∂Èó¥Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2023Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/A&quot;&gt;AÈ¢òÔºöWLANÁΩëÁªú‰ø°ÈÅìÊé•ÂÖ•Êú∫Âà∂Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/B&quot;&gt;BÈ¢òÔºöDFTÁ±ªÁü©ÈòµÁöÑÊï¥Êï∞ÂàÜËß£ÈÄºËøë&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/C&quot;&gt;CÈ¢òÔºöÊûÅÂ∑ÆÁöÑÂÆö‰πâÂèäÊ†áÂáÜÂàÜÁöÑËÆ°ÁÆóÊñπÊ≥ï&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/D&quot;&gt;DÈ¢òÔºöÂå∫ÂüüÂèåÁ¢≥ÁõÆÊ†á‰∏éË∑ØÂæÑËßÑÂàíÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/E&quot;&gt;EÈ¢òÔºöÂá∫Ë°ÄÊÄßËÑëÂçí‰∏≠‰∏¥Â∫äÊô∫ËÉΩËØäÁñóÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98/F&quot;&gt;FÈ¢òÔºöÂº∫ÂØπÊµÅÈôçÊ∞¥‰∏¥ËøëÈ¢ÑÊä•&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2022%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2022Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2021%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2021Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2020%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2020Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2019%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2019Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2018Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2017%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2017Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2016%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2016Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2015%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2015Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2014%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2014Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2013%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2013Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2012%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2012Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2011%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2011Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2010%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2010Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2009%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2009Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2008%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2008Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2007%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2007Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2006%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2006Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2005%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2005Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2004%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2004Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÂõΩËµõËÆ∫Êñá&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2023Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöWLANÁΩëÁªú‰ø°ÈÅìÊé•ÂÖ•Êú∫Âà∂Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöDFTÁ±ªÁü©ÈòµÁöÑÊï¥Êï∞ÂàÜËß£ÈÄºËøë&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÊûÅÂ∑ÆÁöÑÂÆö‰πâÂèäÊ†áÂáÜÂàÜÁöÑËÆ°ÁÆóÊñπ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÂå∫ÂüüÂèåÁ¢≥ÁõÆÊ†á‰∏éË∑ØÂæÑËßÑÂàíÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöÂá∫Ë°ÄÊÄßËÑëÂçí‰∏≠‰∏¥Â∫äÊô∫ËÉΩËØäÁñóÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2023%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöÂº∫ÂØπÊµÅÈôçÊ∞¥‰∏¥ËøëÈ¢ÑÊä•&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2022Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÁßªÂä®Âú∫ÊôØË∂ÖÂàÜËæ®ÂÆö‰ΩçÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÊñπÂΩ¢‰ª∂ÁªÑÊâπ‰ºòÂåñÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÊ±ΩËΩ¶Âà∂ÈÄ†ÂÖ¨Âè∏Ê∂ÇË£Ö-ÊÄªË£ÖÁºìÂ≠òÂå∫Ë∞ÉÂ∫èË∞ÉÂ∫¶‰ºòÂåñÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöPISAÊû∂ÊûÑËäØÁâáËµÑÊ∫êÊéíÂ∏ÉÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöËçâÂéüÊîæÁâßÁ≠ñÁï•Á†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöCOVID-19Áñ´ÊÉÖÊúüÈó¥ÁîüÊ¥ªÁâ©ËµÑÁöÑÁßëÂ≠¶ÁÆ°ÁêÜÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2021Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÁõ∏ÂÖ≥Áü©ÈòµÁªÑÁöÑ‰ΩéÂ§çÊùÇÂ∫¶ËÆ°ÁÆóÂíåÂ≠òÂÇ®Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÁ©∫Ê∞îË¥®ÈáèÈ¢ÑÊä•‰∫åÊ¨°Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂ∏ïÈáëÊ£ÆÁóÖÁöÑËÑëÊ∑±ÈÉ®ÁîµÂà∫ÊøÄÊ≤ªÁñóÂª∫Ê®°Á†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÊäóËÉ∞ËÖ∫ÁôåÂÄôÈÄâËçØÁâ©ÁöÑ‰ºòÂåñÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºö‰ø°Âè∑Âπ≤Êâ∞‰∏ãÁöÑË∂ÖÂÆΩÂ∏¶ÔºàUWBÔºâÁ≤æÁ°ÆÂÆö‰ΩçÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöËà™Á©∫ÂÖ¨Âè∏Êú∫ÁªÑ‰ºòÂåñÊéíÁè≠ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2020Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂçé‰∏∫È¢ò_ËäØÁâáÁõ∏Âô™ÁÆóÊ≥ïËÆæËÆ°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÊ±ΩÊ≤πËæõÁÉ∑ÂÄºÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÈù¢ÂêëÂ∫∑Â§çÂ∑•Á®ãÁöÑËÑëÁîµ‰ø°Âè∑ÂàÜÊûêÂíåÂà§Âà´Ê®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÊó†‰∫∫Êú∫ÈõÜÁæ§ÂçèÂêåÂØπÊäó&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöËÉΩËßÅÂ∫¶‰º∞ËÆ°‰∏éÈ¢ÑÊµã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöÈ£ûË°åÂô®Ë¥®ÂøÉÂπ≥Ë°°‰æõÊ≤πÁ≠ñÁï•‰ºòÂåñ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2019Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÊó†Á∫øÊô∫ËÉΩ‰º†Êí≠Ê®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂ§©ÊñáÂØºËà™‰∏≠ÁöÑÊòüÂõæËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöËßÜËßâÊÉÖÊä•‰ø°ÊÅØÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÊ±ΩËΩ¶Ë°åÈ©∂Â∑•ÂÜµÊûÑÂª∫&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöÂ§öÂÖ®ÁêÉÂèòÊöñÊ∞îÂÄôÈ¢ÑÊµãÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöÂ§öÁ∫¶ÊùüÊù°‰ª∂‰∏ãÊô∫ËÉΩÈ£ûË°åÂô®Ëà™ËøπÂø´ÈÄüËßÑÂàí&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2018Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂÖ≥‰∫éË∑≥Âè∞Ë∑≥Ê∞¥‰ΩìÂûãÁ≥ªÊï∞ËÆæÁΩÆÁöÑÂª∫Ê®°ÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂÖâ‰º†ÈÄÅÁΩëÂª∫Ê®°‰∏é‰ª∑ÂÄºËØÑ‰º∞&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂØπÊÅêÊÄñË¢≠Âáª‰∫ã‰ª∂ËÆ∞ÂΩïÊï∞ÊçÆÁöÑÈáèÂåñÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÂü∫‰∫éÂç´ÊòüÈ´òÂ∫¶ËÆ°Êµ∑Èù¢È´òÂ∫¶ÂºÇÂ∏∏ËµÑÊñôËé∑ÂèñÊΩÆÊ±êË∞ÉÂíåÂ∏∏Êï∞ÊñπÊ≥ïÂèäÂ∫îÁî®&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöÂ§öÊó†‰∫∫Êú∫ÂØπÁªÑÁΩëÈõ∑ËææÁöÑÂçèÂêåÂπ≤Êâ∞&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2018%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöËà™Á´ôÊ•ºÊâ©Â¢ûËØÑ‰º∞&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2017Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÊó†‰∫∫Êú∫Âú®Êä¢Èô©ÊïëÁÅæ‰∏≠ÁöÑ‰ºòÂåñËøêÁî®&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÈù¢Âêë‰∏ã‰∏Ä‰ª£ÂÖâÈÄö‰ø°ÁöÑ VCSEL ÊøÄÂÖâÂô®‰ªøÁúüÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöËà™Áè≠ÊÅ¢Â§çÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÂü∫‰∫éÁõëÊéßËßÜÈ¢ëÁöÑÂâçÊôØÁõÆÊ†áÊèêÂèñ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöÂ§öÊ≥¢Ê¨°ÂØºÂºπÂèëÂ∞Ñ‰∏≠ÁöÑËßÑÂàíÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöÂú∞‰∏ãÁâ©ÊµÅÁ≥ªÁªüÁΩëÁªú&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2016Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂ§öÊó†‰∫∫Êú∫ÂçèÂêå‰ªªÂä°ËßÑÂàí&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂÖ∑ÊúâÈÅó‰º†ÊÄßÁñæÁóÖÂíåÊÄßÁä∂ÁöÑÈÅó‰º†‰ΩçÁÇπÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂü∫‰∫éÊó†Á∫øÈÄö‰ø°Âü∫Á´ôÁöÑÂÆ§ÂÜÖ‰∏âÁª¥ÂÆö‰ΩçÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÂÜõ‰∫ãË°åÂä®ÈÅøÁ©∫‰æ¶ÂØüÁöÑÊó∂Êú∫ÂíåË∑ØÁ∫øÈÄâÊã©&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöÁ≤ÆÈ£üÊúÄ‰ΩéÊî∂Ë¥≠‰ª∑ÊîøÁ≠ñÈóÆÈ¢òÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2015Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÊ∞¥Èù¢Ëà∞ËâáÁºñÈòüÈò≤Á©∫Âíå‰ø°ÊÅØÂåñÊàò‰∫âËØÑ‰º∞Ê®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÊï∞ÊçÆÁöÑÂ§öÊµÅÂΩ¢ÁªìÊûÑÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÁßªÂä®ÈÄö‰ø°‰∏≠ÁöÑÊó†Á∫ø‰ø°ÈÅì‚ÄúÊåáÁ∫π‚ÄùÁâπÂæÅÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÈù¢ÂêëËäÇËÉΩÁöÑÂçï/Â§öÂàóËΩ¶‰ºòÂåñÂÜ≥Á≠ñÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºöÊï∞ÊéßÂä†Â∑•ÂàÄÂÖ∑ËøêÂä®ÁöÑ‰ºòÂåñÊéßÂà∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöÊóÖÊ∏∏Ë∑ØÁ∫øËßÑÂàíÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2014Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂ∞èÈº†ËßÜËßâÊÑüÂèóÂå∫Áîµ‰Ωç‰ø°Âè∑(LFP)‰∏éËßÜËßâÂà∫ÊøÄ‰πãÈó¥ÁöÑÂÖ≥Á≥ªÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÊú∫Âä®ÁõÆÊ†áÁöÑË∑üË∏™‰∏éÂèçË∑üË∏™&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÊó†Á∫øÈÄö‰ø°‰∏≠ÁöÑÂø´Êó∂Âèò‰ø°ÈÅìÂª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºö‰∫∫‰ΩìËê•ÂÖªÂÅ•Â∫∑ËßíÂ∫¶ÁöÑ‰∏≠ÂõΩÊûúËî¨ÂèëÂ±ïÊàòÁï•Á†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºö‰πòÁî®ËΩ¶Áâ©ÊµÅËøêËæìËÆ°ÂàíÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2013Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂèòÂæ™ÁéØÂèëÂä®Êú∫ÈÉ®‰ª∂Ê≥ïÂª∫Ê®°Âèä‰ºòÂåñ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂäüÁéáÊîæÂ§ßÂô®ÈùûÁ∫øÊÄßÁâπÊÄßÂèäÈ¢ÑÂ§±ÁúüÊ®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂæÆËúÇÁ™ùÁéØÂ¢É‰∏≠Êó†Á∫øÊé•Êî∂‰ø°Âè∑ÁöÑÁâπÊÄßÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÁ©∫Ê∞î‰∏≠PM2.5ÈóÆÈ¢òÁöÑÁ†îÁ©∂ attachment&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/E&quot;&gt;EÈ¢òÔºö‰∏≠Á≠âÊî∂ÂÖ•ÂÆö‰Ωç‰∏é‰∫∫Âè£Â∫¶ÈáèÊ®°ÂûãÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/F&quot;&gt;FÈ¢òÔºöÂèØÊåÅÁª≠ÁöÑ‰∏≠ÂõΩÂüé‰π°Â±ÖÊ∞ëÂÖªËÄÅ‰øùÈô©‰ΩìÁ≥ªÁöÑÊï∞Â≠¶Ê®°ÂûãÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2012Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂü∫Âõ†ËØÜÂà´ÈóÆÈ¢òÂèäÂÖ∂ÁÆóÊ≥ïÂÆûÁé∞&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂü∫‰∫éÂç´ÊòüÊó†Ê∫êÊé¢ÊµãÁöÑÁ©∫Èó¥È£ûË°åÂô®‰∏ªÂä®ÊÆµËΩ®ÈÅì‰º∞ËÆ°‰∏éËØØÂ∑ÆÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÊúâÊùÜÊäΩÊ≤πÁ≥ªÁªüÁöÑÊï∞Â≠¶Âª∫Ê®°ÂèäËØäÊñ≠&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÂü∫‰∫éÂç´Êòü‰∫ëÂõæÁöÑÈ£éÂ§±Âú∫(‰∫ëÂØºÈ£é)Â∫¶ÈáèÊ®°Âûã‰∏éÁÆóÊ≥ïÊé¢ËÆ®&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2011Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂü∫‰∫éÂÖâÁöÑÊ≥¢Á≤í‰∫åË±°ÊÄß‰∏ÄÁßçÁåúÊÉ≥ÁöÑÊï∞Â≠¶‰ªøÁúü&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂê∏Ê≥¢ÊùêÊñô‰∏éÂæÆÊ≥¢ÊöóÂÆ§ÈóÆÈ¢òÁöÑÊï∞Â≠¶Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂ∞èÈ∫¶ÂèëËÇ≤ÂêéÊúüËåéÊùÜÊäóÂÄíÊÄßÁöÑÊï∞Â≠¶Ê®°Âûã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÊàøÂú∞‰∫ßË°å‰∏öÁöÑÊï∞Â≠¶Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2010Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÁ°ÆÂÆöËÇøÁò§ÁöÑÈáçË¶ÅÂü∫Âõ†‰ø°ÊÅØ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºö‰∏éÂ∞ÅÂ†µÊ∏çÂè£ÊúâÂÖ≥ÁöÑÈáçÁâ©ËêΩÊ∞¥ÂêéËøêÂä®ËøáÁ®ãÁöÑÊï∞Â≠¶Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÁ•ûÁªèÂÖÉÁöÑÂΩ¢ÊÄÅÂàÜÁ±ªÂíåËØÜÂà´&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÁâπÊÆäÂ∑•‰ª∂Á£®ÂâäÂä†Â∑•ÁöÑÊï∞Â≠¶Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2009Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÊàëÂõΩÂ∞±‰∏ö‰∫∫Êï∞ÊàñÂüéÈïáÁôªËÆ∞Â§±‰∏öÁéáÁöÑÊï∞Â≠¶Âª∫Ê®°&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÊû™ÂºπÂ§¥ÁóïËøπÔºåËá™Âä®ÊØîÂØπÊñπÊ≥ïÁöÑÁ†îÁ©∂&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂ§ö‰º†ÊÑüÂô®Êï∞ÊçÆËûçÂêà‰∏éËà™ËøπÈ¢ÑÊµã&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºö110 Ë≠¶ËΩ¶ÈÖçÁΩÆÂèäÂ∑°ÈÄªÊñπÊ°à&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2008Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÊ±∂Â∑ùÂú∞Èúá‰∏≠ÂîêÂÆ∂Â±±Â†™Â°ûÊπñÊ≥ÑÊ¥™ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÂüéÂ∏ÇÈÅìË∑Ø‰∫§ÈÄö‰ø°Âè∑ÂÆûÊó∂ÊéßÂà∂ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;CÈ¢òÔºöË¥ßËøêÂàóËΩ¶ÁöÑÁºñÁªÑË∞ÉÂ∫¶ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;&quot;&gt;DÈ¢òÔºö‰∏≠Â§ÆÁ©∫Ë∞ÉÁ≥ªÁªüËäÇËÉΩËÆæËÆ°ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2007Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂª∫Á´ãÈ£üÂìÅÂç´ÁîüÂÆâÂÖ®‰øùÈöú‰ΩìÁ≥ªÊï∞Â≠¶Ê®°ÂûãÂèäÊîπËøõÊ®°ÂûãÁöÑËã•Âπ≤ÁêÜËÆ∫ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÊ¢∞ËáÇËøêÂä®Ë∑ØÂæÑËÆæËÆ°ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÊé¢ËÆ®ÊèêÈ´òÈ´òÈÄüÂÖ¨Ë∑ØË∑ØÈù¢Ë¥®ÈáèÁöÑÊîπËøõÊñπÊ°à&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÈÇÆÊîøËøêËæìÁΩëÁªú‰∏≠ÁöÑÈÇÆË∑ØËßÑÂàíÂíåÈÇÆËΩ¶Ë∞ÉËøê&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2006Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöAd Hoc ÁΩëÁªú‰∏≠ÁöÑÂå∫ÂüüÂàíÂàÜÂíåËµÑÊ∫êÂàÜÈÖçÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÁ°ÆÂÆöÈ´òÁ≤æÂ∫¶ÂèÇÊï∞ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÁª¥‰øÆÁ∫øÊÄßÊµÅÈáèÈòÄÊó∂ÁöÑÂÜÖÁ≠íËÆæËÆ°ÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÂ≠¶ÁîüÈù¢ËØïÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2005Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöHighway Traveling time Estimate and Optimal Routing&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºöÁ©∫‰∏≠Âä†Ê≤π&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂüéÂ∏Ç‰∫§ÈÄöÁÆ°ÁêÜ‰∏≠ÁöÑÂá∫ÁßüËΩ¶ËßÑÂàí&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºö‰ªìÂ∫ìÂÆπÈáèÊúâÈôêÊù°‰ª∂‰∏ãÁöÑÈöèÊú∫Â≠òË¥ÆÁÆ°ÁêÜ&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2004Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/A&quot;&gt;AÈ¢òÔºöÂèëÁé∞ÈªÑÁêÉÂπ∂ÂÆö‰Ωç&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/B&quot;&gt;BÈ¢òÔºö‰ΩøÁî®‰∏ãÊñôÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/C&quot;&gt;CÈ¢òÔºöÂîÆÂêéÊúçÂä°Êï∞ÊçÆÁöÑËøêÁî®&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87/D&quot;&gt;DÈ¢òÔºöÁ†îÁ©∂ÁîüÂΩïÂèñÈóÆÈ¢ò&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;‰ª•‰∏ãÊòØÂéÜÂè≤Êõ¥Êñ∞ÁâàÊú¨&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2023%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;2023Âπ¥Ëé∑Â•ñÂêçÂçï&lt;/a&gt;&lt;/strong&gt;(11.14Âá∫ÁöÑÁªìÊûú)&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2023%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2023Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°ËØïÈ¢ò&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2022%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2022 Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/strong&gt;Ôºà2023Âπ¥8Êúà25Êó•Ôºâ&lt;a href=&quot;https://pan.baidu.com/s/1WbDSEvYerB9LqsMTcAQLNg&quot;&gt;‚è¨‰∏ãËΩΩÈìæÊé•&lt;/a&gt;Ôºåkey:&lt;code&gt;i7in&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2023%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2023Âπ¥Êï∞Ê®°Âª∫Ê®° Á¨¨‰∫åÂçÅÂ±äÂ§ßËµõ&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2022%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;2022Âπ¥Ëé∑Â•ñÂêçÂçï&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2022%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2022Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°ÊØîËµõÊ®°ÁâàÔºàWord/LatexÔºâ&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2021%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2021 Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/strong&gt;Ôºà2022Âπ¥6Êúà25Êó•Ôºâ&lt;a href=&quot;https://pan.baidu.com/s/1j0rqd6tvKv4LxGRG0kUaKw&quot;&gt;‚è¨‰∏ãËΩΩÈìæÊé•&lt;/a&gt;Ôºåkey:&lt;code&gt;eyzf&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2022%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2022Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°ÊØîËµõÈÄöÁü•&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2021%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;2021Âπ¥Ëé∑Â•ñÂêçÂçï&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AF%95%E9%A2%98/2021%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AF%95%E9%A2%98&quot;&gt;2021Âπ¥Á†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõËØïÈ¢ò&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2021%E5%B9%B4%E6%95%B0%E6%A8%A1%E6%82%89%E7%9F%A5%26%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2021Âπ¥ËÆ∫ÊñáÊ®°Áâà&lt;/a&gt;&lt;/strong&gt;ÔºàÂåÖÊã¨ word ÁâàÊú¨„ÄÅLatex ÁâàÊú¨Ôºâ&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://www.shumo.com/home/html/category/cpmcm&quot;&gt;Êï∞Â≠¶Âª∫Ê®°ÁΩëËÆ∫Âùõ‰∏äÊèêÈóÆ&lt;/a&gt;&lt;/strong&gt;ÔºàÊØîËµõÊúüÈó¥ÔºåËÆ∫ÂùõËß£Á≠îÔºâ&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2020%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2020 Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/strong&gt;Ôºà2021Âπ¥8Êúà21Êó•Ôºâ&lt;a href=&quot;https://pan.baidu.com/s/1NK3_QXdU6gPH27_gABMUIw&quot;&gt;‚è¨‰∏ãËΩΩÈìæÊé•&lt;/a&gt;Ôºåkey:&lt;code&gt;odt5&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2020%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88&quot;&gt;2020Âπ¥ËÆ∫ÊñáÊ®°Áâà&lt;/a&gt;&lt;/strong&gt;ÔºàÂåÖÊã¨ word ÁâàÊú¨„ÄÅLatex ÁâàÊú¨Ôºâ&lt;/li&gt; 
 &lt;li&gt;Êõ¥Êñ∞ &lt;strong&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E5%9B%BD%E8%B5%9B%E8%AE%BA%E6%96%87/2019%E5%B9%B4%E4%BC%98%E7%A7%80%E8%AE%BA%E6%96%87&quot;&gt;2019 Âπ¥‰ºòÁßÄËÆ∫Êñá&lt;/a&gt;&lt;/strong&gt;Ôºà2020Âπ¥7Êúà21Êó•Ôºâ&lt;a href=&quot;https://pan.baidu.com/s/1xt8R7ad_o7zBEZGZvqA3MA&quot;&gt;‚è¨‰∏ãËΩΩÈìæÊé•&lt;/a&gt;Ôºåkey:&lt;code&gt;2uyl&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;2019Âπ¥‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨ÂçÅÂÖ≠Â±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ‚Äî‚Äî&lt;strong&gt;ÈÄâÈ¢ò„ÄÅÂëΩÈ¢ò‰ªãÁªçÂàÜÊûêÔºö&lt;/strong&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/%E9%80%89%E9%A2%98%E3%80%81%E5%91%BD%E9%A2%98%E4%BB%8B%E7%BB%8D%E5%88%86%E6%9E%90.md&quot;&gt;&lt;strong&gt;ÈÄâÈ¢ò„ÄÅÂëΩÈ¢ò‰ªãÁªçÂàÜÊûê&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;2022 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°&lt;/h2&gt; 
&lt;h4&gt;2022.12.30 ÊØîËµõÁªìÊûúÂÖ¨Á§∫Ôºå&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2022%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;Ëé∑Â•ñÂêçÂçï&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;2022.10.6‚Äî2022.10.10 ÊØîËµõÂ∑≤ÁªèÁªìÊùüÔºåÂ§ßÂÆ∂ËÄêÂøÉÁ≠âÂæÖËé∑Â•ñÂêßÔºà(o^^o)Ôºâ&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;MD5Êñá‰ª∂Ê†°È™åÂíå‰ΩøÁî®ËØ¥ÊòéÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md&quot;&gt;&lt;strong&gt;ËÆ∫ÊñáÊèê‰∫§ÔºàMD5‰ΩøÁî®ÊñπÊ≥ïÔºâ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;LaTex ËÆ∫ÊñáÊ®°Áâà‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/README.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;ÔºàLatex Ê®°Áâà‰∏ÄËà¨Âú®ÂÆòÊñπÁªôÂá∫ word Ê®°ÁâàÂêéÊõ¥Êñ∞ÔºåÊó∂Èó¥Â§ßÊ¶ÇÂú®ÊØîËµõÂâç‰∏ÄÊòüÊúüÂ∑¶Âè≥Ôºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨ÂçÅ‰πùÂ±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;ÔºàÂè™ÂÖÅËÆ∏2022Á∫ßË∑®Ê†°ÁªÑÈòüÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏Ä‰∏ã„ÄÇÔºâ&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2022Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà14Êó•17:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;Êà™Ê≠¢Ôºö2022Âπ¥09Êúà26Êó•17:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2022Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà16Êó•17:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;Êà™Ê≠¢Ôºö2022Âπ¥09Êúà27Êó•17:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2022Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà19Êó•17:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;Êà™Ê≠¢Ôºö2022Âπ¥09Êúà30Êó•17:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2022Âπ¥9Êúà22Êó•8:00‚Äî‚Äî9Êúà26Êó•12:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;2022Âπ¥10Êúà06Êó•8:00‚Äî‚Äî10Êúà10Êó•12:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†ËÆ∫ÊñáMD5Á†ÅÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2022Âπ¥9Êúà25Êó•12:00‚Äî‚Äî9Êúà26Êó•12:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;2022Âπ¥10Êúà09Êó•12:00‚Äî‚Äî10Êúà10Êó•12:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;‰∏ä‰º†PDFÊ†ºÂºèËÆ∫ÊñáÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2022Âπ¥9Êúà26Êó•14:00‚Äî‚Äî9Êúà27Êó•24:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;2022Âπ¥10Êúà10Êó•14:00‚Äî‚Äî10Êúà11Êó•24:00&lt;/font&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;h2&gt;2021 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°&lt;/h2&gt; 
&lt;h4&gt;2021.12.10 ÊØîËµõÁªìÊûúÂÖ¨Á§∫Ôºå&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2021%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;Ëé∑Â•ñÂêçÂçï&lt;/a&gt;&lt;/h4&gt; 
&lt;h4&gt;2021.10.14‚Äî2021.10.28 ÊØîËµõÂ∑≤ÁªèÁªìÊùüÔºåÂ§ßÂÆ∂ËÄêÂøÉÁ≠âÂæÖËé∑Â•ñÂêßÔºà(o^^o)Ôºâ&lt;/h4&gt; 
&lt;h4&gt;2021.10.13 ÊòéÂ§©Êó©ÂÖ´ÁÇπÂºÄÂßãÔºåÁ•ùÂ§ßÂÆ∂ÊØîËµõÂºÄÂøÉ Ôºà^_^Ôºâ&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;MD5Êñá‰ª∂Ê†°È™åÂíå‰ΩøÁî®ËØ¥ÊòéÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md&quot;&gt;&lt;strong&gt;ËÆ∫ÊñáÊèê‰∫§ÔºàMD5‰ΩøÁî®ÊñπÊ≥ïÔºâ&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;LaTex ËÆ∫ÊñáÊ®°Áâà‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/README.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨ÂçÅÂÖ´Â±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;ÔºàÂè™ÂÖÅËÆ∏2021Á∫ßË∑®Ê†°ÁªÑÈòüÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏Ä‰∏ã„ÄÇÔºâ&lt;br /&gt; &lt;font color=&quot;red&quot;&gt;ÂºïÁî®ÂÆòÁΩëÂéüÊñáÔºöÁî±‰∫é‰ªäÂπ¥ÁöÑÊñ∞ÂÜ†Áñ´ÊÉÖÂΩ¢ÂäøÂØºËá¥ÈÉ®ÂàÜÈ´òÊ†°ÁöÑÂ∑•‰ΩúÂÆâÊéíËæÉÂæÄÂπ¥ÂêåÊúüÊúâ‰∏ÄÂÆöÂèòÂåñÔºåÊïôÂ≠¶ÁßëÁ†îËøõÂ∫¶‰πüÁõ∏Â∫î‰ΩúÂá∫Ë∞ÉÊï¥„ÄÇ‰∏∫ËÆ©ÊÑèÂêëÂèÇËµõÁöÑÂ∏àÁîüÊúâÊõ¥Âä†ÂÖÖË£ïÁöÑÊó∂Èó¥Êä•Âêç„ÄÅÂ§áËµõÔºåÁªèÁ†îÁ©∂ÂÜ≥ÂÆöÔºåÁé∞ÂØπÂéüÂÆöÁöÑÂêÑÊó∂Èó¥ËäÇÁÇπ‰∫à‰ª•Âª∂Êúü„ÄÇ&lt;/font&gt;&lt;a href=&quot;https://cpipc.acge.org.cn//cw/detail/4/2c9080127bca23bf017bca55e4060317&quot;&gt;Âª∂Êúü‰∏æÂäûÁ≠âÁõ∏ÂÖ≥‰∫ãÂÆúÁöÑÈÄöÁü•&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2021Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà10Êó•17:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;Êà™Ê≠¢Ôºö2021Âπ¥10Êúà8Êó•17:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2021Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà12Êó•17:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;Êà™Ê≠¢Ôºö2021Âπ¥10Êúà10Êó•17:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2021Âπ¥6Êúà6Êó•8:00‚Äî‚Äî9Êúà13Êó•17:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;Êà™Ê≠¢Ôºö2021Âπ¥10Êúà11Êó•17:00&lt;/font&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;del&gt;&lt;strong&gt;2021Âπ¥9Êúà16Êó•8:00‚Äî‚Äî9Êúà20Êó•12:00&lt;/strong&gt;&lt;/del&gt; &lt;font color=&quot;red&quot;&gt;2021Âπ¥10Êúà14Êó•8:00‚Äî‚Äî10Êúà18Êó•12:00&lt;/font&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.acge.org.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;h2&gt;2020 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°&lt;/h2&gt; 
&lt;h4&gt;2020.11.24 ‰ªäÂπ¥ÊØîÂéªÂπ¥Êôö‰∏§‰∏™Á§ºÊãúÔºåÊØîËµõÁªìÊûúÁªèËøá‰∏Ä‰∏™ÂçäÊúàÁöÑËØÑÂÆ°ÔºåÂú®Ëøô‰∏ÄÂ§©ÂÖ¨Â∏É‰∫Ü&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2020%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;Ëé∑Â•ñÂêçÂçï&lt;/a&gt;ÔºåÂ§ßÂÆ∂ÁöÑÂä™ÂäõÁõ∏‰ø°ÈÉΩ‰ºöÊúâÊâÄÊî∂Ëé∑„ÄÇ‰ΩôÁîüËøòÊúâÂæàÂ§öÊúâÊÑè‰πâÁöÑ‰∫ãÊÉÖÈúÄË¶ÅÊàë‰ª¨ÂéªÂÅöÔºåËÆ©Êàë‰ª¨‰∏ÄËµ∑Âä™ÂäõÂêß„ÄÇ(o^o)&lt;/h4&gt; 
&lt;h4&gt;&lt;font color=&quot;red&quot;&gt;È¢ÑÂëä‰∏Ä‰∏ãÔºåÊåâÁÖßÂæÄÂπ¥ÊàêÁª©ÂÖ¨Â∏ÉÊó∂Èó¥ÔºåÂ∫îËØ•ÊòØÂú®ÂèåÂçÅ‰∏ÄÈÇ£Â§©„ÄÇË¶Å‰πàÂñú‰∏äÂä†ÂñúÔºåË¶Å‰πàÂ∞ë‰π∞ÁÇπ‰∏úË•ø (o^^o)„ÄÇ&lt;/font&gt;&lt;/h4&gt;  
&lt;h4&gt;ËÆ∫ÊñáÊèê‰∫§ÔºàMD5‰ΩøÁî®ÊñπÊ≥ïÔºâ&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;MD5Êñá‰ª∂Ê†°È™åÂíå‰ΩøÁî®ËØ¥ÊòéÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md&quot;&gt;&lt;strong&gt;MD5Êñá‰ª∂Ê†°È™åÂíå‰ΩøÁî®ËØ¥Êòé&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;ËÆ∫ÊñáÊ®°ÁâàÊõ¥Êñ∞&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;LaTex ËÆ∫ÊñáÊ®°Áâà‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/README.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨ÂçÅ‰∏ÉÂ±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;Ôºà‰ªäÂπ¥‰∏çÂÖÅËÆ∏Ë∑®Ê†°ÁªÑÈòüÔºåÂ§ßÂÆ∂Ê≥®ÊÑè‰∏Ä‰∏ã„ÄÇÔºâ&lt;br /&gt; &lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2020Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà10Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2020Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà13Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2020Âπ¥7Êúà1Êó•8:00‚Äî‚Äî9Êúà14Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2020Âπ¥9Êúà17Êó•8:00‚Äî‚Äî9Êúà21Êó•12:00&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;h2&gt;2019 Âπ¥Á†îÁ©∂ÁîüÊï∞Ê®°&lt;/h2&gt; 
&lt;h4&gt;2019.11.11 ÂèàÊòØÂèåÂçÅ‰∏ÄÔºåÊØîËµõÁªìÊûúÁªèËøá‰∏Ä‰∏™ÂçäÊúàÁöÑËØÑÂÆ°ÔºåÂú®Ëøô‰∏ÄÂ§©ÂÖ¨Â∏É‰∫Ü&lt;a href=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/2019%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;Ëé∑Â•ñÂêçÂçï&lt;/a&gt;ÔºåÂ§ßÂÆ∂ÁöÑÂä™ÂäõÁõ∏‰ø°ÈÉΩ‰ºöÊúâÊâÄÊî∂Ëé∑„ÄÇ‰ΩôÁîüËøòÊúâÂæàÂ§öÊúâÊÑè‰πâÁöÑ‰∫ãÊÉÖÈúÄË¶ÅÊàë‰ª¨ÂéªÂÅöÔºåËÆ©Êàë‰ª¨‰∏ÄËµ∑Âä™Âäõ„ÄÇ(o^o)&lt;/h4&gt; 
&lt;h3&gt;2019.9.19‚Äî2019.9.23 ÊØîËµõÂ∑≤ÁªèÁªìÊùüÔºåÂ§ßÂÆ∂ËÄêÂøÉÁ≠âÂæÖËé∑Â•ñÂêßÔºà(o^^o)Ôºâ&lt;/h3&gt; 
&lt;h4&gt;ËÆ∫ÊñáÊèê‰∫§ÔºàMD5‰ΩøÁî®ÊñπÊ≥ïÔºâ&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;MD5Êñá‰ª∂Ê†°È™åÂíå‰ΩøÁî®ËØ¥ÊòéÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/MD5%E6%96%87%E4%BB%B6%E6%A0%A1%E9%AA%8C%E5%92%8C%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E.md&quot;&gt;&lt;strong&gt;MD5Êñá‰ª∂Ê†°È™åÂíå‰ΩøÁî®ËØ¥Êòé&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;ËÆ∫ÊñáÊ®°ÁâàÊõ¥Êñ∞&lt;/h4&gt; 
&lt;p&gt;&lt;em&gt;LaTex ËÆ∫ÊñáÊ®°ÁâàÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/2019%E5%B9%B4Latex%E6%A8%A1%E7%89%88.zip&quot;&gt;&lt;strong&gt;LaTex ËÆ∫ÊñáÊ®°Áâà&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;em&gt;Word ËÆ∫ÊñáÊ®°ÁâàÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/raw/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/%E2%80%9C%E5%8D%8E%E4%B8%BA%E6%9D%AF%E2%80%9D%E7%AC%AC%E5%8D%81%E5%85%AD%E5%B1%8A%E4%B8%AD%E5%9B%BD%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B%E8%AE%BA%E6%96%87%E6%A0%BC%E5%BC%8F%E8%A7%84%E8%8C%83.doc&quot;&gt;&lt;strong&gt;Word ËÆ∫ÊñáÊ®°ÁâàÔºàÂ∑≤Êõ¥Êñ∞ÊúÄÊñ∞Ôºâ&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;em&gt;LaTex ËÆ∫ÊñáÊ®°Áâà‰ΩøÁî®ÊñπÂºèÔºö&lt;/em&gt; &lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2019%E5%B9%B4%E8%AE%BA%E6%96%87%E6%A8%A1%E7%89%88/latex_note.md&quot;&gt;&lt;strong&gt;Â¶Ç‰ΩïÁºñËØë Latex Êñá‰ª∂&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h4&gt;‰∏ãËΩΩÊñπÂºè(‰ªìÂ∫ìÊØîËæÉÂ§ßÔºåÂª∫ËÆÆÂçï‰∏™Êñá‰ª∂‰∏ãËΩΩ)&lt;/h4&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/downloaddemo2.gif&quot; /&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;em&gt;‰∏ªÈ¢òÔºö&lt;/em&gt; &lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/4&quot;&gt;&lt;strong&gt;‚ÄúÂçé‰∏∫ÊùØ‚ÄùÁ¨¨ÂçÅÂÖ≠Â±ä‰∏≠ÂõΩÁ†îÁ©∂ÁîüÊï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;em&gt;Êä•ÂêçÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2019Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà10Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÂÆ°Ê†∏Êó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2019Âπ¥6Êúà1Êó•8:00‚Äî‚Äî9Êúà12Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;‰∫§Ë¥πÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2019Âπ¥7Êúà1Êó•8:00‚Äî‚Äî9Êúà15Êó•17:00&lt;/strong&gt;&lt;br /&gt; &lt;em&gt;ÊØîËµõÊó∂Èó¥Ôºö&lt;/em&gt; &lt;strong&gt;2019Âπ¥9Êúà19Êó•8:00‚Äî‚Äî9Êúà23Êó•12:00&lt;/strong&gt;&lt;/p&gt; 
&lt;h4&gt;ÂÆòÁΩëÊä•ÂêçÂú∞ÂùÄÔºö&lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/4&quot;&gt;ÂÆòÁΩëÂú∞ÂùÄ&lt;/a&gt;&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;h3&gt;2018.9.15 Á•ùÂ§ßÂÆ∂ÊØîËµõÂºÄÂøÉ Ôºà^_^Ôºâ&lt;/h3&gt; 
&lt;h3&gt;2018.9.19 ÊØîËµõÂ∑≤ÁªèÁªìÊùüÔºåÂ§ßÂÆ∂ËÄêÂøÉÁ≠âÂæÖËé∑Â•ñÂêßÔºà^o^Ôºâ&lt;/h3&gt; 
&lt;h4&gt;2018.11.11 ÊØîËµõÁªìÊûúÁªèËøá‰∏Ä‰∏™ÂçäÊúàÁöÑËØÑÂÆ°ÔºåÁªà‰∫éÂú®Êò®Â§©ÂÖ¨Â∏É‰∫Ü&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/2018%E5%B9%B4%E7%A0%94%E7%A9%B6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/2018%E5%B9%B4%E6%9C%80%E7%BB%88%E8%8E%B7%E5%A5%96%E5%90%8D%E5%8D%95&quot;&gt;Ëé∑Â•ñÂêçÂçï&lt;/a&gt;ÔºåÂ§ßÂÆ∂ÁöÑÂä™ÂäõÁõ∏‰ø°ÈÉΩ‰ºöÊúâÊâÄÊî∂Ëé∑„ÄÇ‰ΩôÁîüËøòÊúâÂæàÂ§öÊúâÊÑè‰πâÁöÑ‰∫ãÊÉÖÈúÄË¶ÅÊàë‰ª¨ÂéªÂÅöÔºåËÆ©Êàë‰ª¨‰∏ÄËµ∑Âä™Âäõ„ÄÇ(o^^o)&lt;/h4&gt; 
&lt;hr /&gt; 
&lt;h4&gt;Êõ¥Êñ∞/Ê∑ªÂä†ÊØîËµõÂÆòÁΩëÂú∞ÂùÄ&lt;a href=&quot;https://cpipc.chinadegrees.cn/&quot;&gt;Êà≥ËøôÈáå&lt;/a&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/4&quot;&gt;Êï∞Â≠¶Âª∫Ê®°Á´ûËµõ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/6&quot;&gt;ÁîµÂ≠êËÆæËÆ°Á´ûËµõ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8101510bd&quot;&gt;‰∫∫Â∑•Êô∫ËÉΩÂàõÊñ∞Â§ßËµõ&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://cpipc.chinadegrees.cn/cw/hp/2c9088a5696cbf370169a3f8934810be&quot;&gt;Êú∫Âô®‰∫∫ÂàõÊñ∞ËÆæËÆ°Â§ßËµõ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÁæéËµõËÆ∫Êñá&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2017%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2017Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2016%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2016Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2015%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2015Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2014%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2014Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2013%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2013Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2012%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2012Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2011%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2011Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2010%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2010Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2009%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2009Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2008%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2008Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2007%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2007Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2006%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2006Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2005%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2005Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%BE%8E%E8%B5%9B%E8%AE%BA%E6%96%87/2004%E7%BE%8E%E8%B5%9B%E7%89%B9%E7%AD%89%E5%A5%96%E5%8E%9F%E7%89%88%E8%AE%BA%E6%96%87%E9%9B%86&quot;&gt;2004Âπ¥ÁâπÁ≠âÂ•ñËÆ∫Êñá&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Êï∞Â≠¶Âª∫Ê®°ÁÆóÊ≥ï&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AE%97%E6%B3%95&quot;&gt;ÁªèÂÖ∏ÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95&quot;&gt;Áé∞‰ª£ÁÆóÊ≥ï&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BB%BF%E7%9C%9F&quot;&gt;ËÆ°ÁÆóÊú∫‰ªøÁúü&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%B2%92%E5%AD%90%E7%BE%A4%E7%AE%97%E6%B3%95&quot;&gt;Á≤íÂ≠êÁæ§ÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE&quot;&gt;È©¨Â∞îÂèØÂ§´Èìæ&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%B3%95&quot;&gt;ËíôÁâπÂç°Ê¥õÊ≥ï&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB%E6%B3%95&quot;&gt;Ê®°ÊãüÈÄÄÁÅ´Ê≥ï&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C&quot;&gt;Á•ûÁªèÁΩëÁªú&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90&quot;&gt;Â∞èÊ≥¢ÂàÜÊûê&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E7%8E%B0%E4%BB%A3%E7%AE%97%E6%B3%95/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95&quot;&gt;ÈÅó‰º†ÁÆóÊ≥ï&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;ÊïôÊùêÂèäËØæ‰ª∂&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E5%9B%BD%E9%98%B2%E7%A7%91%E6%8A%80%E6%9C%AF%E5%A4%A7%E5%AD%A6&quot;&gt;ÂõΩÈò≤ÁßëÊäÄÊúØÂ§ßÂ≠¶&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/%E6%95%99%E6%9D%90%E5%8F%8A%E8%AF%BE%E4%BB%B6/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%AF%BE%E4%BB%B6/PPT%E8%AF%BE%E4%BB%B6&quot;&gt;ÊµôÊ±üÂ§ßÂ≠¶ËØæ‰ª∂&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Êï∞Â≠¶Âª∫Ê®°ÁÆóÊ≥ïÊÄùÁª¥ÂØºÂõæ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/Mind&quot;&gt;ÊÄùÁª¥ÂØºÂõæ&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Matlab ÂÖ•Èó®ÊïôÁ®ã&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/zhanwen/MathModel/tree/master/Matlab%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B&quot;&gt;MatlabÂÖ•Èó®ÂíåÂú®Á∫øÊÄß‰ª£Êï∞‰∏≠ÁöÑÂ∫îÁî®&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Â£∞Êòé&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÂÖ∂‰∏≠Êúâ‰∫õÂÜÖÂÆπÊï¥ÁêÜËá™‰∫íËÅîÁΩëÔºåÂ¶ÇÊúâ‰æµÊùÉÔºåËØ∑ËÅîÁ≥ªÔºåÊàëÂ∞ÜÂèäÊó∂Â§ÑÁêÜ„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;‰∏™‰∫∫ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;WellDev&lt;/code&gt;Ôºö‰∏ÄÂêç‰∏çÁæÅÁöÑÂ≠¶ÂÉßÔºåÊàëÁöÑ‰∏ñÁïå‰∏çÂè™ÊúâÂ≠¶ÊúØ„ÄÇ‰∏ÄÊù°Ëø∑ÈÄîÁöÑÂí∏È±ºÔºåÊ≠£Âú®Ê∏∏ÂêëÂ±û‰∫éÂÆÉÁöÑÂ§©Âú∞ÔºÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/donate/common.jpg&quot; width=&quot;150&quot; height=&quot;150&quot; alt=&quot;weixin&quot; /&gt; 
&lt;h3&gt;ËµûÂä©ÂíåÊîØÊåÅ&lt;/h3&gt; 
&lt;p&gt;Ëøô‰∫õÂÜÖÂÆπÈÉΩÊòØÊàëËä±‰∫Ü‰∏çÂ∞ëÊó∂Èó¥Êï¥ÁêÜÂá∫Êù•ÁöÑ, Â¶ÇÊûú‰Ω†ËßâÂæóÂÆÉÂØπ‰Ω†ÂæàÊúâÂ∏ÆÂä©, ËØ∑‰Ω†‰πüÂàÜ‰∫´ÁªôÈúÄË¶ÅÂ≠¶‰π†ÁöÑÊúãÂèã‰ª¨„ÄÇÂ¶ÇÊûú‰Ω†ÁúãÂ•ΩÊàëÁöÑÂÜÖÂÆπÂàÜ‰∫´, ‰πüÂèØ‰ª•ËÄÉËôëÈÄÇÂΩìÁöÑËµûÂä©ÊâìËµè, ËÆ©ÊàëÊúâÊõ¥Â§öÁöÑÂä®ÂäõÂéªÁªßÁª≠ÂàÜ‰∫´Êõ¥Â•ΩÁöÑÂÜÖÂÆπÁªôÂ§ßÂÆ∂„ÄÇ&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;ÂæÆ‰ø°&lt;/th&gt; 
   &lt;th&gt;ÊîØ‰ªòÂÆù&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/donate/weixinpay.jpg&quot; width=&quot;150&quot; height=&quot;150&quot; alt=&quot;pay check&quot; /&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/donate/alipay.jpg&quot; width=&quot;150&quot; height=&quot;150&quot; alt=&quot;pay check&quot; /&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;ËÅîÁ≥ª&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;EmailÔºö&lt;a href=&quot;https://mail.google.com/&quot;&gt;hanwenme@gmail.com&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;WX:ÔºàÊúâ‰ªª‰ΩïÈóÆÈ¢òÈÉΩÂèØ‰ª•Áõ¥Êé•ÊÄºÊàëÔºåÂä†ÁöÑÊó∂ÂÄôÂ§áÊ≥®ËØ¥Êòé‰∏Ä‰∏ãÔºâÔºö&lt;/li&gt; 
&lt;/ul&gt; 
&lt;img src=&quot;https://raw.githubusercontent.com/zhanwen/MathModel/master/images/donate/wechat.png&quot; width=&quot;150&quot; height=&quot;150&quot; alt=&quot;pay check&quot; /&gt;</description>
    </item>
    
    <item>
      <title>dendibakh/perf-book</title>
      <link>https://github.com/dendibakh/perf-book</link>
      <description>&lt;p&gt;The book &quot;Performance Analysis and Tuning on Modern CPU&quot;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&quot;https://twitter.com/dendibakh&quot;&gt;&lt;img src=&quot;https://img.shields.io/twitter/follow/dendibakh&quot; alt=&quot;X (formerly Twitter) Follow&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/github/stars/dendibakh/perf-book&quot; alt=&quot;GitHub Repo stars&quot; /&gt;&lt;/p&gt; 
&lt;h1&gt;&quot;Performance Analysis and Tuning on Modern CPUs&quot; by Denis Bakhvalov, et al.&lt;/h1&gt; 
&lt;h2&gt;Building a book (pdf)&lt;/h2&gt; 
&lt;p&gt;At the moment, building the PDF book only works on Windows and Linux. MacOS requires building some components (e.g. pandoc-crossref) from sources.&lt;/p&gt; 
&lt;p&gt;Requirements:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Python3. Install natsort module: &lt;code&gt;pip install natsort&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://pandoc.org/installing.html&quot;&gt;Pandoc&lt;/a&gt;. Install &lt;a href=&quot;https://github.com/jgm/pandoc/releases/tag/2.9.2.1&quot;&gt;version 2.9&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pandoc-fignos&lt;/code&gt; and &lt;code&gt;pandoc-tablenos&lt;/code&gt;. Run &lt;code&gt;pip install pandoc-fignos pandoc-tablenos&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;pandoc-crossref&lt;/code&gt;. This one requires manual installation. I just downloaded the binary from &lt;a href=&quot;https://github.com/lierdakil/pandoc-crossref/releases/tag/v0.3.6.4&quot;&gt;here&lt;/a&gt; and copied it to the same place where &lt;code&gt;pandoc-fignos&lt;/code&gt; is.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://miktex.org/download&quot;&gt;MiKTeX&lt;/a&gt;. Check &lt;code&gt;Yes&lt;/code&gt; for automatic package installation.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Run:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Linux bash and Windows cmd prompt
python export_book.py &amp;amp;&amp;amp; pdflatex book.tex &amp;amp;&amp;amp; bibtex book &amp;amp;&amp;amp; pdflatex book.tex &amp;amp;&amp;amp; pdflatex book.tex

# Windows powershell
function Run-Block-With-Error($block) {
    $ErrorActionPreference=&quot;Stop&quot;
    Invoke-Command -ScriptBlock $block
}
Run-Block-With-Error {python.exe export_book.py; pdflatex book.tex; bibtex book; pdflatex book.tex; pdflatex book.tex}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;As a result, &lt;code&gt;book.pdf&lt;/code&gt; will be generated. The first compilation may be slow.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/dendibakh/perf-book/main/LICENSE&quot;&gt;Creative Commons Zero v1.0 Universal&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>lervag/vimtex</title>
      <link>https://github.com/lervag/vimtex</link>
      <description>&lt;p&gt;VimTeX: A modern Vim and neovim filetype plugin for LaTeX files.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VimTeX&lt;/h1&gt; 
&lt;p&gt;VimTeX is a modern &lt;a href=&quot;http://www.vim.org/&quot;&gt;Vim&lt;/a&gt; and &lt;a href=&quot;https://neovim.io/&quot;&gt;Neovim&lt;/a&gt; filetype and syntax plugin for LaTeX files.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://gitter.im/vimtex-chat/community?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&quot;&gt;&lt;img src=&quot;https://badges.gitter.im/vimtex-chat/community.svg?sanitize=true&quot; alt=&quot;Gitter&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://github.com/lervag/vimtex/workflows/CI%20tests/badge.svg?sanitize=true&quot; alt=&quot;CI tests&quot; /&gt; &lt;a href=&quot;https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;amp;hosted_button_id=5N4MFVXN7U8NW&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Donate-PayPal-green.svg?sanitize=true&quot; alt=&quot;Donate&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Table of contents&lt;/h2&gt; 
&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#installation&quot;&gt;Installation&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#configuration&quot;&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#quick-start&quot;&gt;Quick Start&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#tutorial&quot;&gt;Tutorial&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#documentation&quot;&gt;Documentation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#screenshots&quot;&gt;Screenshots&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#gifs&quot;&gt;GIFs&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#features&quot;&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#other-relevant-plugins&quot;&gt;Other relevant plugins&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#linting-and-syntax-checking&quot;&gt;Linting and syntax checking&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#snippets-and-templates&quot;&gt;Snippets and templates&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#tag-navigation&quot;&gt;Tag navigation&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#alternatives&quot;&gt;Alternatives&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;VimTeX requires Vim version 9.1 or Neovim version 0.10. The requirements were updated in January 2025 after the release of VimTeX 2.16. If you are stuck on older versions of Vim or Neovim, then you should not use the most recent version of VimTeX, but instead remain at the v2.15 tag (or older).&lt;/p&gt; 
&lt;p&gt;Some features require external tools. For example, the default compiler backend relies on &lt;a href=&quot;https://www.cantab.net/users/johncollins/latexmk/index.html&quot;&gt;latexmk&lt;/a&gt;. Users are encouraged to read the requirements section in the &lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/doc/vimtex.txt&quot;&gt;documentation&lt;/a&gt; (&lt;code&gt;:h vimtex-requirements&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are a lot of methods for installing plugins. The following explains the most common and popular approaches.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!WARNING]&lt;/p&gt; 
 &lt;p&gt;Many plugin managers provide mechanisms to lazy load plugins. Please don&#39;t use this for VimTeX! VimTeX is already lazy loaded by virtue of being a filetype plugin and by using the autoload mechanisms. There is therefore nothing to gain by forcing VimTeX to lazily load through the plugin manager. In fact, doing it will &lt;em&gt;break&lt;/em&gt; the inverse-search mechanism, which relies on a &lt;em&gt;global&lt;/em&gt; command (&lt;code&gt;:VimtexInverseSearch&lt;/code&gt;).&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;lazy.nvim&lt;/h3&gt; 
&lt;p&gt;In Neovim, &lt;a href=&quot;https://github.com/folke/lazy.nvim&quot;&gt;lazy.nvim&lt;/a&gt; is probably the most popular plugin manager. To install VimTeX, add a plugin spec similar to this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-lua&quot;&gt;{
  &quot;lervag/vimtex&quot;,
  lazy = false,     -- we don&#39;t want to lazy load VimTeX
  -- tag = &quot;v2.15&quot;, -- uncomment to pin to a specific release
  init = function()
    -- VimTeX configuration goes here, e.g.
    vim.g.vimtex_view_method = &quot;zathura&quot;
  end
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;VimTeX is mostly implemented with Vimscript and is configured with the classical vimscript variable convention like &lt;code&gt;g:vimtex_OPTION_NAME&lt;/code&gt;. Nowadays, Neovim is often configured with Lua, thus some users may be interested in reading &lt;code&gt;:help lua-vimscript&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;vim-plug&lt;/h3&gt; 
&lt;p&gt;If you use &lt;a href=&quot;https://github.com/junegunn/vim-plug&quot;&gt;vim-plug&lt;/a&gt;, then add &lt;em&gt;one&lt;/em&gt; of the following lines to your configuration. The first will use the latest versions from the &lt;code&gt;master&lt;/code&gt; branch, whereas the second will pin to a release tag.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-vim&quot;&gt;Plug &#39;lervag/vimtex&#39;
Plug &#39;lervag/vimtex&#39;, { &#39;tag&#39;: &#39;v2.15&#39; }
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Other&lt;/h3&gt; 
&lt;p&gt;There are many other plugin managers out there. They are typically well documented, and it should be straightforward to extrapolate the above snippets.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE]&lt;/p&gt; 
 &lt;p&gt;If you use the built-in package feature, then:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Make sure to read and understand the package feature: &lt;code&gt;:help package&lt;/code&gt;!&lt;/li&gt; 
  &lt;li&gt;Use the &lt;code&gt;/pack/foo/start&lt;/code&gt; subdirectory to make sure the filetype plugin is automatically loaded for the &lt;code&gt;tex&lt;/code&gt; filetypes.&lt;/li&gt; 
  &lt;li&gt;Helptags are not generated automatically. Run &lt;code&gt;:helptags&lt;/code&gt; to generate them.&lt;/li&gt; 
  &lt;li&gt;Please note that by default Vim puts custom &lt;code&gt;/start/&lt;/code&gt; plugin directories at the end of the &lt;code&gt;&amp;amp;runtimepath&lt;/code&gt;. This means the built in filetype plugin is loaded, which prevents VimTeX from loading. See &lt;a href=&quot;https://github.com/lervag/vimtex/issues/1413&quot;&gt;#1413&lt;/a&gt; for two suggested solutions to this. To see which scripts are loaded and in which order, use &lt;code&gt;:scriptnames&lt;/code&gt;.&lt;/li&gt; 
  &lt;li&gt;For more information on how to use the Vim native package solution, see &lt;a href=&quot;https://vi.stackexchange.com/questions/9522/what-is-the-vim8-package-feature-and-how-should-i-use-it&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://shapeshed.com/vim-packages/&quot;&gt;here&lt;/a&gt;.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;After installing VimTeX, you should edit your &lt;code&gt;.vimrc&lt;/code&gt; file or &lt;code&gt;init.vim&lt;/code&gt; file to configure VimTeX to your liking. Users should read the documentation to learn the various configuration possibilities, but the below is a simple overview of some of the main aspects.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!CAUTION]&lt;/p&gt; 
 &lt;p&gt;&lt;strong&gt;PLEASE&lt;/strong&gt; don&#39;t just copy this without reading the comments!&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-vim&quot;&gt;&quot; This is necessary for VimTeX to load properly. The &quot;indent&quot; is optional.
&quot; Note: Most plugin managers will do this automatically!
filetype plugin indent on

&quot; This enables Vim&#39;s and neovim&#39;s syntax-related features. Without this, some
&quot; VimTeX features will not work (see &quot;:help vimtex-requirements&quot; for more
&quot; info).
&quot; Note: Most plugin managers will do this automatically!
syntax enable

&quot; Viewer options: One may configure the viewer either by specifying a built-in
&quot; viewer method:
let g:vimtex_view_method = &#39;zathura&#39;

&quot; Or with a generic interface:
let g:vimtex_view_general_viewer = &#39;okular&#39;
let g:vimtex_view_general_options = &#39;--unique file:@pdf\#src:@line@tex&#39;

&quot; VimTeX uses latexmk as the default compiler backend. If you use it, which is
&quot; strongly recommended, you probably don&#39;t need to configure anything. If you
&quot; want another compiler backend, you can change it as follows. The list of
&quot; supported backends and further explanation is provided in the documentation,
&quot; see &quot;:help vimtex-compiler&quot;.
let g:vimtex_compiler_method = &#39;latexrun&#39;

&quot; Most VimTeX mappings rely on localleader and this can be changed with the
&quot; following line. The default is usually fine and is the symbol &quot;\&quot;.
let maplocalleader = &quot;,&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;The following video shows how to use VimTeX&#39;s main features (credits: &lt;a href=&quot;https://github.com/DustyTopology&quot;&gt;@DustyTopology&lt;/a&gt; from &lt;a href=&quot;https://github.com/lervag/vimtex/issues/1946#issuecomment-846345095&quot;&gt;#1946&lt;/a&gt;). The example LaTeX file used in the video is available under &lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/test/example-quick-start/main.tex&quot;&gt;&lt;code&gt;test/example-quick-start/main.tex&lt;/code&gt;&lt;/a&gt; and it may be instructive to copy the file and play with it to learn some of these basic functions.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://user-images.githubusercontent.com/66584581/119213849-1b7d4080-ba77-11eb-8a31-7ff7b9a4a020.mp4&quot;&gt;https://user-images.githubusercontent.com/66584581/119213849-1b7d4080-ba77-11eb-8a31-7ff7b9a4a020.mp4&lt;/a&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP]&lt;/p&gt; 
 &lt;p&gt;If the compiler or the viewer doesn&#39;t start properly, one may type &lt;code&gt;&amp;lt;localleader&amp;gt;li&lt;/code&gt; to view the system commands that were executed to start them. To inspect the compiler output, use &lt;code&gt;&amp;lt;localleader&amp;gt;lo&lt;/code&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Tutorial&lt;/h3&gt; 
&lt;p&gt;Both new and experienced users are encouraged to read the excellent guide by @ejmastnak: &lt;a href=&quot;https://ejmastnak.com/tutorials/vim-latex/vimtex/&quot;&gt;Getting started with the VimTeX plugin&lt;/a&gt;. The guide covers all the fundamentals of setting up a VimTeX-based LaTeX workflow, including usage of the VimTeX plugin, compilation, setting up forward and inverse search with a PDF reader, and Vimscript tools for user-specific customization.&lt;/p&gt; 
&lt;h3&gt;Documentation&lt;/h3&gt; 
&lt;p&gt;Users are of course &lt;em&gt;strongly&lt;/em&gt; encouraged to read the documentation, at least the introduction, to learn about the different features and possibilities provided by VimTeX (see &lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/doc/vimtex.txt&quot;&gt;&lt;code&gt;:h vimtex&lt;/code&gt;&lt;/a&gt;). Advanced users and potential developers may also be interested in reading the supplementary documents:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/DOCUMENTATION.md&quot;&gt;DOCUMENTATION.md&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Screenshots&lt;/h2&gt; 
&lt;p&gt;Here is an example of the syntax highlighting provided by VimTeX. The conceal feature is active on the right-hand side split. The example is made by @DustyTopology with the &lt;a href=&quot;https://github.com/arzg/vim-colors-xcode&quot;&gt;vim-colors-xcode&lt;/a&gt; colorscheme with some minor adjustments &lt;a href=&quot;https://github.com/lervag/vimtex/issues/1946#issuecomment-843674951&quot;&gt;described here&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/lervag/vimtex-media/raw/main/img/syntax.png&quot; alt=&quot;Syntax example&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;GIFs&lt;/h3&gt; 
&lt;p&gt;See the file &lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/VISUALS.md&quot;&gt;VISUALS.md&lt;/a&gt; for screencast-style GIFs demonstrating VimTeX&#39;s core motions, text-editing commands, and text objects.&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;p&gt;Below is a list of features offered by VimTeX. The features are accessible as both commands and mappings. The mappings generally start with &lt;code&gt;&amp;lt;localleader&amp;gt;l&lt;/code&gt;, but if desired one can disable default mappings to define custom mappings. Nearly all features are enabled by default, but each feature may be disabled if desired. The two exceptions are code folding and formating, which are disabled by default and must be manually enabled.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Document compilation with &lt;a href=&quot;https://www.cantab.net/users/johncollins/latexmk/index.html&quot;&gt;latexmk&lt;/a&gt;, &lt;a href=&quot;https://github.com/aclements/latexrun&quot;&gt;latexrun&lt;/a&gt;, &lt;a href=&quot;https://tectonic-typesetting.github.io&quot;&gt;tectonic&lt;/a&gt;, or &lt;a href=&quot;https://github.com/cereda/arara&quot;&gt;arara&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;LaTeX log parsing for quickfix entries using 
  &lt;ul&gt; 
   &lt;li&gt;internal method&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://github.com/stefanhepp/pplatex&quot;&gt;pplatex&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Compilation of selected part of document&lt;/li&gt; 
 &lt;li&gt;Support for several PDF viewers with forward search 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.mupdf.com/&quot;&gt;MuPDF&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://okular.kde.org/&quot;&gt;Okular&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://launchpad.net/qpdfview&quot;&gt;qpdfview&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://skim-app.sourceforge.net/&quot;&gt;Skim&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://www.sumatrapdfreader.org/free-pdf-reader.html&quot;&gt;SumatraPDF&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://pages.uoregon.edu/koch/texshop/&quot;&gt;TeXShop&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://pwmt.org/projects/zathura/&quot;&gt;Zathura&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;Other viewers are supported through a general interface&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Completion of 
  &lt;ul&gt; 
   &lt;li&gt;citations&lt;/li&gt; 
   &lt;li&gt;labels&lt;/li&gt; 
   &lt;li&gt;commands&lt;/li&gt; 
   &lt;li&gt;file names for figures, input/include, includepdf, includestandalone&lt;/li&gt; 
   &lt;li&gt;glossary entries&lt;/li&gt; 
   &lt;li&gt;package and documentclass names based on available &lt;code&gt;.sty&lt;/code&gt; and &lt;code&gt;.cls&lt;/code&gt; files&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Document navigation through 
  &lt;ul&gt; 
   &lt;li&gt;table of contents&lt;/li&gt; 
   &lt;li&gt;table of labels&lt;/li&gt; 
   &lt;li&gt;proper settings for &lt;code&gt;&#39;include&#39;&lt;/code&gt;, &lt;code&gt;&#39;includeexpr&#39;&lt;/code&gt;, &lt;code&gt;&#39;suffixesadd&#39;&lt;/code&gt; and &lt;code&gt;&#39;define&#39;&lt;/code&gt;, which among other things 
    &lt;ul&gt; 
     &lt;li&gt;allow &lt;code&gt;:h include-search&lt;/code&gt; and &lt;code&gt;:h definition-search&lt;/code&gt;&lt;/li&gt; 
     &lt;li&gt;give enhanced &lt;code&gt;gf&lt;/code&gt; command&lt;/li&gt; 
    &lt;/ul&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Easy access to (online) documentation of packages&lt;/li&gt; 
 &lt;li&gt;Word count (through &lt;code&gt;texcount&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Motions (&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/VISUALS.md#motion-commands&quot;&gt;link to GIF demonstrations&lt;/a&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;Move between section boundaries with &lt;code&gt;[[&lt;/code&gt;, &lt;code&gt;[]&lt;/code&gt;, &lt;code&gt;][&lt;/code&gt;, and &lt;code&gt;]]&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Move between environment boundaries with &lt;code&gt;[m&lt;/code&gt;, &lt;code&gt;[M&lt;/code&gt;, &lt;code&gt;]m&lt;/code&gt;, and &lt;code&gt;]M&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Move between math environment boundaries with &lt;code&gt;[n&lt;/code&gt;, &lt;code&gt;[N&lt;/code&gt;, &lt;code&gt;]n&lt;/code&gt;, and &lt;code&gt;]N&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Move between frame environment boundaries with &lt;code&gt;[r&lt;/code&gt;, &lt;code&gt;[R&lt;/code&gt;, &lt;code&gt;]r&lt;/code&gt;, and &lt;code&gt;]R&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Move between comment boundaries with &lt;code&gt;[*&lt;/code&gt; and &lt;code&gt;]*&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Move between matching delimiters with &lt;code&gt;%&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Text objects (&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/VISUALS.md#text-objects&quot;&gt;link to GIF demonstrations&lt;/a&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;ic ac&lt;/code&gt; Commands&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;id ad&lt;/code&gt; Delimiters&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;ie ae&lt;/code&gt; LaTeX environments&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;i$ a$&lt;/code&gt; Math environments&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;iP aP&lt;/code&gt; Sections&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;im am&lt;/code&gt; Items&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Other mappings (&lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/VISUALS.md#deleting-surrounding-latex-content&quot;&gt;link to GIF demonstrations&lt;/a&gt;) 
  &lt;ul&gt; 
   &lt;li&gt;Delete the surrounding command, environment or delimiter with &lt;code&gt;dsc&lt;/code&gt;/&lt;code&gt;dse&lt;/code&gt;/&lt;code&gt;ds$&lt;/code&gt;/&lt;code&gt;dsd&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Change the surrounding command, environment or delimiter with &lt;code&gt;csc&lt;/code&gt;/&lt;code&gt;cse&lt;/code&gt;/&lt;code&gt;cs$&lt;/code&gt;/&lt;code&gt;csd&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Toggle between complementary environments with &lt;code&gt;tse&lt;/code&gt; (see &lt;a href=&quot;https://github.com/lervag/vimtex/releases/tag/v2.16&quot;&gt;v2.16 release notes&lt;/a&gt;)&lt;/li&gt; 
   &lt;li&gt;Toggle starred command or environment with &lt;code&gt;tsc&lt;/code&gt;/&lt;code&gt;tss&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Toggle inline and displaymath with &lt;code&gt;ts$&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Toggle between e.g. &lt;code&gt;()&lt;/code&gt; and &lt;code&gt;\left(\right)&lt;/code&gt; with &lt;code&gt;tsd&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Toggle (inline) fractions with &lt;code&gt;tsf&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Toggle line-break macro &lt;code&gt;\\&lt;/code&gt; with &lt;code&gt;tsb&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Close the current environment/delimiter in insert mode with &lt;code&gt;]]&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Add &lt;code&gt;\left ... \right)&lt;/code&gt; modifiers to surrounding delimiters with &lt;code&gt;&amp;lt;F8&amp;gt;&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Insert new command with &lt;code&gt;&amp;lt;F7&amp;gt;&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;Convenient insert mode mappings for faster typing of e.g. maths&lt;/li&gt; 
   &lt;li&gt;Context menu on citations (e.g. &lt;code&gt;\cite{...}&lt;/code&gt;) mapped to &lt;code&gt;&amp;lt;cr&amp;gt;&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Improved folding (&lt;code&gt;:h &#39;foldexpr&#39;&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Improved indentation (&lt;code&gt;:h &#39;indentexpr&#39;&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Syntax highlighting 
  &lt;ul&gt; 
   &lt;li&gt;A consistent core syntax specification&lt;/li&gt; 
   &lt;li&gt;General syntax highlighting for several popular LaTeX packages&lt;/li&gt; 
   &lt;li&gt;Nested syntax highlighting for several popular LaTeX packages&lt;/li&gt; 
   &lt;li&gt;Highlight matching delimiters&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Support for multi-file project packages 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;http://ctan.uib.no/macros/latex/contrib/import/import.pdf&quot;&gt;import&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;http://ctan.uib.no/macros/latex/contrib/subfiles/subfiles.pdf&quot;&gt;subfiles&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;See the documentation for a thorough introduction to VimTeX (e.g. &lt;code&gt;:h vimtex&lt;/code&gt;).&lt;/p&gt; 
&lt;h2&gt;Other relevant plugins&lt;/h2&gt; 
&lt;p&gt;Even though VimTeX provides a lot of nice features for working with LaTeX documents, there are several features that are better served by other, dedicated plugins. For a more detailed listing of these, please see &lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/doc/vimtex.txt#L540&quot;&gt;&lt;code&gt;:help vimtex-and-friends&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Linting and syntax checking&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/w0rp/ale&quot;&gt;ale&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/neomake/neomake&quot;&gt;neomake&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/vim-syntastic/syntastic&quot;&gt;syntastic&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Snippets and templates&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/SirVer/ultisnips&quot;&gt;UltiSnips&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/Shougo/neosnippet.vim&quot;&gt;neosnippet&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Tag navigation&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ludovicchabant/vim-gutentags&quot;&gt;vim-gutentags&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Alternatives&lt;/h2&gt; 
&lt;p&gt;The following are some alternative LaTeX plugins for Vim:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;http://vim-latex.sourceforge.net&quot;&gt;LaTeX-Suite&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The main difference between VimTeX and LaTeX-Suite (aka vim-latex) is probably that VimTeX does not try to implement a full fledged IDE for LaTeX inside Vim. E.g.:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;VimTeX does not provide a full snippet feature, because this is better handled by &lt;a href=&quot;https://github.com/SirVer/ultisnips&quot;&gt;UltiSnips&lt;/a&gt; or &lt;a href=&quot;https://github.com/Shougo/neosnippet.vim&quot;&gt;neosnippet&lt;/a&gt; or similar snippet engines.&lt;/li&gt; 
   &lt;li&gt;VimTeX builds upon Vim principles: It provides text objects for environments, inline math, it provides motions for sections and paragraphs&lt;/li&gt; 
   &lt;li&gt;VimTeX uses &lt;code&gt;latexmk&lt;/code&gt;, &lt;code&gt;latexrun&lt;/code&gt;, &lt;code&gt;tectonic&lt;/code&gt; or &lt;code&gt;arara&lt;/code&gt; for compilation with a callback feature to get instant feedback on compilation errors&lt;/li&gt; 
   &lt;li&gt;VimTeX is very modular: if you don&#39;t like a feature, you can turn it off.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/jakewvincent/texmagic.nvim&quot;&gt;TexMagic.nvim&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&quot;A simple, lightweight Neovim plugin that facilitates LaTeX build engine selection via magic comments. It is designed with the TexLab LSP server&#39;s build functionality in mind, which at the time of this plugin&#39;s inception had to be specified in init.lua/init.vim and could not be set on a by-project basis.&quot;&lt;/p&gt; &lt;p&gt;This plugin should be combined with the TexLab LSP server, and it only works on neovim.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/LaTeX-Box-Team/LaTeX-Box&quot;&gt;LaTeX-Box&lt;/a&gt;&lt;/p&gt; &lt;p&gt;VimTeX currently has most of the features of LaTeX-Box, as well as some additional ones. See &lt;a href=&quot;https://raw.githubusercontent.com/lervag/vimtex/master/#features&quot;&gt;here&lt;/a&gt; for a relatively complete list of features.&lt;/p&gt; &lt;p&gt;One particular feature that LaTeX-Box has but VimTeX misses, is the ability to do single-shot compilation &lt;em&gt;with callback&lt;/em&gt;. This functionality was removed because it adds a lot of complexity for relatively little gain (IMHO).&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;http://atp-vim.sourceforge.net&quot;&gt;AutomaticTexPlugin&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/xuhdev/vim-latex-live-preview&quot;&gt;vim-latex-live-preview&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For more alternatives and more information and discussions regarding LaTeX plugins for Vim, see:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;http://vi.stackexchange.com/questions/2047/what-are-the-differences-between-latex-plugins&quot;&gt;What are the differences between LaTeX plugins&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://tex.stackexchange.com/questions/339/latex-editors-ides&quot;&gt;List of LaTeX editors (not only Vim)&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>exacity/deeplearningbook-chinese</title>
      <link>https://github.com/exacity/deeplearningbook-chinese</link>
      <description>&lt;p&gt;Deep Learning Book Chinese Translation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning ‰∏≠ÊñáÁøªËØë&lt;/h1&gt; 
&lt;p&gt;Âú®‰ºóÂ§öÁΩëÂèãÁöÑÂ∏ÆÂä©ÂíåÊ†°ÂØπ‰∏ãÔºå‰∏≠ÊñáÁâàÁªà‰∫éÂá∫Áâà‰∫Ü„ÄÇÂ∞ΩÁÆ°ËøòÊúâÂæàÂ§öÈóÆÈ¢òÔºå‰ΩÜËá≥Â∞ë90%ÁöÑÂÜÖÂÆπÊòØÂèØËØªÁöÑÔºåÂπ∂‰∏îÊòØÂáÜÁ°ÆÁöÑ„ÄÇ Êàë‰ª¨Â∞ΩÂèØËÉΩÂú∞‰øùÁïô‰∫ÜÂéü‰π¶&lt;a href=&quot;http://www.deeplearningbook.org/&quot;&gt;Deep Learning&lt;/a&gt;‰∏≠ÁöÑÊÑèÊÄùÂπ∂‰øùÁïôÂéü‰π¶ÁöÑËØ≠Âè•„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÁÑ∂ËÄåÊàë‰ª¨Ê∞¥Âπ≥ÊúâÈôêÔºåÊàë‰ª¨Êó†Ê≥ïÊ∂àÈô§‰ºóÂ§öËØªËÄÖÁöÑÊñπÂ∑Æ„ÄÇÊàë‰ª¨‰ªçÈúÄË¶ÅÂ§ßÂÆ∂ÁöÑÂª∫ËÆÆÂíåÂ∏ÆÂä©Ôºå‰∏ÄËµ∑ÂáèÂ∞èÁøªËØëÁöÑÂÅèÂ∑Æ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Â§ßÂÆ∂ÊâÄË¶ÅÂÅöÁöÑÂ∞±ÊòØÈòÖËØªÔºåÁÑ∂ÂêéÊ±áÊÄª‰Ω†ÁöÑÂª∫ËÆÆÔºåÊèêissueÔºàÊúÄÂ•Ω‰∏çË¶Å‰∏Ä‰∏™‰∏Ä‰∏™Âú∞ÊèêÔºâ„ÄÇÂ¶ÇÊûú‰Ω†Á°ÆÂÆö‰Ω†ÁöÑÂª∫ËÆÆ‰∏çÈúÄË¶ÅÂïÜÈáèÔºåÂèØ‰ª•Áõ¥Êé•ÂèëËµ∑PR„ÄÇ&lt;/p&gt; 
&lt;p&gt;ÂØπÂ∫îÁöÑÁøªËØëËÄÖÔºö&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Á¨¨1„ÄÅ4„ÄÅ7„ÄÅ10„ÄÅ14„ÄÅ20Á´†ÂèäÁ¨¨12.4„ÄÅ12.5ËäÇÁî± @swordyork Ë¥üË¥£&lt;/li&gt; 
 &lt;li&gt;Á¨¨2„ÄÅ5„ÄÅ8„ÄÅ11„ÄÅ15„ÄÅ18Á´†Áî± @liber145 Ë¥üË¥£&lt;/li&gt; 
 &lt;li&gt;Á¨¨3„ÄÅ6„ÄÅ9Á´†Áî± @KevinLee1110 Ë¥üË¥£&lt;/li&gt; 
 &lt;li&gt;Á¨¨13„ÄÅ16„ÄÅ17„ÄÅ19Á´†ÂèäÁ¨¨12.1Ëá≥12.3ËäÇÁî± @futianfan Ë¥üË¥£&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Èù¢ÂêëÁöÑËØªËÄÖ&lt;/h2&gt; 
&lt;p&gt;ËØ∑Áõ¥Êé•‰∏ãËΩΩ&lt;a href=&quot;https://github.com/exacity/deeplearningbook-chinese/releases/download/v0.5-beta/dlbook_cn_v0.5-beta.pdf&quot;&gt;PDF&lt;/a&gt;ÈòÖËØª„ÄÇ ‰∏çÊâìÁÆóÊèê‰æõEPUBÁ≠âÊ†ºÂºèÔºåÂ¶ÇÊúâÈúÄË¶ÅËØ∑Ëá™Ë°å‰øÆÊîπ„ÄÇ&lt;/p&gt; 
&lt;p&gt;Ëøô‰∏ÄÁâàÂáÜÁ°ÆÊÄßÂ∑≤ÁªèÊúâÊâÄÊèêÈ´òÔºåËØªËÄÖÂèØ‰ª•‰ª•‰∏≠ÊñáÁâà‰∏∫‰∏ª„ÄÅËã±ÊñáÁâà‰∏∫ËæÖÊù•ÈòÖËØªÂ≠¶‰π†Ôºå‰ΩÜÊàë‰ª¨‰ªçÂª∫ËÆÆÁ†îÁ©∂ËÄÖÈòÖËØª&lt;a href=&quot;http://www.deeplearningbook.org/&quot;&gt;ÂéüÁâà&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h2&gt;Âá∫ÁâàÂèäÂºÄÊ∫êÂéüÂõ†&lt;/h2&gt; 
&lt;p&gt;Êú¨‰π¶Áî±‰∫∫Ê∞ëÈÇÆÁîµÂá∫ÁâàÁ§æÂá∫ÁâàÔºåÂ¶ÇÊûú‰Ω†ËßâÂæó‰∏≠ÊñáÁâàPDFÂØπ‰Ω†ÊúâÊâÄÂ∏ÆÂä©ÔºåÂ∏åÊúõ‰Ω†ËÉΩÊîØÊåÅ‰∏ãÁ∫∏Ë¥®Ê≠£Áâà‰π¶Á±ç„ÄÇ Â¶ÇÊûú‰Ω†ËßâÂæó‰∏≠ÊñáÁâà‰∏çË°åÔºåÂ∏åÊúõ‰Ω†ËÉΩÂ§öÊèêÂª∫ËÆÆ„ÄÇÈùûÂ∏∏ÊÑüË∞¢ÂêÑ‰ΩçÔºÅ Á∫∏Ë¥®Áâà‰πü‰ºöËøõ‰∏ÄÊ≠•Êõ¥Êñ∞ÔºåÈúÄË¶ÅÂ§ßÂÆ∂Êõ¥Â§öÁöÑÂª∫ËÆÆÂíåÊÑèËßÅÔºå‰∏ÄËµ∑ÂÆåÂñÑ‰∏≠ÊñáÁâà„ÄÇ&lt;/p&gt; 
&lt;p&gt;Á∫∏Ë¥®ÁâàÁõÆÂâçÂú®‰∫∫Ê∞ëÈÇÆÁîµÂá∫ÁâàÁ§æÁöÑÂºÇÊ≠•Á§æÂå∫Âá∫ÂîÆÔºåËßÅ&lt;a href=&quot;http://www.epubit.com.cn/book/details/4278&quot;&gt;Âú∞ÂùÄ&lt;/a&gt;„ÄÇ ‰ª∑Ê†º‰∏ç‰ΩéÔºå‰ΩÜÁúã‰∫ÜÊ†∑Êú¨‰πãÂêéÔºåÊàë‰ª¨ËÆ§‰∏∫Áâ©ÊúâÊâÄÂÄº„ÄÇ Ê≥®ÊÑèÔºåÊàë‰ª¨‰∏ç‰ºöÈÄöËøáÂ™í‰ΩìËøõË°åÂÆ£‰º†ÔºåÂ∏åÊúõÂ§ßÂÆ∂ÂÖàÁúãÁîµÂ≠êÁâàÂÜÖÂÆπÔºåÂÜçÂà§Êñ≠ÊòØÂê¶Ë¥≠‰π∞Á∫∏Ë¥®Áâà„ÄÇ&lt;/p&gt; 
&lt;p&gt;‰ª•‰∏ãÊòØÂºÄÊ∫êÁöÑÂÖ∑‰ΩìÂéüÂõ†Ôºö&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Êàë‰ª¨‰∏çÊòØÊñáÂ≠¶Â∑•‰ΩúËÄÖÔºå‰∏ç‰∏ìËÅåÁøªËØë„ÄÇÂçïÈù†Êàë‰ª¨ÔºåÊó†Ê≥ïÁªôÂá∫‰ªäÂ§©ÁöÑÁøªËØëÔºå‰ºóÂ§öÁΩëÂèãÈÉΩÁªôÊàë‰ª¨ÊèêÂá∫‰∫ÜÂÆùË¥µÁöÑÂª∫ËÆÆÔºåÂõ†Ê≠§ÂºÄÊ∫êÂ∏Æ‰∫ÜÂæàÂ§ßÁöÑÂøô„ÄÇÂá∫ÁâàÁ§æ‰ºöÁªôÊàë‰ª¨Á®øË¥πÔºàÊàë‰ª¨‰πü‰∏çÁü•ÈÅìÂ§öÂ∞ëÔºåÂèØËÉΩ2‰∏áÂ∑¶Âè≥ÔºâÔºåÊàë‰ª¨‰πü‰∏çÂ•ΩÊÑèÊÄùËá™Â∑±Áî®ÔºåÂïÜÈáè‰πãÂêéËßâÂæóÊçêÂá∫ÊòØÊúÄÂêàÈÄÇÁöÑÔºå‰ª•ÊâÄÊúâË¥°ÁåÆËøáÁöÑÁΩëÂèãÁöÑÂêç‰πâÔºàÊàë‰ª¨ÊääÁ®øË¥πÊçêÁªô‰∫ÜÊùâÊ†ëÂÖ¨ÁõäÔºåÁî®‰∫é4ÂêçË¥µÂ∑ûÈ´ò‰∏≠Áîü‰∏âÂπ¥ÁöÑÁîüÊ¥ªË¥πÔºåËßÅ&lt;a href=&quot;https://github.com/exacity/deeplearningbook-chinese/raw/master/donation.pdf&quot;&gt;ÊçêËµ†ÊÉÖÂÜµ&lt;/a&gt;Ôºâ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;PDFÁîµÂ≠êÁâàÂØπ‰∫éÊäÄÊúØÁ±ª‰π¶Á±çÊù•ËØ¥ÊòØÂæàÈáçË¶ÅÁöÑÔºåÈöèÊó∂ÈúÄË¶ÅÊü•ËØ¢ÔºåÊãøÁùÄÁ∫∏Ë¥®ÁâàÂà∞Â§ÑËµ∞ÊòæÁÑ∂‰∏çÂêàÈÄÇ„ÄÇÂõΩÂ§ñÂæàÂ§öÊäÄÊúØ‰π¶Á±çÈÉΩÊúâÂØπÂ∫îÁöÑÁîµÂ≠êÁâàÔºàËôΩÁÑ∂‰∏ç‰∏ÄÂÆöÊòØÊ≠£ÁâàÔºâÔºåËÄåÂõΩÂÜÖÁöÑÂá†‰πéÊ≤°Êúâ„ÄÇ‰∏™‰∫∫ËÆ§‰∏∫ËøôÊòØÂá∫ÁâàÁ§æÊàñËÄÖ‰ΩúËÄÖËÆ§‰∏∫ÂõΩÊ∞ëÁ¥†Ë¥®ËøòÊ≤°ÊúâÈ´òÂà∞‰∏ªÂä®‰∏∫Áü•ËØÜ‰ªòË¥πÁöÑÂ¢ÉÁïåÔºåÊâÄ‰ª•‰∏çÊÑøÊÑè&quot;Ê≥ÑÈú≤&quot;ÁîµÂ≠êÁâà„ÄÇÊó∂‰ª£Âú®ËøõÊ≠•ÔºåÊàë‰ª¨‰πüÈúÄË¶ÅÊîπÂèò„ÄÇÁâπÂà´ÊòØÁøªËØë‰ΩúÂìÅÊôÆÈÅçË¥®Èáè‰∏çÈ´òÁöÑÊÉÖÂÜµ‰∏ãÔºåË¶ÅÊï¢‰∏∫Â§©‰∏ãÂÖà„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Ê∑±Â∫¶Â≠¶‰π†ÂèëÂ±ïÂ§™Âø´ÔºåÊó•Êñ∞ÊúàÂºÇÔºåÊâÄ‰ª•Êàë‰ª¨Â∏åÊúõÂ§ßÂÆ∂Êõ¥Êó©Âú∞Â≠¶Âà∞Áõ∏ÂÖ≥ÁöÑÁü•ËØÜ„ÄÇÊàëËßâÂæóÂéü‰ΩúËÄÖÂºÄÊîæPDFÁîµÂ≠êÁâà‰πüÊúâÁ±ª‰ººÁöÑËÄÉËôëÔºå‰πüÂ∞±ÊòØÂÖàÈòÖËØªÂêé‰ªòË¥π„ÄÇÊàë‰ª¨ËÆ§‰∏∫‰∏≠ÂõΩ‰∫∫Âè£Á¥†Ë¥®Â∑≤ÁªèË∂≥Â§üÈ´òÔºåÊáÇÂæó‰∏∫Áü•ËØÜ‰ªòË¥π„ÄÇÂΩìÁÑ∂Ëøô‰∏çÊòØ‰ªòÁªôÊàë‰ª¨ÁöÑÔºåÊòØ‰ªòÁªôÂá∫ÁâàÁ§æÁöÑÔºåÂá∫ÁâàÁ§æÂÜç‰ªòÁªôÂéü‰ΩúËÄÖ„ÄÇÊàë‰ª¨‰∏çÂ∏åÊúõ‰∏≠ÊñáÁâàÁöÑÈîÄÈáèÂõ†PDFÁîµÂ≠êÁâàÁöÑÂ≠òÂú®ËÄå‰∏ãÊªë„ÄÇÂá∫ÁâàÁ§æÂè™ÊúâÂÄºÂõû‰∫ÜÁâàÊùÉÊâçËÉΩÂú®‰ª•ÂêéÂºïËøõÊõ¥Â§öÁöÑ‰ºòÁßÄ‰π¶Á±ç„ÄÇÊàë‰ª¨Ëøô‰∏™ÂºÄÊ∫êÁøªËØëÂÖà‰æã‰πü‰∏ç‰ºöÊàê‰∏∫‰∏Ä‰∏™ÂèçÈù¢Ê°à‰æãÔºå‰ª•ÂêéÊâç‰ºöÊúâÊõ¥Â§öÁöÑPDFÁîµÂ≠êÁâà„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÂºÄÊ∫ê‰πüÊ∂âÂèäÁâàÊùÉÈóÆÈ¢òÔºåÂá∫‰∫éÁâàÊùÉÂéüÂõ†ÔºåÊàë‰ª¨‰∏çÂÜçÊõ¥Êñ∞Ê≠§ÂàùÁâàPDFÊñá‰ª∂ÔºåËØ∑Â§ßÂÆ∂‰ª•ÊúÄÁªàÁöÑÁ∫∏Ë¥®Áâà‰∏∫ÂáÜ„ÄÇÔºà‰ΩÜÊ∫êÁ†Å‰ºö‰∏ÄÁõ¥Êõ¥Êñ∞Ôºâ&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Ëá¥Ë∞¢&lt;/h2&gt; 
&lt;p&gt;Êàë‰ª¨Êúâ3‰∏™Á±ªÂà´ÁöÑÊ†°ÂØπ‰∫∫Âëò„ÄÇ&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Ë¥üË¥£‰∫∫‰πüÂ∞±ÊòØÂØπÂ∫îÁöÑÁøªËØëËÄÖ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;ÁÆÄÂçïÈòÖËØªÔºåÂØπËØ≠Âè•‰∏çÈÄöÈ°∫ÊàñÈöæ‰ª•ÁêÜËß£ÁöÑÂú∞ÊñπÊèêÂá∫‰øÆÊîπÊÑèËßÅ„ÄÇ&lt;/li&gt; 
 &lt;li&gt;‰∏≠Ëã±ÂØπÊØîÔºåËøõË°å‰∏≠Ëã±ÂØπÂ∫îÈòÖËØªÔºåÊéíÈô§Â∞ëÁøªÈîôÁøªÁöÑÊÉÖÂÜµ„ÄÇ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;ÊâÄÊúâÊ†°ÂØπÂª∫ËÆÆÈÉΩ‰øùÂ≠òÂú®ÂêÑÁ´†ÁöÑ&lt;code&gt;annotations.txt&lt;/code&gt;Êñá‰ª∂‰∏≠„ÄÇ&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Á´†ËäÇ&lt;/th&gt; 
   &lt;th&gt;Ë¥üË¥£‰∫∫&lt;/th&gt; 
   &lt;th&gt;ÁÆÄÂçïÈòÖËØª&lt;/th&gt; 
   &lt;th&gt;‰∏≠Ëã±ÂØπÊØî&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter1_introduction/&quot;&gt;Á¨¨‰∏ÄÁ´† ÂâçË®Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork&lt;/td&gt; 
   &lt;td&gt;lc, @SiriusXDJ, @corenel, @NeutronT&lt;/td&gt; 
   &lt;td&gt;@linzhp&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter2_linear_algebra/&quot;&gt;Á¨¨‰∫åÁ´† Á∫øÊÄß‰ª£Êï∞&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@liber145&lt;/td&gt; 
   &lt;td&gt;@SiriusXDJ, @angrymidiao&lt;/td&gt; 
   &lt;td&gt;@badpoem&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/&quot;&gt;Á¨¨‰∏âÁ´† Ê¶ÇÁéá‰∏é‰ø°ÊÅØËÆ∫&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KevinLee1110&lt;/td&gt; 
   &lt;td&gt;@SiriusXDJ&lt;/td&gt; 
   &lt;td&gt;@kkpoker, @Peiyan&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter4_numerical_computation/&quot;&gt;Á¨¨ÂõõÁ´† Êï∞ÂÄºËÆ°ÁÆó&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork&lt;/td&gt; 
   &lt;td&gt;@zhangyafeikimi&lt;/td&gt; 
   &lt;td&gt;@hengqujushi&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter5_machine_learning_basics/&quot;&gt;Á¨¨‰∫îÁ´† Êú∫Âô®Â≠¶‰π†Âü∫Á°Ä&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@liber145&lt;/td&gt; 
   &lt;td&gt;@wheaio, @huangpingchun&lt;/td&gt; 
   &lt;td&gt;@fairmiracle, @linzhp&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter6_deep_feedforward_networks/&quot;&gt;Á¨¨ÂÖ≠Á´† Ê∑±Â∫¶ÂâçÈ¶àÁΩëÁªú&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KevinLee1110&lt;/td&gt; 
   &lt;td&gt;David_Chow, @linzhp, @sailordiary&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter7_regularization/&quot;&gt;Á¨¨‰∏ÉÁ´† Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÁöÑÊ≠£ÂàôÂåñ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@NBZCC&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter8_optimization_for_training_deep_models/&quot;&gt;Á¨¨ÂÖ´Á´† Ê∑±Â∫¶Ê®°Âûã‰∏≠ÁöÑ‰ºòÂåñ&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@liber145&lt;/td&gt; 
   &lt;td&gt;@happynoom, @codeVerySlow&lt;/td&gt; 
   &lt;td&gt;@huangpingchun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter9_convolutional_networks/&quot;&gt;Á¨¨‰πùÁ´† Âç∑ÁßØÁΩëÁªú&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@KevinLee1110&lt;/td&gt; 
   &lt;td&gt;@zhaoyu611, @corenel&lt;/td&gt; 
   &lt;td&gt;@zhiding&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter10_sequence_modeling_rnn/&quot;&gt;Á¨¨ÂçÅÁ´† Â∫èÂàóÂª∫Ê®°ÔºöÂæ™ÁéØÂíåÈÄíÂΩíÁΩëÁªú&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork&lt;/td&gt; 
   &lt;td&gt;lc&lt;/td&gt; 
   &lt;td&gt;@zhaoyu611, @yinruiqing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter11_practical_methodology/&quot;&gt;Á¨¨ÂçÅ‰∏ÄÁ´† ÂÆûË∑µÊñπÊ≥ïËÆ∫&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@liber145&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter12_applications/&quot;&gt;Á¨¨ÂçÅ‰∫åÁ´† Â∫îÁî®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork, @futianfan&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@corenel&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter13_linear_factor_models/&quot;&gt;Á¨¨ÂçÅ‰∏âÁ´† Á∫øÊÄßÂõ†Â≠êÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@futianfan&lt;/td&gt; 
   &lt;td&gt;@cloudygoose&lt;/td&gt; 
   &lt;td&gt;@ZhiweiYang&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter14_autoencoders/&quot;&gt;Á¨¨ÂçÅÂõõÁ´† Ëá™ÁºñÁ†ÅÂô®&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@Seaball, @huangpingchun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter15_representation_learning/&quot;&gt;Á¨¨ÂçÅ‰∫îÁ´† Ë°®Á§∫Â≠¶‰π†&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@liber145&lt;/td&gt; 
   &lt;td&gt;@cnscottzheng&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter16_structured_probabilistic_modelling/&quot;&gt;Á¨¨ÂçÅÂÖ≠Á´† Ê∑±Â∫¶Â≠¶‰π†‰∏≠ÁöÑÁªìÊûÑÂåñÊ¶ÇÁéáÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@futianfan&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter17_monte_carlo_methods/&quot;&gt;Á¨¨ÂçÅ‰∏ÉÁ´† ËíôÁâπÂç°ÁΩóÊñπÊ≥ï&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@futianfan&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@sailordiary&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter18_confronting_the_partition_function/&quot;&gt;Á¨¨ÂçÅÂÖ´Á´† Èù¢ÂØπÈÖçÂàÜÂáΩÊï∞&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@liber145&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@tankeco&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter19_approximate_inference/&quot;&gt;Á¨¨ÂçÅ‰πùÁ´† Ëøë‰ººÊé®Êñ≠&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@futianfan&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@sailordiary, @hengqujushi, huanghaojun&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese/Chapter20_deep_generative_models/&quot;&gt;Á¨¨‰∫åÂçÅÁ´† Ê∑±Â∫¶ÁîüÊàêÊ®°Âûã&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;@swordyork&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;ÂèÇËÄÉÊñáÁåÆ&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;@pkuwwt&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Êàë‰ª¨‰ºöÂú®Á∫∏Ë¥®ÁâàÊ≠£ÂºèÂá∫ÁâàÁöÑÊó∂ÂÄôÔºåÂú®‰π¶‰∏≠Ëá¥Ë∞¢ÔºåÊ≠£ÂºèÊÑüË∞¢ÂêÑ‰Ωç‰ΩúÂá∫Ë¥°ÁåÆÁöÑÂêåÂ≠¶ÔºÅ&lt;/p&gt; 
&lt;p&gt;ËøòÊúâÂæàÂ§öÂêåÂ≠¶ÊèêÂá∫‰∫Ü‰∏çÂ∞ëÂª∫ËÆÆÔºåÊàë‰ª¨ÈÉΩÂàóÂú®Ê≠§Â§Ñ„ÄÇ&lt;/p&gt; 
&lt;p&gt;@tttwwy @tankeco @fairmiracle @GageGao @huangpingchun @MaHongP @acgtyrant @yanhuibin315 @Buttonwood @titicacafz @weijy026a @RuiZhang1993 @zymiboxpay @xingkongliang @oisc @tielei @yuduowu @Qingmu @HC-2016 @xiaomingabc @bengordai @Bojian @JoyFYan @minoriwww @khty2000 @gump88 @zdx3578 @PassStory @imwebson @wlbksy @roachsinai @Elvinczp @endymecy name:YUE-DaJiong @9578577 @linzhp @cnscottzheng @germany-zhu @zhangyafeikimi @showgood163 @gump88 @kangqf @NeutronT @badpoem @kkpoker @Seaball @wheaio @angrymidiao @ZhiweiYang @corenel @zhaoyu611 @SiriusXDJ @dfcv24 EmisXXY FlyingFire vsooda @friskit-china @poerin @ninesunqian @JiaqiYao @Sofring @wenlei @wizyoung @imageslr @@indam @XuLYC @zhouqingping @freedomRen @runPenguin @pkuwwt @wuqi @tjliupeng @neo0801 @jt827859032 @demolpc @fishInAPool @xiaolangyuxin @jzj1993 @whatbeg LongXiaJun jzd&lt;/p&gt; 
&lt;p&gt;Â¶ÇÊúâÈÅóÊºèÔºåËØ∑Âä°ÂøÖÈÄöÁü•Êàë‰ª¨ÔºåÂèØ‰ª•ÂèëÈÇÆ‰ª∂Ëá≥&lt;code&gt;echo c3dvcmQueW9ya0BnbWFpbC5jb20K | base64 --decode&lt;/code&gt;„ÄÇ ËøôÊòØÊàë‰ª¨ÂøÖÈ°ªË¶ÅÊÑüË∞¢ÁöÑÔºåÊâÄ‰ª•‰∏çË¶Å‰∏çÂ•ΩÊÑèÊÄù„ÄÇ&lt;/p&gt; 
&lt;h2&gt;TODO&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;ÊéíÁâà&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Ê≥®ÊÑè&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ÂêÑÁßçÈóÆÈ¢òÊàñËÄÖÂª∫ËÆÆÂèØ‰ª•ÊèêissueÔºåÂª∫ËÆÆ‰ΩøÁî®‰∏≠Êñá„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Áî±‰∫éÁâàÊùÉÈóÆÈ¢òÔºåÊàë‰ª¨‰∏çËÉΩÂ∞ÜÂõæÁâáÂíåbib‰∏ä‰º†ÔºåËØ∑ËßÅË∞Ö„ÄÇ&lt;/li&gt; 
 &lt;li&gt;Due to copyright issues, we would not upload figures and the bib file.&lt;/li&gt; 
 &lt;li&gt;ÂèØÁî®‰∫éÂ≠¶‰π†Á†îÁ©∂ÁõÆÁöÑÔºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂïÜ‰∏öË°å‰∏∫„ÄÇË∞¢Ë∞¢ÔºÅ&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;MarkdownÊ†ºÂºè&lt;/h2&gt; 
&lt;p&gt;ËøôÁßçÊ†ºÂºèÁ°ÆÂÆûÊØîËæÉÈáçË¶ÅÔºåÊñπ‰æøÊü•ÈòÖÔºå‰πüÊñπ‰æøÁ¥¢Âºï„ÄÇÂàùÊ≠•ËΩ¨Êç¢ÂêéÔºåÁîüÊàêÁΩëÈ°µÔºåÂÖ∑‰ΩìËßÅ&lt;a href=&quot;https://exacity.github.io/deeplearningbook-chinese&quot;&gt;deeplearningbook-chinese&lt;/a&gt;„ÄÇ Ê≥®ÊÑèÔºåËøôÁßçËΩ¨Êç¢Ê≤°ÊúâÊääÂõæÊîæËøõÂéªÔºå‰πü‰∏ç‰ºöÊîæÂõæ„ÄÇÁõÆÂâç‰ΩøÁî®Âçï‰∏™&lt;a href=&quot;https://raw.githubusercontent.com/exacity/deeplearningbook-chinese/master/scripts/convert2md.sh&quot;&gt;ËÑöÊú¨&lt;/a&gt;ÔºåÂü∫‰∫élatexÊñá‰ª∂ËΩ¨Êç¢Ôºå‰ª•ÂêéÂèØËÉΩ‰ºöÊõ¥Êîπ‰ΩÜÂéüÂàôÊòØ‰∏çÁõ¥Êé•‰øÆÊîπ&lt;a href=&quot;https://raw.githubusercontent.com/exacity/deeplearningbook-chinese/master/docs/_posts&quot;&gt;mdÊñá‰ª∂&lt;/a&gt;„ÄÇ ÈúÄË¶ÅÁöÑÂêåÂ≠¶ÂèØ‰ª•Ëá™Ë°å‰øÆÊîπ&lt;a href=&quot;https://raw.githubusercontent.com/exacity/deeplearningbook-chinese/master/scripts/convert2md.sh&quot;&gt;ËÑöÊú¨&lt;/a&gt;„ÄÇ&lt;/p&gt; 
&lt;h2&gt;HTMLÊ†ºÂºè&lt;/h2&gt; 
&lt;p&gt;ËØªËÄÖÂèØ‰ª•‰ΩøÁî®&lt;a href=&quot;https://github.com/coolwanglu/pdf2htmlEX&quot;&gt;pdf2htmlEX&lt;/a&gt;ËøõË°åËΩ¨Êç¢ÔºåÁõ¥Êé•Â∞ÜPDFËΩ¨Êç¢‰∏∫HTML„ÄÇ&lt;/p&gt; 
&lt;p&gt;Updating.....&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>libretro/docs</title>
      <link>https://github.com/libretro/docs</link>
      <description>&lt;p&gt;This is a repo of the RetroArch official document page.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Libretro Docs&lt;/h1&gt; 
&lt;p&gt;This repository contains the source for the official Libretro and RetroArch documentation, available at &lt;a href=&quot;https://docs.libretro.com/&quot;&gt;docs.libretro.com&lt;/a&gt;.&lt;/p&gt; 
&lt;h1&gt;Contribute to the documentation&lt;/h1&gt; 
&lt;p&gt;These docs are written in &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;Markdown.&lt;/a&gt; If you need help with the syntax, use &lt;a href=&quot;https://guides.github.com/features/mastering-markdown/&quot;&gt;this guide&lt;/a&gt;. Mkdocs uses some &lt;a href=&quot;http://www.mkdocs.org/user-guide/writing-your-docs/#markdown-extensions&quot;&gt;Markdown extensions&lt;/a&gt; that you may have to familiarize with.&lt;/p&gt; 
&lt;p&gt;The documentation source is maintained via &lt;a href=&quot;https://en.wikipedia.org/wiki/Git&quot;&gt;Git&lt;/a&gt;. For more info on how to use git, &lt;a href=&quot;https://help.github.com/&quot;&gt;refer to Github&#39;s help page.&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;In order to propose improvements to a document:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/libretro/docs&quot;&gt;Clone the repo&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;Make the changes and update your clone&lt;/li&gt; 
 &lt;li&gt;Follow the &quot;Building the docs&quot; section to render the documentation site locally&lt;/li&gt; 
 &lt;li&gt;Propose your changes using the button &lt;code&gt;New Pull Request&lt;/code&gt; &lt;a href=&quot;https://github.com/libretro/docs&quot;&gt;in the docs repo&lt;/a&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;There is a To-Do list for libretro/docs &lt;em&gt;here&lt;/em&gt; and you can submit suggestions or issues regarding documentation at the &lt;a href=&quot;https://github.com/libretro/docs/issues&quot;&gt;libretro/docs issue tracker&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Building the docs&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Make sure you have &lt;a href=&quot;https://www.python.org/&quot;&gt;Python&lt;/a&gt; and &lt;a href=&quot;https://pip.pypa.io&quot;&gt;pip&lt;/a&gt; installed &lt;pre&gt;&lt;code&gt;python --version
pip --version
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;!!! Note &quot;Building in Windows/msys2&quot; If you are using the standard RetroArch msys2 environment, you will need to install python with the command &lt;code&gt;pacman -S python&lt;/code&gt;. Next you will need to download &lt;a href=&quot;https://bootstrap.pypa.io/get-pip.py&quot;&gt;the &lt;code&gt;get-pip.py&lt;/code&gt; script&lt;/a&gt; from the &lt;code&gt;pip&lt;/code&gt; bootstrap site. Finally, execute the script with the command &lt;code&gt;python get-pip.py&lt;/code&gt;.&lt;/p&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt; &lt;p&gt;Install MkDocs&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install MkDocs-Material&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs-material
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install PyMdown Extensions&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install pymdown-extensions
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install mkdocs-git-revision-date&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs-git-revision-date-plugin
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Install mkdocs-macros&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install mkdocs-macros-plugin
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Build the site&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdocs build
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;The documentation will be built to the &lt;code&gt;site&lt;/code&gt; directory; preview any changes with MkDocs&#39; built-in dev-server before submitting a pull request&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdocs serve
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.mkdocs.org/#installation&quot;&gt;Guide to installing mkdocs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Adding a new core&lt;/h2&gt; 
&lt;p&gt;These are the documents that should be added/updated when a new core is added to the libretro ecosystem.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Add the core to docs/library/ (Follow the latest core template. docs/meta/core-template.md)&lt;/li&gt; 
 &lt;li&gt;Add the core to mkdocs.yml&lt;/li&gt; 
 &lt;li&gt;Add the core to docs/guides/core-list.md&lt;/li&gt; 
 &lt;li&gt;Add the core to docs/meta/see-also.md if it&#39;s related to another core in some way&lt;/li&gt; 
 &lt;li&gt;Add the core to docs/development/licenses.md&lt;/li&gt; 
 &lt;li&gt;Add the core to docs/guides/softpatching.md if it supports softpatching&lt;/li&gt; 
 &lt;li&gt;Add the core to docs/guides/retroachievements.md if it supports cheevos&lt;/li&gt; 
 &lt;li&gt;Add the core to docs/library/bios.md if it needs a BIOS&lt;/li&gt; 
&lt;/ul&gt;</description>
    </item>
    
  </channel>
</rss>
