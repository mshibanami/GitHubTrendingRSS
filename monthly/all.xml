<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>GitHub All Languages Monthly Trending</title>
    <description>Monthly Trending of All Languages in GitHub</description>
    <pubDate>Sat, 27 Sep 2025 01:50:43 GMT</pubDate>
    <link>http://mshibanami.github.io/GitHubTrendingRSS</link>
    
    <item>
      <title>coinbase/x402</title>
      <link>https://github.com/coinbase/x402</link>
      <description>&lt;p&gt;A payments protocol for the internet. Built on HTTP.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;x402 payments protocol&lt;/h1&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&quot;1 line of code to accept digital dollars. No fee, 2 second settlement, $0.001 minimum payment.&quot;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;pre&gt;&lt;code class=&quot;language-typescript&quot;&gt;app.use(
  // How much you want to charge, and where you want the funds to land
  paymentMiddleware(&quot;0xYourAddress&quot;, { &quot;/your-endpoint&quot;: &quot;$0.01&quot; })
);
// That&#39;s it! See examples/typescript/servers/express.ts for a complete example. Instruction below for running on base-sepolia.
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Philosophy&lt;/h2&gt; 
&lt;p&gt;Payments on the internet are fundamentally flawed. Credit Cards are high friction, hard to accept, have minimum payments that are far too high, and don&#39;t fit into the programmatic nature of the internet. It&#39;s time for an open, internet-native form of payments. A payment rail that doesn&#39;t have high minimums + % based fee. Payments that are amazing for humans and AI agents.&lt;/p&gt; 
&lt;h2&gt;Principles&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Open standard:&lt;/strong&gt; the x402 protocol will never force reliance on a single party&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP Native:&lt;/strong&gt; x402 is meant to seamlessly complement the existing HTTP request made by traditional web services, it should not mandate additional requests outside the scope of a typical client / server flow.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Chain and token agnostic:&lt;/strong&gt; we welcome contributions that add support for new chains, signing standards, or schemes, so long as they meet our acceptance criteria laid out in &lt;a href=&quot;https://github.com/coinbase/x402/raw/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Trust minimizing:&lt;/strong&gt; all payment schemes must not allow for the facilitator or resource server to move funds, other than in accordance with client intentions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Easy to use:&lt;/strong&gt; x402 needs to be 10x better than existing ways to pay on the internet. This means abstracting as many details of crypto as possible away from the client and resource server, and into the facilitator. This means the client/server should not need to think about gas, rpc, etc.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Ecosystem&lt;/h2&gt; 
&lt;p&gt;The x402 ecosystem is growing! Check out our &lt;a href=&quot;https://x402.org/ecosystem&quot;&gt;ecosystem page&lt;/a&gt; to see projects building with x402, including:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Client-side integrations&lt;/li&gt; 
 &lt;li&gt;Services and endpoints&lt;/li&gt; 
 &lt;li&gt;Ecosystem infrastructure and tooling&lt;/li&gt; 
 &lt;li&gt;Learning and community resources&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Want to add your project to the ecosystem? See our &lt;a href=&quot;https://github.com/coinbase/x402/tree/main/typescript/site#adding-your-project-to-the-ecosystem&quot;&gt;demo site README&lt;/a&gt; for detailed instructions on how to submit your project.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Roadmap:&lt;/strong&gt; see &lt;a href=&quot;https://github.com/coinbase/x402/raw/main/ROADMAP.md&quot;&gt;ROADMAP.md&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Terms:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;resource&lt;/code&gt;: Something on the internet. This could be a webpage, file server, RPC service, API, any resource on the internet that accepts HTTP / HTTPS requests.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;client&lt;/code&gt;: An entity wanting to pay for a resource.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;facilitator server&lt;/code&gt;: A server that facilitates verification and execution of on-chain payments.&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;resource server&lt;/code&gt;: An HTTP server that provides an API or other resource for a client.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Technical Goals:&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;Permissionless and secure for clients and servers&lt;/li&gt; 
 &lt;li&gt;Gasless for client and resource servers&lt;/li&gt; 
 &lt;li&gt;Minimal integration for the resource server and client (1 line for the server, 1 function for the client)&lt;/li&gt; 
 &lt;li&gt;Ability to trade off speed of response for guarantee of payment&lt;/li&gt; 
 &lt;li&gt;Extensible to different payment flows and chains&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;V1 Protocol&lt;/h2&gt; 
&lt;p&gt;The &lt;code&gt;x402&lt;/code&gt; protocol is a chain agnostic standard for payments on top of HTTP, leverage the existing &lt;code&gt;402 Payment Required&lt;/code&gt; HTTP status code to indicate that a payment is required for access to the resource.&lt;/p&gt; 
&lt;p&gt;It specifies:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;A schema for how servers can respond to clients to facilitate payment for a resource (&lt;code&gt;PaymentRequirements&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A standard header &lt;code&gt;X-PAYMENT&lt;/code&gt; that is set by clients paying for resources&lt;/li&gt; 
 &lt;li&gt;A standard schema and encoding method for data in the &lt;code&gt;X-PAYMENT&lt;/code&gt; header&lt;/li&gt; 
 &lt;li&gt;A recommended flow for how payments should be verified and settled by a resource server&lt;/li&gt; 
 &lt;li&gt;A REST specification for how a resource server can perform verification and settlement against a remote 3rd party server (&lt;code&gt;facilitator&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;A specification for a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header that can be used by resource servers to communicate blockchain transactions details to the client in their HTTP response&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;V1 Protocol Sequencing&lt;/h3&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/coinbase/x402/main/static/x402-protocol-flow.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The following outlines the flow of a payment using the &lt;code&gt;x402&lt;/code&gt; protocol. Note that steps (1) and (2) are optional if the client already knows the payment details accepted for a resource.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; makes an HTTP request to a &lt;code&gt;resource server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; responds with a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; selects one of the &lt;code&gt;paymentRequirements&lt;/code&gt; returned by the server response and creates a &lt;code&gt;Payment Payload&lt;/code&gt; based on the &lt;code&gt;scheme&lt;/code&gt; of the &lt;code&gt;paymentRequirements&lt;/code&gt; they have selected.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Client&lt;/code&gt; sends the HTTP request with the &lt;code&gt;X-PAYMENT&lt;/code&gt; header containing the &lt;code&gt;Payment Payload&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; verifies the &lt;code&gt;Payment Payload&lt;/code&gt; is valid either via local verification or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment Requirements&lt;/code&gt; to the &lt;code&gt;/verify&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; performs verification of the object based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt; and returns a &lt;code&gt;Verification Response&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;If the &lt;code&gt;Verification Response&lt;/code&gt; is valid, the resource server performs the work to fulfill the request. If the &lt;code&gt;Verification Response&lt;/code&gt; is invalid, the resource server returns a &lt;code&gt;402 Payment Required&lt;/code&gt; status and a &lt;code&gt;Payment Required Response&lt;/code&gt; JSON object in the response body.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; either settles the payment by interacting with a blockchain directly, or by POSTing the &lt;code&gt;Payment Payload&lt;/code&gt; and &lt;code&gt;Payment PaymentRequirements&lt;/code&gt; to the &lt;code&gt;/settle&lt;/code&gt; endpoint of a &lt;code&gt;facilitator server&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; submits the payment to the blockchain based on the &lt;code&gt;scheme&lt;/code&gt; and &lt;code&gt;network&lt;/code&gt; of the &lt;code&gt;Payment Payload&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; waits for the payment to be confirmed on the blockchain.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Facilitator server&lt;/code&gt; returns a &lt;code&gt;Payment Execution Response&lt;/code&gt; to the resource server.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;code&gt;Resource server&lt;/code&gt; returns a &lt;code&gt;200 OK&lt;/code&gt; response to the &lt;code&gt;Client&lt;/code&gt; with the resource they requested as the body of the HTTP response, and a &lt;code&gt;X-PAYMENT-RESPONSE&lt;/code&gt; header containing the &lt;code&gt;Settlement Response&lt;/code&gt; as Base64 encoded JSON if the payment was executed successfully.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;Type Specifications&lt;/h3&gt; 
&lt;h4&gt;Data types&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Payment Required Response&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  // Version of the x402 payment protocol
  x402Version: int,

  // List of payment requirements that the resource server accepts. A resource server may accept on multiple chains, or in multiple currencies.
  accepts: [paymentRequirements]

  // Message from the resource server to the client to communicate errors in processing payment
  error: string
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;paymentRequirements&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  // Scheme of the payment protocol to use
  scheme: string;

  // Network of the blockchain to send payment on
  network: string;

  // Maximum amount required to pay for the resource in atomic units of the asset
  maxAmountRequired: uint256 as string;

  // URL of resource to pay for
  resource: string;

  // Description of the resource
  description: string;

  // MIME type of the resource response
  mimeType: string;

  // Output schema of the resource response
  outputSchema?: object | null;

  // Address to pay value to
  payTo: string;

  // Maximum time in seconds for the resource server to respond
  maxTimeoutSeconds: number;

  // Address of the EIP-3009 compliant ERC20 contract
  asset: string;

  // Extra information about the payment details specific to the scheme
  // For `exact` scheme on a EVM network, expects extra to contain the records `name` and `version` pertaining to asset
  extra: object | null;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;&lt;code&gt;Payment Payload&lt;/code&gt;&lt;/strong&gt; (included as the &lt;code&gt;X-PAYMENT&lt;/code&gt; header in base64 encoded json)&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  // Version of the x402 payment protocol
  x402Version: number;

  // scheme is the scheme value of the accepted `paymentRequirements` the client is using to pay
  scheme: string;

  // network is the network id of the accepted `paymentRequirements` the client is using to pay
  network: string;

  // payload is scheme dependent
  payload: &amp;lt;scheme dependent&amp;gt;;
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Facilitator Types &amp;amp; Interface&lt;/h4&gt; 
&lt;p&gt;A &lt;code&gt;facilitator server&lt;/code&gt; is a 3rd party service that can be used by a &lt;code&gt;resource server&lt;/code&gt; to verify and settle payments, without the &lt;code&gt;resource server&lt;/code&gt; needing to have access to a blockchain node or wallet.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;POST /verify&lt;/strong&gt;. Verify a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Request body JSON: &lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  isValid: boolean;
  invalidReason: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;POST /settle&lt;/strong&gt;. Settle a payment with a supported scheme and network:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Request body JSON:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  x402Version: number;
  paymentHeader: string;
  paymentRequirements: paymentRequirements;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Response:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  // Whether the payment was successful
  success: boolean;

  // Error message from the facilitator server
  error: string | null;

  // Transaction hash of the settled payment
  txHash: string | null;

  // Network id of the blockchain the payment was settled on
  networkId: string | null;
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;GET /supported&lt;/strong&gt;. Get supported payment schemes and networks:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Response: &lt;pre&gt;&lt;code class=&quot;language-json5&quot;&gt;{
  kinds: [
    {
      &quot;scheme&quot;: string,
      &quot;network&quot;: string,
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Schemes&lt;/h3&gt; 
&lt;p&gt;A scheme is a logical way of moving money.&lt;/p&gt; 
&lt;p&gt;Blockchains allow for a large number of flexible ways to move money. To help facilitate an expanding number of payment use cases, the &lt;code&gt;x402&lt;/code&gt; protocol is extensible to different ways of settling payments via its &lt;code&gt;scheme&lt;/code&gt; field.&lt;/p&gt; 
&lt;p&gt;Each payment scheme may have different operational functionality depending on what actions are necessary to fulfill the payment. For example &lt;code&gt;exact&lt;/code&gt;, the first scheme shipping as part of the protocol, would have different behavior than &lt;code&gt;upto&lt;/code&gt;. &lt;code&gt;exact&lt;/code&gt; transfers a specific amount (ex: pay $1 to read an article), while a theoretical &lt;code&gt;upto&lt;/code&gt; would transfer up to an amount, based on the resources consumed during a request (ex: generating tokens from an LLM).&lt;/p&gt; 
&lt;p&gt;See &lt;code&gt;specs/schemes&lt;/code&gt; for more details on schemes, and see &lt;code&gt;specs/schemes/exact/scheme_exact_evm.md&lt;/code&gt; to see the first proposed scheme for exact payment on EVM chains.&lt;/p&gt; 
&lt;h3&gt;Schemes vs Networks&lt;/h3&gt; 
&lt;p&gt;Because a scheme is a logical way of moving money, the way a scheme is implemented can be different for different blockchains. (ex: the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Ethereum is very different from the way you need to implement &lt;code&gt;exact&lt;/code&gt; on Solana).&lt;/p&gt; 
&lt;p&gt;Clients and facilitators must explicitly support different &lt;code&gt;(scheme, network)&lt;/code&gt; pairs in order to be able to create proper payloads and verify / settle payments.&lt;/p&gt; 
&lt;h2&gt;Running example&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;Requirements:&lt;/strong&gt; Node.js v24 or higher&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;From &lt;code&gt;examples/typescript&lt;/code&gt; run &lt;code&gt;pnpm install&lt;/code&gt; and &lt;code&gt;pnpm build&lt;/code&gt; to ensure all dependent packages and examples are setup.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a server, i.e. express, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your server&#39;s ethereum address to get paid to into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Select a client, i.e. axios, and &lt;code&gt;cd&lt;/code&gt; into that example. Add your private key for the account making payments into the &lt;code&gt;.env&lt;/code&gt; file, and then run &lt;code&gt;pnpm dev&lt;/code&gt; in that directory.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;You should see activities in the client terminal, which will display a weather report.&lt;/p&gt; 
&lt;h2&gt;Running tests&lt;/h2&gt; 
&lt;ol&gt; 
 &lt;li&gt;Navigate to the typescript directory: &lt;code&gt;cd typescript&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Install dependencies: &lt;code&gt;pnpm install&lt;/code&gt;&lt;/li&gt; 
 &lt;li&gt;Run the unit tests: &lt;code&gt;pnpm test&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This will run the unit tests for the x402 packages.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>pedroslopez/whatsapp-web.js</title>
      <link>https://github.com/pedroslopez/whatsapp-web.js</link>
      <description>&lt;p&gt;A WhatsApp client library for NodeJS that connects through the WhatsApp Web browser app&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;br /&gt; 
 &lt;p&gt; &lt;a href=&quot;https://wwebjs.dev&quot;&gt;&lt;img src=&quot;https://github.com/wwebjs/logos/raw/main/4_Full%20Logo%20Lockup_Small/small_banner_blue.png?raw=true&quot; title=&quot;whatsapp-web.js&quot; alt=&quot;WWebJS Website&quot; width=&quot;500&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;br /&gt; 
 &lt;p&gt; &lt;a href=&quot;https://www.npmjs.com/package/whatsapp-web.js&quot;&gt;&lt;img src=&quot;https://img.shields.io/npm/v/whatsapp-web.js.svg?sanitize=true&quot; alt=&quot;npm&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://depfu.com/github/pedroslopez/whatsapp-web.js?project_id=9765&quot;&gt;&lt;img src=&quot;https://badges.depfu.com/badges/4a65a0de96ece65fdf39e294e0c8dcba/overview.svg?sanitize=true&quot; alt=&quot;Depfu&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/WhatsApp_Web-2.3000.1017054665-brightgreen.svg?sanitize=true&quot; alt=&quot;WhatsApp_Web 2.2346.52&quot; /&gt; &lt;a href=&quot;https://discord.gg/H7DqQs4&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/698610475432411196.svg?logo=discord&quot; alt=&quot;Discord server&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;br /&gt; 
&lt;/div&gt; 
&lt;h2&gt;About&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;A WhatsApp API client that connects through the WhatsApp Web browser app&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;The library works by launching the WhatsApp Web browser application and managing it using Puppeteer to create an instance of WhatsApp Web, thereby mitigating the risk of being blocked. The WhatsApp API client connects through the WhatsApp Web browser app, accessing its internal functions. This grants you access to nearly all the features available on WhatsApp Web, enabling dynamic handling similar to any other Node.js application.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT] &lt;strong&gt;It is not guaranteed you will not be blocked by using this method. WhatsApp does not allow bots or unofficial clients on their platform, so this shouldn&#39;t be considered totally safe.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;Links&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://wwebjs.dev&quot;&gt;Website&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://guide.wwebjs.dev/guide&quot;&gt;Guide&lt;/a&gt; (&lt;a href=&quot;https://github.com/wwebjs/wwebjs.dev/tree/main&quot;&gt;source&lt;/a&gt;) &lt;em&gt;(work in progress)&lt;/em&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.wwebjs.dev/&quot;&gt;Documentation&lt;/a&gt; (&lt;a href=&quot;https://github.com/pedroslopez/whatsapp-web.js/tree/main/docs&quot;&gt;source&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://discord.gg/H7DqQs4&quot;&gt;WWebJS Discord&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/pedroslopez/whatsapp-web.js&quot;&gt;GitHub&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://npmjs.org/package/whatsapp-web.js&quot;&gt;npm&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;The module is now available on npm! &lt;code&gt;npm i whatsapp-web.js&lt;/code&gt;&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] &lt;strong&gt;Node &lt;code&gt;v18+&lt;/code&gt; is required.&lt;/strong&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h2&gt;QUICK STEPS TO UPGRADE NODE&lt;/h2&gt; 
&lt;h3&gt;Windows&lt;/h3&gt; 
&lt;h4&gt;Manual&lt;/h4&gt; 
&lt;p&gt;Just get the latest LTS from the &lt;a href=&quot;https://nodejs.org/en/download/&quot;&gt;official node website&lt;/a&gt;.&lt;/p&gt; 
&lt;h4&gt;npm&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;sudo npm install -g n
sudo n stable
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Choco&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;choco install nodejs-lts
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Winget&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-powershell&quot;&gt;winget install OpenJS.NodeJS.LTS
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Ubuntu / Debian&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash - &amp;amp;&amp;amp;\
sudo apt-get install -y nodejs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Example usage&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-js&quot;&gt;const { Client } = require(&#39;whatsapp-web.js&#39;);

const client = new Client();

client.on(&#39;qr&#39;, (qr) =&amp;gt; {
    // Generate and scan this code with your phone
    console.log(&#39;QR RECEIVED&#39;, qr);
});

client.on(&#39;ready&#39;, () =&amp;gt; {
    console.log(&#39;Client is ready!&#39;);
});

client.on(&#39;message&#39;, msg =&amp;gt; {
    if (msg.body == &#39;!ping&#39;) {
        msg.reply(&#39;pong&#39;);
    }
});

client.initialize();
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Take a look at &lt;a href=&quot;https://github.com/pedroslopez/whatsapp-web.js/raw/master/example.js&quot;&gt;example.js&lt;/a&gt; for another examples with additional use cases.&lt;br /&gt; For further details on saving and restoring sessions, explore the provided &lt;a href=&quot;https://wwebjs.dev/guide/creating-your-bot/authentication.html&quot;&gt;Authentication Strategies&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Supported features&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Feature&lt;/th&gt; 
   &lt;th&gt;Status&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Multi Device&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send messages&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Receive messages&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send media (images/audio/documents)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send media (video)&lt;/td&gt; 
   &lt;td&gt;✅ &lt;a href=&quot;https://wwebjs.dev/guide/creating-your-bot/handling-attachments.html#caveat-for-sending-videos-and-gifs&quot;&gt;(requires Google Chrome)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send stickers&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Receive media (images/audio/video/documents)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send contact cards&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send location&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send buttons&lt;/td&gt; 
   &lt;td&gt;❌ &lt;a href=&quot;https://www.youtube.com/watch?v=hv1R1rLeVVE&quot;&gt;(DEPRECATED)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Send lists&lt;/td&gt; 
   &lt;td&gt;❌ &lt;a href=&quot;https://www.youtube.com/watch?v=hv1R1rLeVVE&quot;&gt;(DEPRECATED)&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Receive location&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Message replies&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Join groups by invite&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Get invite for group&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Modify group info (subject, description)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Modify group settings (send messages, edit info)&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Add group participants&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Kick group participants&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Promote/demote group participants&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mention users&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mention groups&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Mute/unmute chats&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Block/unblock contacts&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Get contact info&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Get profile pictures&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Set user status message&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;React to messages&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Create polls&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Channels&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Vote in polls&lt;/td&gt; 
   &lt;td&gt;🔜&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Communities&lt;/td&gt; 
   &lt;td&gt;🔜&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;Something missing? Make an issue and let us know!&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;Feel free to open pull requests; we welcome contributions! However, for significant changes, it&#39;s best to open an issue beforehand. Make sure to review our &lt;a href=&quot;https://github.com/pedroslopez/whatsapp-web.js/raw/main/CODE_OF_CONDUCT.md&quot;&gt;contribution guidelines&lt;/a&gt; before creating a pull request. Before creating your own issue or pull request, always check to see if one already exists!&lt;/p&gt; 
&lt;h2&gt;Supporting the project&lt;/h2&gt; 
&lt;p&gt;You can support the maintainer of this project through the links below&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/sponsors/pedroslopez&quot;&gt;Support via GitHub Sponsors&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://www.paypal.me/psla/&quot;&gt;Support via PayPal&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://m.do.co/c/73f906a36ed4&quot;&gt;Sign up for DigitalOcean&lt;/a&gt; and get $200 in credit when you sign up (Referral)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Disclaimer&lt;/h2&gt; 
&lt;p&gt;This project is not affiliated, associated, authorized, endorsed by, or in any way officially connected with WhatsApp or any of its subsidiaries or its affiliates. The official WhatsApp website can be found at &lt;a href=&quot;https://whatsapp.com&quot;&gt;whatsapp.com&lt;/a&gt;. &quot;WhatsApp&quot; as well as related names, marks, emblems and images are registered trademarks of their respective owners. Also it is not guaranteed you will not be blocked by using this method. WhatsApp does not allow bots or unofficial clients on their platform, so this shouldn&#39;t be considered totally safe.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;Copyright 2019 Pedro S Lopez&lt;/p&gt; 
&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&lt;br /&gt; you may not use this project except in compliance with the License.&lt;br /&gt; You may obtain a copy of the License at &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Unless required by applicable law or agreed to in writing, software&lt;br /&gt; distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br /&gt; WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br /&gt; See the License for the specific language governing permissions and&lt;br /&gt; limitations under the License.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>CorentinJ/Real-Time-Voice-Cloning</title>
      <link>https://github.com/CorentinJ/Real-Time-Voice-Cloning</link>
      <description>&lt;p&gt;Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Real-Time Voice Cloning&lt;/h1&gt; 
&lt;p&gt;This repository is an implementation of &lt;a href=&quot;https://arxiv.org/pdf/1806.04558.pdf&quot;&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/a&gt; (SV2TTS) with a vocoder that works in real-time. This was my &lt;a href=&quot;https://matheo.uliege.be/handle/2268.2/6801&quot;&gt;master&#39;s thesis&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Video demonstration&lt;/strong&gt; (click the picture):&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=-O_hYhToKoA&quot;&gt;&lt;img src=&quot;https://i.imgur.com/8lFUlgz.png&quot; alt=&quot;Toolbox demo&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h3&gt;Papers implemented&lt;/h3&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;URL&lt;/th&gt; 
   &lt;th&gt;Designation&lt;/th&gt; 
   &lt;th&gt;Title&lt;/th&gt; 
   &lt;th&gt;Implementation source&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1806.04558.pdf&quot;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1802.08435.pdf&quot;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; 
   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/fatchord/WaveRNN&quot;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1703.10135.pdf&quot;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; 
   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/fatchord/WaveRNN&quot;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;a href=&quot;https://arxiv.org/pdf/1710.10467.pdf&quot;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;GE2E (encoder)&lt;/td&gt; 
   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; 
   &lt;td&gt;This repo&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Heads up&lt;/h2&gt; 
&lt;p&gt;Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check out &lt;a href=&quot;https://paperswithcode.com/task/speech-synthesis/&quot;&gt;paperswithcode&lt;/a&gt; for other repositories and recent research in the field of speech synthesis.&lt;/li&gt; 
 &lt;li&gt;Check out &lt;a href=&quot;https://github.com/resemble-ai/chatterbox&quot;&gt;Chatterbox&lt;/a&gt; for a similar project up to date with the 2025 SOTA in voice cloning&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Running the toolbox&lt;/h2&gt; 
&lt;p&gt;Both Windows and Linux are supported.&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install &lt;a href=&quot;https://ffmpeg.org/download.html#get-packages&quot;&gt;ffmpeg&lt;/a&gt;. This is necessary for reading audio files. Check if it&#39;s installed by running in a command line&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;ffmpeg
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;2&quot;&gt; 
 &lt;li&gt;Install uv for python package management&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# On Windows:
powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;
# On Linux
curl -LsSf https://astral.sh/uv/install.sh | sh

# Alternatively, on any platform if you have pip installed you can do
pip install -U uv
&lt;/code&gt;&lt;/pre&gt; 
&lt;ol start=&quot;3&quot;&gt; 
 &lt;li&gt;Run one of the following commands&lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code&gt;# Run the toolbox if you have an NVIDIA GPU
uv run --extra cuda demo_toolbox.py
# Use this if you don&#39;t
uv run --extra cpu demo_toolbox.py

# Run in command line if you don&#39;t want the GUI
uv run --extra cuda demo_cli.py
uv run --extra cpu demo_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Uv will automatically create a .venv directory for you with an appropriate python environment. &lt;a href=&quot;https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues&quot;&gt;Open an issue&lt;/a&gt; if this fails for you&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Pretrained Models&lt;/h3&gt; 
&lt;p&gt;Pretrained models are now downloaded automatically. If this doesn&#39;t work for you, you can manually download them &lt;a href=&quot;https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;(Optional) Download Datasets&lt;/h3&gt; 
&lt;p&gt;For playing with the toolbox alone, I only recommend downloading &lt;a href=&quot;https://www.openslr.org/resources/12/train-clean-100.tar.gz&quot;&gt;&lt;code&gt;LibriSpeech/train-clean-100&lt;/code&gt;&lt;/a&gt;. Extract the contents as &lt;code&gt;&amp;lt;datasets_root&amp;gt;/LibriSpeech/train-clean-100&lt;/code&gt; where &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is a directory of your choosing. Other datasets are supported in the toolbox, see &lt;a href=&quot;https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets&quot;&gt;here&lt;/a&gt;. You&#39;re free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>uutils/coreutils</title>
      <link>https://github.com/uutils/coreutils</link>
      <description>&lt;p&gt;Cross-platform Rust rewrite of the GNU coreutils&lt;/p&gt;&lt;hr&gt;&lt;div class=&quot;oranda-hide&quot;&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/uutils/coreutils/main/docs/src/logo.svg?sanitize=true&quot; alt=&quot;uutils logo&quot; /&gt;&lt;/p&gt; 
  &lt;h1&gt;uutils coreutils&lt;/h1&gt; 
  &lt;p&gt;&lt;a href=&quot;https://crates.io/crates/coreutils&quot;&gt;&lt;img src=&quot;https://img.shields.io/crates/v/coreutils.svg?sanitize=true&quot; alt=&quot;Crates.io&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/wQVJbvJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/discord-join-7289DA.svg?logo=discord&amp;amp;longCache=true&amp;amp;style=flat&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/uutils/coreutils/raw/main/LICENSE&quot;&gt;&lt;img src=&quot;http://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://deps.rs/repo/github/uutils/coreutils&quot;&gt;&lt;img src=&quot;https://deps.rs/repo/github/uutils/coreutils/status.svg?sanitize=true&quot; alt=&quot;dependency status&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
  &lt;p&gt;&lt;a href=&quot;https://codecov.io/gh/uutils/coreutils&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/uutils/coreutils/branch/master/graph/badge.svg?sanitize=true&quot; alt=&quot;CodeCov&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/MSRV-1.85.0-brightgreen&quot; alt=&quot;MSRV&quot; /&gt; &lt;a href=&quot;https://hosted.weblate.org/projects/rust-coreutils/&quot;&gt;&lt;img src=&quot;https://hosted.weblate.org/widget/rust-coreutils/svg-badge.svg?sanitize=true&quot; alt=&quot;Weblate&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
&lt;/div&gt; 
&lt;p&gt;uutils coreutils is a cross-platform reimplementation of the GNU coreutils in &lt;a href=&quot;http://www.rust-lang.org&quot;&gt;Rust&lt;/a&gt;. While all programs have been implemented, some options might be missing or different behavior might be experienced.&lt;/p&gt; 
&lt;div class=&quot;oranda-hide&quot;&gt; 
 &lt;p&gt;To install it:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo install coreutils
~/.cargo/bin/coreutils
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;!-- markdownlint-disable-next-line MD026 --&gt; 
&lt;h2&gt;Goals&lt;/h2&gt; 
&lt;p&gt;uutils coreutils aims to be a drop-in replacement for the GNU utils. Differences with GNU are treated as bugs.&lt;/p&gt; 
&lt;p&gt;Our key objectives include:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Matching GNU&#39;s output (stdout and error code) exactly&lt;/li&gt; 
 &lt;li&gt;Better error messages&lt;/li&gt; 
 &lt;li&gt;Providing comprehensive internationalization support (UTF-8)&lt;/li&gt; 
 &lt;li&gt;Improved performances&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/uutils/coreutils/main/docs/src/extensions.md&quot;&gt;Extensions&lt;/a&gt; when relevant (example: --progress)&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;uutils aims to work on as many platforms as possible, to be able to use the same utils on Linux, macOS, Windows and other platforms. This ensures, for example, that scripts can be easily transferred between platforms.&lt;/p&gt; 
&lt;div class=&quot;oranda-hide&quot;&gt; 
 &lt;h2&gt;Documentation&lt;/h2&gt; 
 &lt;p&gt;uutils has both user and developer documentation available:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;a href=&quot;https://uutils.github.io/coreutils/docs/&quot;&gt;User Manual&lt;/a&gt;&lt;/li&gt; 
  &lt;li&gt;&lt;a href=&quot;https://docs.rs/crate/coreutils/&quot;&gt;Developer Documentation&lt;/a&gt;&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Both can also be generated locally, the instructions for that can be found in the &lt;a href=&quot;https://github.com/uutils/uutils.github.io&quot;&gt;coreutils docs&lt;/a&gt; repository.&lt;/p&gt; 
 &lt;p&gt;Use &lt;a href=&quot;https://hosted.weblate.org/projects/rust-coreutils/&quot;&gt;weblate/rust-coreutils&lt;/a&gt; to translate the Rust coreutils into your language.&lt;/p&gt; 
 &lt;!-- ANCHOR: build (this mark is needed for mdbook) --&gt; 
 &lt;h2&gt;Requirements&lt;/h2&gt; 
 &lt;ul&gt; 
  &lt;li&gt;Rust (&lt;code&gt;cargo&lt;/code&gt;, &lt;code&gt;rustc&lt;/code&gt;)&lt;/li&gt; 
  &lt;li&gt;GNU Make (optional)&lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;h3&gt;Rust Version&lt;/h3&gt; 
 &lt;p&gt;uutils follows Rust&#39;s release channels and is tested against stable, beta and nightly. The current Minimum Supported Rust Version (MSRV) is &lt;code&gt;1.85.0&lt;/code&gt;.&lt;/p&gt; 
 &lt;h2&gt;Building&lt;/h2&gt; 
 &lt;p&gt;There are currently two methods to build the uutils binaries: either Cargo or GNU Make.&lt;/p&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;Building the full package, including all documentation, requires both Cargo and GNU Make on a Unix platform.&lt;/p&gt; 
 &lt;/blockquote&gt; 
 &lt;p&gt;For either method, we first need to fetch the repository:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;git clone https://github.com/uutils/coreutils
cd coreutils
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Cargo&lt;/h3&gt; 
 &lt;p&gt;Building uutils using Cargo is easy because the process is the same as for every other Rust program:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo build --release
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This command builds the most portable common core set of uutils into a multicall (BusyBox-type) binary, named &#39;coreutils&#39;, on most Rust-supported platforms.&lt;/p&gt; 
 &lt;p&gt;Additional platform-specific uutils are often available. Building these expanded sets of uutils for a platform (on that platform) is as simple as specifying it as a feature:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo build --release --features macos
# or ...
cargo build --release --features windows
# or ...
cargo build --release --features unix
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you don&#39;t want to build every utility available on your platform into the final binary, you can also specify which ones you want to build manually. For example:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo build --features &quot;base32 cat echo rm&quot; --no-default-features
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;If you don&#39;t want to build the multicall binary and would prefer to build the utilities as individual binaries, that is also possible. Each utility is contained in its own package within the main repository, named &quot;uu_UTILNAME&quot;. To build individual utilities, use cargo to build just the specific packages (using the &lt;code&gt;--package&lt;/code&gt; [aka &lt;code&gt;-p&lt;/code&gt;] option). For example:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo build -p uu_base32 -p uu_cat -p uu_echo -p uu_rm
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;GNU Make&lt;/h3&gt; 
 &lt;p&gt;Building using &lt;code&gt;make&lt;/code&gt; is a simple process as well.&lt;/p&gt; 
 &lt;p&gt;To simply build all available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;In release mode:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make PROFILE=release
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To build all but a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make SKIP_UTILS=&#39;UTILITY_1 UTILITY_2&#39;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To build only a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make UTILS=&#39;UTILITY_1 UTILITY_2&#39;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h2&gt;Installation&lt;/h2&gt; 
 &lt;h3&gt;Install with Cargo&lt;/h3&gt; 
 &lt;p&gt;Likewise, installing can simply be done using:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo install --path . --locked
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;This command will install uutils into Cargo&#39;s &lt;em&gt;bin&lt;/em&gt; folder (&lt;em&gt;e.g.&lt;/em&gt; &lt;code&gt;$HOME/.cargo/bin&lt;/code&gt;).&lt;/p&gt; 
 &lt;p&gt;This does not install files necessary for shell completion or manpages. For manpages or shell completion to work, use &lt;code&gt;GNU Make&lt;/code&gt; or see &lt;code&gt;Manually install shell completions&lt;/code&gt;/&lt;code&gt;Manually install manpages&lt;/code&gt;.&lt;/p&gt; 
 &lt;h3&gt;Install with GNU Make&lt;/h3&gt; 
 &lt;p&gt;To install all available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install using &lt;code&gt;sudo&lt;/code&gt; switch &lt;code&gt;-E&lt;/code&gt; must be used:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;sudo -E make install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install all but a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make SKIP_UTILS=&#39;UTILITY_1 UTILITY_2&#39; install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install only a few of the available utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make UTILS=&#39;UTILITY_1 UTILITY_2&#39; install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install every program with a prefix (e.g. uu-echo uu-cat):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make PROG_PREFIX=PREFIX_GOES_HERE install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To install the multicall binary:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make MULTICALL=y install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Set install parent directory (default value is /usr/local):&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# DESTDIR is also supported
make PREFIX=/my/path install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;Installing with &lt;code&gt;make&lt;/code&gt; installs shell completions for all installed utilities for &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;fish&lt;/code&gt; and &lt;code&gt;zsh&lt;/code&gt;. Completions for &lt;code&gt;elvish&lt;/code&gt; and &lt;code&gt;powershell&lt;/code&gt; can also be generated; See &lt;code&gt;Manually install shell completions&lt;/code&gt;.&lt;/p&gt; 
 &lt;p&gt;To skip installation of completions and manpages:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make COMPLETIONS=n MANPAGES=n install
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Manually install shell completions&lt;/h3&gt; 
 &lt;p&gt;The &lt;code&gt;coreutils&lt;/code&gt; binary can generate completions for the &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;elvish&lt;/code&gt;, &lt;code&gt;fish&lt;/code&gt;, &lt;code&gt;powershell&lt;/code&gt; and &lt;code&gt;zsh&lt;/code&gt; shells. It prints the result to stdout.&lt;/p&gt; 
 &lt;p&gt;The syntax is:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo run completion &amp;lt;utility&amp;gt; &amp;lt;shell&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;So, to install completions for &lt;code&gt;ls&lt;/code&gt; on &lt;code&gt;bash&lt;/code&gt; to &lt;code&gt;/usr/local/share/bash-completion/completions/ls&lt;/code&gt;, run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo run completion ls bash &amp;gt; /usr/local/share/bash-completion/completions/ls
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Manually install manpages&lt;/h3&gt; 
 &lt;p&gt;To generate manpages, the syntax is:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cargo run manpage &amp;lt;utility&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;So, to install the manpage for &lt;code&gt;ls&lt;/code&gt; to &lt;code&gt;/usr/local/share/man/man1/ls.1&lt;/code&gt; run:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cargo run manpage ls &amp;gt; /usr/local/share/man/man1/ls.1
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h2&gt;Un-installation&lt;/h2&gt; 
 &lt;p&gt;Un-installation differs depending on how you have installed uutils. If you used Cargo to install, use Cargo to uninstall. If you used GNU Make to install, use Make to uninstall.&lt;/p&gt; 
 &lt;h3&gt;Uninstall with Cargo&lt;/h3&gt; 
 &lt;p&gt;To uninstall uutils:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cargo uninstall coreutils
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h3&gt;Uninstall with GNU Make&lt;/h3&gt; 
 &lt;p&gt;To uninstall all utilities:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To uninstall every program with a set prefix:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make PROG_PREFIX=PREFIX_GOES_HERE uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To uninstall the multicall binary:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;make MULTICALL=y uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;To uninstall from a custom parent directory:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# DESTDIR is also supported
make PREFIX=/my/path uninstall
&lt;/code&gt;&lt;/pre&gt; 
 &lt;!-- ANCHOR_END: build (this mark is needed for mdbook) --&gt; 
 &lt;h2&gt;GNU test suite compatibility&lt;/h2&gt; 
 &lt;p&gt;Below is the evolution of how many GNU tests uutils passes. A more detailed breakdown of the GNU test results of the main branch can be found &lt;a href=&quot;https://uutils.github.io/coreutils/docs/test_coverage.html&quot;&gt;in the user manual&lt;/a&gt;.&lt;/p&gt; 
 &lt;p&gt;See &lt;a href=&quot;https://github.com/orgs/uutils/projects/1&quot;&gt;https://github.com/orgs/uutils/projects/1&lt;/a&gt; for the main meta bugs (many are missing).&lt;/p&gt; 
 &lt;p&gt;&lt;img src=&quot;https://github.com/uutils/coreutils-tracking/raw/main/gnu-results.svg?raw=true&quot; alt=&quot;Evolution over time&quot; /&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;!-- close oranda-hide div --&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;To contribute to uutils, please see &lt;a href=&quot;https://raw.githubusercontent.com/uutils/coreutils/main/CONTRIBUTING.md&quot;&gt;CONTRIBUTING&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;uutils is licensed under the MIT License - see the &lt;code&gt;LICENSE&lt;/code&gt; file for details&lt;/p&gt; 
&lt;p&gt;GNU Coreutils is licensed under the GPL 3.0 or later.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>ItzCrazyKns/Perplexica</title>
      <link>https://github.com/ItzCrazyKns/Perplexica</link>
      <description>&lt;p&gt;Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🚀 Perplexica - An AI-powered search engine 🔎 
 &lt;!-- omit in toc --&gt;&lt;/h1&gt; 
&lt;div align=&quot;center&quot; markdown=&quot;1&quot;&gt; 
 &lt;sup&gt;Special thanks to:&lt;/sup&gt; 
 &lt;br /&gt; 
 &lt;br /&gt; 
 &lt;a href=&quot;https://www.warp.dev/perplexica&quot;&gt; &lt;img alt=&quot;Warp sponsorship&quot; width=&quot;400&quot; src=&quot;https://github.com/user-attachments/assets/775dd593-9b5f-40f1-bf48-479faff4c27b&quot; /&gt; &lt;/a&gt; 
 &lt;h3&gt;&lt;a href=&quot;https://www.warp.dev/perplexica&quot;&gt;Warp, the AI Devtool that lives in your terminal&lt;/a&gt;&lt;/h3&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.warp.dev/perplexica&quot;&gt;Available for MacOS, Linux, &amp;amp; Windows&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;p&gt;&lt;a href=&quot;https://discord.gg/26aArMy8tT&quot;&gt;&lt;img src=&quot;https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-screenshot.png?&quot; alt=&quot;preview&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Table of Contents 
 &lt;!-- omit in toc --&gt;&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#overview&quot;&gt;Overview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#preview&quot;&gt;Preview&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#features&quot;&gt;Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#installation&quot;&gt;Installation&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#getting-started-with-docker-recommended&quot;&gt;Getting Started with Docker (Recommended)&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#non-docker-installation&quot;&gt;Non-Docker Installation&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#ollama-connection-errors&quot;&gt;Ollama Connection Errors&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#lemonade-connection-errors&quot;&gt;Lemonade Connection Errors&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-as-a-search-engine&quot;&gt;Using as a Search Engine&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#using-perplexicas-api&quot;&gt;Using Perplexica&#39;s API&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#expose-perplexica-to-network&quot;&gt;Expose Perplexica to a network&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#one-click-deployment&quot;&gt;One-Click Deployment&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features&quot;&gt;Upcoming Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#support-us&quot;&gt;Support Us&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#donations&quot;&gt;Donations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#contribution&quot;&gt;Contribution&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#help-and-support&quot;&gt;Help and Support&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Overview&lt;/h2&gt; 
&lt;p&gt;Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it&#39;s an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.&lt;/p&gt; 
&lt;p&gt;Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.&lt;/p&gt; 
&lt;p&gt;Want to know more about its architecture and how it works? You can read it &lt;a href=&quot;https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Preview&lt;/h2&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/.assets/perplexica-preview.gif&quot; alt=&quot;video-preview&quot; /&gt;&lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Local LLMs&lt;/strong&gt;: You can utilize local LLMs such as Qwen, DeepSeek, Llama, and Mistral.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Two Main Modes:&lt;/strong&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Copilot Mode:&lt;/strong&gt; (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user&#39;s query directly from the page.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Normal Mode:&lt;/strong&gt; Processes your query and performs a web search.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Focus Modes:&lt;/strong&gt; Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;All Mode:&lt;/strong&gt; Searches the entire web to find the best results.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Writing Assistant Mode:&lt;/strong&gt; Helpful for writing tasks that do not require searching the web.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Academic Search Mode:&lt;/strong&gt; Finds articles and papers, ideal for academic research.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;YouTube Search Mode:&lt;/strong&gt; Finds YouTube videos based on the search query.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Wolfram Alpha Search Mode:&lt;/strong&gt; Answers queries that need calculations or data analysis using Wolfram Alpha.&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Reddit Search Mode:&lt;/strong&gt; Searches Reddit for discussions and opinions related to the query.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Current Information:&lt;/strong&gt; Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API&lt;/strong&gt;: Integrate Perplexica into your existing applications and make use of its capibilities.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;It has many more features like image and video search. Some of the planned features are mentioned in &lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features&quot;&gt;upcoming features&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.&lt;/p&gt; 
&lt;h3&gt;Getting Started with Docker (Recommended)&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Ensure Docker is installed and running on your system.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Clone the Perplexica repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/ItzCrazyKns/Perplexica.git
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;After cloning, navigate to the directory containing the project files.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;CUSTOM_OPENAI&lt;/code&gt;: Your OpenAI-API-compliant local server URL, model name, and API key. You should run your local server with host set to &lt;code&gt;0.0.0.0&lt;/code&gt;, take note of which port number it is running on, and then use that port number to set &lt;code&gt;API_URL = http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. You must specify the model name, such as &lt;code&gt;MODEL_NAME = &quot;unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q4_K_XL&quot;&lt;/code&gt;. Finally, set &lt;code&gt;API_KEY&lt;/code&gt; to the appropriate value. If you have not defined an API key, just put anything you want in-between the quotation marks: &lt;code&gt;API_KEY = &quot;whatever-you-want-but-not-blank&quot;&lt;/code&gt; &lt;strong&gt;You only need to configure these settings if you want to use a local OpenAI-compliant server, such as Llama.cpp&#39;s &lt;a href=&quot;https://github.com/ggml-org/llama.cpp/raw/master/tools/server/README.md&quot;&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama&#39;s models instead of OpenAI&#39;s&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;LEMONADE&lt;/code&gt;: Your Lemonade API URL. Since Lemonade runs directly on your local machine (not in Docker), you should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Lemonade on port 8000, use &lt;code&gt;http://host.docker.internal:8000&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Lemonade&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq&#39;s hosted models&lt;/strong&gt;.`&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;Gemini&lt;/code&gt;: Your Gemini API key. &lt;strong&gt;You only need to fill this if you wish to use Google&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;DEEPSEEK&lt;/code&gt;: Your Deepseek API key. &lt;strong&gt;Only needed if you want Deepseek models.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;AIMLAPI&lt;/code&gt;: Your AI/ML API key. &lt;strong&gt;Only needed if you want to use AI/ML API models and embeddings.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker compose up -d
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Wait a few minutes for the setup to complete. You can access Perplexica at &lt;a href=&quot;http://localhost:3000&quot;&gt;http://localhost:3000&lt;/a&gt; in your web browser.&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.&lt;/p&gt; 
&lt;h3&gt;Non-Docker Installation&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Install SearXNG and allow &lt;code&gt;JSON&lt;/code&gt; format in the SearXNG settings.&lt;/li&gt; 
 &lt;li&gt;Clone the repository and rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt; in the root directory. Ensure you complete all required fields in this file.&lt;/li&gt; 
 &lt;li&gt;After populating the configuration run &lt;code&gt;npm i&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Install the dependencies and then execute &lt;code&gt;npm run build&lt;/code&gt;.&lt;/li&gt; 
 &lt;li&gt;Finally, start the app by running &lt;code&gt;npm run start&lt;/code&gt;&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation&quot;&gt;installation documentation&lt;/a&gt; for more information like updating, etc.&lt;/p&gt; 
&lt;h3&gt;Troubleshooting&lt;/h3&gt; 
&lt;h4&gt;Local OpenAI-API-Compliant Servers&lt;/h4&gt; 
&lt;p&gt;If Perplexica tells you that you haven&#39;t configured any chat model providers, ensure that:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Your server is running on &lt;code&gt;0.0.0.0&lt;/code&gt; (not &lt;code&gt;127.0.0.1&lt;/code&gt;) and on the same port you put in the API URL.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct model name loaded by your local LLM server.&lt;/li&gt; 
 &lt;li&gt;You have specified the correct API key, or if one is not defined, you have put &lt;em&gt;something&lt;/em&gt; in the API key field and not left it empty.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Ollama Connection Errors&lt;/h4&gt; 
&lt;p&gt;If you&#39;re encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama&#39;s API. To fix this issue you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check your Ollama API URL:&lt;/strong&gt; Ensure that the API URL is correctly set in the settings menu.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update API URL Based on OS:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Use &lt;code&gt;http://&amp;lt;private_ip_of_host&amp;gt;:11434&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Adjust the port number if you&#39;re using a different one.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Linux Users - Expose Ollama to Network:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt; &lt;p&gt;Inside &lt;code&gt;/etc/systemd/system/ollama.service&lt;/code&gt;, you need to add &lt;code&gt;Environment=&quot;OLLAMA_HOST=0.0.0.0:11434&quot;&lt;/code&gt;. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with &lt;code&gt;systemctl daemon-reload&lt;/code&gt;, and restart Ollama by &lt;code&gt;systemctl restart ollama&lt;/code&gt;. For more information see &lt;a href=&quot;https://github.com/ollama/ollama/raw/main/docs/faq.md#setting-environment-variables-on-linux&quot;&gt;Ollama docs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
   &lt;li&gt; &lt;p&gt;Ensure that the port (default is 11434) is not blocked by your firewall.&lt;/p&gt; &lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h4&gt;Lemonade Connection Errors&lt;/h4&gt; 
&lt;p&gt;If you&#39;re encountering a Lemonade connection error, it is likely due to the backend being unable to connect to Lemonade&#39;s API. To fix this issue you can:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Check your Lemonade API URL:&lt;/strong&gt; Ensure that the API URL is correctly set in the settings menu.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Update API URL Based on OS:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:8000&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Mac:&lt;/strong&gt; Use &lt;code&gt;http://host.docker.internal:8000&lt;/code&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;strong&gt;Linux:&lt;/strong&gt; Use &lt;code&gt;http://&amp;lt;private_ip_of_host&amp;gt;:8000&lt;/code&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;p&gt;Adjust the port number if you&#39;re using a different one.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ensure Lemonade Server is Running:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Make sure your Lemonade server is running and accessible on the configured port (default is 8000).&lt;/li&gt; 
   &lt;li&gt;Verify that Lemonade is configured to accept connections from all interfaces (&lt;code&gt;0.0.0.0&lt;/code&gt;), not just localhost (&lt;code&gt;127.0.0.1&lt;/code&gt;).&lt;/li&gt; 
   &lt;li&gt;Ensure that the port (default is 8000) is not blocked by your firewall.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using as a Search Engine&lt;/h2&gt; 
&lt;p&gt;If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser&#39;s search bar, follow these steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Open your browser&#39;s settings.&lt;/li&gt; 
 &lt;li&gt;Navigate to the &#39;Search Engines&#39; section.&lt;/li&gt; 
 &lt;li&gt;Add a new site search with the following URL: &lt;code&gt;http://localhost:3000/?q=%s&lt;/code&gt;. Replace &lt;code&gt;localhost&lt;/code&gt; with your IP address or domain name, and &lt;code&gt;3000&lt;/code&gt; with the port number if Perplexica is not hosted locally.&lt;/li&gt; 
 &lt;li&gt;Click the add button. Now, you can use Perplexica directly from your browser&#39;s search bar.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;Using Perplexica&#39;s API&lt;/h2&gt; 
&lt;p&gt;Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.&lt;/p&gt; 
&lt;p&gt;For more details, check out the full documentation &lt;a href=&quot;https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Expose Perplexica to network&lt;/h2&gt; 
&lt;p&gt;Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.&lt;/p&gt; 
&lt;h2&gt;One-Click Deployment&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg?sanitize=true&quot; alt=&quot;Deploy to Sealos&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://repocloud.io/details/?app_id=267&quot;&gt;&lt;img src=&quot;https://d16t0pc4846x52.cloudfront.net/deploylobe.svg?sanitize=true&quot; alt=&quot;Deploy to RepoCloud&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&amp;amp;openapp=system-fastdeploy%3FtemplateName%3Dperplexica&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg?sanitize=true&quot; alt=&quot;Run on ClawCloud&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.hostinger.com/vps/docker-hosting?compose_url=https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/refs/heads/master/docker-compose.yaml&quot;&gt;&lt;img src=&quot;https://assets.hostinger.com/vps/deploy.svg?sanitize=true&quot; alt=&quot;Deploy on Hostinger&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Upcoming Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Add settings page&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Adding support for local LLMs&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; History Saving features&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Introducing various Focus Modes&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Adding API support&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; checked disabled /&gt; Adding Discover&lt;/li&gt; 
 &lt;li&gt;&lt;input type=&quot;checkbox&quot; disabled /&gt; Finalizing Copilot Mode&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Support Us&lt;/h2&gt; 
&lt;p&gt;If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.&lt;/p&gt; 
&lt;h3&gt;Donations&lt;/h3&gt; 
&lt;p&gt;We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Ethereum&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Address: &lt;code&gt;0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Contribution&lt;/h2&gt; 
&lt;p&gt;Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the &lt;a href=&quot;https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; file to learn more about Perplexica and how you can contribute to it.&lt;/p&gt; 
&lt;h2&gt;Help and Support&lt;/h2&gt; 
&lt;p&gt;If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. &lt;a href=&quot;https://discord.gg/EFwsmQDgAu&quot;&gt;Click here&lt;/a&gt; to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at &lt;code&gt;itzcrazykns&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don&#39;t forget to check back for updates and new features!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>HKUDS/DeepCode</title>
      <link>https://github.com/HKUDS/DeepCode</link>
      <description>&lt;p&gt;&quot;DeepCode: Open Agentic Coding (Paper2Code &amp; Text2Web &amp; Text2Backend)&quot;&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;table style=&quot;border: none; margin: 0 auto; padding: 0; border-collapse: collapse;&quot;&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot; style=&quot;vertical-align: middle; padding: 10px; border: none; width: 250px;&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/assets/logo.png&quot; alt=&quot;DeepCode Logo&quot; width=&quot;200&quot; style=&quot;margin: 0; padding: 0; display: block;&quot; /&gt; &lt;/td&gt; 
    &lt;td align=&quot;left&quot; style=&quot;vertical-align: middle; padding: 10px 0 10px 30px; border: none;&quot;&gt; &lt;pre style=&quot;font-family: &#39;Courier New&#39;, monospace; font-size: 16px; color: #0EA5E9; margin: 0; padding: 0; text-shadow: 0 0 10px #0EA5E9, 0 0 20px rgba(14,165,233,0.5); line-height: 1.2; transform: skew(-1deg, 0deg); display: block;&quot;&gt;    ██████╗ ███████╗███████╗██████╗  ██████╗ ██████╗ ██████╗ ███████╗
    ██╔══██╗██╔════╝██╔════╝██╔══██╗██╔════╝██╔═══██╗██╔══██╗██╔════╝
    ██║  ██║█████╗  █████╗  ██████╔╝██║     ██║   ██║██║  ██║█████╗
    ██║  ██║██╔══╝  ██╔══╝  ██╔═══╝ ██║     ██║   ██║██║  ██║██╔══╝
    ██████╔╝███████╗███████╗██║     ╚██████╗╚██████╔╝██████╔╝███████╗
    ╚═════╝ ╚══════╝╚══════╝╚═╝      ╚═════╝ ╚═════╝ ╚═════╝ ╚══════╝&lt;/pre&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://trendshift.io/repositories/14665&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14665&quot; alt=&quot;HKUDS%2FDeepCode | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;!-- &lt;img src=&quot;https://readme-typing-svg.herokuapp.com?font=Russo+One&amp;size=28&amp;duration=2000&amp;pause=800&amp;color=06B6D4&amp;background=00000000&amp;center=true&amp;vCenter=true&amp;width=800&amp;height=50&amp;lines=%E2%9A%A1+OPEN+AGENTIC+CODING+%E2%9A%A1&quot; alt=&quot;DeepCode Tech Subtitle&quot; style=&quot;margin-top: 5px; filter: drop-shadow(0 0 12px #06B6D4) drop-shadow(0 0 24px rgba(6,182,212,0.4));&quot;/&gt; --&gt; 
 &lt;h1&gt;&lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/43c585dca3d21b8e4b6390d835cdd34dc4b4b23d/DeepCode_images/title_logo.svg?sanitize=true&quot; alt=&quot;DeepCode Logo&quot; width=&quot;32&quot; height=&quot;32&quot; style=&quot;vertical-align: middle; margin-right: 8px;&quot; /&gt; DeepCode: Open Agentic Coding&lt;/h1&gt; 
 &lt;h3&gt;&lt;em&gt;Advancing Code Generation with Multi-Agent Systems&lt;/em&gt;&lt;/h3&gt; 
 &lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/Version-1.0.0-00d4ff?style=for-the-badge&amp;logo=rocket&amp;logoColor=white&quot; alt=&quot;Version&quot;&gt;

  &lt;img src=&quot;https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;logo=opensourceinitiative&amp;logoColor=white&quot; alt=&quot;License&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/AI-Multi--Agent-9b59b6?style=for-the-badge&amp;logo=brain&amp;logoColor=white&quot; alt=&quot;AI&quot;&gt;
  &lt;img src=&quot;https://img.shields.io/badge/HKU-Data_Intelligence_Lab-f39c12?style=for-the-badge&amp;logo=university&amp;logoColor=white&quot; alt=&quot;HKU&quot;&gt;
&lt;/p&gt; --&gt; 
 &lt;p&gt; &lt;a href=&quot;https://github.com/HKUDS/DeepCode/stargazers&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/HKUDS/DeepCode?color=00d9ff&amp;amp;style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&quot; /&gt;&lt;/a&gt; &lt;img src=&quot;https://img.shields.io/badge/🐍Python-3.13-4ecdc4?style=for-the-badge&amp;amp;logo=python&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&quot; /&gt; &lt;a href=&quot;https://pypi.org/project/deepcode-hku/&quot;&gt;&lt;img src=&quot;https://img.shields.io/pypi/v/deepcode-hku.svg?style=for-the-badge&amp;amp;logo=pypi&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&amp;amp;color=ff6b6b&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;p&gt; &lt;a href=&quot;https://discord.gg/yF2MmDJyGJ&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/💬Discord-Community-7289da?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/HKUDS/DeepCode/issues/11&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/💬WeChat-Group-07c160?style=for-the-badge&amp;amp;logo=wechat&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;div style=&quot;width: 100%; height: 2px; margin: 20px 0; background: linear-gradient(90deg, transparent, #00d9ff, transparent);&quot;&gt;&lt;/div&gt; 
 &lt;/div&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start&quot; style=&quot;text-decoration: none;&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/Quick%20Start-Get%20Started%20Now-00d9ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&amp;amp;labelColor=1a1a2e&quot; /&gt; &lt;/a&gt; 
 &lt;/div&gt; 
 &lt;h3&gt;🖥️ &lt;strong&gt;Interface Showcase&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse; margin: 30px 0;&quot;&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt; &lt;h4&gt;🖥️ &lt;strong&gt;CLI Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Terminal-Based Development&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align=&quot;center&quot;&gt; 
      &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/CLI.gif&quot; alt=&quot;CLI Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(45,55,72,0.3); margin: 15px 0;&quot; /&gt; 
      &lt;div style=&quot;background: linear-gradient(135deg, #2D3748 0%, #4A5568 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt; 
       &lt;strong&gt;🚀 Advanced Terminal Experience&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;⚡ Fast command-line workflow&lt;br /&gt;🔧 Developer-friendly interface&lt;br /&gt;📊 Real-time progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Professional terminal interface for advanced users and CI/CD integration&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td width=&quot;50%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt; &lt;h4&gt;🌐 &lt;strong&gt;Web Interface&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Visual Interactive Experience&lt;/strong&gt;&lt;/p&gt; 
     &lt;div align=&quot;center&quot;&gt; 
      &lt;img src=&quot;https://github.com/Zongwei9888/Experiment_Images/raw/8882a7313c504ca97ead6e7b36c51aa761b6a4f3/DeepCode_images/UI.gif&quot; alt=&quot;Web Interface Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 8px 20px rgba(14,165,233,0.3); margin: 15px 0;&quot; /&gt; 
      &lt;div style=&quot;background: linear-gradient(135deg, #0EA5E9 0%, #00D4FF 100%); border-radius: 12px; padding: 15px; margin: 15px 0; color: white;&quot;&gt; 
       &lt;strong&gt;🎨 Modern Web Dashboard&lt;/strong&gt;
       &lt;br /&gt; 
       &lt;small&gt;🖱️ Intuitive drag-and-drop&lt;br /&gt;📱 Responsive design&lt;br /&gt;🎯 Visual progress tracking&lt;/small&gt; 
      &lt;/div&gt; 
      &lt;p&gt;&lt;em&gt;Beautiful web interface with streamlined workflow for all skill levels&lt;/em&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
 &lt;hr /&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;h3&gt;🎬 &lt;strong&gt;Introduction Video&lt;/strong&gt;&lt;/h3&gt; 
  &lt;div style=&quot;margin: 20px 0;&quot;&gt; 
   &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://img.youtube.com/vi/PRgmP8pOI08/maxresdefault.jpg&quot; alt=&quot;DeepCode Introduction Video&quot; width=&quot;75%&quot; style=&quot;border-radius: 12px; box-shadow: 0 8px 25px rgba(0,0,0,0.15); transition: transform 0.3s ease;&quot; /&gt; &lt;/a&gt; 
  &lt;/div&gt; 
  &lt;p&gt;&lt;em&gt;🎯 &lt;strong&gt;Watch our complete introduction&lt;/strong&gt; - See how DeepCode transforms research papers and natural language into production-ready code&lt;/em&gt;&lt;/p&gt; 
  &lt;p&gt; &lt;a href=&quot;https://youtu.be/PRgmP8pOI08&quot; target=&quot;_blank&quot;&gt; &lt;img src=&quot;https://img.shields.io/badge/▶️_Watch_Video-FF0000?style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&quot; alt=&quot;Watch Video&quot; /&gt; &lt;/a&gt; &lt;/p&gt; 
 &lt;/div&gt; 
 &lt;hr /&gt; 
 &lt;blockquote&gt; 
  &lt;p&gt;&lt;em&gt;&quot;Where AI Agents Transform Ideas into Production-Ready Code&quot;&lt;/em&gt;&lt;/p&gt; 
 &lt;/blockquote&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;📑 Table of Contents&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-key-features&quot;&gt;🚀 Key Features&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#%EF%B8%8F-architecture&quot;&gt;🏗️ Architecture&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start&quot;&gt;🚀 Quick Start&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-examples&quot;&gt;💡 Examples&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-live-demonstrations&quot;&gt;🎬 Live Demonstrations&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-star-history&quot;&gt;⭐ Star History&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-license&quot;&gt;📄 License&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Key Features&lt;/h2&gt; 
&lt;br /&gt; 
&lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; table-layout: fixed;&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt; 
    &lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;🚀 &lt;strong&gt;Paper2Code&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt; 
     &lt;img src=&quot;https://img.shields.io/badge/ALGORITHM-IMPLEMENTATION-ff6b6b?style=for-the-badge&amp;amp;logo=algorithm&amp;amp;logoColor=white&quot; alt=&quot;Algorithm Badge&quot; /&gt; 
    &lt;/div&gt; 
    &lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Implementation of Complex Algorithms&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;p align=&quot;center&quot;&gt;Effortlessly converts complex algorithms from research papers into &lt;strong&gt;high-quality&lt;/strong&gt;, &lt;strong&gt;production-ready&lt;/strong&gt; code, accelerating algorithm reproduction.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt; 
    &lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;🎨 &lt;strong&gt;Text2Web&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt; 
     &lt;img src=&quot;https://img.shields.io/badge/FRONTEND-DEVELOPMENT-4ecdc4?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=white&quot; alt=&quot;Frontend Badge&quot; /&gt; 
    &lt;/div&gt; 
    &lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Front-End Web Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;p align=&quot;center&quot;&gt;Translates plain textual descriptions into &lt;strong&gt;fully functional&lt;/strong&gt;, &lt;strong&gt;visually appealing&lt;/strong&gt; front-end web code for rapid interface creation.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width=&quot;30%&quot; align=&quot;center&quot; style=&quot;vertical-align: top; padding: 20px;&quot;&gt; 
    &lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;h3 style=&quot;margin: 0; padding: 0;&quot;&gt;⚙️ &lt;strong&gt;Text2Backend&lt;/strong&gt;&lt;/h3&gt; 
    &lt;/div&gt; 
    &lt;div align=&quot;center&quot; style=&quot;margin: 15px 0;&quot;&gt; 
     &lt;img src=&quot;https://img.shields.io/badge/BACKEND-DEVELOPMENT-9b59b6?style=for-the-badge&amp;amp;logo=server&amp;amp;logoColor=white&quot; alt=&quot;Backend Badge&quot; /&gt; 
    &lt;/div&gt; 
    &lt;div style=&quot;height: 80px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Automated Back-End Development&lt;/strong&gt;&lt;/p&gt; 
    &lt;/div&gt; 
    &lt;div style=&quot;height: 60px; display: flex; align-items: center; justify-content: center;&quot;&gt; 
     &lt;p align=&quot;center&quot;&gt;Generates &lt;strong&gt;efficient&lt;/strong&gt;, &lt;strong&gt;scalable&lt;/strong&gt;, and &lt;strong&gt;feature-rich&lt;/strong&gt; back-end code from simple text inputs, streamlining server-side development.&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;br /&gt; 
&lt;h3&gt;🎯 &lt;strong&gt;Autonomous Multi-Agent Workflow&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;The Challenges&lt;/strong&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;📄 &lt;strong&gt;Implementation Complexity&lt;/strong&gt;: Converting academic papers and complex algorithms into working code requires significant technical effort and domain expertise&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔬 &lt;strong&gt;Research Bottleneck&lt;/strong&gt;: Researchers spend valuable time implementing algorithms instead of focusing on their core research and discovery work&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;⏱️ &lt;strong&gt;Development Delays&lt;/strong&gt;: Product teams experience long wait times between concept and testable prototypes, slowing down innovation cycles&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔄 &lt;strong&gt;Repetitive Coding&lt;/strong&gt;: Developers repeatedly implement similar patterns and functionality instead of building on existing solutions&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; addresses these workflow inefficiencies by providing reliable automation for common development tasks, streamlining your development workflow from concept to code.&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-mermaid&quot;&gt;flowchart LR
    A[&quot;📄 Research Papers&amp;lt;br/&amp;gt;💬 Text Prompts&amp;lt;br/&amp;gt;🌐 URLs &amp;amp; Document&amp;lt;br/&amp;gt;📎 Files: PDF, DOC, PPTX, TXT, HTML&quot;] --&amp;gt; B[&quot;🧠 DeepCode&amp;lt;br/&amp;gt;Multi-Agent Engine&quot;]
    B --&amp;gt; C[&quot;🚀 Algorithm Implementation &amp;lt;br/&amp;gt;🎨 Frontend Development &amp;lt;br/&amp;gt;⚙️ Backend Development&quot;]

    style A fill:#ff6b6b,stroke:#c0392b,stroke-width:2px,color:#000
    style B fill:#00d4ff,stroke:#0984e3,stroke-width:3px,color:#000
    style C fill:#00b894,stroke:#00a085,stroke-width:2px,color:#000
&lt;/code&gt;&lt;/pre&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🏗️ Architecture&lt;/h2&gt; 
&lt;h3&gt;📊 &lt;strong&gt;System Overview&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;DeepCode&lt;/strong&gt; is an AI-powered development platform that automates code generation and implementation tasks. Our multi-agent system handles the complexity of translating requirements into functional, well-structured code, allowing you to focus on innovation rather than implementation details.&lt;/p&gt; 
&lt;p&gt;🎯 &lt;strong&gt;Technical Capabilities&lt;/strong&gt;:&lt;/p&gt; 
&lt;p&gt;🧬 &lt;strong&gt;Research-to-Production Pipeline&lt;/strong&gt;&lt;br /&gt; Multi-modal document analysis engine that extracts algorithmic logic and mathematical models from academic papers. Generates optimized implementations with proper data structures while preserving computational complexity characteristics.&lt;/p&gt; 
&lt;p&gt;🪄 &lt;strong&gt;Natural Language Code Synthesis&lt;/strong&gt;&lt;br /&gt; Context-aware code generation using fine-tuned language models trained on curated code repositories. Maintains architectural consistency across modules while supporting multiple programming languages and frameworks.&lt;/p&gt; 
&lt;p&gt;⚡ &lt;strong&gt;Automated Prototyping Engine&lt;/strong&gt;&lt;br /&gt; Intelligent scaffolding system generating complete application structures including database schemas, API endpoints, and frontend components. Uses dependency analysis to ensure scalable architecture from initial generation.&lt;/p&gt; 
&lt;p&gt;💎 &lt;strong&gt;Quality Assurance Automation&lt;/strong&gt;&lt;br /&gt; Integrated static analysis with automated unit test generation and documentation synthesis. Employs AST analysis for code correctness and property-based testing for comprehensive coverage.&lt;/p&gt; 
&lt;p&gt;🔮 &lt;strong&gt;CodeRAG Integration System&lt;/strong&gt;&lt;br /&gt; Advanced retrieval-augmented generation combining semantic vector embeddings with graph-based dependency analysis. Automatically discovers optimal libraries and implementation patterns from large-scale code corpus.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;🔧 &lt;strong&gt;Core Techniques&lt;/strong&gt;&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;🧠 &lt;strong&gt;Intelligent Orchestration Agent&lt;/strong&gt;: Central decision-making system that coordinates workflow phases and analyzes requirements. Employs dynamic planning algorithms to adapt execution strategies in real-time based on evolving project complexity. Dynamically selects optimal processing strategies for each implementation step. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;💾 &lt;strong&gt;Efficient Memory Mechanism&lt;/strong&gt;: Advanced context engineering system that manages large-scale code contexts efficiently. Implements hierarchical memory structures with intelligent compression for handling complex codebases. This component enables instant retrieval of implementation patterns and maintains semantic coherence across extended development sessions. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;🔍 &lt;strong&gt;Advanced CodeRAG System&lt;/strong&gt;: Global code comprehension engine that analyzes complex inter-dependencies across repositories. Performs cross-codebase relationship mapping to understand architectural patterns from a holistic perspective. This module leverages dependency graphs and semantic analysis to provide globally-aware code recommendations during implementation.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h3&gt;🤖 &lt;strong&gt;Multi-Agent Architecture of DeepCode&lt;/strong&gt;:&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🎯 Central Orchestrating Agent&lt;/strong&gt;: Orchestrates entire workflow execution and makes strategic decisions. Coordinates specialized agents based on input complexity analysis. Implements dynamic task planning and resource allocation algorithms. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;📝 Intent Understanding Agent&lt;/strong&gt;: Performs deep semantic analysis of user requirements to decode complex intentions. Extracts functional specifications and technical constraints through advanced NLP processing. Transforms ambiguous human descriptions into precise, actionable development specifications with structured task decomposition. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;📄 Document Parsing Agent&lt;/strong&gt;: Processes complex technical documents and research papers with advanced parsing capabilities. Extracts algorithms and methodologies using document understanding models. Converts academic concepts into practical implementation specifications through intelligent content analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🏗️ Code Planning Agent&lt;/strong&gt;: Performs architectural design and technology stack optimization. Dynamic planning for adaptive development roadmaps. Enforces coding standards and generates modular structures through automated design pattern selection.&lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔍 Code Reference Mining Agent&lt;/strong&gt;: Discovers relevant repositories and frameworks through intelligent search algorithms. Analyzes codebases for compatibility and integration potential. Provides recommendations based on similarity metrics and automated dependency analysis. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;📚 Code Indexing Agent&lt;/strong&gt;: Builds comprehensive knowledge graphs of discovered codebases. Maintains semantic relationships between code components. Enables intelligent retrieval and cross-reference capabilities. &lt;br /&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🧬 Code Generation Agent&lt;/strong&gt;: Synthesizes gathered information into executable code implementations. Creates functional interfaces and integrates discovered components. Generates comprehensive test suites and documentation for reproducibility.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h4&gt;🛠️ &lt;strong&gt;Implementation Tools Matrix&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;🔧 Powered by MCP (Model Context Protocol)&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;DeepCode leverages the &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt; standard to seamlessly integrate with various tools and services. This standardized approach ensures reliable communication between AI agents and external systems, enabling powerful automation capabilities.&lt;/p&gt; 
&lt;h5&gt;📡 &lt;strong&gt;MCP Servers &amp;amp; Tools&lt;/strong&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;🛠️ &lt;strong&gt;MCP Server&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;🔧 &lt;strong&gt;Primary Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;💡 &lt;strong&gt;Purpose &amp;amp; Capabilities&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🔍 brave&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Search Engine&lt;/td&gt; 
   &lt;td&gt;Real-time information retrieval via Brave Search API&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌐 bocha-mcp&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Alternative Search&lt;/td&gt; 
   &lt;td&gt;Secondary search option with independent API access&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📂 filesystem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;File System Operations&lt;/td&gt; 
   &lt;td&gt;Local file and directory management, read/write operations&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🌐 fetch&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Web Content Retrieval&lt;/td&gt; 
   &lt;td&gt;Fetch and extract content from URLs and web resources&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📥 github-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Repository Management&lt;/td&gt; 
   &lt;td&gt;Clone and download GitHub repositories for analysis&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📋 file-downloader&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Document Processing&lt;/td&gt; 
   &lt;td&gt;Download and convert files (PDF, DOCX, etc.) to Markdown&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;⚡ command-executor&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;System Commands&lt;/td&gt; 
   &lt;td&gt;Execute bash/shell commands for environment management&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🧬 code-implementation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Code Generation Hub&lt;/td&gt; 
   &lt;td&gt;Comprehensive code reproduction with execution and testing&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📚 code-reference-indexer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Code Search&lt;/td&gt; 
   &lt;td&gt;Intelligent indexing and search of code repositories&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📄 document-segmentation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Smart Document Analysis&lt;/td&gt; 
   &lt;td&gt;Intelligent document segmentation for large papers and technical documents&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h5&gt;🔧 &lt;strong&gt;Legacy Tool Functions&lt;/strong&gt; &lt;em&gt;(for reference)&lt;/em&gt;&lt;/h5&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;🛠️ &lt;strong&gt;Function&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;🎯 &lt;strong&gt;Usage Context&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📄 read_code_mem&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Efficient code context retrieval from memory&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;✍️ write_file&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Direct file content generation and modification&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;🐍 execute_python&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Python code testing and validation&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📁 get_file_structure&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Project structure analysis and organization&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;⚙️ set_workspace&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Dynamic workspace and environment configuration&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;📊 get_operation_history&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;Process monitoring and operation tracking&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;hr /&gt; 
&lt;p&gt;🎛️ &lt;strong&gt;Multi-Interface Framework&lt;/strong&gt;&lt;br /&gt; RESTful API with CLI and web frontends featuring real-time code streaming, interactive debugging, and extensible plugin architecture for CI/CD integration.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;🚀 Multi-Agent Intelligent Pipeline:&lt;/strong&gt;&lt;/p&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;h3&gt;🌟 &lt;strong&gt;Intelligence Processing Flow&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align=&quot;center&quot; width=&quot;100%&quot; style=&quot;border: none; border-collapse: collapse;&quot;&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; font-weight: bold;&quot;&gt; 💡 &lt;strong&gt;INPUT LAYER&lt;/strong&gt;&lt;br /&gt; 📄 Research Papers • 💬 Natural Language • 🌐 URLs • 📋 Requirements &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan=&quot;3&quot; height=&quot;20&quot;&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 15px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 12px; color: white; font-weight: bold;&quot;&gt; 🎯 &lt;strong&gt;CENTRAL ORCHESTRATION&lt;/strong&gt;&lt;br /&gt; Strategic Decision Making • Workflow Coordination • Agent Management &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot; style=&quot;padding: 12px; background: linear-gradient(135deg, #3742fa 0%, #2f3542 100%); border-radius: 10px; color: white; width: 50%;&quot;&gt; 📝 &lt;strong&gt;TEXT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Requirement Processing&lt;/small&gt; &lt;/td&gt; 
    &lt;td width=&quot;10&quot;&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot; style=&quot;padding: 12px; background: linear-gradient(135deg, #8c7ae6 0%, #9c88ff 100%); border-radius: 10px; color: white; width: 50%;&quot;&gt; 📄 &lt;strong&gt;DOCUMENT ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Paper &amp;amp; Spec Processing&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 15px; background: linear-gradient(135deg, #00d2d3 0%, #54a0ff 100%); border-radius: 12px; color: white; font-weight: bold;&quot;&gt; 📋 &lt;strong&gt;REPRODUCTION PLANNING&lt;/strong&gt;&lt;br /&gt; Deep Paper Analysis • Code Requirements Parsing • Reproduction Strategy Development &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot; style=&quot;padding: 12px; background: linear-gradient(135deg, #ffa726 0%, #ff7043 100%); border-radius: 10px; color: white; width: 50%;&quot;&gt; 🔍 &lt;strong&gt;REFERENCE ANALYSIS&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Repository Discovery&lt;/small&gt; &lt;/td&gt; 
    &lt;td width=&quot;10&quot;&gt;&lt;/td&gt; 
    &lt;td align=&quot;center&quot; style=&quot;padding: 12px; background: linear-gradient(135deg, #e056fd 0%, #f368e0 100%); border-radius: 10px; color: white; width: 50%;&quot;&gt; 📚 &lt;strong&gt;CODE INDEXING&lt;/strong&gt;&lt;br /&gt; &lt;small&gt;Knowledge Graph Building&lt;/small&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 15px; background: linear-gradient(135deg, #26de81 0%, #20bf6b 100%); border-radius: 12px; color: white; font-weight: bold;&quot;&gt; 🧬 &lt;strong&gt;CODE IMPLEMENTATION&lt;/strong&gt;&lt;br /&gt; Implementation Generation • Testing • Documentation &lt;/td&gt; 
   &lt;/tr&gt; 
   &lt;tr&gt;
    &lt;td colspan=&quot;3&quot; height=&quot;15&quot;&gt;&lt;/td&gt;
   &lt;/tr&gt; 
   &lt;tr&gt; 
    &lt;td colspan=&quot;3&quot; align=&quot;center&quot; style=&quot;padding: 20px; background: linear-gradient(135deg, #045de9 0%, #09c6f9 100%); border-radius: 15px; color: white; font-weight: bold;&quot;&gt; ⚡ &lt;strong&gt;OUTPUT DELIVERY&lt;/strong&gt;&lt;br /&gt; 📦 Complete Codebase • 🧪 Test Suite • 📚 Documentation • 🚀 Deployment Ready &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;br /&gt; 
 &lt;h3&gt;🔄 &lt;strong&gt;Process Intelligence Features&lt;/strong&gt;&lt;/h3&gt; 
 &lt;table align=&quot;center&quot; style=&quot;border: none;&quot;&gt; 
  &lt;tbody&gt;
   &lt;tr&gt; 
    &lt;td align=&quot;center&quot; width=&quot;25%&quot; style=&quot;padding: 15px;&quot;&gt; 
     &lt;div style=&quot;background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #ff6b6b;&quot;&gt; 
      &lt;h4&gt;🎯 Adaptive Flow&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Dynamic agent selection based on input complexity&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align=&quot;center&quot; width=&quot;25%&quot; style=&quot;padding: 15px;&quot;&gt; 
     &lt;div style=&quot;background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #4ecdc4;&quot;&gt; 
      &lt;h4&gt;🧠 Smart Coordination&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Intelligent task distribution and parallel processing&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align=&quot;center&quot; width=&quot;25%&quot; style=&quot;padding: 15px;&quot;&gt; 
     &lt;div style=&quot;background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #45b7d1;&quot;&gt; 
      &lt;h4&gt;🔍 Context Awareness&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Deep understanding through CodeRAG integration&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
    &lt;td align=&quot;center&quot; width=&quot;25%&quot; style=&quot;padding: 15px;&quot;&gt; 
     &lt;div style=&quot;background: #f8f9fa; border-radius: 10px; padding: 15px; border-left: 4px solid #96ceb4;&quot;&gt; 
      &lt;h4&gt;⚡ Quality Assurance&lt;/h4&gt; 
      &lt;p&gt;&lt;small&gt;Automated testing and validation throughout&lt;/small&gt;&lt;/p&gt; 
     &lt;/div&gt; &lt;/td&gt; 
   &lt;/tr&gt; 
  &lt;/tbody&gt;
 &lt;/table&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h2&gt;🚀 Quick Start&lt;/h2&gt; 
&lt;h3&gt;📦 &lt;strong&gt;Step 1: Installation&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;⚡ &lt;strong&gt;Direct Installation (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 🚀 Install DeepCode package directly
pip install deepcode-hku

# 🔑 Download configuration files
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.config.yaml
curl -O https://raw.githubusercontent.com/HKUDS/DeepCode/main/mcp_agent.secrets.yaml

# 🔑 Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# 🔑 Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: &quot;your_key_here&quot; in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: &quot;your_key_here&quot; in bocha-mcp.env section (line ~74)

# 📄 Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;🔧 &lt;strong&gt;Development Installation (From Source)&lt;/strong&gt;&lt;/h4&gt; 
&lt;details&gt; 
 &lt;summary&gt;&lt;strong&gt;📂 Click to expand development installation options&lt;/strong&gt;&lt;/summary&gt; 
 &lt;h5&gt;🔥 &lt;strong&gt;Using UV (Recommended for Development)&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 🔽 Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# 📦 Install UV package manager
curl -LsSf https://astral.sh/uv/install.sh | sh

# 🔧 Install dependencies with UV
uv venv --python=3.13
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -r requirements.txt

# 🔑 Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# 🔑 Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: &quot;your_key_here&quot; in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: &quot;your_key_here&quot; in bocha-mcp.env section (line ~74)

# 📄 Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
 &lt;h5&gt;🐍 &lt;strong&gt;Using Traditional pip&lt;/strong&gt;&lt;/h5&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 🔽 Clone the repository
git clone https://github.com/HKUDS/DeepCode.git
cd DeepCode/

# 📦 Install dependencies
pip install -r requirements.txt

# 🔑 Configure API keys (required)
# Edit mcp_agent.secrets.yaml with your API keys and base_url:
# - openai: api_key, base_url (for OpenAI/custom endpoints)
# - anthropic: api_key (for Claude models)

# 🔑 Configure search API keys for web search (optional)
# Edit mcp_agent.config.yaml to set your API keys:
# - For Brave Search: Set BRAVE_API_KEY: &quot;your_key_here&quot; in brave.env section (line ~28)
# - For Bocha-MCP: Set BOCHA_API_KEY: &quot;your_key_here&quot; in bocha-mcp.env section (line ~74)

# 📄 Configure document segmentation (optional)
# Edit mcp_agent.config.yaml to control document processing:
# - enabled: true/false (whether to use intelligent document segmentation)
# - size_threshold_chars: 50000 (document size threshold to trigger segmentation)
&lt;/code&gt;&lt;/pre&gt; 
&lt;/details&gt; 
&lt;h4&gt;🪟 &lt;strong&gt;Windows Users: Additional MCP Server Configuration&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;If you&#39;re using Windows, you may need to configure MCP servers manually in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 1. Install MCP servers globally
npm i -g @modelcontextprotocol/server-brave-search
npm i -g @modelcontextprotocol/server-filesystem

# 2. Find your global node_modules path
npm -g root
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then update your &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt; to use absolute paths:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;mcp:
  servers:
    brave:
      command: &quot;node&quot;
      args: [&quot;C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-brave-search/dist/index.js&quot;]
    filesystem:
      command: &quot;node&quot;
      args: [&quot;C:/Program Files/nodejs/node_modules/@modelcontextprotocol/server-filesystem/dist/index.js&quot;, &quot;.&quot;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Replace the path with your actual global node_modules path from step 2.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;🔍 &lt;strong&gt;Search Server Configuration (Optional)&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;DeepCode supports multiple search servers for web search functionality. You can configure your preferred option in &lt;code&gt;mcp_agent.config.yaml&lt;/code&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# Default search server configuration
# Options: &quot;brave&quot; or &quot;bocha-mcp&quot;
default_search_server: &quot;brave&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Available Options:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🔍 Brave Search&lt;/strong&gt; (&lt;code&gt;&quot;brave&quot;&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Default option with high-quality search results&lt;/li&gt; 
   &lt;li&gt;Requires BRAVE_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Recommended for most users&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;🌐 Bocha-MCP&lt;/strong&gt; (&lt;code&gt;&quot;bocha-mcp&quot;&lt;/code&gt;):&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Alternative search server option&lt;/li&gt; 
   &lt;li&gt;Requires BOCHA_API_KEY configuration&lt;/li&gt; 
   &lt;li&gt;Uses local Python server implementation&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;API Key Configuration in mcp_agent.config.yaml:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;# For Brave Search (default) - around line 28
brave:
  command: &quot;npx&quot;
  args: [&quot;-y&quot;, &quot;@modelcontextprotocol/server-brave-search&quot;]
  env:
    BRAVE_API_KEY: &quot;your_brave_api_key_here&quot;

# For Bocha-MCP (alternative) - around line 74
bocha-mcp:
  command: &quot;python&quot;
  args: [&quot;tools/bocha_search_server.py&quot;]
  env:
    PYTHONPATH: &quot;.&quot;
    BOCHA_API_KEY: &quot;your_bocha_api_key_here&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;💡 Tip&lt;/strong&gt;: Both search servers require API key configuration. Choose the one that best fits your API access and requirements.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;⚡ &lt;strong&gt;Step 2: Launch Application&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;🚀 &lt;strong&gt;Using Installed Package (Recommended)&lt;/strong&gt;&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# 🌐 Launch web interface directly
deepcode

# The application will automatically start at http://localhost:8501
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;🛠️ &lt;strong&gt;Using Source Code&lt;/strong&gt;&lt;/h4&gt; 
&lt;p&gt;Choose your preferred interface:&lt;/p&gt; 
&lt;h5&gt;🌐 &lt;strong&gt;Web Interface&lt;/strong&gt; (Recommended)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Using UV
uv run streamlit run ui/streamlit_app.py
# Or using traditional Python
streamlit run ui/streamlit_app.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://img.shields.io/badge/Access-localhost:8501-00d4ff?style=flat-square&amp;amp;logo=streamlit&amp;amp;logoColor=white&quot; alt=&quot;Web Access&quot; /&gt; 
&lt;/div&gt; 
&lt;h5&gt;🖥️ &lt;strong&gt;CLI Interface&lt;/strong&gt; (Advanced Users)&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Using UV
uv run python cli/main_cli.py
# Or using traditional Python
python cli/main_cli.py
&lt;/code&gt;&lt;/pre&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://img.shields.io/badge/Mode-Interactive_Terminal-9b59b6?style=flat-square&amp;amp;logo=terminal&amp;amp;logoColor=white&quot; alt=&quot;CLI Mode&quot; /&gt; 
&lt;/div&gt; 
&lt;h3&gt;🎯 &lt;strong&gt;Step 3: Generate Code&lt;/strong&gt;&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;&lt;strong&gt;📄 Input&lt;/strong&gt;: Upload your research paper, provide requirements, or paste a URL&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;🤖 Processing&lt;/strong&gt;: Watch the multi-agent system analyze and plan&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;⚡ Output&lt;/strong&gt;: Receive production-ready code with tests and documentation&lt;/li&gt; 
&lt;/ol&gt; 
&lt;hr /&gt; 
&lt;h2&gt;💡 Examples&lt;/h2&gt; 
&lt;h3&gt;🎬 &lt;strong&gt;Live Demonstrations&lt;/strong&gt;&lt;/h3&gt; 
&lt;table align=&quot;center&quot;&gt; 
 &lt;tbody&gt;
  &lt;tr&gt; 
   &lt;td width=&quot;33%&quot; align=&quot;center&quot;&gt; &lt;h4&gt;📄 &lt;strong&gt;Paper2Code Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Research to Implementation&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align=&quot;center&quot;&gt; 
     &lt;a href=&quot;https://www.youtube.com/watch?v=MQZYpLkzsbw&quot;&gt; &lt;img src=&quot;https://img.youtube.com/vi/MQZYpLkzsbw/maxresdefault.jpg&quot; alt=&quot;Paper2Code Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);&quot; /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MQZYpLkzsbw&quot;&gt;▶️ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Transform academic papers into production-ready code automatically&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width=&quot;33%&quot; align=&quot;center&quot;&gt; &lt;h4&gt;🖼️ &lt;strong&gt;Image Processing Demo&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;AI-Powered Image Tools&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align=&quot;center&quot;&gt; 
     &lt;a href=&quot;https://www.youtube.com/watch?v=nFt5mLaMEac&quot;&gt; &lt;img src=&quot;https://img.youtube.com/vi/nFt5mLaMEac/maxresdefault.jpg&quot; alt=&quot;Image Processing Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);&quot; /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=nFt5mLaMEac&quot;&gt;▶️ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Intelligent image processing with background removal and enhancement&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
   &lt;td width=&quot;33%&quot; align=&quot;center&quot;&gt; &lt;h4&gt;🌐 &lt;strong&gt;Frontend Implementation&lt;/strong&gt;&lt;/h4&gt; &lt;p&gt;&lt;strong&gt;Complete Web Application&lt;/strong&gt;&lt;/p&gt; 
    &lt;div align=&quot;center&quot;&gt; 
     &lt;a href=&quot;https://www.youtube.com/watch?v=78wx3dkTaAU&quot;&gt; &lt;img src=&quot;https://img.youtube.com/vi/78wx3dkTaAU/maxresdefault.jpg&quot; alt=&quot;Frontend Demo&quot; width=&quot;100%&quot; style=&quot;border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);&quot; /&gt; &lt;/a&gt; 
     &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=78wx3dkTaAU&quot;&gt;▶️ Watch Demo&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
     &lt;p&gt;&lt;em&gt;Full-stack web development from concept to deployment&lt;/em&gt;&lt;/p&gt; 
    &lt;/div&gt; &lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt;
&lt;/table&gt; 
&lt;h3&gt;🆕 &lt;strong&gt;Recent Updates&lt;/strong&gt;&lt;/h3&gt; 
&lt;h4&gt;📄 &lt;strong&gt;Smart Document Segmentation (v1.2.0)&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Intelligent Processing&lt;/strong&gt;: Automatically handles large research papers and technical documents that exceed LLM token limits&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Configurable Control&lt;/strong&gt;: Toggle segmentation via configuration with size-based thresholds&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Semantic Analysis&lt;/strong&gt;: Advanced content understanding with algorithm, concept, and formula preservation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Backward Compatibility&lt;/strong&gt;: Seamlessly falls back to traditional processing for smaller documents&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;🚀 &lt;strong&gt;Coming Soon&lt;/strong&gt;&lt;/h3&gt; 
&lt;p&gt;We&#39;re continuously enhancing DeepCode with exciting new features:&lt;/p&gt; 
&lt;h4&gt;🔧 &lt;strong&gt;Enhanced Code Reliability &amp;amp; Validation&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Automated Testing&lt;/strong&gt;: Comprehensive functionality testing with execution verification and error detection.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Code Quality Assurance&lt;/strong&gt;: Multi-level validation through static analysis, dynamic testing, and performance benchmarking.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Smart Debugging&lt;/strong&gt;: AI-powered error detection with automatic correction suggestions&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;📊 &lt;strong&gt;PaperBench Performance Showcase&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Benchmark Dashboard&lt;/strong&gt;: Comprehensive performance metrics on the PaperBench evaluation suite.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Accuracy Metrics&lt;/strong&gt;: Detailed comparison with state-of-the-art paper reproduction systems.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Success Analytics&lt;/strong&gt;: Statistical analysis across paper categories and complexity levels.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;⚡ &lt;strong&gt;System-wide Optimizations&lt;/strong&gt;&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Performance Boost&lt;/strong&gt;: Multi-threaded processing and optimized agent coordination for faster generation.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Enhanced Reasoning&lt;/strong&gt;: Advanced reasoning capabilities with improved context understanding.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Expanded Support&lt;/strong&gt;: Extended compatibility with additional programming languages and frameworks.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;⭐ Star History&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;em&gt;Community Growth Trajectory&lt;/em&gt;&lt;/p&gt; 
 &lt;a href=&quot;https://star-history.com/#HKUDS/DeepCode&amp;amp;Date&quot;&gt; 
  &lt;picture&gt; 
   &lt;source media=&quot;(prefers-color-scheme: dark)&quot; srcset=&quot;https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&amp;amp;theme=dark&quot; /&gt; 
   &lt;source media=&quot;(prefers-color-scheme: light)&quot; srcset=&quot;https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&quot; /&gt; 
   &lt;img alt=&quot;Star History Chart&quot; src=&quot;https://api.star-history.com/svg?repos=HKUDS/DeepCode&amp;amp;type=Date&quot; style=&quot;border-radius: 15px; box-shadow: 0 0 30px rgba(0, 217, 255, 0.3);&quot; /&gt; 
  &lt;/picture&gt; &lt;/a&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h3&gt;🚀 &lt;strong&gt;Ready to Transform Development?&lt;/strong&gt;&lt;/h3&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt; &lt;a href=&quot;https://raw.githubusercontent.com/HKUDS/DeepCode/main/#-quick-start&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/🚀_Get_Started-00d4ff?style=for-the-badge&amp;amp;logo=rocket&amp;amp;logoColor=white&quot; alt=&quot;Get Started&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/HKUDS&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/🏛️_View_on_GitHub-00d4ff?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&quot; alt=&quot;View on GitHub&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/HKUDS/deepcode-agent&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/⭐_Star_Project-00d4ff?style=for-the-badge&amp;amp;logo=star&amp;amp;logoColor=white&quot; alt=&quot;Star Project&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;h3&gt;📄 &lt;strong&gt;License&lt;/strong&gt;&lt;/h3&gt; 
 &lt;img src=&quot;https://img.shields.io/badge/License-MIT-4ecdc4?style=for-the-badge&amp;amp;logo=opensourceinitiative&amp;amp;logoColor=white&quot; alt=&quot;MIT License&quot; /&gt; 
 &lt;p&gt;&lt;strong&gt;MIT License&lt;/strong&gt; - Copyright (c) 2025 Data Intelligence Lab, The University of Hong Kong&lt;/p&gt; 
 &lt;hr /&gt; 
 &lt;img src=&quot;https://visitor-badge.laobi.icu/badge?page_id=deepcode.readme&amp;amp;style=for-the-badge&amp;amp;color=00d4ff&quot; alt=&quot;Visitors&quot; /&gt; 
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>QuentinFuxa/WhisperLiveKit</title>
      <link>https://github.com/QuentinFuxa/WhisperLiveKit</link>
      <description>&lt;p&gt;Real-time &amp; local speech-to-text, translation, and speaker diarization. With server &amp; web UI.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt;WhisperLiveKit&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/demo.png&quot; alt=&quot;WhisperLiveKit Demo&quot; width=&quot;730&quot; /&gt; &lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;b&gt;Real-time, Fully Local Speech-to-Text with Speaker Identification&lt;/b&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://pypi.org/project/whisperlivekit/&quot;&gt;&lt;img alt=&quot;PyPI Version&quot; src=&quot;https://img.shields.io/pypi/v/whisperlivekit?color=g&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pepy.tech/project/whisperlivekit&quot;&gt;&lt;img alt=&quot;PyPI Downloads&quot; src=&quot;https://static.pepy.tech/personalized-badge/whisperlivekit?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=brightgreen&amp;amp;left_text=installations&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pypi.org/project/whisperlivekit/&quot;&gt;&lt;img alt=&quot;Python Versions&quot; src=&quot;https://img.shields.io/badge/python-3.9--3.15-dark_green&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/LICENSE&quot;&gt;&lt;img alt=&quot;License&quot; src=&quot;https://img.shields.io/badge/License-MIT/Dual Licensed-dark_green&quot; /&gt;&lt;/a&gt; &lt;/p&gt; 
&lt;p&gt;Real-time speech transcription directly to your browser, with a ready-to-use backend+server and a simple frontend. ✨&lt;/p&gt; 
&lt;h4&gt;Powered by Leading Research:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ufalSimulStreaming&quot;&gt;SimulStreaming&lt;/a&gt; (SOTA 2025) - Ultra-low latency transcription using &lt;a href=&quot;https://arxiv.org/pdf/2305.11408&quot;&gt;AlignAtt policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.04672&quot;&gt;NLLB&lt;/a&gt;, (&lt;a href=&quot;https://huggingface.co/entai2965/nllb-200-distilled-600M-ctranslate2&quot;&gt;distilled&lt;/a&gt;) (2024) - Translation to more than 100 languages.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/ufal/whisper_streaming&quot;&gt;WhisperStreaming&lt;/a&gt; (SOTA 2023) - Low latency transcription using &lt;a href=&quot;https://www.isca-archive.org/interspeech_2020/liu20s_interspeech.pdf&quot;&gt;LocalAgreement policy&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2507.18446&quot;&gt;Streaming Sortformer&lt;/a&gt; (SOTA 2025) - Advanced real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/juanmc2005/diart&quot;&gt;Diart&lt;/a&gt; (SOTA 2021) - Real-time speaker diarization&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/snakers4/silero-vad&quot;&gt;Silero VAD&lt;/a&gt; (2024) - Enterprise-grade Voice Activity Detection&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Why not just run a simple Whisper model on every audio batch?&lt;/strong&gt; Whisper is designed for complete utterances, not real-time chunks. Processing small segments loses context, cuts off words mid-syllable, and produces poor transcription. WhisperLiveKit uses state-of-the-art simultaneous speech research for intelligent buffering and incremental processing.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;Architecture&lt;/h3&gt; 
&lt;img alt=&quot;Architecture&quot; src=&quot;https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/architecture.png&quot; /&gt; 
&lt;p&gt;&lt;em&gt;The backend supports multiple concurrent users. Voice Activity Detection reduces overhead when no voice is detected.&lt;/em&gt;&lt;/p&gt; 
&lt;h3&gt;Installation &amp;amp; Quick Start&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install whisperlivekit
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;You can also clone the repo and &lt;code&gt;pip install -e .&lt;/code&gt; for the latest version.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Quick Start&lt;/h4&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the transcription server:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;whisperlivekit-server --model base --language en
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open your browser&lt;/strong&gt; and navigate to &lt;code&gt;http://localhost:8000&lt;/code&gt;. Start speaking and watch your words appear in real-time!&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;blockquote&gt; 
 &lt;ul&gt; 
  &lt;li&gt;See &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py&quot;&gt;tokenizer.py&lt;/a&gt; for the list of all available languages.&lt;/li&gt; 
  &lt;li&gt;For HTTPS requirements, see the &lt;strong&gt;Parameters&lt;/strong&gt; section for SSL configuration options.&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Use it to capture audio from web pages.&lt;/h4&gt; 
&lt;p&gt;Go to &lt;code&gt;chrome-extension&lt;/code&gt; for instructions.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/QuentinFuxa/WhisperLiveKit/refs/heads/main/chrome-extension/demo-extension.png&quot; alt=&quot;WhisperLiveKit Demo&quot; width=&quot;600&quot; /&gt; &lt;/p&gt; 
&lt;h4&gt;Optional Dependencies&lt;/h4&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Optional&lt;/th&gt; 
   &lt;th&gt;&lt;code&gt;pip install&lt;/code&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Speaker diarization with Sortformer&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[asr]&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;Apple Silicon optimized backend&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;mlx-whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;strong&gt;NLLB Translation&lt;/strong&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;huggingface_hub&lt;/code&gt; &amp;amp; &lt;code&gt;transformers&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Speaker diarization with Diart&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Original Whisper backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;em&gt;[Not recommanded]&lt;/em&gt; Improved timestamps backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;whisper-timestamped&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;OpenAI API backend&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;openai&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;See &lt;strong&gt;Parameters &amp;amp; Configuration&lt;/strong&gt; below on how to use them.&lt;/p&gt; 
&lt;h3&gt;Usage Examples&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Command-line Interface&lt;/strong&gt;: Start the transcription server with various options:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Large model and translate from french to danish
whisperlivekit-server --model large-v3 --language fr --target-language da

# Diarization and server listening on */80 
whisperlivekit-server --host 0.0.0.0 --port 80 --model medium --diarization --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Python API Integration&lt;/strong&gt;: Check &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/basic_server.py&quot;&gt;basic_server&lt;/a&gt; for a more complete example of how to use the functions and classes.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from whisperlivekit import TranscriptionEngine, AudioProcessor, parse_args
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
import asyncio

transcription_engine = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global transcription_engine
    transcription_engine = TranscriptionEngine(model=&quot;medium&quot;, diarization=True, lan=&quot;en&quot;)
    yield

app = FastAPI(lifespan=lifespan)

async def handle_websocket_results(websocket: WebSocket, results_generator):
    async for response in results_generator:
        await websocket.send_json(response)
    await websocket.send_json({&quot;type&quot;: &quot;ready_to_stop&quot;})

@app.websocket(&quot;/asr&quot;)
async def websocket_endpoint(websocket: WebSocket):
    global transcription_engine

    # Create a new AudioProcessor for each connection, passing the shared engine
    audio_processor = AudioProcessor(transcription_engine=transcription_engine)    
    results_generator = await audio_processor.create_tasks()
    results_task = asyncio.create_task(handle_websocket_results(websocket, results_generator))
    await websocket.accept()
    while True:
        message = await websocket.receive_bytes()
        await audio_processor.process_audio(message)        
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Frontend Implementation&lt;/strong&gt;: The package includes an HTML/JavaScript implementation &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/web/live_transcription.html&quot;&gt;here&lt;/a&gt;. You can also import it using &lt;code&gt;from whisperlivekit import get_inline_ui_html&lt;/code&gt; &amp;amp; &lt;code&gt;page = get_inline_ui_html()&lt;/code&gt;&lt;/p&gt; 
&lt;h2&gt;Parameters &amp;amp; Configuration&lt;/h2&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Parameter&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Whisper model size. List and recommandations &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/available_models.md&quot;&gt;here&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;small&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;List &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/simul_whisper/whisper/tokenizer.py&quot;&gt;here&lt;/a&gt;. If you use &lt;code&gt;auto&lt;/code&gt;, the model attempts to detect the language automatically, but it tends to bias towards English.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--target-language&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;If sets, activates translation using NLLB. Ex: &lt;code&gt;fr&lt;/code&gt;. &lt;a href=&quot;https://github.com/QuentinFuxa/WhisperLiveKit/raw/main/whisperlivekit/translation/mapping_languages.py&quot;&gt;118 languages available&lt;/a&gt;. If you want to translate to english, you should rather use &lt;code&gt;--task translate&lt;/code&gt;, since Whisper can do it directly.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--task&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Set to &lt;code&gt;translate&lt;/code&gt; to translate &lt;em&gt;only&lt;/em&gt; to english, using Whisper translation.&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transcribe&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Enable speaker identification&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Processing backend. You can switch to &lt;code&gt;faster-whisper&lt;/code&gt; if &lt;code&gt;simulstreaming&lt;/code&gt; does not work correctly&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;simulstreaming&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vac&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Controller&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--no-vad&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Voice Activity Detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--warmup-file&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Audio file path for model warmup&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;jfk.wav&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--host&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server host address&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;localhost&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--port&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Server port&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;8000&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-certfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL certificate file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--ssl-keyfile&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to the SSL private key file (for HTTPS support)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--pcm-input&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;raw PCM (s16le) data is expected as input and FFmpeg will be bypassed. Frontend will use AudioWorklet instead of MediaRecorder&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Translation options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--nllb-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;transformers&lt;/code&gt; or &lt;code&gt;ctranslate2&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;ctranslate2&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--nllb-size&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;600M&lt;/code&gt; or &lt;code&gt;1.3B&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;600M&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Diarization options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--diarization-backend&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;diart&lt;/code&gt; or &lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;sortformer&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-punctuation-split&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable punctuation based splits. See #214&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--segmentation-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart segmentation model. &lt;a href=&quot;https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models&quot;&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--embedding-model&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Hugging Face model ID for Diart embedding model. &lt;a href=&quot;https://github.com/juanmc2005/diart/tree/main?tab=readme-ov-file#pre-trained-models&quot;&gt;Available models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;speechbrain/spkrec-ecapa-voxceleb&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;SimulStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--disable-fast-encoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Disable Faster Whisper or MLX Whisper backends for the encoder (if installed). Inference can be slower but helpful when GPU memory is limited&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--frame-threshold&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;AlignAtt frame threshold (lower = faster, higher = more accurate)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;25&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--beams&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Number of beams for beam search (1 = greedy decoding)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--decoder&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Force decoder type (&lt;code&gt;beam&lt;/code&gt; or &lt;code&gt;greedy&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;auto&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-max-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum audio buffer length (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;30.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--audio-min-len&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Minimum audio length to process (seconds)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;0.0&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--cif-ckpt-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Path to CIF model for word boundary detection&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--never-fire&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Never truncate incomplete words&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Initial prompt for the model&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--static-init-prompt&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Static prompt that doesn&#39;t scroll&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--max-context-tokens&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Maximum context tokens&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--model-path&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Direct path to .pt model file. Download it if not found&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;./base.pt&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--preload-model-count&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Optional. Number of models to preload in memory to speed up loading (set up to the expected number of concurrent users)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;1&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;WhisperStreaming backend options&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Default&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--confidence-validation&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Use confidence scores for faster validation&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;False&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;--buffer_trimming&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Buffer trimming strategy (&lt;code&gt;sentence&lt;/code&gt; or &lt;code&gt;segment&lt;/code&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;segment&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;For diarization using Diart, you need to accept user conditions &lt;a href=&quot;https://huggingface.co/pyannote/segmentation&quot;&gt;here&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation&lt;/code&gt; model, &lt;a href=&quot;https://huggingface.co/pyannote/segmentation-3.0&quot;&gt;here&lt;/a&gt; for the &lt;code&gt;pyannote/segmentation-3.0&lt;/code&gt; model and &lt;a href=&quot;https://huggingface.co/pyannote/embedding&quot;&gt;here&lt;/a&gt; for the &lt;code&gt;pyannote/embedding&lt;/code&gt; model. &lt;strong&gt;Then&lt;/strong&gt;, login to HuggingFace: &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;🚀 Deployment Guide&lt;/h3&gt; 
&lt;p&gt;To deploy WhisperLiveKit in production:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Server Setup&lt;/strong&gt;: Install production ASGI server &amp;amp; launch with multiple workers&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install uvicorn gunicorn
gunicorn -k uvicorn.workers.UvicornWorker -w 4 your_app:app
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Host your customized version of the &lt;code&gt;html&lt;/code&gt; example &amp;amp; ensure WebSocket connection points correctly&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Nginx Configuration&lt;/strong&gt; (recommended for production):&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-nginx&quot;&gt;server {
   listen 80;
   server_name your-domain.com;
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection &quot;upgrade&quot;;
        proxy_set_header Host $host;
}}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;HTTPS Support&lt;/strong&gt;: For secure deployments, use &quot;wss://&quot; instead of &quot;ws://&quot; in WebSocket URL&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;h2&gt;🐋 Docker&lt;/h2&gt; 
&lt;p&gt;Deploy the application easily using Docker with GPU or CPU support.&lt;/p&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Docker installed on your system&lt;/li&gt; 
 &lt;li&gt;For GPU support: NVIDIA Docker runtime installed&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick Start&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;With GPU acceleration (recommended):&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker build -t wlk .
docker run --gpus all -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;CPU only:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker build -f Dockerfile.cpu -t wlk .
docker run -p 8000:8000 --name wlk wlk
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;p&gt;&lt;strong&gt;Custom configuration:&lt;/strong&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Example with custom model and language
docker run --gpus all -p 8000:8000 --name wlk wlk --model large-v3 --language fr
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Memory Requirements&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Large models&lt;/strong&gt;: Ensure your Docker runtime has sufficient memory allocated&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Customization&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;--build-arg&lt;/code&gt; Options: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;EXTRAS=&quot;whisper-timestamped&quot;&lt;/code&gt; - Add extras to the image&#39;s installation (no spaces). Remember to set necessary container options!&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_PRECACHE_DIR=&quot;./.cache/&quot;&lt;/code&gt; - Pre-load a model cache for faster first-time start&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;HF_TKN_FILE=&quot;./token&quot;&lt;/code&gt; - Add your Hugging Face Hub access token to download gated models&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔮 Use Cases&lt;/h2&gt; 
&lt;p&gt;Capture discussions in real-time for meeting transcription, help hearing-impaired users follow conversations through accessibility tools, transcribe podcasts or videos automatically for content creation, transcribe support calls with speaker identification for customer service...&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>openai/codex</title>
      <link>https://github.com/openai/codex</link>
      <description>&lt;p&gt;Lightweight coding agent that runs in your terminal&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt;&lt;code&gt;npm i -g @openai/codex&lt;/code&gt;&lt;br /&gt;or &lt;code&gt;brew install codex&lt;/code&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt;&lt;strong&gt;Codex CLI&lt;/strong&gt; is a coding agent from OpenAI that runs locally on your computer. &lt;br /&gt; &lt;br /&gt;If you want Codex in your code editor (VS Code, Cursor, Windsurf), &lt;a href=&quot;https://developers.openai.com/codex/ide&quot;&gt;install in your IDE&lt;/a&gt; &lt;br /&gt;If you are looking for the &lt;em&gt;cloud-based agent&lt;/em&gt; from OpenAI, &lt;strong&gt;Codex Web&lt;/strong&gt;, go to &lt;a href=&quot;https://chatgpt.com/codex&quot;&gt;chatgpt.com/codex&lt;/a&gt;&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-splash.png&quot; alt=&quot;Codex CLI splash&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;h2&gt;Quickstart&lt;/h2&gt; 
&lt;h3&gt;Installing and running Codex CLI&lt;/h3&gt; 
&lt;p&gt;Install globally with your preferred package manager. If you use npm:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;npm install -g @openai/codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, if you use Homebrew:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;brew install codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Then simply run &lt;code&gt;codex&lt;/code&gt; to get started:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;codex
&lt;/code&gt;&lt;/pre&gt; 
&lt;details&gt; 
 &lt;summary&gt;You can also go to the &lt;a href=&quot;https://github.com/openai/codex/releases/latest&quot;&gt;latest GitHub Release&lt;/a&gt; and download the appropriate binary for your platform.&lt;/summary&gt; 
 &lt;p&gt;Each GitHub Release contains many executables, but in practice, you likely want one of these:&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;macOS 
   &lt;ul&gt; 
    &lt;li&gt;Apple Silicon/arm64: &lt;code&gt;codex-aarch64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;x86_64 (older Mac hardware): &lt;code&gt;codex-x86_64-apple-darwin.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
  &lt;li&gt;Linux 
   &lt;ul&gt; 
    &lt;li&gt;x86_64: &lt;code&gt;codex-x86_64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
    &lt;li&gt;arm64: &lt;code&gt;codex-aarch64-unknown-linux-musl.tar.gz&lt;/code&gt;&lt;/li&gt; 
   &lt;/ul&gt; &lt;/li&gt; 
 &lt;/ul&gt; 
 &lt;p&gt;Each archive contains a single entry with the platform baked into the name (e.g., &lt;code&gt;codex-x86_64-unknown-linux-musl&lt;/code&gt;), so you likely want to rename it to &lt;code&gt;codex&lt;/code&gt; after extracting it.&lt;/p&gt; 
&lt;/details&gt; 
&lt;h3&gt;Using Codex with your ChatGPT plan&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img src=&quot;https://raw.githubusercontent.com/openai/codex/main/.github/codex-cli-login.png&quot; alt=&quot;Codex CLI login&quot; width=&quot;80%&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;Run &lt;code&gt;codex&lt;/code&gt; and select &lt;strong&gt;Sign in with ChatGPT&lt;/strong&gt;. We recommend signing into your ChatGPT account to use Codex as part of your Plus, Pro, Team, Edu, or Enterprise plan. &lt;a href=&quot;https://help.openai.com/en/articles/11369540-codex-in-chatgpt&quot;&gt;Learn more about what&#39;s included in your ChatGPT plan&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;You can also use Codex with an API key, but this requires &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#usage-based-billing-alternative-use-an-openai-api-key&quot;&gt;additional setup&lt;/a&gt;. If you previously used an API key for usage-based billing, see the &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#migrating-from-usage-based-billing-api-key&quot;&gt;migration steps&lt;/a&gt;. If you&#39;re having trouble with login, please comment on &lt;a href=&quot;https://github.com/openai/codex/issues/1243&quot;&gt;this issue&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;Model Context Protocol (MCP)&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp&quot;&gt;MCP servers&lt;/a&gt;. Enable by adding an &lt;code&gt;mcp_servers&lt;/code&gt; section to your &lt;code&gt;~/.codex/config.toml&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Configuration&lt;/h3&gt; 
&lt;p&gt;Codex CLI supports a rich set of configuration options, with preferences stored in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;. For full configuration options, see &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/config.md&quot;&gt;Configuration&lt;/a&gt;.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;h3&gt;Docs &amp;amp; FAQ&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md&quot;&gt;&lt;strong&gt;Getting started&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#cli-usage&quot;&gt;CLI usage&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#running-with-a-prompt-as-input&quot;&gt;Running with a prompt as input&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#example-prompts&quot;&gt;Example prompts&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/getting-started.md#memory-with-agentsmd&quot;&gt;Memory with AGENTS.md&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/config.md&quot;&gt;Configuration&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/sandbox.md&quot;&gt;&lt;strong&gt;Sandbox &amp;amp; approvals&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md&quot;&gt;&lt;strong&gt;Authentication&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#forcing-a-specific-auth-method-advanced&quot;&gt;Auth methods&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/authentication.md#connecting-on-a-headless-machine&quot;&gt;Login on a &quot;Headless&quot; machine&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md&quot;&gt;&lt;strong&gt;Advanced&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#non-interactive--ci-mode&quot;&gt;Non-interactive / CI mode&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#tracing--verbose-logging&quot;&gt;Tracing / verbose logging&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/advanced.md#model-context-protocol-mcp&quot;&gt;Model Context Protocol (MCP)&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/zdr.md&quot;&gt;&lt;strong&gt;Zero data retention (ZDR)&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/contributing.md&quot;&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/install.md&quot;&gt;&lt;strong&gt;Install &amp;amp; build&lt;/strong&gt;&lt;/a&gt; 
  &lt;ul&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/install.md#system-requirements&quot;&gt;System Requirements&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/install.md#dotslash&quot;&gt;DotSlash&lt;/a&gt;&lt;/li&gt; 
   &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/install.md#build-from-source&quot;&gt;Build from source&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/faq.md&quot;&gt;&lt;strong&gt;FAQ&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/docs/open-source-fund.md&quot;&gt;&lt;strong&gt;Open source fund&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;hr /&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;This repository is licensed under the &lt;a href=&quot;https://raw.githubusercontent.com/openai/codex/main/LICENSE&quot;&gt;Apache-2.0 License&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>twitter/the-algorithm</title>
      <link>https://github.com/twitter/the-algorithm</link>
      <description>&lt;p&gt;Source code for the X Recommendation Algorithm&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;X&#39;s Recommendation Algorithm&lt;/h1&gt; 
&lt;p&gt;X&#39;s Recommendation Algorithm is a set of services and jobs that are responsible for serving feeds of posts and other content across all X product surfaces (e.g. For You Timeline, Search, Explore, Notifications). For an introduction to how the algorithm works, please refer to our &lt;a href=&quot;https://blog.x.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm&quot;&gt;engineering blog&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;p&gt;Product surfaces at X are built on a shared set of data, models, and software frameworks. The shared components included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Data&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/tweetypie/server/README.md&quot;&gt;tweetypie&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Core service that handles the reading and writing of post data.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/unified_user_actions/README.md&quot;&gt;unified-user-actions&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Real-time stream of user actions on X.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/user-signal-service/README.md&quot;&gt;user-signal-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Centralized platform to retrieve explicit (e.g. likes, replies) and implicit (e.g. profile visits, tweet clicks) user signals.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Model&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/simclusters_v2/README.md&quot;&gt;SimClusters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Community detection and sparse embeddings into those communities.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/twitter/the-algorithm-ml/raw/main/projects/twhin/README.md&quot;&gt;TwHIN&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Dense knowledge graph embeddings for Users and Posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/trust_and_safety_models/README.md&quot;&gt;trust-and-safety-models&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Models for detecting NSFW or abusive content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/interaction_graph/README.md&quot;&gt;real-graph&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Model to predict the likelihood of an X User interacting with another User.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/graph/batch/job/tweepcred/README&quot;&gt;tweepcred&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Page-Rank algorithm for calculating X User reputation.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/recos-injector/README.md&quot;&gt;recos-injector&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Streaming event processor for building input streams for &lt;a href=&quot;https://github.com/twitter/GraphJet&quot;&gt;GraphJet&lt;/a&gt; based services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/graph-feature-service/README.md&quot;&gt;graph-feature-service&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Serves graph features for a directed pair of users (e.g. how many of User A&#39;s following liked posts from User B).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/topic-social-proof/README.md&quot;&gt;topic-social-proof&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Identifies topics related to individual posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-scorer/README.md&quot;&gt;representation-scorer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Compute scores between pairs of entities (Users, Posts, etc.) using embedding similarity.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Software framework&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/navi/README.md&quot;&gt;navi&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;High performance, machine learning model serving written in Rust.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md&quot;&gt;product-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Software framework for building feeds of content.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/timelines/data_processing/ml_util/aggregation_framework/README.md&quot;&gt;timelines-aggregation-framework&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Framework for generating aggregate features in batch or real time.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/representation-manager/README.md&quot;&gt;representation-manager&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Service to retrieve embeddings (i.e. SimClusers and TwHIN).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/twml/README.md&quot;&gt;twml&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy machine learning framework built on TensorFlow v1.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The product surfaces currently included in this repository are the For You Timeline and Recommended Notifications.&lt;/p&gt; 
&lt;h3&gt;For You Timeline&lt;/h3&gt; 
&lt;p&gt;The diagram below illustrates how major services and jobs interconnect to construct a For You Timeline.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/docs/system-diagram.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt; 
&lt;p&gt;The core components of the For You Timeline included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Candidate Source&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/java/com/twitter/search/README.md&quot;&gt;search-index&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Find and rank In-Network posts. ~50% of posts come from this candidate source.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/tweet-mixer&quot;&gt;tweet-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Coordination layer for fetching Out-of-Network tweet candidates from underlying compute services.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos/user_tweet_entity_graph/README.md&quot;&gt;user-tweet-entity-graph&lt;/a&gt; (UTEG)&lt;/td&gt; 
   &lt;td&gt;Maintains an in memory User to Post interaction graph, and finds candidates based on traversals of this graph. This is built on the &lt;a href=&quot;https://github.com/twitter/GraphJet&quot;&gt;GraphJet&lt;/a&gt; framework. Several other GraphJet based features and candidate sources are located &lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/scala/com/twitter/recos&quot;&gt;here&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/follow-recommendations-service/README.md&quot;&gt;follow-recommendation-service&lt;/a&gt; (FRS)&lt;/td&gt; 
   &lt;td&gt;Provides Users with recommendations for accounts to follow, and posts from those accounts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/README.md&quot;&gt;light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by search index (Earlybird) to rank posts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/README.md&quot;&gt;heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Neural network for ranking candidate posts. One of the main signals used to select timeline posts post candidate sourcing.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Post mixing &amp;amp; filtering&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/home-mixer/README.md&quot;&gt;home-mixer&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main service used to construct and serve the Home Timeline. Built on &lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/product-mixer/README.md&quot;&gt;product-mixer&lt;/a&gt;.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/visibilitylib/README.md&quot;&gt;visibility-filters&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Responsible for filtering X content to support legal compliance, improve product quality, increase user trust, protect revenue through the use of hard-filtering, visible product treatments, and coarse-grained downranking.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/timelineranker/README.md&quot;&gt;timelineranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Legacy service which provides relevance-scored posts from the Earlybird Search Index and UTEG service.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Recommended Notifications&lt;/h3&gt; 
&lt;p&gt;The core components of Recommended Notifications included in this repository are listed below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Type&lt;/th&gt; 
   &lt;th&gt;Component&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Service&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/README.md&quot;&gt;pushservice&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Main recommendation service at X used to surface recommendations to our users via notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Ranking&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/light_ranking/README.md&quot;&gt;pushservice-light-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Light Ranker model used by pushservice to rank posts. Bridges candidate generation and heavy ranking by pre-selecting highly-relevant candidates from the initial huge candidate pool.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;a href=&quot;https://raw.githubusercontent.com/twitter/the-algorithm/main/pushservice/src/main/python/models/heavy_ranking/README.md&quot;&gt;pushservice-heavy-ranker&lt;/a&gt;&lt;/td&gt; 
   &lt;td&gt;Multi-task learning model to predict the probabilities that the target users will open and engage with the sent notifications.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h2&gt;Build and test code&lt;/h2&gt; 
&lt;p&gt;We include Bazel BUILD files for most components, but not a top-level BUILD or WORKSPACE file. We plan to add a more complete build and test system in the future.&lt;/p&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We invite the community to submit GitHub issues and pull requests for suggestions on improving the recommendation algorithm. We are working on tools to manage these suggestions and sync changes to our internal repository. Any security concerns or issues should be routed to our official &lt;a href=&quot;https://hackerone.com/x&quot;&gt;bug bounty program&lt;/a&gt; through HackerOne. We hope to benefit from the collective intelligence and expertise of the global community in helping us identify issues and suggest improvements, ultimately leading to a better X.&lt;/p&gt; 
&lt;p&gt;Read our blog on the open source initiative &lt;a href=&quot;https://blog.x.com/en_us/topics/company/2023/a-new-era-of-transparency-for-twitter&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Alibaba-NLP/DeepResearch</title>
      <link>https://github.com/Alibaba-NLP/DeepResearch</link>
      <description>&lt;p&gt;Tongyi Deep Research, the Leading Open-source Deep Research Agent&lt;/p&gt;&lt;hr&gt;&lt;div align=&quot;center&quot;&gt; 
 &lt;picture&gt; 
  &lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/logo.png&quot; width=&quot;100%&quot; /&gt; 
 &lt;/picture&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;div align=&quot;center&quot; style=&quot;line-height: 1;&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Models-5EDDD2?style=for-the-badge&amp;amp;logo=huggingface&amp;amp;logoColor=ffffff&amp;amp;labelColor&quot; alt=&quot;MODELS&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Github-24292F?style=for-the-badge&amp;amp;logo=github&amp;amp;logoColor=white&quot; alt=&quot;GITHUB&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&amp;amp;logo=google-chrome&amp;amp;logoColor=white&quot; alt=&quot;Blog&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; 🤗 &lt;a href=&quot;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&quot; target=&quot;_blank&quot;&gt;HuggingFace&lt;/a&gt; ｜ &lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot; /&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B&quot; target=&quot;_blank&quot;&gt;ModelScope&lt;/a&gt; | 💬 &lt;a href=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/wechat.jpg&quot;&gt;WeChat(微信)&lt;/a&gt; &lt;/p&gt;
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://trendshift.io/repositories/14895&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://trendshift.io/api/badge/repositories/14895&quot; alt=&quot;Alibaba-NLP%2FDeepResearch | Trendshift&quot; style=&quot;width: 250px; height: 55px;&quot; width=&quot;250&quot; height=&quot;55&quot; /&gt;&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;👏 Welcome to try Tongyi DeepResearch via our &lt;strong&gt;&lt;a href=&quot;https://www.modelscope.cn/studios/jialongwu/Tongyi-DeepResearch&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot; /&gt; Modelscope online demo&lt;/a&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;a href=&quot;https://huggingface.co/spaces/Alibaba-NLP/Tongyi-DeepResearch&quot;&gt;🤗 Huggingface online demo&lt;/a&gt;&lt;/strong&gt; or &lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/assets/aliyun.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot; /&gt; &lt;strong&gt;&lt;a href=&quot;https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&amp;amp;tab=app#/app/app-market/deep-search/&quot;&gt;bailian service&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!NOTE] This demo is for quick exploration only. Response times may vary or fail intermittently due to model latency and tool QPS limits. For a stable experience we recommend local deployment; for a production-ready service, visit &lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/assets/aliyun.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot; /&gt; &lt;a href=&quot;https://bailian.console.aliyun.com/?spm=a2ty02.31808181.d_app-market.1.6c4974a1tFmoFc&amp;amp;tab=app#/app/app-market/deep-search/&quot;&gt;bailian&lt;/a&gt; and follow the guided setup.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h1&gt;Introduction&lt;/h1&gt; 
&lt;p&gt;We present &lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot; /&gt; &lt;strong&gt;Tongyi DeepResearch&lt;/strong&gt;, an agentic large language model featuring 30.5 billion total parameters, with only 3.3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for &lt;strong&gt;long-horizon, deep information-seeking&lt;/strong&gt; tasks. Tongyi DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity&#39;s Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA,xbench-DeepSearch, FRAMES and SimpleQA.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;Tongyi DeepResearch builds upon our previous work on the &lt;img src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/tongyi.png&quot; width=&quot;14px&quot; style=&quot;display:inline;&quot; /&gt; &lt;a href=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/WebAgent/&quot;&gt;WebAgent&lt;/a&gt; project.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;p&gt;More details can be found in our 📰&amp;nbsp;&lt;a href=&quot;https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&quot;&gt;Tech Blog&lt;/a&gt;.&lt;/p&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/performance.png&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Features&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;⚙️ &lt;strong&gt;Fully automated synthetic data generation pipeline&lt;/strong&gt;: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.&lt;/li&gt; 
 &lt;li&gt;🔄 &lt;strong&gt;Large-scale continual pre-training on agentic data&lt;/strong&gt;: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.&lt;/li&gt; 
 &lt;li&gt;🔁 &lt;strong&gt;End-to-end reinforcement learning&lt;/strong&gt;: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‑stationary environment.&lt;/li&gt; 
 &lt;li&gt;🤖 &lt;strong&gt;Agent Inference Paradigm Compatibility&lt;/strong&gt;: At inference, Tongyi DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model&#39;s core intrinsic abilities, and an IterResearch-based &#39;Heavy&#39; mode, which uses a test-time scaling strategy to unlock the model&#39;s maximum performance ceiling.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Model Download&lt;/h1&gt; 
&lt;p&gt;You can directly download the model by following the links below.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th align=&quot;center&quot;&gt;Model&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Download Links&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Model Size&lt;/th&gt; 
   &lt;th align=&quot;center&quot;&gt;Context Length&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td align=&quot;center&quot;&gt;Tongyi-DeepResearch-30B-A3B&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;&lt;a href=&quot;https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B&quot;&gt;🤗 HuggingFace&lt;/a&gt;&lt;br /&gt; &lt;a href=&quot;https://modelscope.cn/models/iic/Tongyi-DeepResearch-30B-A3B&quot;&gt;🤖 ModelScope&lt;/a&gt;&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;30B-A3B&lt;/td&gt; 
   &lt;td align=&quot;center&quot;&gt;128K&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h1&gt;News&lt;/h1&gt; 
&lt;p&gt;[2025/09/20]🚀 Tongyi-DeepResearch-30B-A3B is now on &lt;a href=&quot;https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b&quot;&gt;OpenRouter&lt;/a&gt;! Follow the &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch?tab=readme-ov-file#6-you-can-use-openrouters-api-to-call-our-model&quot;&gt;Quick-start&lt;/a&gt; guide.&lt;/p&gt; 
&lt;p&gt;[2025/09/17]🔥 We have released &lt;strong&gt;Tongyi-DeepResearch-30B-A3B&lt;/strong&gt;.&lt;/p&gt; 
&lt;h1&gt;Deep Research Benchmark Results&lt;/h1&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/benchmark.png&quot; /&gt; &lt;/p&gt; 
&lt;h2&gt;Quick Start&lt;/h2&gt; 
&lt;p&gt;This guide provides instructions for setting up the environment and running inference scripts located in the &lt;a href=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/inference/&quot;&gt;inference&lt;/a&gt; folder.&lt;/p&gt; 
&lt;h3&gt;1. Environment Setup&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Recommended Python version: &lt;strong&gt;3.10.0&lt;/strong&gt; (using other versions may cause dependency issues).&lt;/li&gt; 
 &lt;li&gt;It is strongly advised to create an isolated environment using &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Example with Conda
conda create -n react_infer_env python=3.10.0
conda activate react_infer_env
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;2. Installation&lt;/h3&gt; 
&lt;p&gt;Install the required dependencies:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;3. Environment Configuration and Prepare Evaluation Data&lt;/h3&gt; 
&lt;h4&gt;Environment Configuration&lt;/h4&gt; 
&lt;p&gt;Configure your API keys and settings by copying the example environment file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Copy the example environment file
cp .env.example .env
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Edit the &lt;code&gt;.env&lt;/code&gt; file and provide your actual API keys and configuration values:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;SERPER_KEY_ID&lt;/strong&gt;: Get your key from &lt;a href=&quot;https://serper.dev/&quot;&gt;Serper.dev&lt;/a&gt; for web search and Google Scholar&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JINA_API_KEYS&lt;/strong&gt;: Get your key from &lt;a href=&quot;https://jina.ai/&quot;&gt;Jina.ai&lt;/a&gt; for web page reading&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;API_KEY/API_BASE&lt;/strong&gt;: OpenAI-compatible API for page summarization from &lt;a href=&quot;https://platform.openai.com/&quot;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DASHSCOPE_API_KEY&lt;/strong&gt;: Get your key from &lt;a href=&quot;https://dashscope.aliyun.com/&quot;&gt;Dashscope&lt;/a&gt; for file parsing&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;SANDBOX_FUSION_ENDPOINT&lt;/strong&gt;: Python interpreter sandbox endpoints (see &lt;a href=&quot;https://github.com/bytedance/SandboxFusion&quot;&gt;SandboxFusion&lt;/a&gt;)&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;MODEL_PATH&lt;/strong&gt;: Path to your model weights&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DATASET&lt;/strong&gt;: Name of your evaluation dataset&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;OUTPUT_PATH&lt;/strong&gt;: Directory for saving results&lt;/li&gt; 
&lt;/ul&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The &lt;code&gt;.env&lt;/code&gt; file is gitignored, so your secrets will not be committed to the repository.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h4&gt;Prepare Evaluation Data&lt;/h4&gt; 
&lt;p&gt;The system supports two input file formats: &lt;strong&gt;JSON&lt;/strong&gt; and &lt;strong&gt;JSONL&lt;/strong&gt;.&lt;/p&gt; 
&lt;h4&gt;Supported File Formats:&lt;/h4&gt; 
&lt;p&gt;&lt;strong&gt;Option 1: JSONL Format (recommended)&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.jsonl&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.jsonl&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;Each line must be a valid JSON object with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;{&quot;question&quot;: &quot;What is the capital of France?&quot;, &quot;answer&quot;: &quot;Paris&quot;}
{&quot;question&quot;: &quot;Explain quantum computing&quot;, &quot;answer&quot;: &quot;&quot;}
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Option 2: JSON Format&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Create your data file with &lt;code&gt;.json&lt;/code&gt; extension (e.g., &lt;code&gt;my_questions.json&lt;/code&gt;)&lt;/li&gt; 
 &lt;li&gt;File must contain a JSON array of objects, each with &lt;code&gt;question&lt;/code&gt; and &lt;code&gt;answer&lt;/code&gt; keys: &lt;pre&gt;&lt;code class=&quot;language-json&quot;&gt;[
  {&quot;question&quot;: &quot;What is the capital of France?&quot;, &quot;answer&quot;: &quot;Paris&quot;},
  {&quot;question&quot;: &quot;Explain quantum computing&quot;, &quot;answer&quot;: &quot;&quot;}
]
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Important Note:&lt;/strong&gt; The &lt;code&gt;answer&lt;/code&gt; field contains the &lt;strong&gt;ground truth/reference answer&lt;/strong&gt; used for evaluation. The system generates its own responses to the questions, and these reference answers are used to automatically judge the quality of the generated responses during benchmark evaluation.&lt;/p&gt; 
&lt;h4&gt;File References for Document Processing:&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;If using the &lt;em&gt;file parser&lt;/em&gt; tool, &lt;strong&gt;prepend the filename to the &lt;code&gt;question&lt;/code&gt; field&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;Place referenced files in &lt;code&gt;eval_data/file_corpus/&lt;/code&gt; directory&lt;/li&gt; 
 &lt;li&gt;Example: &lt;code&gt;{&quot;question&quot;: &quot;report.pdf What are the key findings?&quot;, &quot;answer&quot;: &quot;...&quot;}&lt;/code&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;File Organization:&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;project_root/
├── eval_data/
│   ├── my_questions.jsonl          # Your evaluation data
│   └── file_corpus/                # Referenced documents
│       ├── report.pdf
│       └── data.xlsx
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;4. Configure the Inference Script&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;Open &lt;code&gt;run_react_infer.sh&lt;/code&gt; and modify the following variables as instructed in the comments: 
  &lt;ul&gt; 
   &lt;li&gt;&lt;code&gt;MODEL_PATH&lt;/code&gt; - path to the local or remote model weights.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;DATASET&lt;/code&gt; - full path to your evaluation file, e.g. &lt;code&gt;eval_data/my_questions.jsonl&lt;/code&gt; or &lt;code&gt;/path/to/my_questions.json&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;&lt;code&gt;OUTPUT_PATH&lt;/code&gt; - path for saving the prediction results, e.g. &lt;code&gt;./outputs&lt;/code&gt;.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Depending on the tools you enable (retrieval, calculator, web search, etc.), provide the required &lt;code&gt;API_KEY&lt;/code&gt;, &lt;code&gt;BASE_URL&lt;/code&gt;, or other credentials. Each key is explained inline in the bash script.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;5. Run the Inference Script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;bash run_react_infer.sh
&lt;/code&gt;&lt;/pre&gt; 
&lt;hr /&gt; 
&lt;p&gt;With these steps, you can fully prepare the environment, configure the dataset, and run the model. For more details, consult the inline comments in each script or open an issue.&lt;/p&gt; 
&lt;h3&gt;6. You can use OpenRouter&#39;s API to call our model&lt;/h3&gt; 
&lt;p&gt;Tongyi-DeepResearch-30B-A3B is now available at &lt;a href=&quot;https://openrouter.ai/alibaba/tongyi-deepresearch-30b-a3b&quot;&gt;OpenRouter&lt;/a&gt;. You can run the inference without any GPUs.&lt;/p&gt; 
&lt;p&gt;You need to modify the following in the file &lt;a href=&quot;https://github.com/Alibaba-NLP/DeepResearch/raw/main/inference/react_agent.py&quot;&gt;inference/react_agent.py&lt;/a&gt;:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;In the call_server function: Set the API key and URL to your OpenRouter account’s API and URL.&lt;/li&gt; 
 &lt;li&gt;Change the model name to alibaba/tongyi-deepresearch-30b-a3b.&lt;/li&gt; 
 &lt;li&gt;Adjust the content concatenation way as described in the comments on lines &lt;strong&gt;88–90.&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Benchmark Evaluation&lt;/h2&gt; 
&lt;p&gt;We provide benchmark evaluation scripts for various datasets. Please refer to the &lt;a href=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/evaluation/&quot;&gt;evaluation scripts&lt;/a&gt; directory for more details.&lt;/p&gt; 
&lt;h2&gt;Deep Research Agent Family&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;img width=&quot;100%&quot; src=&quot;https://raw.githubusercontent.com/Alibaba-NLP/DeepResearch/main/assets/family.png&quot; /&gt; &lt;/p&gt; 
&lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following paper:&lt;/p&gt; 
&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/pdf/2501.07572&quot;&gt;WebWalker: Benchmarking LLMs in Web Traversal&lt;/a&gt; (ACL 2025)&lt;br /&gt; [2] &lt;a href=&quot;https://arxiv.org/pdf/2505.22648&quot;&gt;WebDancer: Towards Autonomous Information Seeking Agency&lt;/a&gt; (NeurIPS 2025)&lt;br /&gt; [3] &lt;a href=&quot;https://arxiv.org/pdf/2507.02592&quot;&gt;WebSailor: Navigating Super-human Reasoning for Web Agent&lt;/a&gt;&lt;br /&gt; [4] &lt;a href=&quot;https://arxiv.org/pdf/2507.15061&quot;&gt;WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization&lt;/a&gt;&lt;br /&gt; [5] &lt;a href=&quot;https://arxiv.org/pdf/2508.05748&quot;&gt;WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent&lt;/a&gt;&lt;br /&gt; [6] &lt;a href=&quot;https://arxiv.org/pdf/2509.13309&quot;&gt;WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents&lt;/a&gt;&lt;br /&gt; [7] &lt;a href=&quot;https://arxiv.org/pdf/2509.13313&quot;&gt;ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization&lt;/a&gt;&lt;br /&gt; [8] &lt;a href=&quot;https://arxiv.org/pdf/2509.13312&quot;&gt;WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research&lt;/a&gt;&lt;br /&gt; [9] &lt;a href=&quot;https://arxiv.org/pdf/2509.13305&quot;&gt;WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning&lt;/a&gt;&lt;br /&gt; [10] &lt;a href=&quot;https://arxiv.org/pdf/2509.13310&quot;&gt;Scaling Agents via Continual Pre-training&lt;/a&gt;&lt;br /&gt; [11] &lt;a href=&quot;https://arxiv.org/pdf/2509.13311&quot;&gt;Towards General Agentic Intelligence via Environment Scaling&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;🌟 Misc&lt;/h2&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://www.star-history.com/#Alibaba-NLP/DeepResearch&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=Alibaba-NLP/DeepResearch&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;h2&gt;🚩 Talent Recruitment&lt;/h2&gt; 
&lt;p&gt;🔥🔥🔥 We are hiring! Research intern positions are open (based in Hangzhou、Beijing、Shanghai)&lt;/p&gt; 
&lt;p&gt;📚 &lt;strong&gt;Research Area&lt;/strong&gt;：Web Agent, Search Agent, Agent RL, MultiAgent RL, Agentic RAG&lt;/p&gt; 
&lt;p&gt;☎️ &lt;strong&gt;Contact&lt;/strong&gt;：&lt;a href=&quot;&quot;&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Contact Information&lt;/h2&gt; 
&lt;p&gt;For communications, please contact Yong Jiang (&lt;a href=&quot;mailto:yongjiang.jy@alibaba-inc.com&quot;&gt;yongjiang.jy@alibaba-inc.com&lt;/a&gt;).&lt;/p&gt; 
&lt;h2&gt;Citation&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi-DeepResearch},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>Physical-Intelligence/openpi</title>
      <link>https://github.com/Physical-Intelligence/openpi</link>
      <description>&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;openpi&lt;/h1&gt; 
&lt;p&gt;openpi holds open-source models and packages for robotics, published by the &lt;a href=&quot;https://www.physicalintelligence.company/&quot;&gt;Physical Intelligence team&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Currently, this repo contains three types of models:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;the &lt;a href=&quot;https://www.physicalintelligence.company/blog/pi0&quot;&gt;π₀ model&lt;/a&gt;, a flow-based vision-language-action model (VLA).&lt;/li&gt; 
 &lt;li&gt;the &lt;a href=&quot;https://www.physicalintelligence.company/research/fast&quot;&gt;π₀-FAST model&lt;/a&gt;, an autoregressive VLA, based on the FAST action tokenizer.&lt;/li&gt; 
 &lt;li&gt;the &lt;a href=&quot;https://www.physicalintelligence.company/blog/pi05&quot;&gt;π₀.₅ model&lt;/a&gt;, an upgraded version of π₀ with better open-world generalization trained with &lt;a href=&quot;https://www.physicalintelligence.company/research/knowledge_insulation&quot;&gt;knowledge insulation&lt;/a&gt;. Note that, in this repository, we currently only support the flow matching head for both $\pi_{0.5}$ training and inference.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;For all models, we provide &lt;em&gt;base model&lt;/em&gt; checkpoints, pre-trained on 10k+ hours of robot data, and examples for using them out of the box or fine-tuning them to your own datasets.&lt;/p&gt; 
&lt;p&gt;This is an experiment: $\pi_0$ was developed for our own robots, which differ from the widely used platforms such as &lt;a href=&quot;https://tonyzhaozh.github.io/aloha/&quot;&gt;ALOHA&lt;/a&gt; and &lt;a href=&quot;https://droid-dataset.github.io/&quot;&gt;DROID&lt;/a&gt;, and though we are optimistic that researchers and practitioners will be able to run creative new experiments adapting $\pi_0$ to their own platforms, we do not expect every such attempt to be successful. All this is to say: $\pi_0$ may or may not work for you, but you are welcome to try it and see!&lt;/p&gt; 
&lt;h2&gt;Updates&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;[Sept 2025] We released PyTorch support in openpi.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025] We released pi05, an upgraded version of pi0 with better open-world generalization.&lt;/li&gt; 
 &lt;li&gt;[Sept 2025]: We have added an &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md#data-filtering&quot;&gt;improved idle filter&lt;/a&gt; for DROID training.&lt;/li&gt; 
 &lt;li&gt;[Jun 2025]: We have added &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README_train.md&quot;&gt;instructions&lt;/a&gt; for using &lt;code&gt;openpi&lt;/code&gt; to train VLAs on the full &lt;a href=&quot;https://droid-dataset.github.io/&quot;&gt;DROID dataset&lt;/a&gt;. This is an approximate open-source implementation of the training pipeline used to train pi0-FAST-DROID.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Requirements&lt;/h2&gt; 
&lt;p&gt;To run the models in this repository, you will need an NVIDIA GPU with at least the following specifications. These estimations assume a single GPU, but you can also use multiple GPUs with model parallelism to reduce per-GPU memory requirements by configuring &lt;code&gt;fsdp_devices&lt;/code&gt; in the training config. Please also note that the current training script does not yet support multi-node training.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Mode&lt;/th&gt; 
   &lt;th&gt;Memory Required&lt;/th&gt; 
   &lt;th&gt;Example GPU&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 8 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (LoRA)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 22.5 GB&lt;/td&gt; 
   &lt;td&gt;RTX 4090&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Fine-Tuning (Full)&lt;/td&gt; 
   &lt;td&gt;&amp;gt; 70 GB&lt;/td&gt; 
   &lt;td&gt;A100 (80GB) / H100&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;The repo has been tested with Ubuntu 22.04, we do not currently support other operating systems.&lt;/p&gt; 
&lt;h2&gt;Installation&lt;/h2&gt; 
&lt;p&gt;When cloning this repo, make sure to update submodules:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone --recurse-submodules git@github.com:Physical-Intelligence/openpi.git

# Or if you already cloned the repo:
git submodule update --init --recursive
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We use &lt;a href=&quot;https://docs.astral.sh/uv/&quot;&gt;uv&lt;/a&gt; to manage Python dependencies. See the &lt;a href=&quot;https://docs.astral.sh/uv/getting-started/installation/&quot;&gt;uv installation instructions&lt;/a&gt; to set it up. Once uv is installed, run the following to set up the environment:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;GIT_LFS_SKIP_SMUDGE=1 uv sync
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;NOTE: &lt;code&gt;GIT_LFS_SKIP_SMUDGE=1&lt;/code&gt; is needed to pull LeRobot as a dependency.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;: As an alternative to uv installation, we provide instructions for installing openpi using Docker. If you encounter issues with your system setup, consider using Docker to simplify installation. See &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/docker.md&quot;&gt;Docker Setup&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Model Checkpoints&lt;/h2&gt; 
&lt;h3&gt;Base Models&lt;/h3&gt; 
&lt;p&gt;We provide multiple base VLA model checkpoints. These checkpoints have been pre-trained on 10k+ hours of robot data, and can be used for fine-tuning.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href=&quot;https://www.physicalintelligence.company/blog/pi0&quot;&gt;π₀ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base autoregressive &lt;a href=&quot;https://www.physicalintelligence.company/research/fast&quot;&gt;π₀-FAST model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;Base &lt;a href=&quot;https://www.physicalintelligence.company/blog/pi05&quot;&gt;π₀.₅ model&lt;/a&gt; for fine-tuning&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_base&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;h3&gt;Fine-Tuned Models&lt;/h3&gt; 
&lt;p&gt;We also provide &quot;expert&quot; checkpoints for various robot platforms and tasks. These models are fine-tuned from the base models above and intended to run directly on the target robot. These may or may not work on your particular robot. Since these checkpoints were fine-tuned on relatively small datasets collected with more widely available robots, such as ALOHA and the DROID Franka setup, they might not generalize to your particular setup, though we found some of these, especially the DROID checkpoint, to generalize quite broadly in practice.&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Model&lt;/th&gt; 
   &lt;th&gt;Use Case&lt;/th&gt; 
   &lt;th&gt;Description&lt;/th&gt; 
   &lt;th&gt;Checkpoint Path&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-FAST-DROID&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$-FAST model fine-tuned on the &lt;a href=&quot;https://droid-dataset.github.io/&quot;&gt;DROID dataset&lt;/a&gt;: can perform a wide range of simple table-top manipulation tasks 0-shot in new scenes on the DROID robot platform&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_fast_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-DROID&lt;/td&gt; 
   &lt;td&gt;Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on the &lt;a href=&quot;https://droid-dataset.github.io/&quot;&gt;DROID dataset&lt;/a&gt;: faster inference than $\pi_0$-FAST-DROID, but may not follow language commands as well&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-towel&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href=&quot;https://tonyzhaozh.github.io/aloha/&quot;&gt;ALOHA&lt;/a&gt; data: can fold diverse towels 0-shot on ALOHA robot platforms&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_towel&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-tupperware&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on internal &lt;a href=&quot;https://tonyzhaozh.github.io/aloha/&quot;&gt;ALOHA&lt;/a&gt; data: can unpack food from a tupperware container&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_tupperware&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_0$-ALOHA-pen-uncap&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_0$ model fine-tuned on public &lt;a href=&quot;https://dit-policy.github.io/&quot;&gt;ALOHA&lt;/a&gt; data: can uncap a pen&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi0_aloha_pen_uncap&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-LIBERO&lt;/td&gt; 
   &lt;td&gt;Inference&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned for the &lt;a href=&quot;https://libero-project.github.io/datasets&quot;&gt;LIBERO&lt;/a&gt; benchmark: gets state-of-the-art performance (see &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md&quot;&gt;LIBERO README&lt;/a&gt;)&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_libero&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;$\pi_{0.5}$-DROID&lt;/td&gt; 
   &lt;td&gt;Inference / Fine-Tuning&lt;/td&gt; 
   &lt;td&gt;$\pi_{0.5}$ model fine-tuned on the &lt;a href=&quot;https://droid-dataset.github.io/&quot;&gt;DROID dataset&lt;/a&gt; with &lt;a href=&quot;https://www.physicalintelligence.company/research/knowledge_insulation&quot;&gt;knowledge insulation&lt;/a&gt;: fast inference and good language-following&lt;/td&gt; 
   &lt;td&gt;&lt;code&gt;gs://openpi-assets/checkpoints/pi05_droid&lt;/code&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;By default, checkpoints are automatically downloaded from &lt;code&gt;gs://openpi-assets&lt;/code&gt; and are cached in &lt;code&gt;~/.cache/openpi&lt;/code&gt; when needed. You can overwrite the download path by setting the &lt;code&gt;OPENPI_DATA_HOME&lt;/code&gt; environment variable.&lt;/p&gt; 
&lt;h2&gt;Running Inference for a Pre-Trained Model&lt;/h2&gt; 
&lt;p&gt;Our pre-trained model checkpoints can be run with a few lines of code (here our $\pi_0$-FAST-DROID model):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config(&quot;pi05_droid&quot;)
checkpoint_dir = download.maybe_download(&quot;gs://openpi-assets/checkpoints/pi05_droid&quot;)

# Create a trained policy.
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference on a dummy example.
example = {
    &quot;observation/exterior_image_1_left&quot;: ...,
    &quot;observation/wrist_image_left&quot;: ...,
    ...
    &quot;prompt&quot;: &quot;pick up the fork&quot;
}
action_chunk = policy.infer(example)[&quot;actions&quot;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can also test this out in the &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/inference.ipynb&quot;&gt;example notebook&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We provide detailed step-by-step examples for running inference of our pre-trained checkpoints on &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/droid/README.md&quot;&gt;DROID&lt;/a&gt; and &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real/README.md&quot;&gt;ALOHA&lt;/a&gt; robots.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Remote Inference&lt;/strong&gt;: We provide &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md&quot;&gt;examples and code&lt;/a&gt; for running inference of our models &lt;strong&gt;remotely&lt;/strong&gt;: the model can run on a different server and stream actions to the robot via a websocket connection. This makes it easy to use more powerful GPUs off-robot and keep robot and policy environments separate.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Test inference without a robot&lt;/strong&gt;: We provide a &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md&quot;&gt;script&lt;/a&gt; for testing inference without a robot. This script will generate a random observation and run inference with the model. See &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/simple_client/README.md&quot;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h2&gt;Fine-Tuning Base Models on Your Own Data&lt;/h2&gt; 
&lt;p&gt;We will fine-tune the $\pi_{0.5}$ model on the &lt;a href=&quot;https://libero-project.github.io/datasets&quot;&gt;LIBERO dataset&lt;/a&gt; as a running example for how to fine-tune a base model on your own data. We will explain three steps:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Convert your data to a LeRobot dataset (which we use for training)&lt;/li&gt; 
 &lt;li&gt;Defining training configs and running training&lt;/li&gt; 
 &lt;li&gt;Spinning up a policy server and running inference&lt;/li&gt; 
&lt;/ol&gt; 
&lt;h3&gt;1. Convert your data to a LeRobot dataset&lt;/h3&gt; 
&lt;p&gt;We provide a minimal example script for converting LIBERO data to a LeRobot dataset in &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/convert_libero_data_to_lerobot.py&quot;&gt;&lt;code&gt;examples/libero/convert_libero_data_to_lerobot.py&lt;/code&gt;&lt;/a&gt;. You can easily modify it to convert your own data! You can download the raw LIBERO dataset from &lt;a href=&quot;https://huggingface.co/datasets/openvla/modified_libero_rlds&quot;&gt;here&lt;/a&gt;, and run the script with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run examples/libero/convert_libero_data_to_lerobot.py --data_dir /path/to/your/libero/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you just want to fine-tune on LIBERO, you can skip this step, because our LIBERO fine-tuning configs point to a pre-converted LIBERO dataset. This step is merely an example that you can adapt to your own data.&lt;/p&gt; 
&lt;h3&gt;2. Defining training configs and running training&lt;/h3&gt; 
&lt;p&gt;To fine-tune a base model on your own data, you need to define configs for data processing and training. We provide example configs with detailed comments for LIBERO below, which you can modify for your own dataset:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/policies/libero_policy.py&quot;&gt;&lt;code&gt;LiberoInputs&lt;/code&gt; and &lt;code&gt;LiberoOutputs&lt;/code&gt;&lt;/a&gt;: Defines the data mapping from the LIBERO environment to the model and vice versa. Will be used for both, training and inference.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py&quot;&gt;&lt;code&gt;LeRobotLiberoDataConfig&lt;/code&gt;&lt;/a&gt;: Defines how to process raw LIBERO data from LeRobot dataset for training.&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py&quot;&gt;&lt;code&gt;TrainConfig&lt;/code&gt;&lt;/a&gt;: Defines fine-tuning hyperparameters, data config, and weight loader.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;We provide example fine-tuning configs for &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py&quot;&gt;π₀&lt;/a&gt;, &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py&quot;&gt;π₀-FAST&lt;/a&gt;, and &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/src/openpi/training/config.py&quot;&gt;π₀.₅&lt;/a&gt; on LIBERO data.&lt;/p&gt; 
&lt;p&gt;Before we can run training, we need to compute the normalization statistics for the training data. Run the script below with the name of your training config:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run scripts/compute_norm_stats.py --config-name pi05_libero
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Now we can kick off training with the following command (the &lt;code&gt;--overwrite&lt;/code&gt; flag is used to overwrite existing checkpoints if you rerun fine-tuning with the same config):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 uv run scripts/train.py pi05_libero --exp-name=my_experiment --overwrite
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The command will log training progress to the console and save checkpoints to the &lt;code&gt;checkpoints&lt;/code&gt; directory. You can also monitor training progress on the Weights &amp;amp; Biases dashboard. For maximally using the GPU memory, set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; before running training -- this enables JAX to use up to 90% of the GPU memory (vs. the default of 75%).&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We provide functionality for &lt;em&gt;reloading&lt;/em&gt; normalization statistics for state / action normalization from pre-training. This can be beneficial if you are fine-tuning to a new task on a robot that was part of our pre-training mixture. For more details on how to reload normalization statistics, see the &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/norm_stats.md&quot;&gt;norm_stats.md&lt;/a&gt; file.&lt;/p&gt; 
&lt;h3&gt;3. Spinning up a policy server and running inference&lt;/h3&gt; 
&lt;p&gt;Once training is complete, we can run inference by spinning up a policy server and then querying it from a LIBERO evaluation script. Launching a model server is easy (we use the checkpoint for iteration 20,000 for this example, modify as needed):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run scripts/serve_policy.py policy:checkpoint --policy.config=pi05_libero --policy.dir=checkpoints/pi05_libero/my_experiment/20000
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This will spin up a server that listens on port 8000 and waits for observations to be sent to it. We can then run an evaluation script (or robot runtime) that queries the server.&lt;/p&gt; 
&lt;p&gt;For running the LIBERO eval in particular, we provide (and recommend using) a Dockerized workflow that handles both the policy server and the evaluation script together. See the &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/libero/README.md&quot;&gt;LIBERO README&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;p&gt;If you want to embed a policy server call in your own robot runtime, we have a minimal example of how to do so in the &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/docs/remote_inference.md&quot;&gt;remote inference docs&lt;/a&gt;.&lt;/p&gt; 
&lt;h3&gt;More Examples&lt;/h3&gt; 
&lt;p&gt;We provide more examples for how to fine-tune and run inference with our models on the ALOHA platform in the following READMEs:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_sim&quot;&gt;ALOHA Simulator&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/aloha_real&quot;&gt;ALOHA Real&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/examples/ur5&quot;&gt;UR5&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;PyTorch Support&lt;/h2&gt; 
&lt;p&gt;openpi now provides PyTorch implementations of π₀ and π₀.₅ models alongside the original JAX versions! The PyTorch implementation has been validated on the LIBERO benchmark (both inference and finetuning). A few features are currently not supported (this may change in the future):&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;The π₀-FAST model&lt;/li&gt; 
 &lt;li&gt;Mixed precision training&lt;/li&gt; 
 &lt;li&gt;FSDP (fully-sharded data parallelism) training&lt;/li&gt; 
 &lt;li&gt;LoRA (low-rank adaptation) training&lt;/li&gt; 
 &lt;li&gt;EMA (exponential moving average) weights during training&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Setup&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Make sure that you have the latest version of all dependencies installed: &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Double check that you have transformers 4.53.2 installed: &lt;code&gt;uv pip show transformers&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Apply the transformers library patches:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;cp -r ./src/openpi/models_pytorch/transformers_replace/* .venv/lib/python3.11/site-packages/transformers/
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;This overwrites several files in the transformers library with necessary model changes: 1) supporting AdaRMS, 2) correctly controlling the precision of activations, and 3) allowing the KV cache to be used without being updated.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: With the default uv link mode (hardlink), this will permanently affect the transformers library in your uv cache, meaning the changes will survive reinstallations of transformers and could even propagate to other projects that use transformers. To fully undo this operation, you must run &lt;code&gt;uv cache clean transformers&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Converting JAX Models to PyTorch&lt;/h3&gt; 
&lt;p&gt;To convert a JAX model checkpoint to PyTorch format:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --checkpoint_dir /path/to/jax/checkpoint \
    --config_name &amp;lt;config name&amp;gt; \
    --output_path /path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Running Inference with PyTorch&lt;/h3&gt; 
&lt;p&gt;The PyTorch implementation uses the same API as the JAX version - you only need to change the checkpoint path to point to the converted PyTorch model:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;from openpi.training import config as _config
from openpi.policies import policy_config
from openpi.shared import download

config = _config.get_config(&quot;pi05_droid&quot;)
checkpoint_dir = &quot;/path/to/converted/pytorch/checkpoint&quot;

# Create a trained policy (automatically detects PyTorch format)
policy = policy_config.create_trained_policy(config, checkpoint_dir)

# Run inference (same API as JAX)
action_chunk = policy.infer(example)[&quot;actions&quot;]
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Policy Server with PyTorch&lt;/h3&gt; 
&lt;p&gt;The policy server works identically with PyTorch models - just point to the converted checkpoint directory:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run scripts/serve_policy.py policy:checkpoint \
    --policy.config=pi05_droid \
    --policy.dir=/path/to/converted/pytorch/checkpoint
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Finetuning with PyTorch&lt;/h3&gt; 
&lt;p&gt;To finetune a model in PyTorch:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Convert the JAX base model to PyTorch format:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;uv run examples/convert_jax_model_to_pytorch.py \
    --config_name &amp;lt;config name&amp;gt; \
    --checkpoint_dir /path/to/jax/base/model \
    --output_path /path/to/pytorch/base/model
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Specify the converted PyTorch model path in your config using &lt;code&gt;pytorch_weight_path&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Launch training using one of these modes:&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Single GPU training:
uv run scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;

# Example:
uv run scripts/train_pytorch.py debug --exp_name pytorch_test
uv run scripts/train_pytorch.py debug --exp_name pytorch_test --resume  # Resume from latest checkpoint

# Multi-GPU training (single node):
uv run torchrun --standalone --nnodes=1 --nproc_per_node=&amp;lt;num_gpus&amp;gt; scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name &amp;lt;run_name&amp;gt;

# Example:
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test
uv run torchrun --standalone --nnodes=1 --nproc_per_node=2 scripts/train_pytorch.py pi0_aloha_sim --exp_name pytorch_ddp_test --resume

# Multi-Node Training:
uv run torchrun \
    --nnodes=&amp;lt;num_nodes&amp;gt; \
    --nproc_per_node=&amp;lt;gpus_per_node&amp;gt; \
    --node_rank=&amp;lt;rank_of_node&amp;gt; \
    --master_addr=&amp;lt;master_ip&amp;gt; \
    --master_port=&amp;lt;port&amp;gt; \
    scripts/train_pytorch.py &amp;lt;config_name&amp;gt; --exp_name=&amp;lt;run_name&amp;gt; --save_interval &amp;lt;interval&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Precision Settings&lt;/h3&gt; 
&lt;p&gt;JAX and PyTorch implementations handle precision as follows:&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;JAX:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: most weights and computations in bfloat16, with a few computations in float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: defaults to mixed precision: weights and gradients in float32, (most) activations and computations in bfloat16. You can change to full float32 training by setting &lt;code&gt;dtype&lt;/code&gt; to float32 in the config.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;PyTorch:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt;Inference: matches JAX -- most weights and computations in bfloat16, with a few weights converted to float32 for stability&lt;/li&gt; 
 &lt;li&gt;Training: supports either full bfloat16 (default) or full float32. You can change it by setting &lt;code&gt;pytorch_training_precision&lt;/code&gt; in the config. bfloat16 uses less memory but exhibits higher losses compared to float32. Mixed precision is not yet supported.&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;With torch.compile, inference speed is comparable between JAX and PyTorch.&lt;/p&gt; 
&lt;h2&gt;Troubleshooting&lt;/h2&gt; 
&lt;p&gt;We will collect common issues and their solutions here. If you encounter an issue, please check here first. If you can&#39;t find a solution, please file an issue on the repo (see &lt;a href=&quot;https://raw.githubusercontent.com/Physical-Intelligence/openpi/main/CONTRIBUTING.md&quot;&gt;here&lt;/a&gt; for guidelines).&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Issue&lt;/th&gt; 
   &lt;th&gt;Resolution&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;uv sync&lt;/code&gt; fails with dependency conflicts&lt;/td&gt; 
   &lt;td&gt;Try removing the virtual environment directory (&lt;code&gt;rm -rf .venv&lt;/code&gt;) and running &lt;code&gt;uv sync&lt;/code&gt; again. If issues persist, check that you have the latest version of &lt;code&gt;uv&lt;/code&gt; installed (&lt;code&gt;uv self update&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Training runs out of GPU memory&lt;/td&gt; 
   &lt;td&gt;Make sure you set &lt;code&gt;XLA_PYTHON_CLIENT_MEM_FRACTION=0.9&lt;/code&gt; (or higher) before running training to allow JAX to use more GPU memory. You can also use &lt;code&gt;--fsdp-devices &amp;lt;n&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;n&amp;gt;&lt;/code&gt; is your number of GPUs, to enable &lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;fully-sharded data parallelism&lt;/a&gt;, which reduces memory usage in exchange for slower training (the amount of slowdown depends on your particular setup). If you are still running out of memory, you may way to consider disabling EMA.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Policy server connection errors&lt;/td&gt; 
   &lt;td&gt;Check that the server is running and listening on the expected port. Verify network connectivity and firewall settings between client and server.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Missing norm stats error when training&lt;/td&gt; 
   &lt;td&gt;Run &lt;code&gt;scripts/compute_norm_stats.py&lt;/code&gt; with your config name before starting training.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Dataset download fails&lt;/td&gt; 
   &lt;td&gt;Check your internet connection. For HuggingFace datasets, ensure you&#39;re logged in (&lt;code&gt;huggingface-cli login&lt;/code&gt;).&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;CUDA/GPU errors&lt;/td&gt; 
   &lt;td&gt;Verify NVIDIA drivers are installed correctly. For Docker, ensure nvidia-container-toolkit is installed. Check GPU compatibility. You do NOT need CUDA libraries installed at a system level --- they will be installed via uv. You may even want to try &lt;em&gt;uninstalling&lt;/em&gt; system CUDA libraries if you run into CUDA issues, since system libraries can sometimes cause conflicts.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Import errors when running examples&lt;/td&gt; 
   &lt;td&gt;Make sure you&#39;ve installed all dependencies with &lt;code&gt;uv sync&lt;/code&gt;. Some examples may have additional requirements listed in their READMEs.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Action dimensions mismatch&lt;/td&gt; 
   &lt;td&gt;Verify your data processing transforms match the expected input/output dimensions of your robot. Check the action space definitions in your policy classes.&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Diverging training loss&lt;/td&gt; 
   &lt;td&gt;Check the &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, and &lt;code&gt;std&lt;/code&gt; values in &lt;code&gt;norm_stats.json&lt;/code&gt; for your dataset. Certain dimensions that are rarely used can end up with very small &lt;code&gt;q01&lt;/code&gt;, &lt;code&gt;q99&lt;/code&gt;, or &lt;code&gt;std&lt;/code&gt; values, leading to huge states and actions after normalization. You can manually adjust the norm stats as a workaround.&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt;</description>
    </item>
    
    <item>
      <title>resemble-ai/chatterbox</title>
      <link>https://github.com/resemble-ai/chatterbox</link>
      <description>&lt;p&gt;SoTA open-source TTS&lt;/p&gt;&lt;hr&gt;&lt;img width=&quot;1200&quot; height=&quot;600&quot; alt=&quot;Chatterbox-Multilingual&quot; src=&quot;https://www.resemble.ai/wp-content/uploads/2025/09/Chatterbox-Multilingual-1.png&quot; /&gt; 
&lt;h1&gt;Chatterbox TTS&lt;/h1&gt; 
&lt;p&gt;&lt;a href=&quot;https://resemble-ai.github.io/chatterbox_demopage/&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/listen-demo_samples-blue&quot; alt=&quot;Alt Text&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://huggingface.co/spaces/ResembleAI/Chatterbox&quot;&gt;&lt;img src=&quot;https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm.svg?sanitize=true&quot; alt=&quot;Alt Text&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://podonos.com/resembleai/chatterbox&quot;&gt;&lt;img src=&quot;https://static-public.podonos.com/badges/insight-on-pdns-sm-dark.svg?sanitize=true&quot; alt=&quot;Alt Text&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://discord.gg/rJq9cRJBJ6&quot;&gt;&lt;img src=&quot;https://img.shields.io/discord/1377773249798344776?label=join%20discord&amp;amp;logo=discord&amp;amp;style=flat&quot; alt=&quot;Discord&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;_Made with ♥️ by &lt;a href=&quot;https://resemble.ai&quot; target=&quot;_blank&quot;&gt;&lt;img width=&quot;100&quot; alt=&quot;resemble-logo-horizontal&quot; src=&quot;https://github.com/user-attachments/assets/35cf756b-3506-4943-9c72-c05ddfa4e525&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;We&#39;re excited to introduce &lt;strong&gt;Chatterbox Multilingual&lt;/strong&gt;, &lt;a href=&quot;https://resemble.ai&quot;&gt;Resemble AI&#39;s&lt;/a&gt; first production-grade open source TTS model supporting &lt;strong&gt;23 languages&lt;/strong&gt; out of the box. Licensed under MIT, Chatterbox has been benchmarked against leading closed-source systems like ElevenLabs, and is consistently preferred in side-by-side evaluations.&lt;/p&gt; 
&lt;p&gt;Whether you&#39;re working on memes, videos, games, or AI agents, Chatterbox brings your content to life across languages. It&#39;s also the first open source TTS model to support &lt;strong&gt;emotion exaggeration control&lt;/strong&gt; with robust &lt;strong&gt;multilingual zero-shot voice cloning&lt;/strong&gt;. Try the english only version now on our &lt;a href=&quot;https://huggingface.co/spaces/ResembleAI/Chatterbox&quot;&gt;English Hugging Face Gradio app.&lt;/a&gt;. Or try the multilingual version on our &lt;a href=&quot;https://huggingface.co/spaces/ResembleAI/Chatterbox-Multilingual-TTS&quot;&gt;Multilingual Hugging Face Gradio app.&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;If you like the model but need to scale or tune it for higher accuracy, check out our competitively priced TTS service (&lt;a href=&quot;https://resemble.ai&quot;&gt;link&lt;/a&gt;). It delivers reliable performance with ultra-low latency of sub 200ms—ideal for production use in agents, applications, or interactive media.&lt;/p&gt; 
&lt;h1&gt;Key Details&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;Multilingual, zero-shot TTS supporting 23 languages&lt;/li&gt; 
 &lt;li&gt;SoTA zeroshot English TTS&lt;/li&gt; 
 &lt;li&gt;0.5B Llama backbone&lt;/li&gt; 
 &lt;li&gt;Unique exaggeration/intensity control&lt;/li&gt; 
 &lt;li&gt;Ultra-stable with alignment-informed inference&lt;/li&gt; 
 &lt;li&gt;Trained on 0.5M hours of cleaned data&lt;/li&gt; 
 &lt;li&gt;Watermarked outputs&lt;/li&gt; 
 &lt;li&gt;Easy voice conversion script&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://podonos.com/resembleai/chatterbox&quot;&gt;Outperforms ElevenLabs&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Supported Languages&lt;/h1&gt; 
&lt;p&gt;Arabic (ar) • Danish (da) • German (de) • Greek (el) • English (en) • Spanish (es) • Finnish (fi) • French (fr) • Hebrew (he) • Hindi (hi) • Italian (it) • Japanese (ja) • Korean (ko) • Malay (ms) • Dutch (nl) • Norwegian (no) • Polish (pl) • Portuguese (pt) • Russian (ru) • Swedish (sv) • Swahili (sw) • Turkish (tr) • Chinese (zh)&lt;/p&gt; 
&lt;h1&gt;Tips&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;General Use (TTS and Voice Agents):&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Ensure that the reference clip matches the specified language tag. Otherwise, language transfer outputs may inherit the accent of the reference clip’s language. To mitigate this, set &lt;code&gt;cfg_weight&lt;/code&gt; to &lt;code&gt;0&lt;/code&gt;.&lt;/li&gt; 
   &lt;li&gt;The default settings (&lt;code&gt;exaggeration=0.5&lt;/code&gt;, &lt;code&gt;cfg_weight=0.5&lt;/code&gt;) work well for most prompts across all languages.&lt;/li&gt; 
   &lt;li&gt;If the reference speaker has a fast speaking style, lowering &lt;code&gt;cfg_weight&lt;/code&gt; to around &lt;code&gt;0.3&lt;/code&gt; can improve pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;Expressive or Dramatic Speech:&lt;/strong&gt;&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Try lower &lt;code&gt;cfg_weight&lt;/code&gt; values (e.g. &lt;code&gt;~0.3&lt;/code&gt;) and increase &lt;code&gt;exaggeration&lt;/code&gt; to around &lt;code&gt;0.7&lt;/code&gt; or higher.&lt;/li&gt; 
   &lt;li&gt;Higher &lt;code&gt;exaggeration&lt;/code&gt; tends to speed up speech; reducing &lt;code&gt;cfg_weight&lt;/code&gt; helps compensate with slower, more deliberate pacing.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Installation&lt;/h1&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;pip install chatterbox-tts
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can install from source:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;# conda create -yn chatterbox python=3.11
# conda activate chatterbox

git clone https://github.com/resemble-ai/chatterbox.git
cd chatterbox
pip install -e .
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;We developed and tested Chatterbox on Python 3.11 on Debian 11 OS; the versions of the dependencies are pinned in &lt;code&gt;pyproject.toml&lt;/code&gt; to ensure consistency. You can modify the code or dependencies in this installation mode.&lt;/p&gt; 
&lt;h1&gt;Usage&lt;/h1&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import torchaudio as ta
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS

# English example
model = ChatterboxTTS.from_pretrained(device=&quot;cuda&quot;)

text = &quot;Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy&#39;s Nexus in an epic late-game pentakill.&quot;
wav = model.generate(text)
ta.save(&quot;test-english.wav&quot;, wav, model.sr)

# Multilingual examples
multilingual_model = ChatterboxMultilingualTTS.from_pretrained(device=device)

french_text = &quot;Bonjour, comment ça va? Ceci est le modèle de synthèse vocale multilingue Chatterbox, il prend en charge 23 langues.&quot;
wav_french = multilingual_model.generate(spanish_text, language_id=&quot;fr&quot;)
ta.save(&quot;test-french.wav&quot;, wav_french, model.sr)

chinese_text = &quot;你好，今天天气真不错，希望你有一个愉快的周末。&quot;
wav_chinese = multilingual_model.generate(chinese_text, language_id=&quot;zh&quot;)
ta.save(&quot;test-chinese.wav&quot;, wav_chinese, model.sr)

# If you want to synthesize with a different voice, specify the audio prompt
AUDIO_PROMPT_PATH = &quot;YOUR_FILE.wav&quot;
wav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)
ta.save(&quot;test-2.wav&quot;, wav, model.sr)
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;code&gt;example_tts.py&lt;/code&gt; and &lt;code&gt;example_vc.py&lt;/code&gt; for more examples.&lt;/p&gt; 
&lt;h1&gt;Acknowledgements&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/FunAudioLLM/CosyVoice&quot;&gt;Cosyvoice&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/CorentinJ/Real-Time-Voice-Cloning&quot;&gt;Real-Time-Voice-Cloning&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/yl4579/HiFTNet&quot;&gt;HiFT-GAN&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/meta-llama/llama3&quot;&gt;Llama 3&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/xingchensong/S3Tokenizer&quot;&gt;S3Tokenizer&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;Built-in PerTh Watermarking for Responsible AI&lt;/h1&gt; 
&lt;p&gt;Every audio file generated by Chatterbox includes &lt;a href=&quot;https://github.com/resemble-ai/perth&quot;&gt;Resemble AI&#39;s Perth (Perceptual Threshold) Watermarker&lt;/a&gt; - imperceptible neural watermarks that survive MP3 compression, audio editing, and common manipulations while maintaining nearly 100% detection accuracy.&lt;/p&gt; 
&lt;h2&gt;Watermark extraction&lt;/h2&gt; 
&lt;p&gt;You can look for the watermark using the following script.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;import perth
import librosa

AUDIO_PATH = &quot;YOUR_FILE.wav&quot;

# Load the watermarked audio
watermarked_audio, sr = librosa.load(AUDIO_PATH, sr=None)

# Initialize watermarker (same as used for embedding)
watermarker = perth.PerthImplicitWatermarker()

# Extract watermark
watermark = watermarker.get_watermark(watermarked_audio, sample_rate=sr)
print(f&quot;Extracted watermark: {watermark}&quot;)
# Output: 0.0 (no watermark) or 1.0 (watermarked)
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Official Discord&lt;/h1&gt; 
&lt;p&gt;👋 Join us on &lt;a href=&quot;https://discord.gg/rJq9cRJBJ6&quot;&gt;Discord&lt;/a&gt; and let&#39;s build something awesome together!&lt;/p&gt; 
&lt;h1&gt;Citation&lt;/h1&gt; 
&lt;p&gt;If you find this model useful, please consider citing.&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;@misc{chatterboxtts2025,
  author       = {{Resemble AI}},
  title        = {{Chatterbox-TTS}},
  year         = {2025},
  howpublished = {\url{https://github.com/resemble-ai/chatterbox}},
  note         = {GitHub repository}
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;Disclaimer&lt;/h1&gt; 
&lt;p&gt;Don&#39;t use this model to do bad things. Prompts are sourced from freely available data on the internet.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dockur/windows</title>
      <link>https://github.com/dockur/windows</link>
      <description>&lt;p&gt;Windows inside a Docker container.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&quot;center&quot;&gt;Windows&lt;br /&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;a href=&quot;https://github.com/dockur/windows&quot;&gt;&lt;img src=&quot;https://github.com/dockur/windows/raw/master/.github/logo.png&quot; title=&quot;Logo&quot; style=&quot;max-width:100%;&quot; width=&quot;128&quot; /&gt;&lt;/a&gt; 
 &lt;/div&gt; 
 &lt;div align=&quot;center&quot;&gt; 
  &lt;p&gt;&lt;a href=&quot;https://github.com/dockur/windows/&quot;&gt;&lt;img src=&quot;https://github.com/dockur/windows/actions/workflows/build.yml/badge.svg?sanitize=true&quot; alt=&quot;Build&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/dockurr/windows/tags&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/v/dockurr/windows/latest?arch=amd64&amp;amp;sort=semver&amp;amp;color=066da5&quot; alt=&quot;Version&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/dockurr/windows/tags&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/image-size/dockurr/windows/latest?color=066da5&amp;amp;label=size&quot; alt=&quot;Size&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/dockur/windows/pkgs/container/windows&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdockur%2Fwindows%2Fwindows.json&amp;amp;query=%24.downloads&amp;amp;logo=github&amp;amp;style=flat&amp;amp;color=066da5&amp;amp;label=pulls&quot; alt=&quot;Package&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/dockurr/windows/&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/dockurr/windows.svg?style=flat&amp;amp;label=pulls&amp;amp;logo=docker&quot; alt=&quot;Pulls&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
 &lt;/div&gt;&lt;/h1&gt; 
&lt;p&gt;Windows inside a Docker container.&lt;/p&gt; 
&lt;h2&gt;Features ✨&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;ISO downloader&lt;/li&gt; 
 &lt;li&gt;KVM acceleration&lt;/li&gt; 
 &lt;li&gt;Web-based viewer&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Video 📺&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xhGYobuG508&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/xhGYobuG508/0.jpg&quot; alt=&quot;Youtube&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;Usage 🐳&lt;/h2&gt; 
&lt;h5&gt;Via Docker Compose:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;services:
  windows:
    image: dockurr/windows
    container_name: windows
    environment:
      VERSION: &quot;11&quot;
    devices:
      - /dev/kvm
      - /dev/net/tun
    cap_add:
      - NET_ADMIN
    ports:
      - 8006:8006
      - 3389:3389/tcp
      - 3389:3389/udp
    volumes:
      - ./windows:/storage
    restart: always
    stop_grace_period: 2m
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Via Docker CLI:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run -it --rm --name windows -p 8006:8006 --device=/dev/kvm --device=/dev/net/tun --cap-add NET_ADMIN -v &quot;${PWD:-.}/windows:/storage&quot; --stop-timeout 120 dockurr/windows
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Via Kubernetes:&lt;/h5&gt; 
&lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;kubectl apply -f https://raw.githubusercontent.com/dockur/windows/refs/heads/master/kubernetes.yml
&lt;/code&gt;&lt;/pre&gt; 
&lt;h5&gt;Via Github Codespaces:&lt;/h5&gt; 
&lt;p&gt;&lt;a href=&quot;https://codespaces.new/dockur/windows&quot;&gt;&lt;img src=&quot;https://github.com/codespaces/badge.svg?sanitize=true&quot; alt=&quot;Open in GitHub Codespaces&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;FAQ 💬&lt;/h2&gt; 
&lt;h3&gt;How do I use it?&lt;/h3&gt; 
&lt;p&gt;Very simple! These are the steps:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;Start the container and connect to &lt;a href=&quot;http://127.0.0.1:8006/&quot;&gt;port 8006&lt;/a&gt; using your web browser.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Sit back and relax while the magic happens, the whole installation will be performed fully automatic.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once you see the desktop, your Windows installation is ready for use.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Enjoy your brand new machine, and don&#39;t forget to star this repo!&lt;/p&gt; 
&lt;h3&gt;How do I select the Windows version?&lt;/h3&gt; 
&lt;p&gt;By default, Windows 11 Pro will be installed. But you can add the &lt;code&gt;VERSION&lt;/code&gt; environment variable to your compose file, in order to specify an alternative Windows version to be downloaded:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  VERSION: &quot;11&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Select from the values below:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Size&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;11&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 11 Pro&lt;/td&gt; 
   &lt;td&gt;5.4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;11l&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 11 LTSC&lt;/td&gt; 
   &lt;td&gt;4.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;11e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 11 Enterprise&lt;/td&gt; 
   &lt;td&gt;5.3 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;10&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 10 Pro&lt;/td&gt; 
   &lt;td&gt;5.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;10l&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 10 LTSC&lt;/td&gt; 
   &lt;td&gt;4.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;10e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 10 Enterprise&lt;/td&gt; 
   &lt;td&gt;5.2 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;8e&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 8.1 Enterprise&lt;/td&gt; 
   &lt;td&gt;3.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;7u&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 7 Ultimate&lt;/td&gt; 
   &lt;td&gt;3.1 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;vu&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Vista Ultimate&lt;/td&gt; 
   &lt;td&gt;3.0 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;xp&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows XP Professional&lt;/td&gt; 
   &lt;td&gt;0.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2k&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows 2000 Professional&lt;/td&gt; 
   &lt;td&gt;0.4 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
   &lt;td&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2025&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2025&lt;/td&gt; 
   &lt;td&gt;6.7 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2022&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2022&lt;/td&gt; 
   &lt;td&gt;6.0 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2019&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2019&lt;/td&gt; 
   &lt;td&gt;5.3 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2016&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2016&lt;/td&gt; 
   &lt;td&gt;6.5 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2012&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2012&lt;/td&gt; 
   &lt;td&gt;4.3 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2008&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2008&lt;/td&gt; 
   &lt;td&gt;3.0 GB&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;&lt;code&gt;2003&lt;/code&gt;&lt;/td&gt; 
   &lt;td&gt;Windows Server 2003&lt;/td&gt; 
   &lt;td&gt;0.6 GB&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] To install ARM64 versions of Windows use &lt;a href=&quot;https://github.com/dockur/windows-arm/&quot;&gt;dockur/windows-arm&lt;/a&gt;.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How do I change the storage location?&lt;/h3&gt; 
&lt;p&gt;To change the storage location, include the following bind mount in your compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;volumes:
  - ./windows:/storage
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace the example path &lt;code&gt;./windows&lt;/code&gt; with the desired storage folder or named volume.&lt;/p&gt; 
&lt;h3&gt;How do I change the size of the disk?&lt;/h3&gt; 
&lt;p&gt;To expand the default size of 64 GB, add the &lt;code&gt;DISK_SIZE&lt;/code&gt; setting to your compose file and set it to your preferred capacity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  DISK_SIZE: &quot;256G&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] This can also be used to resize the existing disk to a larger capacity without any data loss. However you will need to &lt;a href=&quot;https://learn.microsoft.com/en-us/windows-server/storage/disk-management/extend-a-basic-volume?tabs=disk-management&quot;&gt;manually extend the disk partition&lt;/a&gt; since the added disk space will appear as unallocated.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How do I share files with the host?&lt;/h3&gt; 
&lt;p&gt;Open &#39;File Explorer&#39; and click on the &#39;Network&#39; section, you will see a computer called &lt;code&gt;host.lan&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;Double-click it and it will show a folder called &lt;code&gt;Data&lt;/code&gt;, which can be bound to any folder on your host via the compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;volumes:
  -  ./example:/data
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The example folder &lt;code&gt;./example&lt;/code&gt; will be available as &lt;code&gt; \\host.lan\Data&lt;/code&gt;.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!TIP] You can map this path to a drive letter in Windows, for easier access.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How do I change the amount of CPU or RAM?&lt;/h3&gt; 
&lt;p&gt;By default, the container will be allowed to use a maximum of 2 CPU cores and 4 GB of RAM.&lt;/p&gt; 
&lt;p&gt;If you want to adjust this, you can specify the desired amount using the following environment variables:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  RAM_SIZE: &quot;8G&quot;
  CPU_CORES: &quot;4&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I configure the username and password?&lt;/h3&gt; 
&lt;p&gt;By default, a user called &lt;code&gt;Docker&lt;/code&gt; is created during installation and its password is &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;If you want to use different credentials, you can configure them in your compose file (only before installation):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  USERNAME: &quot;bill&quot;
  PASSWORD: &quot;gates&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I select the Windows language?&lt;/h3&gt; 
&lt;p&gt;By default, the English version of Windows will be downloaded.&lt;/p&gt; 
&lt;p&gt;But before installation you can add the &lt;code&gt;LANGUAGE&lt;/code&gt; environment variable to your compose file, in order to specify an alternative language:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  LANGUAGE: &quot;French&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You can choose between: 🇦🇪 Arabic, 🇧🇬 Bulgarian, 🇨🇳 Chinese, 🇭🇷 Croatian, 🇨🇿 Czech, 🇩🇰 Danish, 🇳🇱 Dutch, 🇬🇧 English, 🇪🇪 Estonian, 🇫🇮 Finnish, 🇫🇷 French, 🇩🇪 German, 🇬🇷 Greek, 🇮🇱 Hebrew, 🇭🇺 Hungarian, 🇮🇹 Italian, 🇯🇵 Japanese, 🇰🇷 Korean, 🇱🇻 Latvian, 🇱🇹 Lithuanian, 🇳🇴 Norwegian, 🇵🇱 Polish, 🇵🇹 Portuguese, 🇷🇴 Romanian, 🇷🇺 Russian, 🇷🇸 Serbian, 🇸🇰 Slovak, 🇸🇮 Slovenian, 🇪🇸 Spanish, 🇸🇪 Swedish, 🇹🇭 Thai, 🇹🇷 Turkish and 🇺🇦 Ukrainian.&lt;/p&gt; 
&lt;h3&gt;How do I select the keyboard layout?&lt;/h3&gt; 
&lt;p&gt;If you want to use a keyboard layout or locale that is not the default for your selected language, you can add &lt;code&gt;KEYBOARD&lt;/code&gt; and &lt;code&gt;REGION&lt;/code&gt; variables like this (before installation):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  REGION: &quot;en-US&quot;
  KEYBOARD: &quot;en-US&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I select the edition?&lt;/h3&gt; 
&lt;p&gt;Windows Server offers a minimalistic Core edition without a GUI. To select those non-standard editions, you can add a &lt;code&gt;EDITION&lt;/code&gt; variable like this (before installation):&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  EDITION: &quot;core&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I install a custom image?&lt;/h3&gt; 
&lt;p&gt;In order to download an unsupported ISO image, specify its URL in the &lt;code&gt;VERSION&lt;/code&gt; environment variable:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  VERSION: &quot;https://example.com/win.iso&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Alternatively, you can also skip the download and use a local file instead, by binding it in your compose file in this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;volumes:
  - ./example.iso:/boot.iso
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Replace the example path &lt;code&gt;./example.iso&lt;/code&gt; with the filename of your desired ISO file. The value of &lt;code&gt;VERSION&lt;/code&gt; will be ignored in this case.&lt;/p&gt; 
&lt;h3&gt;How do I run a script after installation?&lt;/h3&gt; 
&lt;p&gt;To run your own script after installation, you can create a file called &lt;code&gt;install.bat&lt;/code&gt; and place it in a folder together with any additional files it needs (software to be installed for example).&lt;/p&gt; 
&lt;p&gt;Then bind that folder in your compose file like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;volumes:
  -  ./example:/oem
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;The example folder &lt;code&gt;./example&lt;/code&gt; will be copied to &lt;code&gt;C:\OEM&lt;/code&gt; and the containing &lt;code&gt;install.bat&lt;/code&gt; will be executed during the last step of the automatic installation.&lt;/p&gt; 
&lt;h3&gt;How do I perform a manual installation?&lt;/h3&gt; 
&lt;p&gt;It&#39;s recommended to stick to the automatic installation, as it adjusts various settings to prevent common issues when running Windows inside a virtual environment.&lt;/p&gt; 
&lt;p&gt;However, if you insist on performing the installation manually at your own risk, add the following environment variable to your compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  MANUAL: &quot;Y&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I connect using RDP?&lt;/h3&gt; 
&lt;p&gt;The web-viewer is mainly meant to be used during installation, as its picture quality is low, and it has no audio or clipboard for example.&lt;/p&gt; 
&lt;p&gt;So for a better experience you can connect using any Microsoft Remote Desktop client to the IP of the container, using the username &lt;code&gt;Docker&lt;/code&gt; and password &lt;code&gt;admin&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;There is a RDP client for &lt;a href=&quot;https://play.google.com/store/apps/details?id=com.microsoft.rdc.androidx&quot;&gt;Android&lt;/a&gt; available from the Play Store and one for &lt;a href=&quot;https://apps.apple.com/nl/app/microsoft-remote-desktop/id714464092?l=en-GB&quot;&gt;iOS&lt;/a&gt; in the Apple Store. For Linux you can use &lt;a href=&quot;https://www.freerdp.com/&quot;&gt;FreeRDP&lt;/a&gt; and on Windows just type &lt;code&gt;mstsc&lt;/code&gt; in the search box.&lt;/p&gt; 
&lt;h3&gt;How do I assign an individual IP address to the container?&lt;/h3&gt; 
&lt;p&gt;By default, the container uses bridge networking, which shares the IP address with the host.&lt;/p&gt; 
&lt;p&gt;If you want to assign an individual IP address to the container, you can create a macvlan network as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker network create -d macvlan \
    --subnet=192.168.0.0/24 \
    --gateway=192.168.0.1 \
    --ip-range=192.168.0.100/28 \
    -o parent=eth0 vlan
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Be sure to modify these values to match your local subnet.&lt;/p&gt; 
&lt;p&gt;Once you have created the network, change your compose file to look as follows:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;services:
  windows:
    container_name: windows
    ..&amp;lt;snip&amp;gt;..
    networks:
      vlan:
        ipv4_address: 192.168.0.100

networks:
  vlan:
    external: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;An added benefit of this approach is that you won&#39;t have to perform any port mapping anymore, since all ports will be exposed by default.&lt;/p&gt; 
&lt;blockquote&gt; 
 &lt;p&gt;[!IMPORTANT]&lt;br /&gt; This IP address won&#39;t be accessible from the Docker host due to the design of macvlan, which doesn&#39;t permit communication between the two. If this is a concern, you need to create a &lt;a href=&quot;https://blog.oddbit.com/post/2018-03-12-using-docker-macvlan-networks/#host-access&quot;&gt;second macvlan&lt;/a&gt; as a workaround.&lt;/p&gt; 
&lt;/blockquote&gt; 
&lt;h3&gt;How can Windows acquire an IP address from my router?&lt;/h3&gt; 
&lt;p&gt;After configuring the container for &lt;a href=&quot;https://raw.githubusercontent.com/dockur/windows/master/#how-do-i-assign-an-individual-ip-address-to-the-container&quot;&gt;macvlan&lt;/a&gt;, it is possible for Windows to become part of your home network by requesting an IP from your router, just like a real PC.&lt;/p&gt; 
&lt;p&gt;To enable this mode, in which the container and Windows will have separate IP addresses, add the following lines to your compose file:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  DHCP: &quot;Y&quot;
devices:
  - /dev/vhost-net
device_cgroup_rules:
  - &#39;c *:* rwm&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I add multiple disks?&lt;/h3&gt; 
&lt;p&gt;To create additional disks, modify your compose file like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  DISK2_SIZE: &quot;32G&quot;
  DISK3_SIZE: &quot;64G&quot;
volumes:
  - ./example2:/storage2
  - ./example3:/storage3
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;How do I pass-through a disk?&lt;/h3&gt; 
&lt;p&gt;It is possible to pass-through disk devices or partitions directly by adding them to your compose file in this way:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;devices:
  - /dev/sdb:/disk1
  - /dev/sdc1:/disk2
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Use &lt;code&gt;/disk1&lt;/code&gt; if you want it to become your main drive (which will be formatted during installation), and use &lt;code&gt;/disk2&lt;/code&gt; and higher to add them as secondary drives (which will stay untouched).&lt;/p&gt; 
&lt;h3&gt;How do I pass-through a USB device?&lt;/h3&gt; 
&lt;p&gt;To pass-through a USB device, first lookup its vendor and product id via the &lt;code&gt;lsusb&lt;/code&gt; command, then add them to your compose file like this:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;environment:
  ARGUMENTS: &quot;-device usb-host,vendorid=0x1234,productid=0x1234&quot;
devices:
  - /dev/bus/usb
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If the device is a USB disk drive, please wait until after the installation is fully completed before connecting it. Otherwise the installation may fail, as the order of the disks can get rearranged.&lt;/p&gt; 
&lt;h3&gt;How do I verify if my system supports KVM?&lt;/h3&gt; 
&lt;p&gt;First check if your software is compatible using this chart:&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;&lt;strong&gt;Product&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Linux&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Win11&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;Win10&lt;/strong&gt;&lt;/th&gt; 
   &lt;th&gt;&lt;strong&gt;macOS&lt;/strong&gt;&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docker CLI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Docker Desktop&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Podman CLI&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;Podman Desktop&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;✅&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
   &lt;td&gt;❌&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;p&gt;After that you can run the following commands in Linux to check your system:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;sudo apt install cpu-checker
sudo kvm-ok
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you receive an error from &lt;code&gt;kvm-ok&lt;/code&gt; indicating that KVM cannot be used, please check whether:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;the virtualization extensions (&lt;code&gt;Intel VT-x&lt;/code&gt; or &lt;code&gt;AMD SVM&lt;/code&gt;) are enabled in your BIOS.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;you enabled &quot;nested virtualization&quot; if you are running the container inside a virtual machine.&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;you are not using a cloud provider, as most of them do not allow nested virtualization for their VPS&#39;s.&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;If you did not receive any error from &lt;code&gt;kvm-ok&lt;/code&gt; but the container still complains about a missing KVM device, it could help to add &lt;code&gt;privileged: true&lt;/code&gt; to your compose file (or &lt;code&gt;sudo&lt;/code&gt; to your &lt;code&gt;docker&lt;/code&gt; command) to rule out any permission issue.&lt;/p&gt; 
&lt;h3&gt;How do I run macOS in a container?&lt;/h3&gt; 
&lt;p&gt;You can use &lt;a href=&quot;https://github.com/dockur/macos&quot;&gt;dockur/macos&lt;/a&gt; for that. It shares many of the same features, except for the automatic installation.&lt;/p&gt; 
&lt;h3&gt;How do I run a Linux desktop in a container?&lt;/h3&gt; 
&lt;p&gt;You can use &lt;a href=&quot;https://github.com/qemus/qemu&quot;&gt;qemus/qemu&lt;/a&gt; in that case.&lt;/p&gt; 
&lt;h3&gt;Is this project legal?&lt;/h3&gt; 
&lt;p&gt;Yes, this project contains only open-source code and does not distribute any copyrighted material. Any product keys found in the code are just generic placeholders provided by Microsoft for trial purposes. So under all applicable laws, this project will be considered legal.&lt;/p&gt; 
&lt;h2&gt;Disclaimer ⚖️&lt;/h2&gt; 
&lt;p&gt;&lt;em&gt;The product names, logos, brands, and other trademarks referred to within this project are the property of their respective trademark holders. This project is not affiliated, sponsored, or endorsed by Microsoft Corporation.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>gin-gonic/gin</title>
      <link>https://github.com/gin-gonic/gin</link>
      <description>&lt;p&gt;Gin is a high-performance HTTP web framework written in Go. It provides a Martini-like API but with significantly better performance—up to 40 times faster—thanks to httprouter. Gin is designed for building REST APIs, web applications, and microservices.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gin Web Framework&lt;/h1&gt; 
&lt;img align=&quot;right&quot; width=&quot;159px&quot; src=&quot;https://raw.githubusercontent.com/gin-gonic/logo/master/color.png&quot; /&gt; 
&lt;p&gt;&lt;a href=&quot;https://github.com/gin-gonic/gin/actions/workflows/gin.yml&quot;&gt;&lt;img src=&quot;https://github.com/gin-gonic/gin/actions/workflows/gin.yml/badge.svg?branch=master&quot; alt=&quot;Build Status&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://codecov.io/gh/gin-gonic/gin&quot;&gt;&lt;img src=&quot;https://codecov.io/gh/gin-gonic/gin/branch/master/graph/badge.svg?sanitize=true&quot; alt=&quot;codecov&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://goreportcard.com/report/github.com/gin-gonic/gin&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/github.com/gin-gonic/gin&quot; alt=&quot;Go Report Card&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://pkg.go.dev/github.com/gin-gonic/gin?tab=doc&quot;&gt;&lt;img src=&quot;https://pkg.go.dev/badge/github.com/gin-gonic/gin?status.svg?sanitize=true&quot; alt=&quot;Go Reference&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://sourcegraph.com/github.com/gin-gonic/gin?badge&quot;&gt;&lt;img src=&quot;https://sourcegraph.com/github.com/gin-gonic/gin/-/badge.svg?sanitize=true&quot; alt=&quot;Sourcegraph&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://www.codetriage.com/gin-gonic/gin&quot;&gt;&lt;img src=&quot;https://www.codetriage.com/gin-gonic/gin/badges/users.svg?sanitize=true&quot; alt=&quot;Open Source Helpers&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/gin-gonic/gin/releases&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/release/gin-gonic/gin.svg?style=flat-square&quot; alt=&quot;Release&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;📰 &lt;a href=&quot;https://gin-gonic.com/en/blog/news/gin-1-11-0-release-announcement/&quot;&gt;Announcing Gin 1.11.0!&lt;/a&gt;&lt;/h2&gt; 
&lt;p&gt;Read about the latest features and improvements in Gin 1.11.0 on our official blog.&lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;Gin is a high-performance HTTP web framework written in &lt;a href=&quot;https://go.dev/&quot;&gt;Go&lt;/a&gt;. It provides a Martini-like API but with significantly better performance—up to 40 times faster—thanks to &lt;a href=&quot;https://github.com/julienschmidt/httprouter&quot;&gt;httprouter&lt;/a&gt;. Gin is designed for building REST APIs, web applications, and microservices where speed and developer productivity are essential.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Why choose Gin?&lt;/strong&gt;&lt;/p&gt; 
&lt;p&gt;Gin combines the simplicity of Express.js-style routing with Go&#39;s performance characteristics, making it ideal for:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Building high-throughput REST APIs&lt;/li&gt; 
 &lt;li&gt;Developing microservices that need to handle many concurrent requests&lt;/li&gt; 
 &lt;li&gt;Creating web applications that require fast response times&lt;/li&gt; 
 &lt;li&gt;Prototyping web services quickly with minimal boilerplate&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;strong&gt;Gin&#39;s key features:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Zero allocation router&lt;/strong&gt; - Extremely memory-efficient routing with no heap allocations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;High performance&lt;/strong&gt; - Benchmarks show superior speed compared to other Go web frameworks&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Middleware support&lt;/strong&gt; - Extensible middleware system for authentication, logging, CORS, etc.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Crash-free&lt;/strong&gt; - Built-in recovery middleware prevents panics from crashing your server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;JSON validation&lt;/strong&gt; - Automatic request/response JSON binding and validation&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Route grouping&lt;/strong&gt; - Organize related routes and apply common middleware&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Error management&lt;/strong&gt; - Centralized error handling and logging&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Built-in rendering&lt;/strong&gt; - Support for JSON, XML, HTML templates, and more&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Extensible&lt;/strong&gt; - Large ecosystem of community middleware and plugins&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Getting Started&lt;/h2&gt; 
&lt;h3&gt;Prerequisites&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Go version&lt;/strong&gt;: Gin requires &lt;a href=&quot;https://go.dev/&quot;&gt;Go&lt;/a&gt; version &lt;a href=&quot;https://go.dev/doc/devel/release#go1.23.0&quot;&gt;1.23&lt;/a&gt; or above&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Basic Go knowledge&lt;/strong&gt;: Familiarity with Go syntax and package management is helpful&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Installation&lt;/h3&gt; 
&lt;p&gt;With &lt;a href=&quot;https://go.dev/wiki/Modules#how-to-use-modules&quot;&gt;Go&#39;s module support&lt;/a&gt;, simply import Gin in your code and Go will automatically fetch it during build:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;import &quot;github.com/gin-gonic/gin&quot;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Your First Gin Application&lt;/h3&gt; 
&lt;p&gt;Here&#39;s a complete example that demonstrates Gin&#39;s simplicity:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;package main

import (
  &quot;net/http&quot;

  &quot;github.com/gin-gonic/gin&quot;
)

func main() {
  // Create a Gin router with default middleware (logger and recovery)
  r := gin.Default()
  
  // Define a simple GET endpoint
  r.GET(&quot;/ping&quot;, func(c *gin.Context) {
    // Return JSON response
    c.JSON(http.StatusOK, gin.H{
      &quot;message&quot;: &quot;pong&quot;,
    })
  })
  
  // Start server on port 8080 (default)
  // Server will listen on 0.0.0.0:8080 (localhost:8080 on Windows)
  r.Run()
}
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;&lt;strong&gt;Running the application:&lt;/strong&gt;&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Save the code above as &lt;code&gt;main.go&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Run the application:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-sh&quot;&gt;go run main.go
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Open your browser and visit &lt;a href=&quot;http://localhost:8080/ping&quot;&gt;&lt;code&gt;http://localhost:8080/ping&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;You should see: &lt;code&gt;{&quot;message&quot;:&quot;pong&quot;}&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;What this example demonstrates:&lt;/strong&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Creating a Gin router with default middleware&lt;/li&gt; 
 &lt;li&gt;Defining HTTP endpoints with simple handler functions&lt;/li&gt; 
 &lt;li&gt;Returning JSON responses&lt;/li&gt; 
 &lt;li&gt;Starting an HTTP server&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Next Steps&lt;/h3&gt; 
&lt;p&gt;After running your first Gin application, explore these resources to learn more:&lt;/p&gt; 
&lt;h4&gt;📚 Learning Resources&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/gin-gonic/gin/master/docs/doc.md&quot;&gt;Gin Quick Start Guide&lt;/a&gt;&lt;/strong&gt; - Comprehensive tutorial with API examples and build configurations&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/gin-gonic/examples&quot;&gt;Example Repository&lt;/a&gt;&lt;/strong&gt; - Ready-to-run examples demonstrating various Gin use cases: 
  &lt;ul&gt; 
   &lt;li&gt;REST API development&lt;/li&gt; 
   &lt;li&gt;Authentication &amp;amp; middleware&lt;/li&gt; 
   &lt;li&gt;File uploads and downloads&lt;/li&gt; 
   &lt;li&gt;WebSocket connections&lt;/li&gt; 
   &lt;li&gt;Template rendering&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;📖 Documentation&lt;/h2&gt; 
&lt;h3&gt;API Reference&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pkg.go.dev/github.com/gin-gonic/gin&quot;&gt;Go.dev API Documentation&lt;/a&gt;&lt;/strong&gt; - Complete API reference with examples&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;User Guides&lt;/h3&gt; 
&lt;p&gt;The comprehensive documentation is available on &lt;a href=&quot;https://gin-gonic.com&quot;&gt;gin-gonic.com&lt;/a&gt; in multiple languages:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gin-gonic.com/en/docs/&quot;&gt;English&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/zh-cn/docs/&quot;&gt;简体中文&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/zh-tw/docs/&quot;&gt;繁體中文&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gin-gonic.com/ja/docs/&quot;&gt;日本語&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/ko-kr/docs/&quot;&gt;한국어&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/es/docs/&quot;&gt;Español&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gin-gonic.com/tr/docs/&quot;&gt;Turkish&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/fa/docs/&quot;&gt;Persian&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/pt/docs/&quot;&gt;Português&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://gin-gonic.com/ru/docs/&quot;&gt;Russian&lt;/a&gt; | &lt;a href=&quot;https://gin-gonic.com/id/docs/&quot;&gt;Indonesian&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Official Tutorials&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://go.dev/doc/tutorial/web-service-gin&quot;&gt;Go.dev Tutorial: Developing a RESTful API with Go and Gin&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;⚡ Performance Benchmarks&lt;/h2&gt; 
&lt;p&gt;Gin demonstrates exceptional performance compared to other Go web frameworks. It uses a custom version of &lt;a href=&quot;https://github.com/julienschmidt/httprouter&quot;&gt;HttpRouter&lt;/a&gt; for maximum efficiency. &lt;a href=&quot;https://raw.githubusercontent.com/gin-gonic/gin/master/BENCHMARKS.md&quot;&gt;View detailed benchmarks →&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Gin vs. Other Go Frameworks&lt;/strong&gt; (GitHub API routing benchmark):&lt;/p&gt; 
&lt;table&gt; 
 &lt;thead&gt; 
  &lt;tr&gt; 
   &lt;th&gt;Benchmark name&lt;/th&gt; 
   &lt;th align=&quot;right&quot;&gt;(1)&lt;/th&gt; 
   &lt;th align=&quot;right&quot;&gt;(2)&lt;/th&gt; 
   &lt;th align=&quot;right&quot;&gt;(3)&lt;/th&gt; 
   &lt;th align=&quot;right&quot;&gt;(4)&lt;/th&gt; 
  &lt;/tr&gt; 
 &lt;/thead&gt; 
 &lt;tbody&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGin_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;&lt;strong&gt;43550&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;&lt;strong&gt;27364 ns/op&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;&lt;strong&gt;0 B/op&lt;/strong&gt;&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;&lt;strong&gt;0 allocs/op&lt;/strong&gt;&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkAce_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;40543&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;29670 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkAero_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;57632&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;20648 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkBear_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;9234&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;216179 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;86448 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;943 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkBeego_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;7407&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;243496 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;71456 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkBone_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;420&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2922835 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;720160 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;8620 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkChi_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;7620&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;238331 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;87696 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkDenco_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;18355&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;64494 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;20224 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;167 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkEcho_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;31251&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;38479 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGocraftWeb_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;4117&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;300062 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;131656 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1686 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGoji_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;3274&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;416158 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;56112 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;334 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGojiv2_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1402&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;870518 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;352720 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;4321 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGoJsonRest_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2976&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;401507 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;134371 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2737 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGoRestful_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;410&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2913158 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;910144 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2938 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGorillaMux_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;346&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;3384987 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;251650 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1994 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkGowwwRouter_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;10000&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;143025 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;72144 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;501 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkHttpRouter_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;55938&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;21360 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkHttpTreeMux_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;10000&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;153944 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;65856 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;671 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkKocha_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;10000&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;106315 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;23304 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;843 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkLARS_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;47779&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;25084 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;0 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkMacaron_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;3266&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;371907 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;149409 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1624 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkMartini_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;331&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;3444706 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;226551 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2325 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkPat_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;273&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;4381818 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1483152 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;26963 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkPossum_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;10000&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;164367 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;84448 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkR2router_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;10000&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;160220 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;77328 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;979 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkRivet_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;14625&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;82453 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;16272 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;167 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkTango_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;6255&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;279611 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;63826 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;1618 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkTigerTonic_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;2008&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;687874 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;193856 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;4474 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkTraffic_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;355&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;3478508 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;820744 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;14114 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
  &lt;tr&gt; 
   &lt;td&gt;BenchmarkVulcan_GithubAll&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;6885&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;193333 ns/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;19894 B/op&lt;/td&gt; 
   &lt;td align=&quot;right&quot;&gt;609 allocs/op&lt;/td&gt; 
  &lt;/tr&gt; 
 &lt;/tbody&gt; 
&lt;/table&gt; 
&lt;ul&gt; 
 &lt;li&gt;(1): Total Repetitions achieved in constant time, higher means more confident result&lt;/li&gt; 
 &lt;li&gt;(2): Single Repetition Duration (ns/op), lower is better&lt;/li&gt; 
 &lt;li&gt;(3): Heap Memory (B/op), lower is better&lt;/li&gt; 
 &lt;li&gt;(4): Average Allocations per Repetition (allocs/op), lower is better&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🔌 Middleware Ecosystem&lt;/h2&gt; 
&lt;p&gt;Gin has a rich ecosystem of middleware for common web development needs. Explore community-contributed middleware:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/gin-contrib&quot;&gt;gin-contrib&lt;/a&gt;&lt;/strong&gt; - Official middleware collection including:&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;Authentication (JWT, Basic Auth, Sessions)&lt;/li&gt; 
   &lt;li&gt;CORS, Rate limiting, Compression&lt;/li&gt; 
   &lt;li&gt;Logging, Metrics, Tracing&lt;/li&gt; 
   &lt;li&gt;Static file serving, Template engines&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/gin-gonic/contrib&quot;&gt;gin-gonic/contrib&lt;/a&gt;&lt;/strong&gt; - Additional community middleware&lt;/p&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🏢 Production Usage&lt;/h2&gt; 
&lt;p&gt;Gin powers many high-traffic applications and services in production:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/appleboy/gorush&quot;&gt;gorush&lt;/a&gt;&lt;/strong&gt; - High-performance push notification server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/fnproject/fn&quot;&gt;fnproject&lt;/a&gt;&lt;/strong&gt; - Container-native, serverless platform&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/photoprism/photoprism&quot;&gt;photoprism&lt;/a&gt;&lt;/strong&gt; - AI-powered personal photo management&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/luraproject/lura&quot;&gt;lura&lt;/a&gt;&lt;/strong&gt; - Ultra-performant API Gateway framework&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/thoas/picfit&quot;&gt;picfit&lt;/a&gt;&lt;/strong&gt; - Real-time image processing server&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/distribworks/dkron&quot;&gt;dkron&lt;/a&gt;&lt;/strong&gt; - Distributed job scheduling system&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;🤝 Contributing&lt;/h2&gt; 
&lt;p&gt;Gin is the work of hundreds of contributors from around the world. We welcome and appreciate your contributions!&lt;/p&gt; 
&lt;h3&gt;How to Contribute&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;🐛 &lt;strong&gt;Report bugs&lt;/strong&gt; - Help us identify and fix issues&lt;/li&gt; 
 &lt;li&gt;💡 &lt;strong&gt;Suggest features&lt;/strong&gt; - Share your ideas for improvements&lt;/li&gt; 
 &lt;li&gt;📝 &lt;strong&gt;Improve documentation&lt;/strong&gt; - Help make our docs clearer&lt;/li&gt; 
 &lt;li&gt;🔧 &lt;strong&gt;Submit code&lt;/strong&gt; - Fix bugs or implement new features&lt;/li&gt; 
 &lt;li&gt;🧪 &lt;strong&gt;Write tests&lt;/strong&gt; - Improve our test coverage&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Getting Started with Contributing&lt;/h3&gt; 
&lt;ol&gt; 
 &lt;li&gt;Check out our &lt;a href=&quot;https://raw.githubusercontent.com/gin-gonic/gin/master/CONTRIBUTING.md&quot;&gt;CONTRIBUTING.md&lt;/a&gt; for detailed guidelines&lt;/li&gt; 
 &lt;li&gt;Join our community discussions and ask questions&lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;&lt;strong&gt;All contributions are valued and help make Gin better for everyone!&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>dataease/SQLBot</title>
      <link>https://github.com/dataease/SQLBot</link>
      <description>&lt;p&gt;🔥 基于大模型和 RAG 的智能问数系统。Text-to-SQL Generation via LLMs using RAG.&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://resource-fit2cloud-com.oss-cn-hangzhou.aliyuncs.com/sqlbot/sqlbot.png&quot; alt=&quot;SQLBot&quot; width=&quot;300&quot; /&gt;&lt;/p&gt; 
&lt;h3 align=&quot;center&quot;&gt;基于大模型和 RAG 的智能问数系统&lt;/h3&gt; 
&lt;p align=&quot;center&quot;&gt; &lt;a href=&quot;https://github.com/dataease/SQLBot/releases/latest&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/v/release/dataease/SQLBot&quot; alt=&quot;Latest release&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://github.com/dataease/SQLBot&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/stars/dataease/SQLBot?color=%231890FF&amp;amp;style=flat-square&quot; alt=&quot;Stars&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://hub.docker.com/r/dataease/SQLbot&quot;&gt;&lt;img src=&quot;https://img.shields.io/docker/pulls/dataease/sqlbot?label=downloads&quot; alt=&quot;Download&quot; /&gt;&lt;/a&gt;&lt;br /&gt; &lt;/p&gt; 
&lt;hr /&gt; 
&lt;p&gt;SQLBot 是一款基于大模型和 RAG 的智能问数系统。SQLBot 的优势包括：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;开箱即用&lt;/strong&gt;: 只需配置大模型和数据源即可开启问数之旅，通过大模型和 RAG 的结合来实现高质量的 text2sql；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;易于集成&lt;/strong&gt;: 支持快速嵌入到第三方业务系统，也支持被 n8n、MaxKB、Dify、Coze 等 AI 应用开发平台集成调用，让各类应用快速拥有智能问数能力；&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;安全可控&lt;/strong&gt;: 提供基于工作空间的资源隔离机制，能够实现细粒度的数据权限控制。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;工作原理&lt;/h2&gt; 
&lt;img width=&quot;1105&quot; height=&quot;577&quot; alt=&quot;system-arch&quot; src=&quot;https://github.com/user-attachments/assets/462603fc-980b-4b8b-a6d4-a821c070a048&quot; /&gt; 
&lt;h2&gt;快速开始&lt;/h2&gt; 
&lt;h3&gt;安装部署&lt;/h3&gt; 
&lt;p&gt;准备一台 Linux 服务器，安装好 &lt;a href=&quot;https://docs.docker.com/get-docker/&quot;&gt;Docker&lt;/a&gt;，执行以下一键安装脚本：&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run -d \
  --name sqlbot \
  --restart unless-stopped \
  -p 8000:8000 \
  -p 8001:8001 \
  -v ./data/sqlbot/excel:/opt/sqlbot/data/excel \
  -v ./data/sqlbot/file:/opt/sqlbot/data/file \
  -v ./data/sqlbot/images:/opt/sqlbot/images \
  -v ./data/sqlbot/logs:/opt/sqlbot/app/logs \
  -v ./data/postgresql:/var/lib/postgresql/data \
  --privileged=true \
  dataease/sqlbot
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;你也可以通过 &lt;a href=&quot;https://apps.fit2cloud.com/1panel&quot;&gt;1Panel 应用商店&lt;/a&gt; 快速部署 SQLBot。&lt;/p&gt; 
&lt;p&gt;如果是内网环境，你可以通过 &lt;a href=&quot;https://community.fit2cloud.com/#/products/sqlbot/downloads&quot;&gt;离线安装包方式&lt;/a&gt; 部署 SQLBot。&lt;/p&gt; 
&lt;h3&gt;访问方式&lt;/h3&gt; 
&lt;ul&gt; 
 &lt;li&gt;在浏览器中打开: http://&amp;lt;你的服务器IP&amp;gt;:8000/&lt;/li&gt; 
 &lt;li&gt;用户名: admin&lt;/li&gt; 
 &lt;li&gt;密码: SQLBot@123456&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;联系我们&lt;/h3&gt; 
&lt;p&gt;如你有更多问题，可以加入我们的技术交流群与我们交流。&lt;/p&gt; 
&lt;img width=&quot;180&quot; height=&quot;180&quot; alt=&quot;contact_me_qr&quot; src=&quot;https://github.com/user-attachments/assets/2594ff29-5426-4457-b051-279855610030&quot; /&gt; 
&lt;h2&gt;UI 展示&lt;/h2&gt;  
&lt;img alt=&quot;q&amp;amp;a&quot; src=&quot;https://github.com/user-attachments/assets/55526514-52f3-4cfe-98ec-08a986259280&quot; /&gt;  
&lt;h2&gt;Star History&lt;/h2&gt; 
&lt;p&gt;&lt;a href=&quot;https://www.star-history.com/#dataease/sqlbot&amp;amp;Date&quot;&gt;&lt;img src=&quot;https://api.star-history.com/svg?repos=dataease/sqlbot&amp;amp;type=Date&quot; alt=&quot;Star History Chart&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;h2&gt;飞致云旗下的其他明星项目&lt;/h2&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/dataease/dataease/&quot;&gt;DataEase&lt;/a&gt; - 人人可用的开源 BI 工具&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1panel-dev/1panel/&quot;&gt;1Panel&lt;/a&gt; - 现代化、开源的 Linux 服务器运维管理面板&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1panel-dev/MaxKB/&quot;&gt;MaxKB&lt;/a&gt; - 强大易用的企业级智能体平台&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/jumpserver/jumpserver/&quot;&gt;JumpServer&lt;/a&gt; - 广受欢迎的开源堡垒机&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/1Panel-dev/CordysCRM&quot;&gt;Cordys CRM&lt;/a&gt; - 新一代的开源 AI CRM 系统&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/halo-dev/halo/&quot;&gt;Halo&lt;/a&gt; - 强大易用的开源建站工具&lt;/li&gt; 
 &lt;li&gt;&lt;a href=&quot;https://github.com/metersphere/metersphere/&quot;&gt;MeterSphere&lt;/a&gt; - 新一代的开源持续测试工具&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;License&lt;/h2&gt; 
&lt;p&gt;本仓库遵循 &lt;a href=&quot;https://raw.githubusercontent.com/dataease/SQLBot/main/LICENSE&quot;&gt;FIT2CLOUD Open Source License&lt;/a&gt; 开源协议，该许可证本质上是 GPLv3，但有一些额外的限制。&lt;/p&gt; 
&lt;p&gt;你可以基于 SQLBot 的源代码进行二次开发，但是需要遵守以下规定：&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;不能替换和修改 SQLBot 的 Logo 和版权信息；&lt;/li&gt; 
 &lt;li&gt;二次开发后的衍生作品必须遵守 GPL V3 的开源义务。&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;如需商业授权，请联系 &lt;a href=&quot;mailto:support@fit2cloud.com&quot;&gt;support@fit2cloud.com&lt;/a&gt; 。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>modelcontextprotocol/registry</title>
      <link>https://github.com/modelcontextprotocol/registry</link>
      <description>&lt;p&gt;A community driven registry service for Model Context Protocol (MCP) servers.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MCP Registry&lt;/h1&gt; 
&lt;p&gt;The MCP registry provides MCP clients with a list of MCP servers, like an app store for MCP servers.&lt;/p&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md&quot;&gt;&lt;strong&gt;📤 Publish my MCP server&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://registry.modelcontextprotocol.io/docs&quot;&gt;&lt;strong&gt;⚡️ Live API docs&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/explanations/ecosystem-vision.md&quot;&gt;&lt;strong&gt;👀 Ecosystem vision&lt;/strong&gt;&lt;/a&gt; | 📖 &lt;strong&gt;&lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs&quot;&gt;Full documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; 
&lt;h2&gt;Development Status&lt;/h2&gt; 
&lt;p&gt;&lt;strong&gt;2025-09-08 update&lt;/strong&gt;: The registry has launched in preview 🎉 (&lt;a href=&quot;https://blog.modelcontextprotocol.io/posts/2025-09-08-mcp-registry-preview/&quot;&gt;announcement blog post&lt;/a&gt;). While the system is now more stable, this is still a preview release and breaking changes or data resets may occur. A general availability (GA) release will follow later. We&#39;d love your feedback in &lt;a href=&quot;https://github.com/modelcontextprotocol/registry/discussions/new?category=ideas&quot;&gt;GitHub discussions&lt;/a&gt; or in the &lt;a href=&quot;https://discord.com/channels/1358869848138059966/1369487942862504016&quot;&gt;#registry-dev Discord&lt;/a&gt; (&lt;a href=&quot;https://modelcontextprotocol.io/community/communication&quot;&gt;joining details here&lt;/a&gt;).&lt;/p&gt; 
&lt;p&gt;Current key maintainers:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Adam Jones&lt;/strong&gt; (Anthropic) &lt;a href=&quot;https://github.com/domdomegg&quot;&gt;@domdomegg&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Tadas Antanavicius&lt;/strong&gt; (PulseMCP) &lt;a href=&quot;https://github.com/tadasant&quot;&gt;@tadasant&lt;/a&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Toby Padilla&lt;/strong&gt; (GitHub) &lt;a href=&quot;https://github.com/toby&quot;&gt;@toby&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;Contributing&lt;/h2&gt; 
&lt;p&gt;We use multiple channels for collaboration - see &lt;a href=&quot;https://modelcontextprotocol.io/community/communication&quot;&gt;modelcontextprotocol.io/community/communication&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Often (but not always) ideas flow through this pipeline:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://modelcontextprotocol.io/community/communication&quot;&gt;Discord&lt;/a&gt;&lt;/strong&gt; - Real-time community discussions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/modelcontextprotocol/registry/discussions&quot;&gt;Discussions&lt;/a&gt;&lt;/strong&gt; - Propose and discuss product/technical requirements&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/modelcontextprotocol/registry/issues&quot;&gt;Issues&lt;/a&gt;&lt;/strong&gt; - Track well-scoped technical work&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/modelcontextprotocol/registry/pulls&quot;&gt;Pull Requests&lt;/a&gt;&lt;/strong&gt; - Contribute work towards issues&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h3&gt;Quick start:&lt;/h3&gt; 
&lt;h4&gt;Pre-requisites&lt;/h4&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Go 1.24.x&lt;/strong&gt;&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;golangci-lint v2.4.0&lt;/strong&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h4&gt;Running the server&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Start full development environment
make dev-compose
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;This starts the registry at &lt;a href=&quot;http://localhost:8080&quot;&gt;&lt;code&gt;localhost:8080&lt;/code&gt;&lt;/a&gt; with PostgreSQL and seed data. The database uses ephemeral storage and is reset each time you restart the containers, ensuring a clean state for development and testing.&lt;/p&gt; 
&lt;p&gt;The setup can be configured with environment variables in &lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docker-compose.yml&quot;&gt;docker-compose.yml&lt;/a&gt; - see &lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/.env.example&quot;&gt;.env.example&lt;/a&gt; for a reference.&lt;/p&gt; 
&lt;details&gt; 
 &lt;summary&gt;Alternative: Running a pre-built Docker image&lt;/summary&gt; 
 &lt;p&gt;Pre-built Docker images are automatically published to GitHub Container Registry:&lt;/p&gt; 
 &lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Run latest stable release
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:latest

# Run latest from main branch (continuous deployment)
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main

# Run specific release version
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:v1.0.0

# Run development build from main branch
docker run -p 8080:8080 ghcr.io/modelcontextprotocol/registry:main-20250906-abc123d
&lt;/code&gt;&lt;/pre&gt; 
 &lt;p&gt;&lt;strong&gt;Available tags:&lt;/strong&gt;&lt;/p&gt; 
 &lt;ul&gt; 
  &lt;li&gt;&lt;strong&gt;Releases&lt;/strong&gt;: &lt;code&gt;latest&lt;/code&gt;, &lt;code&gt;v1.0.0&lt;/code&gt;, &lt;code&gt;v1.1.0&lt;/code&gt;, etc.&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Continuous&lt;/strong&gt;: &lt;code&gt;main&lt;/code&gt; (latest main branch build)&lt;/li&gt; 
  &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt;: &lt;code&gt;main-&amp;lt;date&amp;gt;-&amp;lt;sha&amp;gt;&lt;/code&gt; (specific commit builds)&lt;/li&gt; 
 &lt;/ul&gt; 
&lt;/details&gt; 
&lt;h4&gt;Publishing a server&lt;/h4&gt; 
&lt;p&gt;To publish a server, we&#39;ve built a simple CLI. You can use it with:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Build the latest CLI
make publisher

# Use it!
./bin/mcp-publisher --help
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;See &lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs/guides/publishing/publish-server.md&quot;&gt;the publisher guide&lt;/a&gt; for more details.&lt;/p&gt; 
&lt;h4&gt;Other commands&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# Run lint, unit tests and integration tests
make check
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;There are also a few more helpful commands for development. Run &lt;code&gt;make help&lt;/code&gt; to learn more, or look in &lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/Makefile&quot;&gt;Makefile&lt;/a&gt;.&lt;/p&gt; 
&lt;!--
For Claude and other AI tools: Always prefer make targets over custom commands where possible.
--&gt; 
&lt;h2&gt;Architecture&lt;/h2&gt; 
&lt;h3&gt;Project Structure&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;├── cmd/                     # Application entry points
│   └── publisher/           # Server publishing tool
├── data/                    # Seed data
├── deploy/                  # Deployment configuration (Pulumi)
├── docs/                    # Documentation
├── internal/                # Private application code
│   ├── api/                 # HTTP handlers and routing
│   ├── auth/                # Authentication (GitHub OAuth, JWT, namespace blocking)
│   ├── config/              # Configuration management
│   ├── database/            # Data persistence (PostgreSQL)
│   ├── service/             # Business logic
│   ├── telemetry/           # Metrics and monitoring
│   └── validators/          # Input validation
├── pkg/                     # Public packages
│   ├── api/                 # API types and structures
│   │   └── v0/              # Version 0 API types
│   └── model/               # Data models for server.json
├── scripts/                 # Development and testing scripts
├── tests/                   # Integration tests
└── tools/                   # CLI tools and utilities
    └── validate-*.sh        # Schema validation tools
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Authentication&lt;/h3&gt; 
&lt;p&gt;Publishing supports multiple authentication methods:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OAuth&lt;/strong&gt; - For publishing by logging into GitHub&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;GitHub OIDC&lt;/strong&gt; - For publishing from GitHub Actions&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;DNS verification&lt;/strong&gt; - For proving ownership of a domain and its subdomains&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;HTTP verification&lt;/strong&gt; - For proving ownership of a domain&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;The registry validates namespace ownership when publishing. E.g. to publish...:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;code&gt;io.github.domdomegg/my-cool-mcp&lt;/code&gt; you must login to GitHub as &lt;code&gt;domdomegg&lt;/code&gt;, or be in a GitHub Action on domdomegg&#39;s repos&lt;/li&gt; 
 &lt;li&gt;&lt;code&gt;me.adamjones/my-cool-mcp&lt;/code&gt; you must prove ownership of &lt;code&gt;adamjones.me&lt;/code&gt; via DNS or HTTP challenge&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;More documentation&lt;/h2&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/modelcontextprotocol/registry/main/docs&quot;&gt;documentation&lt;/a&gt; for more details if your question has not been answered here!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>trufflesecurity/trufflehog</title>
      <link>https://github.com/trufflesecurity/trufflehog</link>
      <description>&lt;p&gt;Find, verify, and analyze leaked credentials&lt;/p&gt;&lt;hr&gt;&lt;p align=&quot;center&quot;&gt; &lt;img alt=&quot;GoReleaser Logo&quot; src=&quot;https://storage.googleapis.com/trufflehog-static-sources/pixel_pig.png&quot; height=&quot;140&quot; /&gt; &lt;/p&gt;
&lt;h2 align=&quot;center&quot;&gt;TruffleHog&lt;/h2&gt; 
&lt;p align=&quot;center&quot;&gt;Find leaked credentials.&lt;/p&gt; 
&lt;p&gt;&lt;/p&gt; 
&lt;hr /&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;p&gt;&lt;a href=&quot;https://goreportcard.com/report/github.com/trufflesecurity/trufflehog/v3&quot;&gt;&lt;img src=&quot;https://goreportcard.com/badge/github.com/trufflesecurity/trufflehog/v3&quot; alt=&quot;Go Report Card&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/LICENSE&quot;&gt;&lt;img src=&quot;https://img.shields.io/badge/license-AGPL--3.0-brightgreen&quot; alt=&quot;License&quot; /&gt;&lt;/a&gt; &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/pkg/detectors&quot;&gt;&lt;img src=&quot;https://img.shields.io/github/directory-file-count/trufflesecurity/truffleHog/pkg/detectors?label=Total%20Detectors&amp;amp;type=dir&quot; alt=&quot;Total Detectors&quot; /&gt;&lt;/a&gt;&lt;/p&gt; 
&lt;/div&gt; 
&lt;hr /&gt; 
&lt;h1&gt;&lt;span&gt;🔎&lt;/span&gt; &lt;em&gt;Now Scanning&lt;/em&gt;&lt;/h1&gt; 
&lt;div align=&quot;center&quot;&gt; 
 &lt;img src=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/assets/scanning_logos.svg?sanitize=true&quot; /&gt; 
 &lt;p&gt;&lt;strong&gt;...and more&lt;/strong&gt;&lt;/p&gt; 
 &lt;p&gt;To learn more about TruffleHog and its features and capabilities, visit our &lt;a href=&quot;https://trufflesecurity.com/trufflehog?gclid=CjwKCAjwouexBhAuEiwAtW_Zx5IW87JNj97Ci7heFnA5ar6-DuNzT2Y5nIl9DuZ-FOUqx0Qg3vb9nxoClcEQAvD_BwE&quot;&gt;product page&lt;/a&gt;.&lt;/p&gt; 
&lt;/div&gt; 
&lt;h1&gt;&lt;span&gt;🌐&lt;/span&gt; TruffleHog Enterprise&lt;/h1&gt; 
&lt;p&gt;Are you interested in continuously monitoring &lt;strong&gt;Git, Jira, Slack, Confluence, Microsoft Teams, Sharepoint, and more..&lt;/strong&gt; for credentials? We have an enterprise product that can help! Learn more at &lt;a href=&quot;https://trufflesecurity.com/trufflehog-enterprise&quot;&gt;https://trufflesecurity.com/trufflehog-enterprise&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We take the revenue from the enterprise product to fund more awesome open source projects that the whole community can benefit from.&lt;/p&gt;  
&lt;h1&gt;What is TruffleHog 🐽&lt;/h1&gt; 
&lt;p&gt;TruffleHog is the most powerful secrets &lt;strong&gt;Discovery, Classification, Validation,&lt;/strong&gt; and &lt;strong&gt;Analysis&lt;/strong&gt; tool. In this context, secret refers to a credential a machine uses to authenticate itself to another machine. This includes API keys, database passwords, private encryption keys, and more...&lt;/p&gt; 
&lt;h2&gt;Discovery 🔍&lt;/h2&gt; 
&lt;p&gt;TruffleHog can look for secrets in many places including Git, chats, wikis, logs, API testing platforms, object stores, filesystems and more&lt;/p&gt; 
&lt;h2&gt;Classification 📁&lt;/h2&gt; 
&lt;p&gt;TruffleHog classifies over 800 secret types, mapping them back to the specific identity they belong to. Is it an AWS secret? Stripe secret? Cloudflare secret? Postgres password? SSL Private key? Sometimes it&#39;s hard to tell looking at it, so TruffleHog classifies everything it finds.&lt;/p&gt; 
&lt;h2&gt;Validation ✅&lt;/h2&gt; 
&lt;p&gt;For every secret TruffleHog can classify, it can also log in to confirm if that secret is live or not. This step is critical to know if there’s an active present danger or not.&lt;/p&gt; 
&lt;h2&gt;Analysis 🔬&lt;/h2&gt; 
&lt;p&gt;For the 20 some of the most commonly leaked out credential types, instead of sending one request to check if the secret can log in, TruffleHog can send many requests to learn everything there is to know about the secret. Who created it? What resources can it access? What permissions does it have on those resources?&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;📢&lt;/span&gt; Join Our Community&lt;/h1&gt; 
&lt;p&gt;Have questions? Feedback? Jump into Slack or Discord and hang out with us.&lt;/p&gt; 
&lt;p&gt;Join our &lt;a href=&quot;https://join.slack.com/t/trufflehog-community/shared_invite/zt-pw2qbi43-Aa86hkiimstfdKH9UCpPzQ&quot;&gt;Slack Community&lt;/a&gt;&lt;/p&gt; 
&lt;p&gt;Join the &lt;a href=&quot;https://discord.gg/8Hzbrnkr7E&quot;&gt;Secret Scanning Discord&lt;/a&gt;&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;📺&lt;/span&gt; Demo&lt;/h1&gt; 
&lt;p&gt;&lt;img src=&quot;https://storage.googleapis.com/truffle-demos/non-interactive.svg?sanitize=true&quot; alt=&quot;GitHub scanning demo&quot; /&gt;&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -it -v &quot;$PWD:/pwd&quot; trufflesecurity/trufflehog:latest github --org=trufflesecurity
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;💾&lt;/span&gt; Installation&lt;/h1&gt; 
&lt;p&gt;Several options are available for you:&lt;/p&gt; 
&lt;h3&gt;MacOS users&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;brew install trufflehog
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Docker:&lt;/h3&gt; 
&lt;p&gt;&lt;sub&gt;&lt;i&gt;&lt;em&gt;Ensure Docker engine is running before executing the following commands:&lt;/em&gt;&lt;/i&gt;&lt;/sub&gt;&lt;/p&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Unix&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -it -v &quot;$PWD:/pwd&quot; trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Windows Command Prompt&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -it -v &quot;%cd:/=\%:/pwd&quot; trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Windows PowerShell&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -it -v &quot;${PWD}:/pwd&quot; trufflesecurity/trufflehog github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;M1 and M2 Mac&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --platform linux/arm64 --rm -it -v &quot;$PWD:/pwd&quot; trufflesecurity/trufflehog:latest github --repo https://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Binary releases&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;Download and unpack from https://github.com/trufflesecurity/trufflehog/releases
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Compile from source&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;git clone https://github.com/trufflesecurity/trufflehog.git
cd trufflehog; go install
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using installation script&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using installation script, verify checksum signature (requires cosign to be installed)&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -v -b /usr/local/bin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Using installation script to install a specific version&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin &amp;lt;ReleaseTag like v3.56.0&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;🔐&lt;/span&gt; Verifying the artifacts&lt;/h1&gt; 
&lt;p&gt;Checksums are applied to all artifacts, and the resulting checksum file is signed using cosign.&lt;/p&gt; 
&lt;p&gt;You need the following tool to verify signature:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;&lt;a href=&quot;https://docs.sigstore.dev/cosign/system_config/installation/&quot;&gt;Cosign&lt;/a&gt;&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Verification steps are as follows:&lt;/p&gt; 
&lt;ol&gt; 
 &lt;li&gt; &lt;p&gt;Download the artifact files you want, and the following files from the &lt;a href=&quot;https://github.com/trufflesecurity/trufflehog/releases&quot;&gt;releases&lt;/a&gt; page.&lt;/p&gt; 
  &lt;ul&gt; 
   &lt;li&gt;trufflehog_{version}_checksums.txt&lt;/li&gt; 
   &lt;li&gt;trufflehog_{version}_checksums.txt.pem&lt;/li&gt; 
   &lt;li&gt;trufflehog_{version}_checksums.txt.sig&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Verify the signature:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;cosign verify-blob &amp;lt;path to trufflehog_{version}_checksums.txt&amp;gt; \
--certificate &amp;lt;path to trufflehog_{version}_checksums.txt.pem&amp;gt; \
--signature &amp;lt;path to trufflehog_{version}_checksums.txt.sig&amp;gt; \
--certificate-identity-regexp &#39;https://github\.com/trufflesecurity/trufflehog/\.github/workflows/.+&#39; \
--certificate-oidc-issuer &quot;https://token.actions.githubusercontent.com&quot;
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
 &lt;li&gt; &lt;p&gt;Once the signature is confirmed as valid, you can proceed to validate that the SHA256 sums align with the downloaded artifact:&lt;/p&gt; &lt;pre&gt;&lt;code class=&quot;language-shell&quot;&gt;sha256sum --ignore-missing -c trufflehog_{version}_checksums.txt
&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; 
&lt;/ol&gt; 
&lt;p&gt;Replace &lt;code&gt;{version}&lt;/code&gt; with the downloaded files version&lt;/p&gt; 
&lt;p&gt;Alternatively, if you are using the installation script, pass &lt;code&gt;-v&lt;/code&gt; option to perform signature verification. This requires Cosign binary to be installed prior to running the installation script.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;🚀&lt;/span&gt; Quick Start&lt;/h1&gt; 
&lt;h2&gt;1: Scan a repo for only verified secrets&lt;/h2&gt; 
&lt;p&gt;Command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Expected output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;🐷🔑🐷  TruffleHog. Unearth your secrets. 🐷🔑🐷

Found verified result 🐷🔑
Detector Type: AWS
Decoder Type: PLAIN
Raw result: AKIAYVP4CIPPERUVIFXG
Line: 4
Commit: fbc14303ffbf8fb1c2c1914e8dda7d0121633aca
File: keys
Email: counter &amp;lt;counter@counters-MacBook-Air.local&amp;gt;
Repository: https://github.com/trufflesecurity/test_keys
Timestamp: 2022-06-16 10:17:40 -0700 PDT
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;2: Scan a GitHub Org for only verified secrets&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog github --org=trufflesecurity --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;3: Scan a GitHub Repo for only verified keys and get JSON output&lt;/h2&gt; 
&lt;p&gt;Command:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog git https://github.com/trufflesecurity/test_keys --results=verified,unknown --json
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Expected output:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;{&quot;SourceMetadata&quot;:{&quot;Data&quot;:{&quot;Git&quot;:{&quot;commit&quot;:&quot;fbc14303ffbf8fb1c2c1914e8dda7d0121633aca&quot;,&quot;file&quot;:&quot;keys&quot;,&quot;email&quot;:&quot;counter \u003ccounter@counters-MacBook-Air.local\u003e&quot;,&quot;repository&quot;:&quot;https://github.com/trufflesecurity/test_keys&quot;,&quot;timestamp&quot;:&quot;2022-06-16 10:17:40 -0700 PDT&quot;,&quot;line&quot;:4}}},&quot;SourceID&quot;:0,&quot;SourceType&quot;:16,&quot;SourceName&quot;:&quot;trufflehog - git&quot;,&quot;DetectorType&quot;:2,&quot;DetectorName&quot;:&quot;AWS&quot;,&quot;DecoderName&quot;:&quot;PLAIN&quot;,&quot;Verified&quot;:true,&quot;Raw&quot;:&quot;AKIAYVP4CIPPERUVIFXG&quot;,&quot;Redacted&quot;:&quot;AKIAYVP4CIPPERUVIFXG&quot;,&quot;ExtraData&quot;:{&quot;account&quot;:&quot;595918472158&quot;,&quot;arn&quot;:&quot;arn:aws:iam::595918472158:user/canarytokens.com@@mirux23ppyky6hx3l6vclmhnj&quot;,&quot;user_id&quot;:&quot;AIDAYVP4CIPPJ5M54LRCY&quot;},&quot;StructuredData&quot;:null}
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;4: Scan a GitHub Repo + its Issues and Pull Requests&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog github --repo=https://github.com/trufflesecurity/test_keys --issue-comments --pr-comments
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;5: Scan an S3 bucket for verified keys&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog s3 --bucket=&amp;lt;bucket name&amp;gt; --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;6: Scan S3 buckets using IAM Roles&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog s3 --role-arn=&amp;lt;iam role arn&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;7: Scan a Github Repo using SSH authentication in Docker&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;docker run --rm -v &quot;$HOME/.ssh:/root/.ssh:ro&quot; trufflesecurity/trufflehog:latest git ssh://github.com/trufflesecurity/test_keys
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;8: Scan individual files or directories&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog filesystem path/to/file1.txt path/to/file2.txt path/to/dir
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;9: Scan a local git repo&lt;/h2&gt; 
&lt;p&gt;Clone the git repo. For example &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/git@github.com:trufflesecurity/test_keys.git&quot;&gt;test keys&lt;/a&gt; repo.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ git clone git@github.com:trufflesecurity/test_keys.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Run trufflehog from the parent directory (outside the git repo).&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;$ trufflehog git file://test_keys --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;10: Scan GCS buckets for verified secrets&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog gcs --project-id=&amp;lt;project-ID&amp;gt; --cloud-environment --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;11: Scan a Docker image for verified secrets&lt;/h2&gt; 
&lt;p&gt;Use the &lt;code&gt;--image&lt;/code&gt; flag multiple times to scan multiple images.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;# to scan from a remote registry
trufflehog docker --image trufflesecurity/secrets --results=verified,unknown

# to scan from the local docker daemon
trufflehog docker --image docker://new_image:tag --results=verified,unknown

# to scan from an image saved as a tarball
trufflehog docker --image file://path_to_image.tar --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;12: Scan in CI&lt;/h2&gt; 
&lt;p&gt;Set the &lt;code&gt;--since-commit&lt;/code&gt; flag to your default branch that people merge into (ex: &quot;main&quot;). Set the &lt;code&gt;--branch&lt;/code&gt; flag to your PR&#39;s branch name (ex: &quot;feature-1&quot;). Depending on the CI/CD platform you use, this value can be pulled in dynamically (ex: &lt;a href=&quot;https://circleci.com/docs/variables/&quot;&gt;CIRCLE_BRANCH in Circle CI&lt;/a&gt; and &lt;a href=&quot;https://docs.travis-ci.com/user/environment-variables/&quot;&gt;TRAVIS_PULL_REQUEST_BRANCH in Travis CI&lt;/a&gt;). If the repo is cloned and the target branch is already checked out during the CI/CD workflow, then &lt;code&gt;--branch HEAD&lt;/code&gt; should be sufficient. The &lt;code&gt;--fail&lt;/code&gt; flag will return an 183 error code if valid credentials are found.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog git file://. --since-commit main --branch feature-1 --results=verified,unknown --fail
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;13: Scan a Postman workspace&lt;/h2&gt; 
&lt;p&gt;Use the &lt;code&gt;--workspace-id&lt;/code&gt;, &lt;code&gt;--collection-id&lt;/code&gt;, &lt;code&gt;--environment&lt;/code&gt; flags multiple times to scan multiple targets.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog postman --token=&amp;lt;postman api token&amp;gt; --workspace-id=&amp;lt;workspace id&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;14: Scan a Jenkins server&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog jenkins --url https://jenkins.example.com --username admin --password admin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;15: Scan an Elasticsearch server&lt;/h2&gt; 
&lt;h3&gt;Scan a Local Cluster&lt;/h3&gt; 
&lt;p&gt;There are two ways to authenticate to a local cluster with TruffleHog: (1) username and password, (2) service token.&lt;/p&gt; 
&lt;h4&gt;Connect to a local cluster with username and password&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --username truffle --password hog
&lt;/code&gt;&lt;/pre&gt; 
&lt;h4&gt;Connect to a local cluster with a service token&lt;/h4&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog elasticsearch --nodes 192.168.14.3 192.168.14.4 --service-token ‘AAEWVaWM...Rva2VuaSDZ’
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Scan an Elastic Cloud Cluster&lt;/h3&gt; 
&lt;p&gt;To scan a cluster on Elastic Cloud, you’ll need a Cloud ID and API key.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog elasticsearch \
  --cloud-id &#39;search-prod:dXMtY2Vx...YjM1ODNlOWFiZGRlNjI0NA==&#39; \
  --api-key &#39;MlVtVjBZ...ZSYlduYnF1djh3NG5FQQ==&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;16. Scan a GitHub Repository for Cross Fork Object References and Deleted Commits&lt;/h2&gt; 
&lt;p&gt;The following command will enumerate deleted and hidden commits on a GitHub repository and then scan them for secrets. This is an alpha release feature.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog github-experimental --repo https://github.com/&amp;lt;USER&amp;gt;/&amp;lt;REPO&amp;gt;.git --object-discovery
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In addition to the normal TruffleHog output, the &lt;code&gt;--object-discovery&lt;/code&gt; flag creates two files in a new &lt;code&gt;$HOME/.trufflehog&lt;/code&gt; directory: &lt;code&gt;valid_hidden.txt&lt;/code&gt; and &lt;code&gt;invalid.txt&lt;/code&gt;. These are used to track state during commit enumeration, as well as to provide users with a complete list of all hidden and deleted commits (&lt;code&gt;valid_hidden.txt&lt;/code&gt;). If you&#39;d like to automatically remove these files after scanning, please add the flag &lt;code&gt;--delete-cached-data&lt;/code&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Enumerating all valid commits on a repository using this method takes between 20 minutes and a few hours, depending on the size of your repository. We added a progress bar to keep you updated on how long the enumeration will take. The actual secret scanning runs extremely fast.&lt;/p&gt; 
&lt;p&gt;For more information on Cross Fork Object References, please &lt;a href=&quot;https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github&quot;&gt;read our blog post&lt;/a&gt;.&lt;/p&gt; 
&lt;h2&gt;17. Scan Hugging Face&lt;/h2&gt; 
&lt;h3&gt;Scan a Hugging Face Model, Dataset or Space&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog huggingface --model &amp;lt;model_id&amp;gt; --space &amp;lt;space_id&amp;gt; --dataset &amp;lt;dataset_id&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;h3&gt;Scan all Models, Datasets and Spaces belonging to a Hugging Face Organization or User&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog huggingface --org &amp;lt;orgname&amp;gt; --user &amp;lt;username&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;(Optionally) When scanning an organization or user, you can skip an entire class of resources with &lt;code&gt;--skip-models&lt;/code&gt;, &lt;code&gt;--skip-datasets&lt;/code&gt;, &lt;code&gt;--skip-spaces&lt;/code&gt; OR a particular resource with &lt;code&gt;--ignore-models &amp;lt;model_id&amp;gt;&lt;/code&gt;, &lt;code&gt;--ignore-datasets &amp;lt;dataset_id&amp;gt;&lt;/code&gt;, &lt;code&gt;--ignore-spaces &amp;lt;space_id&amp;gt;&lt;/code&gt;.&lt;/p&gt; 
&lt;h3&gt;Scan Discussion and PR Comments&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog huggingface --model &amp;lt;model_id&amp;gt; --include-discussions --include-prs
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;18. Scan stdin Input&lt;/h2&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;aws s3 cp s3://example/gzipped/data.gz - | gunzip -c | trufflehog stdin
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;❓&lt;/span&gt; FAQ&lt;/h1&gt; 
&lt;ul&gt; 
 &lt;li&gt;All I see is &lt;code&gt;🐷🔑🐷 TruffleHog. Unearth your secrets. 🐷🔑🐷&lt;/code&gt; and the program exits, what gives? 
  &lt;ul&gt; 
   &lt;li&gt;That means no secrets were detected&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Why is the scan taking a long time when I scan a GitHub org 
  &lt;ul&gt; 
   &lt;li&gt;Unauthenticated GitHub scans have rate limits. To improve your rate limits, include the &lt;code&gt;--token&lt;/code&gt; flag with a personal access token&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;It says a private key was verified, what does that mean? 
  &lt;ul&gt; 
   &lt;li&gt;Check out our Driftwood blog post to learn how to do this, in short we&#39;ve confirmed the key can be used live for SSH or SSL &lt;a href=&quot;https://trufflesecurity.com/blog/driftwood-know-if-private-keys-are-sensitive/&quot;&gt;Blog post&lt;/a&gt;&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
 &lt;li&gt;Is there an easy way to ignore specific secrets? 
  &lt;ul&gt; 
   &lt;li&gt;If the scanned source &lt;a href=&quot;https://github.com/trufflesecurity/trufflehog/raw/d6375ba92172fd830abb4247cca15e3176448c5d/pkg/engine/engine.go#L358-L365&quot;&gt;supports line numbers&lt;/a&gt;, then you can add a &lt;code&gt;trufflehog:ignore&lt;/code&gt; comment on the line containing the secret to ignore that secrets.&lt;/li&gt; 
  &lt;/ul&gt; &lt;/li&gt; 
&lt;/ul&gt; 
&lt;h1&gt;&lt;span&gt;📰&lt;/span&gt; What&#39;s new in v3?&lt;/h1&gt; 
&lt;p&gt;TruffleHog v3 is a complete rewrite in Go with many new powerful features.&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;We&#39;ve &lt;strong&gt;added over 700 credential detectors that support active verification against their respective APIs&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;We&#39;ve also added native &lt;strong&gt;support for scanning GitHub, GitLab, Docker, filesystems, S3, GCS, Circle CI and Travis CI&lt;/strong&gt;.&lt;/li&gt; 
 &lt;li&gt;&lt;strong&gt;Instantly verify private keys&lt;/strong&gt; against millions of github users and &lt;strong&gt;billions&lt;/strong&gt; of TLS certificates using our &lt;a href=&quot;https://trufflesecurity.com/blog/driftwood&quot;&gt;Driftwood&lt;/a&gt; technology.&lt;/li&gt; 
 &lt;li&gt;Scan binaries, documents, and other file formats&lt;/li&gt; 
 &lt;li&gt;Available as a GitHub Action and a pre-commit hook&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;What is credential verification?&lt;/h2&gt; 
&lt;p&gt;For every potential credential that is detected, we&#39;ve painstakingly implemented programmatic verification against the API that we think it belongs to. Verification eliminates false positives. For example, the &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/pkg/detectors/aws/aws.go&quot;&gt;AWS credential detector&lt;/a&gt; performs a &lt;code&gt;GetCallerIdentity&lt;/code&gt; API call against the AWS API to verify if an AWS credential is active.&lt;/p&gt; 
&lt;h1&gt;&lt;span&gt;📝&lt;/span&gt; Usage&lt;/h1&gt; 
&lt;p&gt;TruffleHog has a sub-command for each source of data that you may want to scan:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;git&lt;/li&gt; 
 &lt;li&gt;github&lt;/li&gt; 
 &lt;li&gt;gitlab&lt;/li&gt; 
 &lt;li&gt;docker&lt;/li&gt; 
 &lt;li&gt;s3&lt;/li&gt; 
 &lt;li&gt;filesystem (files and directories)&lt;/li&gt; 
 &lt;li&gt;syslog&lt;/li&gt; 
 &lt;li&gt;circleci&lt;/li&gt; 
 &lt;li&gt;travisci&lt;/li&gt; 
 &lt;li&gt;gcs (Google Cloud Storage)&lt;/li&gt; 
 &lt;li&gt;postman&lt;/li&gt; 
 &lt;li&gt;jenkins&lt;/li&gt; 
 &lt;li&gt;elasticsearch&lt;/li&gt; 
 &lt;li&gt;stdin&lt;/li&gt; 
 &lt;li&gt;multi-scan&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;Each subcommand can have options that you can see with the &lt;code&gt;--help&lt;/code&gt; flag provided to the sub command:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;$ trufflehog git --help
usage: TruffleHog git [&amp;lt;flags&amp;gt;] &amp;lt;uri&amp;gt;

Find credentials in git repositories.

Flags:
  -h, --help                Show context-sensitive help (also try --help-long and --help-man).
      --log-level=0         Logging verbosity on a scale of 0 (info) to 5 (trace). Can be disabled with &quot;-1&quot;.
      --profile             Enables profiling and sets a pprof and fgprof server on :18066.
  -j, --json                Output in JSON format.
      --json-legacy         Use the pre-v3.0 JSON format. Only works with git, gitlab, and github sources.
      --github-actions      Output in GitHub Actions format.
      --concurrency=20           Number of concurrent workers.
      --no-verification     Don&#39;t verify the results.
      --results=RESULTS          Specifies which type(s) of results to output: verified, unknown, unverified, filtered_unverified. Defaults to all types.
      --allow-verification-overlap
                                 Allow verification of similar credentials across detectors
      --filter-unverified   Only output first unverified result per chunk per detector if there are more than one results.
      --filter-entropy=FILTER-ENTROPY
                                 Filter unverified results with Shannon entropy. Start with 3.0.
      --config=CONFIG            Path to configuration file.
      --print-avg-detector-time
                                 Print the average time spent on each detector.
      --no-update           Don&#39;t check for updates.
      --fail                Exit with code 183 if results are found.
      --verifier=VERIFIER ...    Set custom verification endpoints.
      --custom-verifiers-only   Only use custom verification endpoints.
      --archive-max-size=ARCHIVE-MAX-SIZE
                                 Maximum size of archive to scan. (Byte units eg. 512B, 2KB, 4MB)
      --archive-max-depth=ARCHIVE-MAX-DEPTH
                                 Maximum depth of archive to scan.
      --archive-timeout=ARCHIVE-TIMEOUT
                                 Maximum time to spend extracting an archive.
      --include-detectors=&quot;all&quot;  Comma separated list of detector types to include. Protobuf name or IDs may be used, as well as ranges.
      --exclude-detectors=EXCLUDE-DETECTORS
                                 Comma separated list of detector types to exclude. Protobuf name or IDs may be used, as well as ranges. IDs defined here take precedence over the include list.
      --version             Show application version.
  -i, --include-paths=INCLUDE-PATHS
                                 Path to file with newline separated regexes for files to include in scan.
  -x, --exclude-paths=EXCLUDE-PATHS
                                 Path to file with newline separated regexes for files to exclude in scan.
      --exclude-globs=EXCLUDE-GLOBS
                                 Comma separated list of globs to exclude in scan. This option filters at the `git log` level, resulting in faster scans.
      --since-commit=SINCE-COMMIT
                                 Commit to start scan from.
      --branch=BRANCH            Branch to scan.
      --max-depth=MAX-DEPTH      Maximum depth of commits to scan.
      --bare                Scan bare repository (e.g. useful while using in pre-receive hooks)

Args:
  &amp;lt;uri&amp;gt;  Git repository URL. https://, file://, or ssh:// schema expected.
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;For example, to scan a &lt;code&gt;git&lt;/code&gt; repository, start with&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;trufflehog git https://github.com/trufflesecurity/trufflehog.git
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;Configuration&lt;/h2&gt; 
&lt;p&gt;TruffleHog supports defining &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/#regex-detector-alpha&quot;&gt;custom regex detectors&lt;/a&gt; and multiple sources in a configuration file provided via the &lt;code&gt;--config&lt;/code&gt; flag. The regex detectors can be used with any subcommand, while the sources defined in configuration are only for the &lt;code&gt;multi-scan&lt;/code&gt; subcommand.&lt;/p&gt; 
&lt;p&gt;The configuration format for sources can be found on Truffle Security&#39;s &lt;a href=&quot;https://docs.trufflesecurity.com/scan-data-for-secrets&quot;&gt;source configuration documentation page&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;Example GitHub source configuration and &lt;a href=&quot;https://docs.trufflesecurity.com/github#Fvm1I&quot;&gt;options reference&lt;/a&gt;:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;sources:
- connection:
    &#39;@type&#39;: type.googleapis.com/sources.GitHub
    repositories:
    - https://github.com/trufflesecurity/test_keys.git
    unauthenticated: {}
  name: example config scan
  type: SOURCE_TYPE_GITHUB
  verify: true
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;You may define multiple connections under the &lt;code&gt;sources&lt;/code&gt; key (see above), and TruffleHog will scan all of the sources concurrently.&lt;/p&gt; 
&lt;h2&gt;S3&lt;/h2&gt; 
&lt;p&gt;The S3 source supports assuming IAM roles for scanning in addition to IAM users. This makes it easier for users to scan multiple AWS accounts without needing to rely on hardcoded credentials for each account.&lt;/p&gt; 
&lt;p&gt;The IAM identity that TruffleHog uses initially will need to have &lt;code&gt;AssumeRole&lt;/code&gt; privileges as a principal in the &lt;a href=&quot;https://aws.amazon.com/blogs/security/how-to-use-trust-policies-with-iam-roles/&quot;&gt;trust policy&lt;/a&gt; of each IAM role to assume.&lt;/p&gt; 
&lt;p&gt;To scan a specific bucket using locally set credentials or instance metadata if on an EC2 instance:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog s3 --bucket=&amp;lt;bucket-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;To scan a specific bucket using an assumed role:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog s3 --bucket=&amp;lt;bucket-name&amp;gt; --role-arn=&amp;lt;iam-role-arn&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Multiple roles can be passed as separate arguments. The following command will attempt to scan every bucket each role has permissions to list in the S3 API:&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog s3 --role-arn=&amp;lt;iam-role-arn-1&amp;gt; --role-arn=&amp;lt;iam-role-arn-2&amp;gt;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Exit Codes:&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;0: No errors and no results were found.&lt;/li&gt; 
 &lt;li&gt;1: An error was encountered. Sources may not have completed scans.&lt;/li&gt; 
 &lt;li&gt;183: No errors were encountered, but results were found. Will only be returned if &lt;code&gt;--fail&lt;/code&gt; flag is used.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;h2&gt;&lt;img alt=&quot;octocat&quot; src=&quot;https://github.githubassets.com/images/icons/emoji/octocat.png?v8&quot; /&gt;) TruffleHog Github Action&lt;/h2&gt; 
&lt;h3&gt;General Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code&gt;on:
  push:
    branches:
      - main
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - name: Secret Scanning
      uses: trufflesecurity/trufflehog@main
      with:
        extra_args: --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In the example config above, we&#39;re scanning for live secrets in all PRs and Pushes to &lt;code&gt;main&lt;/code&gt;. Only code changes in the referenced commits are scanned. If you&#39;d like to scan an entire branch, please see the &quot;Advanced Usage&quot; section below.&lt;/p&gt; 
&lt;h3&gt;Shallow Cloning&lt;/h3&gt; 
&lt;p&gt;If you&#39;re incorporating TruffleHog into a standalone workflow and aren&#39;t running any other CI/CD tooling alongside TruffleHog, then we recommend using &lt;a href=&quot;https://git-scm.com/docs/git-clone#Documentation/git-clone.txt---depthltdepthgt&quot;&gt;Shallow Cloning&lt;/a&gt; to speed up your workflow. Here&#39;s an example of how to do it:&lt;/p&gt; 
&lt;pre&gt;&lt;code&gt;...
      - shell: bash
        run: |
          if [ &quot;${{ github.event_name }}&quot; == &quot;push&quot; ]; then
            echo &quot;depth=$(($(jq length &amp;lt;&amp;lt;&amp;lt; &#39;${{ toJson(github.event.commits) }}&#39;) + 2))&quot; &amp;gt;&amp;gt; $GITHUB_ENV
            echo &quot;branch=${{ github.ref_name }}&quot; &amp;gt;&amp;gt; $GITHUB_ENV
          fi
          if [ &quot;${{ github.event_name }}&quot; == &quot;pull_request&quot; ]; then
            echo &quot;depth=$((${{ github.event.pull_request.commits }}+2))&quot; &amp;gt;&amp;gt; $GITHUB_ENV
            echo &quot;branch=${{ github.event.pull_request.head.ref }}&quot; &amp;gt;&amp;gt; $GITHUB_ENV
          fi
      - uses: actions/checkout@v3
        with:
          ref: ${{env.branch}}
          fetch-depth: ${{env.depth}}
      - uses: trufflesecurity/trufflehog@main
        with:
          extra_args: --results=verified,unknown
...
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;Depending on the event type (push or PR), we calculate the number of commits present. Then we add 2, so that we can reference a base commit before our code changes. We pass that integer value to the &lt;code&gt;fetch-depth&lt;/code&gt; flag in the checkout action in addition to the relevant branch. Now our checkout process should be much shorter.&lt;/p&gt; 
&lt;h3&gt;Canary detection&lt;/h3&gt; 
&lt;p&gt;TruffleHog statically detects &lt;a href=&quot;https://canarytokens.org/&quot;&gt;https://canarytokens.org/&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;&lt;img src=&quot;https://github.com/trufflesecurity/trufflehog/assets/52866392/74ace530-08c5-4eaf-a169-84a73e328f6f&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt; 
&lt;h3&gt;Advanced Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;- name: TruffleHog
  uses: trufflesecurity/trufflehog@main
  with:
    # Repository path
    path:
    # Start scanning from here (usually main branch).
    base:
    # Scan commits until here (usually dev branch).
    head: # optional
    # Extra args to be passed to the trufflehog cli.
    extra_args: --log-level=2 --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;If you&#39;d like to specify specific &lt;code&gt;base&lt;/code&gt; and &lt;code&gt;head&lt;/code&gt; refs, you can use the &lt;code&gt;base&lt;/code&gt; argument (&lt;code&gt;--since-commit&lt;/code&gt; flag in TruffleHog CLI) and the &lt;code&gt;head&lt;/code&gt; argument (&lt;code&gt;--branch&lt;/code&gt; flag in the TruffleHog CLI). We only recommend using these arguments for very specific use cases, where the default behavior does not work.&lt;/p&gt; 
&lt;h4&gt;Advanced Usage: Scan entire branch&lt;/h4&gt; 
&lt;pre&gt;&lt;code&gt;- name: scan-push
        uses: trufflesecurity/trufflehog@main
        with:
          base: &quot;&quot;
          head: ${{ github.ref_name }}
          extra_args: --results=verified,unknown
&lt;/code&gt;&lt;/pre&gt; 
&lt;h2&gt;TruffleHog GitLab CI&lt;/h2&gt; 
&lt;h3&gt;Example Usage&lt;/h3&gt; 
&lt;pre&gt;&lt;code class=&quot;language-yaml&quot;&gt;stages:
  - security

security-secrets:
  stage: security
  allow_failure: false
  image: alpine:latest
  variables:
    SCAN_PATH: &quot;.&quot; # Set the relative path in the repo to scan
  before_script:
    - apk add --no-cache git curl jq
    - curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
  script:
    - trufflehog filesystem &quot;$SCAN_PATH&quot; --results=verified,unknown --fail --json | jq
  rules:
    - if: &#39;$CI_PIPELINE_SOURCE == &quot;merge_request_event&quot;&#39;
&lt;/code&gt;&lt;/pre&gt; 
&lt;p&gt;In the example pipeline above, we&#39;re scanning for live secrets in all repository directories and files. This job runs only when the pipeline source is a merge request event, meaning it&#39;s triggered when a new merge request is created.&lt;/p&gt; 
&lt;h2&gt;Pre-commit Hook&lt;/h2&gt; 
&lt;p&gt;TruffleHog can be used in a pre-commit hook to prevent credentials from leaking before they ever leave your computer.&lt;/p&gt; 
&lt;p&gt;See the &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/PreCommit.md&quot;&gt;pre-commit hook documentation&lt;/a&gt; for more information.&lt;/p&gt; 
&lt;h2&gt;Regex Detector (alpha)&lt;/h2&gt; 
&lt;p&gt;TruffleHog supports detection and verification of custom regular expressions. For detection, at least one &lt;strong&gt;regular expression&lt;/strong&gt; and &lt;strong&gt;keyword&lt;/strong&gt; is required. A &lt;strong&gt;keyword&lt;/strong&gt; is a fixed literal string identifier that appears in or around the regex to be detected. To allow maximum flexibility for verification, a webhook is used containing the regular expression matches.&lt;/p&gt; 
&lt;p&gt;TruffleHog will send a JSON POST request containing the regex matches to a configured webhook endpoint. If the endpoint responds with a &lt;code&gt;200 OK&lt;/code&gt; response status code, the secret is considered verified.&lt;/p&gt; 
&lt;p&gt;Custom Detectors support a few different filtering mechanisms: entropy, regex targeting the entire match, regex targeting the captured secret, and excluded word lists checked against the secret (captured group if present, entire match if capture group is not present). Note that if your custom detector has multiple &lt;code&gt;regex&lt;/code&gt; set (in this example &lt;code&gt;hogID&lt;/code&gt;, and &lt;code&gt;hogToken&lt;/code&gt;), then the filters get applied to each regex. &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/examples/generic_with_filters.yml&quot;&gt;Here&lt;/a&gt; is an example of a custom detector using these filters.&lt;/p&gt; 
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; This feature is alpha and subject to change.&lt;/p&gt; 
&lt;h3&gt;Regex Detector Example&lt;/h3&gt; 
&lt;p&gt;&lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/pkg/custom_detectors/CUSTOM_DETECTORS.md&quot;&gt;Here&lt;/a&gt; is how to setup a custom regex detector with verification server.&lt;/p&gt; 
&lt;h2&gt;&lt;span&gt;🔍&lt;/span&gt; Analyze&lt;/h2&gt; 
&lt;p&gt;TruffleHog supports running a deeper analysis of a credential to view its permissions and the resources it has access to.&lt;/p&gt; 
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;trufflehog analyze
&lt;/code&gt;&lt;/pre&gt; 
&lt;h1&gt;&lt;span&gt;❤️&lt;/span&gt; Contributors&lt;/h1&gt; 
&lt;p&gt;This project exists thanks to all the people who contribute. [&lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/CONTRIBUTING.md&quot;&gt;Contribute&lt;/a&gt;].&lt;/p&gt; 
&lt;a href=&quot;https://github.com/trufflesecurity/trufflehog/graphs/contributors&quot;&gt; &lt;img src=&quot;https://contrib.rocks/image?repo=trufflesecurity/trufflehog&quot; /&gt; &lt;/a&gt; 
&lt;h1&gt;&lt;span&gt;💻&lt;/span&gt; Contributing&lt;/h1&gt; 
&lt;p&gt;Contributions are very welcome! Please see our &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/CONTRIBUTING.md&quot;&gt;contribution guidelines first&lt;/a&gt;.&lt;/p&gt; 
&lt;p&gt;We no longer accept contributions to TruffleHog v2, but that code is available in the &lt;code&gt;v2&lt;/code&gt; branch.&lt;/p&gt; 
&lt;h2&gt;Adding new secret detectors&lt;/h2&gt; 
&lt;p&gt;We have published some &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/hack/docs/Adding_Detectors_external.md&quot;&gt;documentation and tooling to get started on adding new secret detectors&lt;/a&gt;. Let&#39;s improve detection together!&lt;/p&gt; 
&lt;h1&gt;Use as a library&lt;/h1&gt; 
&lt;p&gt;Currently, trufflehog is in heavy development and no guarantees can be made on the stability of the public APIs at this time.&lt;/p&gt; 
&lt;h1&gt;License Change&lt;/h1&gt; 
&lt;p&gt;Since v3.0, TruffleHog is released under a AGPL 3 license, included in &lt;a href=&quot;https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/LICENSE&quot;&gt;&lt;code&gt;LICENSE&lt;/code&gt;&lt;/a&gt;. TruffleHog v3.0 uses none of the previous codebase, but care was taken to preserve backwards compatibility on the command line interface. The work previous to this release is still available licensed under GPL 2.0 in the history of this repository and the previous package releases and tags. A completed CLA is required for us to accept contributions going forward.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
